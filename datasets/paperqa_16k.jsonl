{"question": "Which datasets are mentioned in the paper as part of knowledge-intensive tasks for evaluation?", "evidence": "  We conduct experiments on three types of datasets: in-domain (ID) tasks that require general-domain knowledge; out-of-domain (OOD) tasks that require domain-specific knowledge; knowledge-intensive (KI) tasks that require commonsense knowledge. FEVER and CSQA, two knowledge-intensive tasks, have the greatest improvement: 10.3% for FEVER and 1.2% for CSQA. ", "options": ["A. MNLI, CoLA, MRPC", "B. ChemProt, RCT, IMDB, Amazon", "C. FEVER, Common-senseQA", "D. STS-B, WNLI, QNLI "], "answer": "C", "content": "\nIntroduction\nPre-trained language models (PLMs) have achieved a multitude of successful applications in natural language understanding (Devlin et al., 2018; Liu et al., 2019; He et al., 2021b) and generation (Lewis et al., 2019; Zhang et al., 2020; Yang et al., 2020; Brown et al., 2020) . The predominant methodology for domain adaptation is fine-tuning on labeled domain-specific data or continued pre-training (Gururangan et al., 2020) on unlabeled domain-specific data. Although effective, both fine-tuning and continued pre-training methods require tuning all the parameters of a PLM, raising high costs beyond many institutions' reach. To mitigate this, multiple parameter-efficient fine-tuning (PEFT) methods are proposed, including prompt-based tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . However, they are more concerned about task adaptation and it is still unclear how to regularly, and inexpensively inject domain knowledge into PLMs for different domain-specific tasks. Moreover, directly tuning PLMs on a domain-specific corpus with PEFT methods will lead to the catastrophic forgetting problem (Yogatama et al., 2019; Gururangan et al., 2020) . These limitations highlight an important research question: how to adapt PLMs with the new domain knowledge while keeping the old-domain knowledge unmodified?\nInspired by the recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022 ) that found knowledge is stored in feed-forward networks (FFNs), we decouple the FFNs into two parts: the original pre-trained FFNs to maintain the olddomain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Specifically, we propose Mixture-of-Domain-Adapters (MixDA), a mixture of several domain adapters to inject domain-specific knowledge without affecting the old-domain knowledge. Our model has two stages: piq domain-specific tuning multiple knowledge adapters on unlabeled data and then piiq task-specific tuning adapters on labeled data. In the first stage, we train several domain adapters on both domain-specific corpus and pre-training corpus simultaneously while keeping the original feed-forward networks unchanged. In the second stage, we train a mixture-of-adapters gate to dynamically select the desired knowledge adapter and a task-specific adapter for task adaptation.\nWe conduct experiments on a broad range of tasks, including 4 out-of-domain datasets, 9 in-domain datasets, and 2 knowledge-intensive datasets. Our experimental results demonstrate the effectiveness of MixDA on 15 datasets, spanning biomedical, computer science publications, news, and reviews. Further analysis displays three key properties of our proposed approach: piq Reliability: it shows superior performance on both in-domain and out-of-domain tasks. piiq Scalability: it scales well to the increasing number of domains. piiiq Efficiency: it adds only a small number of parameters per domain. We claim that these properties are helpful for language models as a service, where a frozen PLM is served, and multiple adapters are inserted to support different customized services.\n\nRelated Work\nIn this section, we will review four research lines related to injecting domain knowledge into pretrained language models: knowledge injection, domain adaptation, parameter-efficient fine-tuning, and mixture-of-adapters.\n\nKnowledge Injection\nKnowledge can be injected into PLMs by pretraining or fine-tuning, each corresponding to a separate research direction. During pre-training, the knowledge carried by knowledge graphs (Zhang et al., 2019; He et al., 2020) , entities (Sun et al., 2019; Xiong et al., 2020) , n-grams (Diao et al., 2020) , knowledge embedding (Wang et al., 2021b) , synonym and hyponym-hypernym relations in WordNet (Lauscher et al., 2019) , word-supersense knowledge (Levine et al., 2020) , and knowledge bases (Peters et al., 2019) can be injected into PLMs by feeding knowledge inputs and designing new objectives. However, pre-training-based methods are costly, making the application to huge PLMs (e.g., models with 175 Billion parameters) impossible. Fine-tuning-based methods only require an additional fine-tuning process. Some studies inject extra information into the input sentences, like knowledge triples from knowledge graphs (Liu et al., 2020) and knowledge context (Faldu et al., 2021) , while other studies explored specific model and training designs, like knowledge adapter networks (Wang et al., 2021a) , graph convolutional networks and LSTMs (Lin et al., 2019) , and metalearning (Sinitsin et al., 2020) . Zhu et al. (2020) formulated knowledge injection as a constrained optimization problem by adding a constraint on the loss on the unmodified facts. Recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) reveal that knowledge is stored in the feed-forward networks in PLMs. Inspired by these studies, we propose a new efficient tuning method to inject domain knowledge into feed-forward networks with minimal costs.\n\nDomain Adaptation\nPrevious studies have observed that language models suffer from a significant performance drop during the domain shift (Beltagy et al., 2019; Alsentzer et al., 2019; Huang et al., 2019; Lee et al., 2020; Ke et al., 2022b) . Effective strategies that can bridge the domain gap are introduced. Pre-training language models from scratch is effective but costly, like SciBERT (Beltagy et al., 2019) , BioBERT (Lee et al., 2020) , and ClinicalBERT (Alsentzer et al., 2019) . Recent studies explored continued pretraining (Gururangan et al., 2020) and adapter networks (Diao et al., 2021) to save time by training on unlabeled downstream task data. In this paper, we introduce plug-in domain adaptors for domain adaptation, which are effective and mitigate catastrophic forgetting issues because of the explicit learning strategy and efficient model architecture.\n\nParameter-Efficient Fine-tuning\nAnother relevant research direction is parameterefficient fine-tuning (PEFT), which only fine-tunes a small number of parameters. Existing works solve this problem from two perspectives: promptbased tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . Several works in adapter-based tuning are closely related to ours. AdapterFusion (Pfeiffer et al., 2021) aims to combine multiple task adapters but does not offer specific architecture or training strategies to learn external knowledge. DEMix (Gururangan et al., 2022) and MixDA both train adapters that specialize in domains and use mechanisms to route different adapters, but differ in routing methods, base models, and training strategies. K-Adapter (Wang et al., 2021a ) is re-stricted by its training on T-REx triples and lacks the flexibility to train on unstructured knowledge. Similar to MixDA, CPT (Ke et al., 2022a) integrates domain knowledge into LMs, but it employs a different approach. While MixDA uses domain adapters to substitute FFN layers and task adapters to perform end tasks, CPT adds CL-Plugins that learn domain knowledge. Recent work by He et al. (2021a) presents a unified framework that establishes connections across different PEFT methods. Our work can leverage any PEFT method and complement them.\n\nMixture-of-Experts\nMixture-of-Experts (MoE) (Shazeer et al., 2017) is introduced with several expert networks, gating networks, and load-balancing techniques. The following studies improve MoE on initialization and training schemes (Fedus et al., 2022) , routing mechanisms (Zuo et al., 2021; Yang et al., 2021) , and load-balancing issues (Lewis et al., 2021; Roller et al., 2021) . AdaMix (Wang et al., 2022) proposed a mixture of adapters to improve the downstream task performance. Instead of mixing different designs of adapters, our domain adapter is a feedforward network specifically designed for domain knowledge.\n\nApproach\nGiven a pre-trained language model M, the input is a sentence X \" t 1 t 2 \u00a8\u00a8\u00a8t i \u00a8\u00a8\u00a8t T (t i indicates the i-th token) and the output is the representation of each token. The overall architecture of our model is shown in Figure 1 . The training process is divided into two-stage. In Stage 1 (Figure 1 (a)), we inject new feed-forward networks (FFNs) (namely domain-adapter) paralleled to the original pre-trained FFNs in some Transformer layers, acting as a key-value memory. The newly injected domain-adapter is trained on both domain-specific unlabeled data and original pre-training unlabeled data to store new factual associations while keeping old-domain ones. All modules are frozen except domain-adapter in this stage. In Stage 2 (Figure 1 (b)), we train a mixture-of-adapters (MoA) gate and a task-adapter on downstream tasks with labeled data, and only these two new modules are updated. The MoA gate receives outputs from the old-domain FFNs and domain-adapter, then outputs a weighted sum of them. An additional taskadapter is inserted in each Transformer block to facilitate downstream tasks. Figure 1 (c) shows the structures of the domain-adapter and the MoA gate.\nIn this section, we first introduce domainadapter, which learns and stores domain-specific knowledge, and then describe task-adapters that perform the downstream task. Finally, we discuss how the MoA gate integrates the outputs from the FFN and the domain-adapter.\n\nDomain-Adapter\nPrevious studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) suggest that factual associations are stored in the FFNs of some Transformer layers. To help models learn domain-specific knowledge, we propose a lightweight domain-adapter that works parallel to the FFNs, and a training method to learn domain-specific knowledge alongside keeping old-domain ones. Domain-adapter has a simple bottleneck architecture consisting of a down projection layer, a nonlinearity (such as ReLU (Agarap, 2018)), and an up projection layer. This helps keep the parameter size low (Houlsby et al., 2019) with competitive performance.\nIn Stage 1, the domain-adapter is trained with the domain-specific and old-domain datasets in one batch. Note that all other parameters are frozen except the domain-adapter at this stage. Let L K denote the knowledge loss related to domain-specific knowledge, and L S denote the sampling loss related to old-domain knowledge. The knowledge loss is a cross-entropy loss on predicting masked tokens, and the sampling loss is designed to align the latent spaces of the old-domain knowledge and new domain-specific knowledge. The total loss L is given by a weighted sum of the two, that is:\nEQUATION\nwhere \u03bb is a weight for the knowledge loss.\nThe knowledge loss is implemented by using cross-entropy loss. Given a sentence with M mask tokens whose answers are m 1 , m 2 , \u00a8\u00a8\u00a8, m M , respectively, the knowledge loss L K is given by\nEQUATION\nwhere ppm i q is the probability for token m i output by M. 2016)), we translate each relation into a sentence, and then mask out its object. For example, the relation \"the Eiffel tower-/r/LocatedAt-Paris\" is translated into \"The Eiffel Tower is located at Paris.\", then \"Paris\" is substituted with the mask token, and the model is trained to fill the mask. \u201a Unstructured knowledge For unstructured knowledge (e.g., downstream unlabeled texts), we use the masked language model (MLM) similar to RoBERTa pretraining. Some tokens are randomly sampled from the input sentence and replaced with the special token <mask>, and the model is trained to predict the masked token. The cross-entropy loss is calculated to optimize the model. For old-domain knowledge and sampling loss, we train the model on general corpora including Wikipedia and BookCorpus (Zhu et al., 2015) . Specifically, for each batch, sentences randomly sampled from the dataset are input into the model. Given L layers that have domain-adapters installed, for each such layer l, we collect token representations from the FFN F l , and representations from the domain-adapter K l . The goal is to keep them as similar as possible. Thus, we calculate the sampling loss L S with L2 loss:\nL S \" 1 L L \u00ff l\"1 ||F l \u00b4Kl || 2 2 .\n(3)\n\nTask-Adapter\nAfter training domain-adapters, the model is aware of the domain knowledge, which is not directly related to downstream tasks though. Therefore, we add task-adapters on top of the domain-adapter to adapt to downstream tasks. For example, a domainadapter trained in biomedical knowledge can sup- \n\nMixture-of-Adapters Gate\nOn downstream tasks, it is possible that the output from the FFN, or a weighted sum of the two, produces better results. Therefore, in Stage 2, we train an additional mixture-of-adapters (MoA) gate simultaneously. The MoA gate receives the outputs from the attention layer q, the domain-adapter K, and the FFN F . q is first sent into a multi-layer perceptron (MLP):\nEQUATION\n)\nThe MLP is composed of a down-projection layer W d and an up-projection layer W u , and h \" W u \u03c3pW d qq, where \u03c3 is the nonlinearity function.\nThen, h is input into a Sigmoid layer to generate the weights of the FFNs and other domain-adapters:\nw \" Sigmoidphq.\n(5)\nThe final output o is a weighted sum of the outputs of the FFNs and the domain-adapter:\nEQUATION\nwhere r; s denotes matrix concatenation.\n\nExperimental Settings\nIn this section, we first introduce the datasets, then the baseline models, the evaluation metrics, and implementation details in the following four subsections, respectively.\n\nDatasets\nWe conduct experiments on three types of datasets: in-domain (ID) tasks that require general-domain knowledge; out-of-domain (OOD) tasks that require domain-specific knowledge; knowledgeintensive (KI) tasks that require commonsense knowledge.\n\u201a ID: GLUE Benchmark (Wang et al., 2018) including MNLI (Williams et al., 2017) , CoLA (Warstadt et al., 2019) , MRPC (Dolan and Brockett, 2005) , SST-2 (Socher et al., 2013) , RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) , STS-B (Cer et al., 2017) , WNLI (Levesque et al., 2012) , QNLI (Rajpurkar et al., 2016) , and QQP (Iyer et al., 2017) . \u201a OOD: ChemProt (Kringelum et al., 2016) , RCT (Dernoncourt and Lee, 2017) , IMDB (Maas et al., 2011) , and Amazon (He and McAuley, 2016) . ChemProt is a manually annotated chemical-protein interaction dataset extracted from 5,031 abstractions. RCT is a dataset based on PubMed for sentence classification. IMDB provides 25,000 movie reviews for sentiment analysis. Amazon is a dataset containing product reviews from Amazon, annotated with user ratings. \u201a KI: FEVER (Thorne et al., 2018) and Common-senseQA (CSQA) (Talmor et al., 2019) . FEVER consists of 185,445 claims that correspond to Wikipedia articles and are classified as supported, refuted, and not enough information. Common-senseQA consists of 12,247 questions with 5 choices and requires commonsense knowledge to predict the correct answers.\nFor Stage 1, we train the domain-adapter with unstructured knowledge related to the dataset following Section 3.1. The unstructured knowledge used is listed in Table 1 . We also experiment with structured knowledge in Section 6.2. For Stage 2, we adopt the true few-shot setting following (Perez et al., 2021) to demonstrate the effectiveness of MixDA. For each dataset class, we randomly sample K \" 16 examples from the original training set as the new training set, and another different K \" 16 examples as the validation set. The original validation set will be used as the test set. The Pfeiffer adapter is used in Stage 2 unless stated otherwise.\n\nBaselines\nIn our experiments, we use the following models as the main baselines. For convenience, we refer to them with the abbreviations in the parentheses later. \u201a HOULSBY (HO): Houlsby adapter (Houlsby et al., 2019) Prefix-Tuning trains a number of prompt embeddings for each task and pre-pends it before tokens. \u201a FINE-TUNING (FT): Fine-tuning all of the parameters of the RoBERTa-large model on downstream tasks.\n\nEvaluation Metrics\nWe adopt the Pearson correlation for STS-B since it is a regression task. The remaining are text classification tasks. Following Wang et al. (2018) ; Gururangan et al. ( 2020); Diao et al. (2021) , we adopt macro-F1 for MRPC and QQP, and micro-F1 for others as evaluation metrics. Macro-F1 computes the F1 independently for each metric, while micro-F1 computes an average metric of all classes. To account for the instability of small datasets, we report the average performance and the standard deviation of 3 runs with different random seeds.\n\nImplementation\nWe implement our RoBERTa-large model based on the Transformers library from HuggingFace 2 . The Houlsby adapter, the Pfeiffer adapter, and Prefix-Tuning are implemented based on the adaptertransformers library (Pfeiffer et al., 2020a) . LoRA is implemented based on OpenDelta (Ding et al., 2022) . During Stage 1, we train the domain-adapter with learning rate 1e-4, batch size 20, and weight decay 0.05. The knowledge loss factor \u03bb is set to 0.5. We train the 7 and 11 layers of RoBERTa-large with domain-adapter in 10 epochs. In Stage 2, we use the Pfeiffer adapter as the default task-adapter and train 20 epochs. All the experiments are conducted on Nvidia 2080Ti GPUs. We find the best hyper-parameters through grid search and the best results are listed in Appendix A. The computation time can be found in Appendix B.\n\nExperimental Results\nWe compare the performance of MixDA with our baselines on 15 datasets. First, we train the domainadapter for each domain individually and then perform each task with its corresponding domainadapter, which shows significant improvement over our baselines. Next, we plug in several domainadapters trained on different domains parallelly to verify the scalability of our model. detect the chemical-protein interaction. For example, MixDA shows more familiarity with words associated with that field, such as \"gefitinib\" and \"tyrosine kinase inhibitor\". In contrast, MixDA falters on STS-B, falling behind Pfeiffer by 0.8%. This is because the knowledge in Stage 1 is not effectively utilized. STS-B consists of sentence pairs like \"The cat sat on the mat\" and \"The cat did not sit on the mat\", with little need for additional knowledge. Across the three task domains, MixDA has an average improvement of 4.8% over RoBERTa + Pfeiffer on out-of-domain tasks, 2.5% on indomain tasks, and 5.0% on knowledge-intensive tasks. It shows that MixDA is not only effective for out-of-domain tasks and knowledge-intensive tasks that require additional knowledge but is helpful for general-domain language tasks as well, demonstrating its ability to excel at both in-domain and out-of-domain tasks (reliability).\n\nParallel Domain Adapters\nIn the previous section, we explored using a single domain-adapter for each downstream task. Next, we show the scalability of MixDA by using parallel domain-adapters and only train the MoA layer and task-adapters in Stage 2. The training process in Stage 2 follows the previous experiments. Table 3 shows the comparison across single domainadapter, parallel domain-adapters, and RoBERTa + Pfeiffer on 7 datasets. On average, parallel domainadapters show an improvement of 0.6% over vanilla RoBERTa + Pfeiffer, even though they fall behind the single domain adapter by 1.9%. This could be attributed to the MoA gate choosing the suboptimal domain-adapter for some test data. Still, considering its improvement over Pfeiffer, the MoA gate chooses the correct domain-adapter in most cases. Therefore, MixDA demonstrates its scalability, allowing end users to train Stage 1 on different datasets and combine them later. Overall, in both single and parallel situations, MixDA significantly improves upon the vanilla RoBERTa + Pfeif- fer model with a small increase in model size. This is due to the ability of MixDA to capture knowledge and the MoA to select useful knowledge for downstream tasks.\n\nAnalysis\nIn this section, we analyze the respective contributions of each part of MixDA through detailed analysis, including the Stage 1 training, task-adapters in Stage 2, and the mixture-of-adapters gate.\n\nAblation Study\nIn this section, we conduct an ablation study to reveal the contributions of each part of the model. There are three variants: (1) We remove the MoA gate and choose the domain-adapter instead of the RoBERTa feed-forward layer (-MoA). ( 2 \n\nStructured and Unstructured Knowledge\nIn Section 5, the MixDA is only trained on unstructured knowledge. As a comparison, we also train the domain adapter on ConceptNet, a structured knowledge dataset, and then attach both the unstructured and structured to our model and train the MoA layer and the task-adapter during Stage 2.\nTable 5 shows the result of combining structured and unstructured knowledge in Stage 1. FEVER and CSQA, two knowledge-intensive tasks, have the greatest improvement: 10.3% for FEVER and 1.2% for CSQA. This is because ConceptNet stores commonsense knowledge that can help both tasks. Meanwhile, MRPC and STS-B also obtain improvement, showing that ConceptNet can benefit general language tasks as well. In conclusion, the experiment demonstrates the ability of MixDA to utilize structured knowledge, the extensibility of our model, and the possible benefits of structured knowledge.\n\nEffectiveness of Task-Adapters\nIn most experiments of this paper, we adopt Pfeiffer as the task-adapter unless otherwise specified. In this section, we test the performance of MixDA combined with other kinds of task-adapters, including Houlsby, Prefix-Tuning, LoRA, and Pfeiffer. parameters compared to Houlsby, making it the optimal choice of task-adapters in our experiment.\n\nConclusion\nIn this paper, we proposed MixDA, a mixture of adapters for domain adaptation. We first decouple the knowledge modules (i.e., FFNs) into the old-domain and domain-specific FFNs. Then we propose a two-stage adapter tuning strategy: first tuning the domain adapter on each domain and then tuning the task adapter on each task. Moreover, our model could be scaled to multiple domains easily with the introduction of the mixture-of-adapters gate. Empirically, MixDA achieved significant improvement over in-domain tasks, out-of-domain tasks, and knowledge-intensive tasks. Further analyses demonstrate the reliability, scalability, and efficiency of our method.\n"}
{"question": "In the context of metalinguistic QA datasets, what distinguishes this work from previous research?", "evidence": "  So far, metalanguage has not been a focus in the QA domain-ours is the first publicly available English metalinguistic QA dataset. Most QA tasks are set up to have a question and a reference document, where the objective is to find the answer based on the document (Fan et al., 2019; Kwiatkowski et al., 2019) . In this paper, we explored a type of \"closed-book\" question answering task (Roberts et al., 2020; Khashabi et al., 2021) .  To the best of our knowledge, this task has not been explored to date within the realm of English language questions that require significant generalization and adaptation rather than looking up facts.  ", "options": ["A. It focuses on annotating and quantifying mentions of metalinguistic expressions.", "B. It primarily analyzes the usage of metalinguistic expressions of mock politeness.", "C. It introduces the first publicly available English metalinguistic QA dataset.", "D. It explores \"closed-book\" question answering tasks with reference documents."], "answer": "C", "content": "\nIntroduction\nLanguage is so powerful that it can be reflected back on itself. Statements like \"In informal usage, a steep learning curve means something that is difficult (and takes much effort) to learn\" or \"In some cases, an adjective has both -ic and -ical forms, with no difference in meaning\" expressly concern linguistic inventories, structures, and behaviors. In other words, they are metalinguistic-they use language to discuss language (cf. Wilson, 2013) . They may concern a particular instance of language use, or properties of a language or speaker in general; either way, they are metalinguistic in making linguistic phenomena the subject matter of a linguistic utterance. For the rest of this paper, the term metalanguage is used for natural language text in which natural language is also the subject matter.\nWhile NLP models have become powerful at predicting text in many settings, it remains to be seen whether such capability extends to metalanguagewhere linguistic strings are not being deployed to contribute to the discourse with their normal denotations, but rather, are treated as entities with linguistic properties (e.g., grammar, meaning). One way this can be explored is in a question answering framework, which requires suitable datasets, ideally based on questions that are realistic and paired with high-quality answers.\nIn this paper, we present a corpus of metalinguistic questions and answers about English. The corpus is collected and carefully processed from two Stack Exchange forum sites: English Language & Usage (ENG) and English Language Learners (ELL). It covers more than 70k questions on numerous topics about English such as grammar, meaning, fluency, and etymology along with answers. Our corpus, ELQA (English Language Questions and Answers), can serve as a tool to facilitate metalinguistic studies. Moreover, since questions in ELQA cover a variety of topics in English, it can be used in the educational and English language learning domains.\nAs the first case study of ELQA, we investigate the performance of current state-of-the-art NLP technology on free-form question answering in the English language domain. Additionally, we explore the possibility of building NLP models that can directly answer questions from language learners. We process a subset of ELQA and make it appropriate for this task. Then, we report on the results of both automatic and human evaluations using different experimental settings of T5 1 and GPT-3 2 models. Although most of these models achieve high ratings for well-formedness, the validity of their answers is significantly lower than that of human-authored answers, indicating that this type of metalinguistic QA task is challenging even for large language models.\nOur main contributions are: 1) we release the first publicly available metalinguistic QA dataset, 3 focused on the English language; 2) we present a taxonomy of questions in the corpus along with analysis; and 3) we investigate to what extent LLMs are able to articulate appropriate generalizations about language in response to these questions.\n\nRelated Work\nStack Exchange is a network of numerous CQA sites (originally and most famously, Stack Over-3 https://github.com/shabnam-b/ELQA flow) built on a common platform. Stack Exchange forums have been featured in a number of previous datasets (Yao et al., 2013; Hoogeveen et al., 2015; Ahmad et al., 2018; Penha et al., 2019; Campos et al., 2020; Kumar and Black, 2020; Rogers et al., 2023) , including the English site (our ENG) along with others such as Ask Ubuntu, Android, Gaming and WordPress (dos Santos et al., 2015; Nakov et al., 2017) . We focus on ENG and ELL as they concern the English language itself; we show that these datasets cover a wide range of metalinguistic questions.\nOur use of these forums contrasts with previous work on metalanguage in corpora, which annotated and quantified mentions (Anderson et al., 2004; Wilson, 2010 Wilson, , 2011 Wilson, , 2012 Wilson, , 2017)) , but did not consider entire questions and answers about language. Taylor (2015) studied metalanguage in online forums, but with a focus on the usage of metalinguistic expressions of mock politeness. More recently, Bogetic (2021) published the first corpus of contemporary Slovene, Croatian and Serbian media metalanguage texts.\nSo far, metalanguage has not been a focus in the QA domain-ours is the first publicly available English metalinguistic QA dataset. Most QA tasks are set up to have a question and a reference document, where the objective is to find the answer based on the document (Fan et al., 2019; Kwiatkowski et al., 2019) . In this paper, we explored a type of \"closed-book\" question answering task (Roberts et al., 2020; Khashabi et al., 2021) . To the best of our knowledge, this task has not been explored to date within the realm of English language questions that require significant generalization and adaptation rather than looking up facts.\n\nConstructing the Dataset\nWe collect our data from two sites on Stack Exchange: English Language & Usage (ENG) 4 and English Language Learners (ELL). 5 Sample screenshots of the site are shown in Figure 1 . The Stack Exchange data is publicly released under the CC-BY-SA 3.0 license. We preprocessed the data until 2021-12-06 collected from the Internet Archive 6 to be suitable for NLP studies and release it as ELQA. Additionally, some cleanup (e.g., removing posts marked as \"spam\" or \"offensive\") was done. Fields for each entry (question) include the title, body, user bio (if available), score (which is calculated based on up-votes and down-votes by other users), tags (user-assigned, related to the area/topic of the question), favorite count, and a list of answers. Textual content (body and user bio) is provided in two formats: HTML and plain text without HTML tags. We release two versions of ELQA based on different preprocessing steps. In ELQA-large, we keep questions as long as they don't include any images (<img> HTML tag) and have an answer with a score of at least 2 (meaning at least two people other than the user posting the answer found it helpful). For ELQA-small, we applied further filtering to ensure that the data has the least amount of noise: a) questions should have a score of at least 4 https://english.stackexchange.com/ 5 https://ell.stackexchange.com/ 6 https://archive.org/ 2 (ensuring questions are clear and coherent), b) question has an answer with a score higher than 3 and c) there are no hyperlinks in at least one of the high-rated answers. The last step reduces noise and facilitates a fair comparison for the closed-book question-answering task ( \u00a74) with model-generated answers, as models cannot be expected to have access to the web to suggest valid URLs compared to humans who would search the web for appropriate resources to include in their answers.\nFor quality assurance, we also did a human annotation on ELQA-small. Two of the authors annotated 250 question and answer pairs for the following: 1) Is the question answerable? and 2) Does the answer fully address the question? We found 99.2% of the questions answerable and 91.8% of the answers acceptable.\nTable 1 contains overall statistics on both versions. Figure 2 shows the distribution of the 10 most common tags in each of the sites. Since users assign these tags to their questions (0 to multiple), similar or near-duplicate tags are common within the collection. Some form more general and more fine-grained variants, e.g. 'meaning' and 'meaningin-context'. In addition to available user-assigned tags, we manually inspected a large subset of the data to identify salient types of questions. These are defined below and illustrated in Table 2 . We then labeled 100 random questions to get a rough estimate of their frequencies (two annotators annotated these 100 samples and they agreed on 92% of cases in an overlapping subset).\n\u2022 Fluency (\u224838% of questions): Usually asking about a particular sentence, comparison of multiple sentences, and/or probing how an expression should be used in general. The user wants to know if X is correct, or to decide between multiple choices, which one is correct. \"Correct\" could mean grammatical, most natural/idiomatic, stylistically appropriate, conveying the intended meaning, etc. In Qs where options are provided by the user, there are cases in which 1) none of the choices are correct, 2) multiple choices are correct, and 3) only one is correct. \u2022 Form to Meaning (Interpretation) (\u224819% of questions): Questions such as \"What does X mean?\" (of an expression in general, or an encountered passage) or \"What's the difference in meaning between X and Y?\". \u2022 Meaning to Form (Encoding) (\u224820% of questions): In these questions, the user gives some As can be seen from the examples in Table 2 , it is common for questions and answers to contain example usages, often visually distinguished with Markdown formatting (such as blockquotes, bullets, and italics) which we retain in the processed corpus markup. Examples can be incorporated into a post in a variety of ways-e.g., asking for an interpretation of one usage, as in the Form to Meaning example in Table 2 , or contrasting multiple usages such as in the following question:\n\nDid VS Have done\nWhat is difference between the following statements: Did you tell your parents yet? Have you told your parents yet? Haven't you told your parents yet? Are these questions correct? why do we use one over another in some cases? What is the difference in meaning?\nUsage examples provided in a question may be instances that the author encountered \"in the wild\" (such as in a novel or film), or in a grammar book or dictionary, or they may have been constructed by the user. Answers sometimes include examples found through a corpus search.\n\nEnglish Language Question Answering\nLarge language models can produce output that is fluent and (at times) informationally adequate when presented with factual questions about entities in the world (Roberts et al., 2020) . But how do such models perform when asked questions about the language itself? In this section, we investigate the free-form English language question answering task.\nThis task has the potential to benefit educational applications for language learners. Research on NLP for educational purposes has investigated tasks such as automated grammatical error correction (Dale et al., 2012; Ng et al., 2014; Bryant et al., 2019; Wang et al., 2021, inter alia) , question and quiz generation for language learning (Sakaguchi et al., 2013; Chinkina and Meurers, 2017; Marrese-Taylor et al., 2018; Vachev et al., 2021) , and automated essay scoring (Burstein, 2003; Farag et al., 2018, inter alia) . Nevertheless, an application that has not been taken up by the educational NLP community is free-form question answering about language. Second language learners possess a degree of metalinguistic awareness about the language they are learning, and often turn to teachers or more advanced speakers with explicit questions about vocabulary, grammar, and usage. Community Question Answering (CQA) websites such as Stack Exchange have sites for language learners' questions and answers. These sites require consid- erable effort by volunteers, and learners may have to wait for an answer-if an answer is provided at all. In fact, looking at the data from 2021-12-06 for ENG and ELL, 9% of questions have no answers.\n\nData\nWe randomly divided ELQA-small into train/test/dev splits. This resulted in 21,175 Q&A pairs in the train split and 3,107 Q&A pairs in each of the dev and test splits. Answers in these splits have a score of at least 4. If there are multiple high-rated answers to a question, we include all of them for training. Some of these questions can be answered by looking at a dictionary or vocabulary list for descriptions. But many of them are explanations in relation to particular instances of language use and require significant reasoning rather than looking up facts. Thus in this setup, we do not have any external context/reference available at evaluation time, i.e. this is a closed-book QA task.\nThe input for the task is Title:\n[Q title] <sep> Body: [Q body].\nWe use the HTML version of ELQA for this task since metalinguistic mentions are usually distinguished via formatting (e.g., blockquotes, bullets) and the ultimate goal is a system that humans can easily use to get answers to their language-related questions.\n\nSetup\nWe use T5 (Raffel et al., 2020; Roberts et al., 2022) and GPT-3 (Brown et al., 2020) as our models since they have been shown to be strong baselines in other QA domains. We believe the questions in ELQA offer new challenges for the QA task since they require different types of knowledge/understanding to be able to generate answers. Addition-ally, these questions contain noise (grammatical errors) and cases of textual metalanguage which is likely harder to comprehend for a model.\nWe fine-tune T5-l and T5-xxl for this task. 7 We saved multiple checkpoints during fine-tuning and evaluated them with the interpolation of BLEU (Papineni et al., 2002) , BERTScore (Zhang et al., 2020) and ROUGE (Lin, 2004) on the dev set to choose the best-performing one (checkpoint at 75k updates, hyperparameters available in Table 8 in the Appendix).\nWith GPT-3 we used text-davinci-003 and experimented with both fine-tuning (FT) on 100 and 1000 samples and a few-shot (FS) setting in which the model is given a few demonstrations of the questions and answers at inference time as conditioning, but no weights are updated (Radford et al., 2019) . In the FS setting, we show the model four Q&A pairs since we wanted the model to see different question types but there were also limits on the input length. To select these 4 pairs, we randomly created 5 different sets of Q&A pairs, evaluated on a subset of dev, and chose the best-performing set for the experiments (dev results available in Appendix, Table 9 ).\n\nAutomatic Evaluation\nResults are shown in Table 3 . GPT-3 FS outperforms all other methods in all metrics with a large margin except for BLEU Score. We also observed that using GPT-3 in a few-shot setup worked much better than the fine-tuned version. Looking at some of the model-generated answers, we noticed that the fine-tuned model tends to generate longer an- swers containing redundant text. We observed improvements when we used 1000 samples instead of 100 for fine-tuning and hence, fine-tuning on larger data might result in better performance, however, we only experimented with 100 and 1000 samples in this paper due to having limited resources.\nBased on Table 3 , T5-xxl seems to perform similarly to GPT-3 FT-1000. However, a small manual evaluation showed otherwise (GPT-3 FT-1000 answers were slightly better). Furthermore, we observe that the scores for even the best system are very low, but manual evaluations showed that the GPT-3 FS generates fairly good answers in many cases. Due to these observations and also given the well-known limitations of automatic metrics for evaluating generation tasks (Kasai et al., 2022; Celikyilmaz et al., 2020; Bhakthavatsalam et al., 2021) , we believe conducting human evaluation for deeper analysis is necessary for this task.\nIn Table 4 , we show results for each site to see if one is more challenging than the other. Overall, models perform slightly better on ELL based on automatic metrics-but we see in the next section (Table 5 ) that there isn't really a meaningful difference between the sites when humans evaluate the answers.\n\nHuman Evaluation\nHuman evaluators were presented with the question title and body, and then asked to rate 5 answers: a top-rated human-provided answer, a low-rated human-provided answer, and answers generated by 3 of our best models: GPT-3 FS, GPT3 FT-1000, T5-xxl.\nThey were asked to give ratings (via a slider widget, on a 1-5 integer scale-the higher, the better) for two criteria (C1 & C2): 8 1. Does the answer look grammatically/ structurally like a good answer (ignoring whether it answers the question)? 2. Is the information in this answer a valid response to the question (ignoring formatting/ stylistic issues)?\nThe first criterion aims to get a score for fluency and coherence and the second one for correctness and completeness.\nWe collected ratings for a set of 75 questions (375 different answers). Each question with its set of answers was evaluated by at least 2 raters, and then the average score was calculated based on their responses. 9 We also report the average z-score which is calculated over each annotator's raw score distribution for each metric, intended to normalize interannotator variation in how the scale is interpreted for each of the two metrics (details in Appendix B).\nThe results of this study are shown in Table 5 . Overall, answers generated by GPT-3 FS have a small gap with human answers in both C1 and C2. 10 GPT-3 FT-1000 comes next, with less accurate answers containing redundant text and hallucinations. The smallest model, T5-xxl, ranks last.\nRankings based on human evaluations are available in Table 6 . These results are also indicating that model-generated answers are fluent in most cases, but they are not as good as human answers Title Question Explain the verb tense in \"All that glitters is not gold\" I am confused by the maxim All that glitters is not gold. May I know why the word is is used there? In my known according the English grammar glitters is plural, So after plural we add verb+s. As in above example use auxiliary verb so why not arewhy is? Source Answer Top-rated human Good question, but it's worth pointing out that glitters in this context is a verb, not a noun. The subject of the sentence is all. When all is used as the subject of a sentence, it functions as a pronoun, and it can function as a singular or a plural pronoun.\nFrom NOAD <blockquote> all (pronoun) [with clause ] the only thing (used for emphasis) : all I want is to be left alone.\n(used to refer to surroundings or a situation in general) everything : all was well | it was all very strange. </blockquote> So: <blockquote> All that glitters is not gold </blockquote> is roughly equivalent to <blockquote> Everything that glitters is not gold </blockquote> and the verb tense agrees. but: <blockquote> All who eat are not gluttons </blockquote> is roughly equivalent to: <blockquote> All the people who eat are not gluttons </blockquote> so there is no problem there, either.\n\nLow-rated human\nGrammatically is is used with third person singular and also s form of verb is used. \"All\" indicates a third person single group.\n\nGPT-3 FS\nThe phrase all that glitters is not gold is an idiom, which means that things that appear to be valuable may not actually be so. It is an old proverb, and the verb tense used in the phrase is an example of the so-called \"historic present\". The historic present is a literary device used to give a sense of immediacy to a narrative. It is used to describe events that happened in the past, but are being described as if they are happening in the present. In this case, the phrase is used to emphasize the timelessness of the proverb. So, while the verb glitters is indeed plural, the verb is is used in the historic present to give the phrase a sense of immediacy and timelessness. GPT-3 FT-1000\nThe maxim is in the present tense, so the verb is also in the present tense: Glitters. The past tense of glitter is glittered, and the past participle is glittered. So if the maxim was in the past tense, it would be: <blockquote> All that glittered was not gold. </blockquote> The present tense maxim is a generalisation about something that is true in the speaker's experience. The past tense maxim would be a historical statement about what was true in the past.\n\nT5-xxl\nThis is an interesting question, because it is not immediately obvious what the grammar rules are here. It is not a fixed phrase, and so subject-verb agreement must be based on the context. In this case, the that is a prepositional phrase acting as a complement to the preposition all. It could be rephrased as <blockquote> All that glitters is not gold </blockquote> It is true that all means \"everything that glitters\" -but it is equally true that not everything that glitters means \"not everything that glitters is gold\". It is much the same as <blockquote> Everything that glitters is not gold. </blockquote> The subject all is singular, so the verb must be singular as well -is. If it was twice plural, then are would be used, but it is not. It is interesting to note that Shakespeare used the same construction in Hamlet: <blockquote> Everything that glitters is not gold. </blockquote> The subject that is singular, and so the auxiliary verb is agrees with the singular noun. when correctness/completeness is considered (GPT-3 FS is ranked first or as good as a top-rated human answer in only 45% of cases).\nFor each criterion and Top-rated human, Lowrated human and GPT-3 FS, histograms of the average score of the two annotators are plotted in Figure 3 . We can observe that GPT-3 FS and Lowrated human have very similar numbers of highscoring answers (human evaluation scores), but the number of low-scoring human answers drops off gradually as quality decreases, while the distribution is more spread out for GPT-3 FS. I.e., the model has some moderately bad answers as well as some good ones, whereas Low-rated human answers cluster more on the upper end of the scale. C1 (fluency/coherence). All models generated fairly fluent and well-structured answers. We even notice that GPT-3 FS scores higher in wellformedness than human answers. We looked at those samples and we believe there are two main reasons for this: 1) Some human answers were very long, containing multiple different quotes from different sources. On average, our evaluators preferred the structure of answers from GPT-3 FS, which took the form of a short paragraph addressing the question. 2) Some human answers have a more casual/conversational nature. On the other hand, GPT-3 FS generated more authoritativesounding, teacher-like answers with complete sentences, which were preferred by our evaluators in some cases. C2 (correctness/completeness). On average, models are worse on this metric, though sometimes they did produce acceptable answers (perhaps because variants of the question are commonly discussed on the web).\nOne challenge in this domain is that questions, particularly from ELL, may not be fully fluent. In the Fluency example from Table 7 we see that there are some grammatical errors in the question that are unrelated to the topic of the question. In addition, the questioner uses incorrect terminology, mentioning verb tense in the post title even though the question is actually about subject-verb agreement with respect to number (as can be inferred from the examples). While the good human response correctly flags the incorrect terminology and answers the underlying question, GPT-3 models give irrelevant answers about tense.\nAnother correctness failure from GPT-3 FS can be seen in the following pair involving a Meaningto-Form question:\nWhat is the analog equivalent of 'digitally'? In electronics, we can solve a problem digitally or analoguely ? That doesn't sound right, but surely there must be a word I can use? Update: some definitions: digitally: Relating to or being a service that provides information expressed in discrete numerical form. analoguely: Relating to or being a service that provides information expressed in continuous, non-discrete form. (This is my made up definition of a word I made up). analogously: Similar or alike in such a way as to permit the drawing of an analogy. As you can see, the word analogously doesn't mean the opposite of digitally, it means something else. Answer: The analog equivalent of 'digitally' is 'analogously'. Analogously means \"in a similar way\" or \"by comparison\". It is used to describe a situation where two things are similar in some way, but not necessarily opposite. For example, you could say that a digital signal is analogous to an analog signal, because they are both signals, but they are different in terms of how they are represented.\nWe see that the model begins by ignoring the user's explanation that analogously does not have the desired meaning, and suggests it anyway. The rest of the model's answer gives a (valid) general definition of analogously and clumsily attempts to apply it to the two kinds of signals, which is not responsive to the question. It may be the case that models particularly struggle with questions for which there is no straightforward solution (in this case, no adverb derived from analogue has the user's intended meaning).\n\nMetrics\nOur human evaluations found that the high-rated human answers are the best, and GPT-3 FS is roughly on par with the low-rated human answer, while other models are worse.\nAs noted in \u00a74.3.1, we did not find the automatic metrics very informative. We believe this is due to the long and free-form nature of this task: concepts can be explained in different ways and answers can include different examples which make automatic evaluations very challenging. For example, for the question Why is this sentence: \"Additional nine features were added. . . \" incorrect? a human answer is:\nIt has to do with the order of the adjectives. For example, consider this sentence: <blockquote> Happy nine men walk into a bar. </blockquote> Both nine and happy are adjectives, but we are really intending nine to describe the happy men, not happy describing the nine men. I don't .... Model-generated answers mostly mirror the examples in the question. But answers given by humans mention new examples-which are not in the question (and therefore unlikely to come up in other answers). Thus a good answer may mention superficially off-topic terms metalinguistically, which presents evaluation challenges unlike those of summarization or MT, for example (Reiter, 2018; Celikyilmaz et al., 2020) . For QA tasks in metalinguistic domains such as ours, caution is warranted when applying automatic metrics that are unaware of the use/mention distinction. Human evaluation is likely to be more illuminating. We release our full human evaluation data to enable detailed comparisons in future research.\n\nQualitative Analysis\nWe examined behavioral patterns for the best model, GPT-3 FS, organized by question type, with observations as follows. Interpretation and Encoding: These are the categories where the model performs the best, especially if the question has a straightforward answer (for example, there aren't several possible answers based on context). Topics in these categories often center on definitions of vocabulary and idioms. These questions may involve less reasoning to answer, and models may be leveraging content from the online dictionaries and thesauri in their web training data.\nGrammatical Analysis: When asked about general concepts and rules of grammar, the modelgenerated answers are usually acceptable:\n-ed and -ing adjectives What is the difference? <blockquote> I am interesting in mathematics. </blockquote> versus <blockquote> I am interested in mathematics. </blockquote> Murphy intermediate grammar Unit 97 tells me that I cannot use the former. Extended and general explanations will be appreciated. Answer: The difference between -ed and -ing adjectives is that the -ed adjectives describe a feeling or emotion that is felt by the subject, while the -ing adjectives describe a quality of the subject itself. In the example sentence, interested is the correct word to use because it describes the feeling of interest that the subject has in mathematics. Interesting would be incorrect because it describes the quality of the subject itself, not the feeling of the subject. For example, if you were to say \"This book is interesting,\" you would be describing the book itself, not the feeling you have towards the book. On the other hand, if you were to say \"I am interested in this book,\" you would be describing the feeling you have towards the book.\nHowever, when questions become more specific to examples, sometimes the model struggles:\nWhat does \"in\" refer to? <blockquote> A symbiotic relationship is an interaction between two or more species in which one species lives in or on another species. Answer: It refers to the other species. The symbiotic species lives in the other species.\nThe example in the question uses the word in twice. Rather than addressing the first token, which the questioner has bolded, the answer addresses the second one (denoting the physical relationship between one species and another). Fluency: The model correctly answers many of the fluency questions. Others require reasoning and understanding of context, and the model struggles to generate a correct answer. For example:\nReturning for the nth time Suppose I went to the supermarket three times. Is my third trip considered my \"second time returning there\" or my \"third time returning there?\" Thanks. (I know this question sounds silly, but English is my second language...) Answer: Your third trip to the supermarket would be considered your third time returning there. The phrase returning for the nth time is used to refer to the nth time you have returned to a place, not the n-1th time.\nWe also observe that when the correct usage of a case is not mentioned in the question itself, the model sometimes suggests incorrect corrections and wrong explanations:\nWhy is this usage of \"to\" incorrect? I am in communication with a friend who is learn-ing English. I noticed her using \"to\" in the wrong context, but I'm unable to explain which rule she is breaking. <blockquote> I can't access to Gmail. </blockquote> Answer: In this sentence, the word to is incorrect because it is being used as a preposition. The correct word to use in this context is for. <block-quote> I can't access for Gmail. </blockquote> The word to is used as a preposition when it is followed by a verb, such as go, come, run, etc. For example: <blockquote> I am going to the store. </blockquote> However, when to is followed by a noun, it is usually incorrect. In this case, the correct word to use is for. <blockquote> I need a gift for my friend.</blockquote>\n\nOutlook\nOverall, we were surprised by the quality of many of the answers from GPT-3 FS: many would likely have received high user ratings if submitted as answers on the site. At the same time, the model is not to the point where we would want to trust its answers without human review. We believe that answer confidence estimation-so that users can be shown only the best model-generated answers-is an important direction for using learner QA models in practice (Jiang et al., 2021) .\n\nConclusion\nWe presented ELQA, a dataset containing metalinguistic questions and answers about the English language. We provided analysis and a taxonomy of the data, along with experiments on free-form answer generation and investigated the extent to which language models can articulate their generalizations about language. Since many of the questions in ELQA were asked by language learners, it forms a potentially useful and so far untapped resource for educational NLP purposes and metalinguistic question answering. We release the dataset to enable further studies of this task.\n"}
{"question": "What is the purpose of the ACU protocol in the evaluation of summarization systems?", "evidence": "  The ACU protocol is designed to reduce the subjectivity of reference-based human evaluation by simplifying the basic annotation unit -the annotators only need to decide on the presence of a single fact, extracted from one text sequence, in another text sequence, to which a binary label can be assigned with more objectivity Specifically, the evaluation process is decomposed into two steps: (1) ACU Writing -extracting facts from one text sequence, and (2) ACU Matching -checking for the presence of the extracted facts in another sequence. ", "options": ["A. To assess the popularity of summarization systems.", "B. To evaluate the linguistic quality of summaries.", "C. To measure the information overlap between system-generated summaries and reference summaries.", "D. To calculate the summary length of system-generated summaries. "], "answer": "C", "content": "\nIntroduction\nHuman evaluation plays an essential role in both assessing the rapid development of summarization systems in recent years (Lewis et al., 2020a; Zhang et al., 2020a; Brown et al., 2020; Sanh et al., 2022; He et al., 2022) and in assessing the ability of automatic metrics to evaluate such systems as a proxy * Equal contribution for manual evaluation (Bhandari et al., 2020; Fabbri et al., 2022a; Gao and Wan, 2022) . However, while human evaluation is regarded as the gold standard for evaluating both summarization systems and automatic metrics, as suggested by Clark et al. (2021) an evaluation study does not become \"gold\" automatically without proper practices. For example, achieving a high inter-annotator agreement among annotators can be difficult (Goyal et al., 2022) , and there can be a near-zero correlation between the annotations of crowd-workers and expert annotators (Fabbri et al., 2022a) . Also, a human evaluation study without a large enough sample size can fail to find statistically significant results due to insufficient statistical power (Card et al., 2020) .\nTherefore, we believe it is important to ensure that human evaluation can indeed serve as a solid foundation for evaluating summarization systems and automatic metrics. For this, we propose using a robust human evaluation protocol for evaluating the salience of summaries that is more objective by dissecting the summaries into finegrained content units and defining the annotation task based on those units. Specifically, we introduce the Atomic Content Unit (ACU) protocol for summary salience evaluation ( \u00a73), which is modified from the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. We demonstrate that with the ACU protocol, a high inter-annotator agreement can be established among crowd-workers, which leads to more stable system evaluation results and better reproducibility.\nWe then collect, through both in-house annotation and crowdsourcing, RoSE, a large human evaluation benchmark of human-annotated summaries with the ACU evaluation protocol on recent state-of-the-art summarization systems, which yields higher statistical power ( \u00a74). To support evaluation across datasets and domains, our benchmark consists of test sets over three summarization datasets, CNN/DailyMail (CNNDM) (Nalla-\n\nStatistical Power\n\u2212 High statistical power is difficult to reach for human evaluation of similar-performing systems. \u00a74.1 \u2212 Increasing the sample size of human evaluation effectively raises statistical power.\n\nSummary Length\n\u2212 Summaries from different summarization systems show a large difference in average length. \u00a74.2 \u2212 Difference in summary length is not well-reflected by automatic evaluation metrics.\n\u2212 Reference-free and reference-based human evaluation results have a near-zero correlation. Evaluation \u2212 Reference-free human evaluation strongly correlates with input-agnostic, annotator preference. Protocol Comparison \u2212 Annotator's input-agnostic preference has a strong positive correlation with summary lengths.\n\u00a75.2 \u2212 Annotator's input-agnostic preference does not favor reference summaries. \u2212 Compared to smaller, fine-tuned models, zero-shot large language models (e.g. GPT-3) perform better under reference-free evaluation, but worse under reference-based evaluation.\nEvaluating \u2212 A higher-powered human evaluation dataset can lead to a more robust automatic metric evaluation, as shown by a tighter confidence interval and higher statistical power of metric evaluation. Automatic Metrics \u2212 Automatic metric performance differs greatly under different human evaluation protocols.\n\u00a76.1 & \u00a76.2 \u2212 Automatic metrics show relatively strong system-level correlation and moderate summary-level correlation with our robust human evaluation protocol.\nTable 1 : Summary of the key findings in our work. pati et al., 2016) , XSum (Narayan et al., 2018), and SamSum (Gliwa et al., 2019) , and annotations on the validation set of CNNDM to facilitate automatic metric training. To gain further insights into the characteristics of different evaluation protocols, we conduct human evaluation with three other protocols ( \u00a75). Specifically, we analyze protocol differences in the context of both fine-tuned models and large language models (LLMs) in a zero-shot setting such as GPT-3 (Brown et al., 2020) . We find that different protocols can lead to drastically different results, which can be affected by annotators' prior preferences, highlighting the importance of aligning the protocol with the summary quality intended to be evaluated. We note that our benchmark enables a more trustworthy evaluation of automatic metrics ( \u00a76), as shown by statistical characteristics such as tighter confidence intervals and more statistically significant comparisons ( \u00a76.2).\nOur evaluation includes recent methods based on LLMs (Fu et al., 2023; Liu et al., 2023) , and we found that they cannot outperform traditional metrics despite their successes on related benchmarks such as SummEval (Fabbri et al., 2022a) . We summarize our key findings in Tab. 1. Our contributions are the following: (1) We propose the ACU protocol for high-agreement human evaluation of summary salience. (2) We curate the RoSE benchmark, consisting of 22000 summary-level annotations and requiring over 150 hours of in-house annotation, across three summarization datasets, which can lay a solid foundation for training and evaluating automatic metrics. 1 (3) We compare four human evaluation protocols for summarization and show how they can lead to drastically different model preferences. (4) We evaluate automatic metrics across different human evaluation protocols and call for human evaluation to be conducted with a clear evaluation target aligned with the evaluated systems or metrics, such that task-specific qualities can be evaluated without the impact of general, input-agnostic preferences of annotators. We note that the implications of our findings can become even more critical with the progress of LLMs trained with human preference feedback (Ouyang et al., 2022) and call for a more rigorous human evaluation of LLM performance.\n\nRelated Work\nHuman Evaluation Benchmarks Human annotations are essential to the analysis of summarization research progress. Thus, recent efforts have focused on aggregating model outputs and annotating them according to specific quality dimensions (Huang et al., 2020; Bhandari et al., 2020; Stiennon et al., 2020; Zhang and Bansal, 2021; Fabbri et al., 2022a; Gao and Wan, 2022) . The most relevant work to ours is Bhandari et al. (2020) , which annotates summaries according to semantic content units, motivated by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. However, this benchmark only covers a single dataset (CNNDM) without a focus on similarly-performing state-of-the-art systems, which may skew metric analysis (Tang et al., 2022a) and not fully reflect realistic scenarios (Deutsch et al., 2022) . In contrast, our benchmark consists only of outputs from recently-introduced models over three datasets.\nSummarization Meta-Evaluation With a human evaluation dataset, there exist many directions of meta-evaluation, or re-evaluation of the current state of evaluation, such as metric performance analyses, understanding model strengths, and human evaluation protocol comparisons.\nWithin metric meta-analysis, several studies have focused on the analysis of ROUGE (Lin, 2004b) , and its variations (Rankel et al., 2013; Graham, 2015) , across domains such as news (Lin, 2004a) , meeting summarization (Liu and Liu, 2008) , and scientific articles (Cohan and Goharian, 2016) . Other studies analyze a broader set of metrics (Peyrard, 2019; Bhandari et al., 2020; Deutsch and Roth, 2020; Fabbri et al., 2022a; Gabriel et al., 2021; Kasai et al., 2022b) , including those specific to factual consistency evaluation (Kryscinski et al., 2020; Durmus et al., 2020; Wang et al., 2020; Maynez et al., 2020; Laban et al., 20d; Fabbri et al., 2022b; Honovich et al., 2022; Tam et al., 2022) .\nRegarding re-evaluating model performance, a recent line of work has focused on evaluating zeroshot large language models (Goyal et al., 2022; Liang et al., 2022; Tam et al., 2022) , noting their high performance compared to smaller models.\nAs for the further understanding of human evaluation, prior work has compared approaches to human evaluation (Hardy et al., 2019) , studied annotation protocols for quality dimensions such as linguistic quality (Steen and Markert, 2021) and factual consistency (Tang et al., 2022b) , and noted the effects of human annotation inconsistencies on system rankings (Owczarzak et al., 2012) . The unreliability and cost of human evaluation in certain settings have been emphasized (Chaganty et al., 2018; Clark et al., 2021) , with some work noting that thousands of costly data points may need to be collected in order to draw statistically significant conclusions (Wei and Jia, 2021). Our meta-analysis focuses on this latter aspect, and we further analyze potential confounding factors in evaluation such as length and protocol design, with respect to both small and large zero-shot language models.\n\nAtomic Content Units for Summarization Evaluation\nWe now describe our Atomic Content Unit (ACU) annotation protocol for reference-based summary salience evaluation, including the procedure of writing ACUs based on reference summaries and matching the written ACUs with system outputs.\n\nPreliminaries\nIn this work, we focus on a specific summarization meta-evaluation study on summary salience. Salience is a desired summary quality that requires the summary to include all and only important information of the input article. The human evaluation of summary salience can be conducted in either reference-free or reference-based manners. The former asks the annotators to assess the summary directly based on the input article (Fabbri et al., 2022a) , while the latter requires the annotators to assess the information overlap between the system output and reference summary (Bhandari et al., 2020) , under the assumption that the reference summary is the gold standard of summary salience. 2 Given that reference-based protocols are more constrained, we focus on reference-based evaluation for our human judgment dataset collection, and we conduct a comparison of protocols in \u00a75.\n\nACU Annotation Protocol\nInspired by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols and subsequent annotation collection efforts (Bhandari et al., 2020; Zhang and Bansal, 2021) , the ACU protocol is designed to reduce the subjectivity of reference-based human evaluation by simplifying the basic annotation unit -the annotators only need to decide on the presence of a single fact, extracted from one text sequence, in another text sequence, to which a binary label can be assigned with more objectivity. Specifically, the evaluation process is decomposed into two steps: (1) ACU Writing -extracting facts from one text sequence, and (2) ACU Matching -checking for the presence of the extracted facts in another sequence. We formulate the ACU protocol as a recall-based protocol, such that the first step only needs to be performed once for the reference summary, allowing for reproducibility and reuse of these units when performing matching on new system outputs. ACU Writing While the LitePyramid approach defines its basic content unit as a sentence containing a brief fact, we follow Bhandari et al. (2020) to emphasize a shorter, more fine-grained information unit. Specifically, we define the ACU protocol with the concept of atomic facts -elementary information units in the reference summaries, which no\nThe clash occurred inside the box.\nOscar is Brazilian.\nOscar was taken off at half time.\nDidier Drogba replaced Oscar. longer need to be further split for the purpose of reducing ambiguity in human evaluation. 3 Then, ACUs are constructed based on one atomic fact and other minimal, necessary information. Fig. 1 shows an example of the written ACUs. To ensure annotation quality, we (the authors) write all the ACUs used in this work. We define guidelines to standardize the annotation process; for each summary sentence the annotator creates an ACU constituting the main information from the subject of the main clause (e.g., root), followed by additional ACUs for other facts while including the minimal necessary information from the root. We provide rules for dealing with quotations, extraneous adjectives, noisy summaries, and additional cases. We note that there can still be inherent subjectivity in the written ACUs among different annotators even with the provided guidelines. However, such subjectivity should be unbiased in summary comparison because all the candidate summaries are evaluated by the same set of written ACUs. ACU Matching Given ACUs written for a set of reference summaries, our protocol evaluates summarization system performance by checking the presence of the ACUs in the system-generated summaries as illustrated in Fig. 1 . For this step, we recruit annotators on Amazon Mechanical Turk 4 (MTurk). The annotators must pass a qualification test, and additional requirements are specified in Appendix A. Besides displaying the ACUs and the system outputs, we also provide the reference summaries to be used as context for the ACUs. Scoring Summaries with ACU ACU matching annotations can be aggregated into summary scores. We first define an un-normalized ACU score f of a candidate summary s given a set of ACUs A as: where A s is a subset of A that is matched with s.\n\nOscar collided with\nEQUATION\nWe note that f by default is a recall based score with respect to the reference summary r. Therefore, we also define a normalized ACU score f as:\nf\u03b1(s, A, r) = e min (0,\nEQUATION\nwhere |s|, |r| are the length (i.e., number of words) of the candidate summary s and the reference summary r respectively, and \u03b1 is a positive number controlling the strength of the normalization. This normalization is in effect a redundancy penalty, which penalizes the summaries longer than the reference and resembles the brevity penalty in BLEU (Papineni et al., 2002) . In practice, we set the value of \u03b1 by de-correlating f with the summary length using the collected ACU annotations.\n\nACU Annotation Collection\nWe collect ACU annotations on three summarization datasets: CNNDM (Nallapati et al., 2016 ), XSum (Narayan et al., 2018 ), and SamSum (Gliwa et al., 2019) . To reflect the latest progress in text summarization, we collect and annotate the generated summaries of pre-trained summarization systems proposed in recent years. 5 Detailed informa-tion about the summarization systems we used can be found in Appendix A.2. Table 2 shows the statistics of the collected annotations. The annotations are collected from the test set of the above datasets, and additionally from the validation set of CNNDM to facilitate the training of automatic evaluation metrics. In total, we collect around 21.8k ACU-level annotations and around 22k summary-level annotations, aggregated over around 50k individual summary-level judgments.\nTo calculate inter-annotator agreement, we use Krippendorff's alpha (Krippendorff, 2011) . The aggregated summary-level agreement score of ACU matching is 0.7571, and the ACU-level agreement score is 0.7528. These agreement scores are higher than prior collections, such as RealSumm (Bhandari et al., 2020) and SummEval (Fabbri et al., 2022a) , which have an average agreement score of crowd-workers 0.66 and 0.49, respectively.\n\nRoSE Benchmark Analysis\nWe first analyze the robustness of our collected annotations and a case study on the system outputs.\n\nPower Analysis\nWe analyze the statistical power of our collected human annotations to study whether it can yield stable and trustworthy results (Card et al., 2020) . Statistical power is the probability that the null hypothesis of a statistical significance test is rejected given there is a real effect. For example, for a human evaluation study that compares the performance of two genuinely different systems, a statistical power of 0.80 means there is an 80% chance that a significant difference will be observed. Further details can be found in Appendix B.1.\nWe conduct the power analysis for pair-wise system comparisons with ACU scores (Eq. 1) focusing on two factors, the number of test examples and the observed system difference. Specifically, we run the power analysis with varying sample sizes, and group the system pairs into buckets according to their performance difference, as determined by ROUGE1 recall scores (Fig. 2 ). 6 We observe the following: (1) A high statistical power 7 is difficult to reach when the system performance is similar. 6 We note that these scores are proxies of the true system differences, and the power analysis is based on the assumption that the systems have significantly different performance. 7 An experiment is usually considered sufficiently powered if the statistical power is over 0.80. Notably, while the sample size of the human evaluation performed in recent work is typically around 50-100, 8 such sample size can only reach a power of 0.80 when the ROUGE1 recall score difference is above 5. (2) Increasing the sample size can effectively raise the statistical power. For example, when the system performance difference is within the range of 1-2 points, the power of a 500-sample set is around 0.50 while a 100-sample set only has a power of around 0.20. The results of power analysis on three datasets with both ROUGE and ACU score differences are provided in Appendix B.2 with the same patterns, which indicates that our dataset can provide more stable summarization system evaluation thanks to its higher statistical power.\n\nSummarization System Analysis\nAs a case study, in Tab. summaries. Meanwhile, the systems that generate longer summaries may be favored by users who prefer more informative summaries. Therefore, we join the previous work (Sun et al., 2019; Song et al., 2021; Gehrmann et al., 2022; Goyal et al., 2022) in advocating treating summary lengths as a separate aspect of summary quality in evaluation, as in earlier work in summarization research. 9\n\nEvaluating Annotation Protocols\nApart from ACU annotations, we collect human annotations with three different protocols to better understand their characteristics. Specifically, two reference-free protocols are investigated: Prior protocol evaluates the annotators' preferences of summaries without the input document, while Ref-free protocol evaluates if summaries cover the salient information of the input document. We also consider one reference-based protocol, Ref-based, which evaluates the content similarity between the generated and reference summaries. Appendix D.1 provides detailed instructions for each protocol.\n\nAnnotation Collection\nWe collected three annotations per summary on a 100-example subset of the above CNNDM test set using the same pool of workers from our ACU qualification. Except for ACU, all of the summaries from different systems are evaluated within a single task with a score from 1 (worst) to 5 (best), similar (2) annotations for summaries from GPT-3 (Brown et al., 2020), 10 T0 (Sanh et al., 2022), BRIO, and BART to better understand annotation protocols with respect to recently introduced large language models applied to zero-shot summarization.\n\nResults Analysis\nWe investigate both the summary-level and systemlevel correlations of evaluation results of different protocols to study their inherent similarity. Details of correlation calculation are in Appendix C.\nResults on Fine-tuned Models We show the system-level protocol correlation when evaluating the fine-tuned models in Tab. 4, and the summarylevel correlation can be found in Appendix D.2. We use the normalized ACU score (Eq. 2) because the other evaluation protocols are supposed to resemble an F1 score, while the ACU score is by definition recall-based. We have the following observations:\n(1) The Ref-free protocol has a strong correlation with the Prior protocol, suggesting that the latter may have a large impact on the annotator's document-based judgments.\n(2) Both the Prior and Ref-free protocols have a strong correlation with summary length, showing that annotators may favor longer summaries.\n(3) The Ref-free protocol and the Ref-based protocol have a negative correlation while ideally they are supposed to measure similar quality aspects.\nWe perform power analysis on the results following the procedure in \u00a74.1 and found that ACU protocol can yield higher statistical power than the Ref-based protocol, suggesting that the ACU protocol leads to more robust evaluation results. We also found that the reference-free Prior and Ref-free Table 6 : The Kendall's correlation between the automatic metric scores and ACU scores of system outputs on CNNDM, XSum, and SamSum datasets. The correlation is calculated at both the system level and the summary level. We use the recall score of the automatic metrics when available to align with the ACU scores.\nautomatic metric variants. We focus the metric evaluation on ACU annotations because of two insights from \u00a75: (1) Reference-based metrics should be evaluated with reference-based human evaluation.\n(2) ACU protocol provides higher statistical power than the summary-level Ref-based protocol.\n\nMetric Evaluation with ACU Annotations\nWe use the correlations between automatic metric scores and ACU annotation scores of system outputs to analyze and compare automatic metric performance. The following metrics are evaluated:\n(1) lexical overlap based metrics, ROUGE (Lin, 2004b), METEOR (Lavie and Agarwal, 2007) 5) evaluation methods based on large language models, GPTScore (Fu et al., 2023) and G-Eval (Liu et al., 2023) , with two variants that are based on GPT-3.5 11 (G-Eval-3.5) and GPT-4 12 (OpenAI, 2023) (G-Eval-4) respectively. We note that for LLM-based evaluation we require the metric to calculate the recall score. For G- -3.5 -.091 -.273 -.091 .818 1.00 1.00 G-Eval-3.5-S -.091 -.273 -.273 .818 1.00 1.00 G-Eval-4\n.091 .818 .636 1.00 1.00 1.00\nTable 7 : The system-level Kendall's correlation between the automatic metric and ACU scores on different system pairs grouped by their ACU score differences on the CNNDM dataset, into six equal-sized buckets. We use the recall score of the automatic metrics when available.\nEval-3.5 we report two variants that are based on greedy decoding (G-Eval-3.5) and sampling (G-Eval-3.5-S) respectively, Details of the LLM-based evaluation are in Appendix E.2. Tab. 6 shows the results, with additional results of more metrics in Appendix E.3. We note:\n(1) Several automatic metrics from the different families of methods (e.g., ROUGE, BARTScore) are all able to achieve a relatively high correlation with the ACU scores, especially at the system level.\n(2) Metric performance varies across different datasets. In particular, metrics tend to have stronger correlations on the SamSum dataset and weaker correlations on the XSum dataset. We hypothesize that one reason is that the reference summaries of the XSum dataset contain more complex structures.\n(3) Despite their successes (Fu et al., 2023; Liu et al., 2023) in other human evaluation benchmarks such as SummEval, LLM-based automatic evaluation cannot outperform traditional methods such as ROUGE on RoSE. Moreover, their low summarylevel correlation with ACU scores suggests that their predicted scores may not be well-calibrated.\nFollowing Deutsch et al. ( 2022), we further investigate metric performance when evaluating system pairs with varying performance differences. Specifically, we group the system pairs based on the difference of their ACU scores into different buckets and calculate the modified Kendall's correlation (Deutsch et al., 2022) on each bucket. The system pairs in each bucket are provided in Appendix E.4. Tab. 7 shows that the automatic metrics generally perform worse when they are used to evaluate similar-performing systems. \n\nAnalysis of Metric Evaluation\nWe analyze the metric evaluation with respect to the statistical characteristics and the impact of different human evaluation protocols on metric evaluation.\nConfidence Interval We select several representative automatic metrics and calculate the confidence intervals of their system-level correlations with the ACU scores using bootstrapping. Similar to Deutsch et al. ( 2021b), we find that the confidence intervals are large. However, we found that having a larger sample size can effectively reduce the confidence interval, which further shows the importance of increasing the statistical power of the human evaluation dataset as discussed in \u00a74.1. We provide further details in Appendix E.5.\n\nPower Analysis of Metric Comparison\nWe conduct a power analysis of pair-wise metric comparison with around 200 pairs, which corresponds to the chance of a statistical significance result being found. More details can be found in Appendix E.6. The results are in Fig. 3 , showing similar patterns as in the power analysis of summarization system comparison ( \u00a74.1):\n(1) Significant results are difficult to find when the metric performance is similar;\n(2) Increasing the sample size can effectively increase the chance of finding significant results. automatic metrics generally perform better under reference-based evaluation protocols, but can have negative correlations with reference-free protocols.\n\nConclusion and Implications\nWe introduce RoSE, a benchmark whose underlying protocol and scale allow for more robust summarization evaluation across three datasets. With our benchmark, we re-evaluate the current state of human evaluation and its implications for both summarization system and automatic metric development, and we suggest the following:\n(1) Alignment in metric evaluation. To evaluate automatic metrics, it is important to use an appropriate human evaluation protocol that captures the intended quality dimension to be measured. For example, reference-based automatic metrics should be evaluated by reference-based human evaluation, which disentangles metric performance from the impact of reference summaries.\n(2) Alignment in system evaluation. We advocate for targeted evaluation, which clearly defines the intended evaluation quality. Specifically, text summarization, as a conditional generation task, should be defined by both the source and target texts along with pre-specified, desired characteristics. Clearly specifying characteristics to be measured can lead to more reliable and objective evaluation results. This will be even more important for LLMs pretrained with human preference feedback for disentangling annotators' prior preferences for LLMs with the task-specific summary quality.\n(3) Alignment between NLP datasets and tasks.\nHuman judgments for summary quality can be diverse and affected by various factors such as summary lengths, and reference summaries are not al-ways favored. Therefore, existing summarization datasets (e.g. CNNDM) should only be used for the appropriate tasks. For example, they can be used to define a summarization task with specific requirements (e.g. maximum summary lengths), and be important for studying reference-based metrics.\n"}
{"question": "Which method is not use in Correction Scoring", "evidence": "The final correction is produced by scoring the faithfulness of each candidate correction from the previous steps w.r.t. the evidence. We use entailment score to approximate faithfulness. Here, DocNLI (Yin et al., 2021) is used to compute such document-sentence entailment relations. Doc-NLI is more generalizable than other documentsentence entailment models, such as FactCC (Kryscinski et al., 2020) , since it was trained on five datasets of various tasks and domains. Conventional NLI models trained on sentence-level NLI datasets, such as MNLI (Williams et al., 2018) , are not applicable since previous work has found that these models are ill-suited for measuring entailment beyond the sentence level (Falke et al., 2019) . In addition, to prevent the final correction from deviating too much from the original claim, we also consider ROUGE-1 scores, motivated by Wan and Bansal (2022) . The final metric used for scoring is the sum of ROUGE-1 score 5 and DocNLI entailment score. Formally,", "options": ["A. DocNLI ", "B. entailment score", "C. Conventional NLI models trained on sentence-level NLI datasets", "D.  ROUGE-1 scores", "Correction Scoring\nThe final correction is produced by scoring the faithfulness of each candidate correction from the previous steps w.r.t. the evidence. We use entailment score to approximate faithfulness. Here, DocNLI (Yin et al., 2021) is used to compute such document-sentence entailment relations. Doc-NLI is more generalizable than other documentsentence entailment models, such as FactCC (Kryscinski et al., 2020) , since it was trained on five datasets of various tasks and domains. Conventional NLI models trained on sentence-level NLI datasets, such as MNLI (Williams et al., 2018) , are not applicable since previous work has found that these models are ill-suited for measuring entailment beyond the sentence level (Falke et al., 2019) . In addition, to prevent the final correction from deviating too much from the original claim, we also consider ROUGE-1 scores, motivated by Wan and Bansal (2022) . The final metric used for scoring is the sum of ROUGE-1 score 5 and DocNLI entailment score. Formally,\n"], "answer": "C", "content": "\nIntroduction\nThe task of correcting factual errors is in high demand and requires a significant amount of human effort. The English Wikipedia serves as a notable case in point. It is continually updated by over 120K editors, with an average of around six factual edits made per minute 2 . Using machines to correct factual errors could allow the articles to be updated with the most current information automatically. This process, due to its high speed, can help retain the integrity of the content and prevent the spread of false or misleading information.\nIn addition, the hallucination issues have been shown to be a prime concern for neural models,\n\nEvidence\nThe novel COVID-19 is highly contagious and is transmitted mostly through respiratory droplets. But, whether its transmission can be forwarded by touching a surface (i.e., a fomite) is uncertain.... COVID-19 has a case fatality rate of below 2%.\n\nFinal Correction\nCOVID-19 is not infectious.\n\nInput Claim\nFigure 1 : An example of a factual but unfaithful correction leading to misleading information. While it is technically true that the majority of people infected with COVID-19 will recover, there is no information in the evidence that supports the final correction. Additionally, when this statement is taken out of context, it could mislead people to believe that COVID-19 is not dangerous and that there is no need for precautions, which is false. A factual and faithful correction is \"COVID-19 is highly contagious.\".\nwhere they are prone to generate content factually inconsistent with the input sources due to the unfaithful training samples (Maynez et al., 2020) and the implicit \"knowledge\" it learned during pre-training (Niven and Kao, 2019) . Factual error correction can be used in both pre-processing and post-processing steps to rectify the factual inconsistencies in training data and generated texts, respectively. This can help build trust and confidence in the reliability of language models.\nPrior work typically formulates factual error correction as a sequence-to-sequence task, either in a fully supervised or in a distantly supervised manner (Shah et al., 2020; Thorne and Vlachos, 2021) . While these approaches have made great strides in generating fluent and grammatically valid corrections, they only focus on the aspect of factuality: whether the outputs are aligned with facts. Little emphasis was placed on faithfulness: the factual consistency of the outputs with the evidence. Faithfulness is critical in this task as it indicates whether a generated correction reflects the information we intend to update. If faithfulness is not ensured, this could lead to the spread of misleading content, causing serious consequences. Figure 1 shows a concrete example. In the context of automatically updating textual knowledge bases, the topic of an unfaithful output would likely deviate much from that of the expected correction. Therefore, such an edit is not desirable, even if it is factual.\nIn this work, we present the first study on the faithfulness aspect of factual error correction. To address faithfulness, we propose a zero-shot factual error correction framework (ZEROFEC), inspired by how humans verify and correct factual errors. When humans find a piece of information suspicious, they tend to first identify potentially false information units, such as noun phrases, then ask questions about each information unit, and finally look for the correct answers in trustworthy evidence (Saeed et al., 2022; Chen et al., 2022) . Following a similar procedure, ZEROFEC breaks the factual error correction task into five sub-tasks:\n(1) claim answer generation: extracting all information units, such as noun phrases and verb phrases, from the input claim; (2) question generation: generating question given each claim answer and the original claim such that each claim answer is the answer to each generated question; (3) question answering: answering each generated question using the evidence as context; (4) QA-to-claim: converting each pair of generated question and answer to a declarative statement; (5) correction scoring: evaluating corrections based on their faithfulness to the evidence, where faithfulness is approximated by the entailment score between the evidence and each candidate correction. The highest-scoring correction is selected as the final output. An overview of our framework is shown in Figure 2 . Our method ensures the corrected information units are derived from the evidence, which helps improve the faithfulness of the generated corrections. In addition, our approach is naturally interpretable since the questions and answers generated directly reflect which information units are being compared with the evidence.\nOur contributions can be summarized as follows:\n\u2022 We propose ZEROFEC, a factual error correction framework that effectively addresses faithfulness by asking questions about the input claim, seeking answers in the evidence, and scoring the outputs by faithfulness. \u2022 Our approach outperforms all prior methods, including fully-supervised approaches trained on 58K instances, in ensuring faithfulness on two factual error correction datasets, FEVER (Thorne et al., 2018) and SCIFACT (Wadden et al., 2020) . \u2022 We analyze the correlation of human judgments with automatic metrics to provide intuition for future research on evaluating the faithfulness, factuality, and intelligibility of factual error corrections.\n\nTask\nIn Thorne and Vlachos (2021)'s setting, retrieved evidence is used, which means the model may be able to correct factual errors, even though there is no supporting information in the evidence. In this case, although the prediction is considered correct, the model is hallucinating, which is not a desired property. Additionally, due to the way data was collected, they require systems to alter the input claim even if the input claim is already faithful to the evidence. We argue that no edit is needed for claims that are faithful to the evidence.\nTo address these shortcomings, our setup aims to edit a claim using a given piece of grounded evidence that supports or refutes the original claim (see Figure 2 ). Using gold-standard evidence avoids the issue where a system outputs the correct answer by chance due to hallucinations. In our setting, a system must be faithful to the evidence to correct factual errors, allowing us to evaluate system performance more fairly. Furthermore, we require the model not to edit the original claim if it is already factually consistent with the provided evidence.\nConcretely, the input to our task is a claim C and a piece of gold-standard evidence E that supports or refutes C. The goal of factual error correction is to produce a corrected claim \u0108 that fixes factual errors in C while being faithful to E. If C is already supported by E, models should output the original claim (i.e. \u0108 = C).\n\nProposed Methods\nOur framework, ZEROFEC, faithfully corrects factual errors using question-answering and entailment.\nSpecifically, we represent the input claim C as question-answer pairs \n{(Q 1 , A C 1 ), ..., (Q n , A C n )} such that each question Q i reflects the corresponding information unit A C i ,\n\nCandidate Corrections\nNight of the Living Dead is a horror film.\n\nFinal Correction\nFigure 2 : An overview of our framework. First, given an input claim, we generate the claim answers by enumerating all information units in the input claim. Second, conditioned on each extracted answer and the input claim, a question is generated. Third, each question is then fed to a question answering model to produce an evidence answer using the given evidence as context. Fourth, using a sequence-to-sequence approach, each evidence answer and the corresponding question are transformed into a statement, which serves as a candidate correction. Finally, the final correction is produced by scoring candidate corrections based on faithfulness.\nanswer A E i in the given evidence E using a learned QA model ( \u00a73.3). Each candidate correction S i is obtained by converting the corresponding pair of Q i and A E i into a declarative statement ( \u00a73.4). This guarantees that the corrected information units we replace factual errors with are derived from the evidence and thus ensures high faithfulness. The final output of ZEROFEC is the S i with the highest faithfulness score computed by an entailment model ( \u00a73.5). An overview of our framework is shown in Figure 2 .\nOne major challenge that makes our task more difficult than prior studies on faithfulness (Wang et al., 2020; Fabbri et al., 2022a ) is that we need to handle more diverse factual errors, such as negation errors and errors that can only be abstractively corrected. For instance, in the second example of in Table 2 , the QA model should output \"Yes\" as the answer, which cannot be produced by extractive QA systems. To address this issue, we adopt abstractive QG and QA models that can handle diverse question types and train our QA-to-claim model on multiple datasets to cover cases that cannot be handled by extractive systems. The following subsections illustrate the details of each component in our framework.\n\nClaim Answer Generation\nThe goal of claim answer generation is to identify information units in the input claim that may be unfaithful to E. We aim to maximize the recall in this step since the missed candidates cannot be recovered in later steps. Therefore, we extract all noun chunks and named entities using Spacy 3 and extract nouns, verbs, adjectives, adverbs, noun phrases, verb phrases using Stanza 4 . Additionally, we also extract negation terms, such as \"not\" and \"never\", from the input claim. We name the extracted information units claim answers, denoted as\nA C = {A C 1 , A C 2 , ..., A C n }.\n\nQuestion Generation\nUpon claim answers are produced, we generate questions that will be later used to look for correct information units in the evidence. Questions are generated conditioned on the claim answers using the input claim as context. We denote the question generator as G. Each claim answer\nA C i is concatenated with the input claim C to generate a question Q i = G(A C i , C).\nWe utilize MixQG (Murakhovs 'ka et al., 2022) as our question generator G to cover the wide diversity of factual errors and candidates extracted. MixQG was trained on nine question generation datasets with various answer types, including boolean, multiple-choice, extractive, and abstractive answers.\n\nQuestion Answering\nThe question answering step identifies the correct information units A E i corresponding to each question Q i in the given evidence E. Our QA module answers questions from the question generation steps with the given evidence as context. Let F denote our QA model. We feed the concatenation of a generated question and the evidence to the QA model to produce an evidence answer (Khashabi et al., 2022) is used as our question answering model. UnifiedQA-v2 is a T5-based (Raffel et al., 2020b) abstractive QA model trained on twenty QA datasets that can handle diverse question types.\nA E i = F(Q i , E). UnifiedQA-v2\n\nQA-to-Claim\nAfter questions and answers are generated, we transform each pair of question and answer into a declarative statement, which serves as a candidate correction that will be scored in the next step. Previous studies on converting QAs to claims focus on extractive answer types only (Pan et al., 2021) . To accommodate diverse types of questions and answers, we train a sequence-to-sequence model that generates a claim given a question-answer pair on three datasets: QA2D (Demszky et al., 2018) for extractive answers, BoolQ (Clark et al., 2019) for boolean answers, and SciTail (Khot et al., 2018) for covering scientific domain QAs. Note that samples in BoolQ do not contain converted declarative statements. Using Stanza's constituency parser, we apply heuristics to transform all QAs to their declarative forms in BoolQ. Our QA-to-claim model is a T5-base fine-tuned on these three datasets. Concretely, let M denote our QA-to-claim model. M takes in a generated question Q i and an evidence answer A E i as inputs and outputs a statement\nS i = M(Q i , A E i ).\n\nCorrection Scoring\nThe final correction is produced by scoring the faithfulness of each candidate correction from the previous steps w.r.t. the evidence. We use entailment score to approximate faithfulness. Here, DocNLI (Yin et al., 2021) is used to compute such document-sentence entailment relations. Doc-NLI is more generalizable than other documentsentence entailment models, such as FactCC (Kryscinski et al., 2020) , since it was trained on five datasets of various tasks and domains. Conventional NLI models trained on sentence-level NLI datasets, such as MNLI (Williams et al., 2018) , are not applicable since previous work has found that these models are ill-suited for measuring entailment beyond the sentence level (Falke et al., 2019) . In addition, to prevent the final correction from deviating too much from the original claim, we also consider ROUGE-1 scores, motivated by Wan and Bansal (2022) . The final metric used for scoring is the sum of ROUGE-1 score 5 and DocNLI entailment score. Formally,\nEQUATION\nEQUATION\nwhere C \u2032 is the final correction produced by our framework. Furthermore, to handle cases where the input claim is already faithful to the evidence, we include the input claim in the candidate correction list to be scored.\n\nDomain Adaptation\nDuring the early stage of our experiments, we found that our proposed framework did not perform well in correcting factual errors in biomedical claims. This results from the fact that our QA and entailment models were not fine-tuned on datasets in the biomedical domain. To address this issue, we adapt UNIFIEDQA-V2 and DOCNLI on two biomedical QA datasets, PUBMEDQA (Jin et al., 2019) and BIOASQ (Tsatsaronis et al., 2015) , by further fine-tuning them for a few thousand steps. We later show that this simple domain adaptation technique successfully improves our overall factual error correction performance on a biomedical dataset without decreasing performance in the Wikipedia domain (see \u00a75.1).\n4 Experimental Setup\n\nDatasets\nWe conduct experiments on two English datasets, FEVER and SCIFACT. FEVER (Thorne and Vla-chos, 2021 ) is repurposed from the corresponding fact-checking dataset (Thorne et al., 2018 ) that consists of evidence collected from Wikipedia and claims written by humans that are supported or refuted by the evidence. Similarly, SCIFACT is another fact-checking dataset in the biomedical domain (Wadden et al., 2020) . We repurpose it for the factual error correction task using the following steps. First, we form faithful claims by taking all claims supported by evidence. Then, unfaithful claims are generated by applying Knowledge Base Informed Negations (Wright et al., 2022) , a semantic altering transformation technique guided by knowledge base, to a subset of the faithful claims. Appendix A shows detailed statistics.\n\nEvaluation Metrics\nOur evaluation focuses on faithfulness. Therefore, we adopt some recently developed metrics that have been shown to correlate well with human judgments in terms of faithfulness. BARTScore (Yuan et al., 2021) computes the semantic overlap between the input claim and the evidence by calculating the logarithmic probability of generating the evidence conditioned on the claim. FactCC (Kryscinski et al., 2020) is an entailment-based metric that predicts the faithfulness probability of a claim w.r.t. the evidence. We report the average of the COR-RECT probability across all samples. In addition, we consider QAFACTEVAL (Fabbri et al., 2022a) , a recently released QA-based metric that achieves the highest performance on the SUMMAC factual consistency evaluation benchmark (Laban et al., 2022) . Furthermore, we also report performance on SARI (Xu et al., 2016) , a lexical-based metric that has been widely used in the factual error correction task (Thorne and Vlachos, 2021; Shah et al., 2020) .\n\nBaselines\nWe compare our framework with the following baseline systems. T5-FULL (Thorne and Vlachos, 2021) is a fully-supervised model based on T5-base (Raffel et al., 2020a) that generates the correction conditioned on the input claim and the given evidence. MASKCORRECT (Shah et al., 2020) and T5-DISTANT (Thorne and Vlachos, 2021) are both distantly-supervised methods that are composed of a masker and a sequence-to-sequence (seq2seq) corrector. The masker learns to mask out information units that are possibly false based on a learned fact verifier or an explanation model (Ribeiro et al., 2016) and the seq2seq corrector learns to fill in the masks with factual information. The biggest difference is in the choice of seq2seq corrector. T5-DISTANT uses T5-base, while MASKCOR-RECT utilizes a two-encoder pointer generator. For zero-shot baselines, we selected two post-hoc editing frameworks that are trained to remove hallucinations from summaries, REVISEREF (Adams et al., 2022) and COMPEDIT (Fabbri et al., 2022b) .\nREVISEREF is trained on synthetic data where hallucinating samples are created by entity swaps.\nCOMPEDIT learns to remove factual errors with sentence compression, where training data are generated with a separate perturber that inserts entities into faithful sentences.\n\nImplementation Details\nNo training is needed for ZEROFEC. As for ZEROFEC-DA, we fine-tune UNIFIEDQA-V2 and DOCNLI on the BIOASQ and PUBMEDQA datasets for a maximum of 5,000 steps using AdamW (Loshchilov and Hutter, 2019) with a learning rate of 3e-6 and a weight decay of 1e-6.\nDuring inference time, all generative components use beam search with a beam width of 4.\n\nMain Results\nTable 1 summarizes the main results on the FEVER and SCIFACT datasets. Both ZEROFEC and ZEROFEC-DA achieve significantly better performance than the distantly-supervised and zeroshot baselines. More impressively, they surpass the performance of the fully-supervised model on most metrics, even though the fully-supervised model is trained on 58K samples in the FEVER experiment.\nThe improvements demonstrate the effectiveness of our approach in producing faithful factual error correction by combining question answering and entailment predictions. In addition, even though our domain adaptation technique is simple, it successfully boosts the performance on the SCIFACT dataset while pertaining great performance on the FEVER dataset. The first example in It is true that ZEROFEC-DA requires additional training, which is different from typical zero-shot methods. However, the key point remains that our framework does not require any task-specific training data. Hence, our approach still offers the benefits of zero-shot learning by not requiring any additional training data beyond what was already available for the question answering task, a field with much richer resources compared to the factchecking field.\n\nQualitative Analysis\nTo provide intuition for our framework's ability to produce faithful factual error corrections, we manually examined 50 correct and 50 incorrect outputs made by ZEROFEC on the FEVER dataset. The interpretability of ZEROFEC allows for insightful examinations of the outputs. Among the correct samples, our framework produces faithful corrections because all intermediate outputs are accurately produced rather than \"being correct by chance\". For the incorrect outputs, we analyze the source of mistakes, as shown in Figure 3 . The vast majority of failed cases result from DocNLI's failure to score candidate corrections faithfully. In addition to the mediocre performance of DocNLI, one primary reason is that erroneous outputs from other compo-nents would not be considered mistakes so long as the correction scoring module determines the resulting candidate corrections unfaithful to the evidence. A possible solution to improve DocNLI is to further fine-tune it on synthetic data generated by perturbing samples in FEVER and SCIFACT. Examples of correct and incorrect outputs are presented in Table 7 and Table 8 \n\nHuman Evaluation\nTo further validate the effectiveness of our proposed method, we recruited three graduate students who are not authors to conduct human evaluations on 100 and 40 claims from FEVER and SCIFACT, respectively. For each claim, human judges are presented with the ground-truth correction, the goldstandard evidence, and output produced by a factual error correction system and tasked to assess the quality of the correction with respect to three dimensions. Intelligibility evaluates the fluency of the correction. An intelligible output is free of grammatical mistakes, and its meaning must be T5-DISTANT's output: Fuller House ( TV series ) isn't airing on HBO.\nTable 2 : Example outputs from different approaches. The outputs from our framework are directly interpretable, as the generated questions and answers reflect which information units in the input claim are erroneous and which information in the evidence supports the final correction. We only show the generated answers and questions directly related to the gold correction. In the first example, ZEROFEC-DA corrects a mistake made by ZEROFEC thanks to domain adaptation. In the second example, ZEROFEC successfully produces a faithful factual error correction, whereas the output of T5-DISTANT, the distantly-supervised baseline, is factual yet unfaithful to the evidence.\nunderstandable by humans without further explanation. Factuality considers whether the input claim is aligned with facts. Systems' output can be factual and semantically different from the gold correction as long as it is consistent with the world's knowledge. Faithfulness examines whether the input is factually consistent with the given evidence. Note that a faithful output must be factual since we assume all evidence is free of factual error. To evaluate the annotation quality, we compute the inter-annotator agreement. Krippendorff's Alpha (Krippendorff, 2011 ) is 68.85%, which indicates a moderate level of agreement. Details of our human evaluation can be found in Appendix B.\nThe human evaluation results are demonstrated in Table 3 . We observe that: (1) ZEROFEC and ZEROFEC-DA achieve the best overall performance in Factuality and Faithfulness on both datasets, even when compared to the fully-supervised method, suggesting that our approach is the best in ensuring faithfulness for factual error correction.\n(2) Our domain adaptation for the biomedical domain surprisingly improves faithfulness and factuality in the Wikipedia domain (i.e. FEVER). This suggests that fine-tuning the components of our framework on more datasets helps improve robustness in terms of faithfulness.\n(3) Factual output produced by ZEROFEC and ZEROFEC-DA are always faithful to the evidence, preventing the potential spread of misleading information caused by factual but unfaithful corrections. The second example in Table 2 demonstrates an instance of factual but unfaithful correction made by baseline models. Here, the output of T5-DISTANT is unfaithful since the evidence does not mention whether Fuller House airs on HBO. In fact, although Fuller House was not on HBO when it premiered, it was later accessible on HBO Max. Therefore, the correction produced by T5-DISTANT is misleading.\n\nCorrelation with Human Judgments\nRecent efforts on faithfulness metrics have been mostly focusing on the summarization task. No prior work has studied the transferability of these metrics to the factual error correction task. We seek to bridge this gap by showing the correlation between the automatic metrics used in measure, the results are summarized in Table 4 .\nWe have the following observations. (1) SARI is the most consistent and reliable metric for evaluating Factuality and Faithfulness across two datasets. Although the other three metrics developed more recently demonstrate high correlations with human judgments of faithfulness in multiple summarization datasets, their transferability to the factual error correction task is limited due to their incompatible design for this particular task. For example, QA-based metrics like QAFACTEVAL are less reliable for evaluating faithfulness in this task due to their inability to extract a sufficient number of answers from a single-sentence input claim. In contrast, summaries in summarization datasets generally consist of multiple sentences, enabling the extraction of a greater number of answers. To validate this, we analyzed the intermediate outputs of QAFACTEVAL. Our analysis confirms that it extracts an average of only 1.95 answers on the FEVER dataset, significantly lower than the more than 10 answers typically extracted for summaries. (2) Across the two datasets, the correlations between all automatic metrics and Intelligibility are low. The extremely high proportion of intelligible outputs may explain the low correlation. (3) The correlation for learning-based metrics, including QAFACTEVAL and FACTCC, drop significantly when applied to SCIFACT. This is likely caused by the lack of fine-tuning or pre-training with biomedical data.\n6 Related Work\n\nFactual Error Correction\nAn increasing number of work began to explore factual error correction in recent years, following the rise of fact-checking (Thorne et al., 2018; Wadden et al., 2020; Gupta and Srikumar, 2021; Huang et al., 2022b) and fake news detection (Shu et al., 2020; Fung et al., 2021; Wu et al., 2022; Huang et al., 2022a) . Shah et al. (2020) propose a distant supervision learning method based on a masker-corrector architecture, which assumes access to a learned fact verifier. Thorne and Vlachos (2021) created the first factual error correction dataset by repurposing the FEVER (Thorne et al., 2018) dataset, which allows for fully-supervised training of factual error correctors. They also extended Shah et al. (2020) 's method with more advanced pre-trained sequence-to-sequence models. Most recently, Schick et al. (2022) proposed PEER, a collaborative language model that demonstrates superior text editing capabilities due to its multiple text-infilling pre-training objectives, such as planning and realizing edits as well as explaining the intention behind each edit 6 .\n\nFaithfulness\nPrevious studies addressing faithfulness are mostly in the summarization field and can be roughly divided into two categories, evaluation and enhancement. Within faithfulness evaluation, one line of work developed entailment-based metrics by training document-sentence entailment models on synthetic data (Kryscinski et al., 2020; Yin et al., 2021) or human-annotated data (Ribeiro et al., 2022; Chan et al., 2023) , or applying conventional NLI models at the sentence level (Laban et al., 2022) . Another line of work evaluates faithfulness by comparing information units extracted from summaries and input sources using QA (Wang et al., 2020; Deutsch et al., 2021) . There is a recent study that integrates QA into entailment by feeding QA outputs as features to an entailment model (Fabbri et al., 2022a) . We combine QA and entailment by using entailment to score the correction candidates produced by QA. Within faithfulness enhancement, some work improves factual consistency by incorporating auxiliary losses into the training process (Nan et al., 2021; Cao and Wang, 2021; Tang et al., 2022; Huang et al., 2023) . Some other work devises factuality-aware pre-training and fine-tuning objectives to reduce hallucinations (Wan and Bansal, 2022) . The most similar to our work are studies that utilize a separate rewriting model to fix hallucinations in summaries. For example, Cao et al. (2020) present a post-hoc corrector trained on synthetic data, where negative samples are created via perturbations. Adams et al. (2022) fix factually inconsistent information in the reference summaries to prevent the summarization from learning hallucinating examples. Fabbri et al. (2022b) propose a compression-based post-editor to correct extrinsic errors in the generated summaries. By contrast, we leverage the power of QA and entailment together to address faithfulness.\n\nConclusions and Future Work\nWe have presented ZEROFEC, a zero-shot framework that asks questions about an input claim and seeks answers from the given evidence to correct factual errors faithfully. The experimental results demonstrate the superiority of our approach over prior methods, including fully-supervised methods, as indicated by both automatic metrics and human evaluations. More importantly, the decomposability of ZEROFEC naturally offers interpretability, as the questions and answers generated directly reflect which information units in the input claim are incorrect and why. Furthermore, we reveal the most suitable metric for assessing faithfulness of factual error correction by analyzing the correlation between the reported automatic metrics and human judgments. For future work, we plan to extend our framework to faithfully correct misinformation in social media posts and news articles to inhibit the dissemination of false information. In addition, it may be meaningful to explore extending zero-shot factual error correction to multimedia task settings, such as identifying inconsistencies between chart and text (Zhou et al., 2023) .\n"}
{"question": "What is the main focus of the paper?", "evidence": "  We study grammar induction with mildly context-sensitive grammars for unsupervised discontinuous parsing. This work studied unsupervised discontinuous constituency parsing with mildly context-sensitive grammars, focusing on the formalism of linear context-free rewriting systems. ", "options": ["A. Supervised parsing", "B. Tensor decomposition-based rank-space dynamic programming", "C. Unsupervised discontinuous constituency parsing with mildly context-sensitive grammars", "D. Linguistic structure modeling "], "answer": "C", "content": "\nIntroduction\nUnsupervised parsing aims to induce hierarchical linguistic structures given only the strings in a language. A classic approach to unsupervised parsing is through probabilistic grammar induction (Lari and Young, 1990) , which learns a probabilistic grammar (i.e., a set of rewrite rules and their probabilities) from raw text. Recent work has shown that neural parameterizations of probabilistic contextfree grammars (PCFG), wherein the grammar's rule probabilities are given by a neural network over shared symbol embeddings, can achieve promising results on unsupervised constituency parsing (Kim et al., 2019; Jin et al., 2019 Jin et al., , 2021;; Yang et al., 2021b Yang et al., , 2022)) .\nHowever, context-free rules are not natural for modeling discontinuous language phenomena such as extrapositions, cross-serial dependencies, and Code: https://github.com/sustcsonglin/TN-LCFRS. wh-movements. Mildly context-sensitive grammars (Joshi, 1985) , which sit between context-free and context-sensitive grammars in the classic Chomsky-Sch\u00fctzenberger hierarchy (Chomsky, 1959; Chomsky and Sch\u00fctzenberger, 1963 ), 1 are powerful enough to model richer aspects of natural language including discontinuous and non-local phenomena. And despite their expressivity they enjoy polynomial-time inference algorithms, making them attractive both as cognitively plausible models of human language processing and as targets for unsupervised learning.\nThere are several weakly equivalent formalisms for generating the mildly context-sensitive languages which might serve as potential targets for grammar induction: tree adjoining grammars (Joshi, 1975) , head grammars (Pollard, 1985) , combinatory categorial grammars (Steedman, 1987) , and linear indexed grammars (Gazdar, 1988) . In this paper we work with linear context-free rewriting systems (LCFGS, Vijay-Shanker et al., 1987) , which generalize the above formalisms and are weakly equivalent to multiple context-free grammars (Seki et al., 1991) . Derivation trees in an LCFRS directly correspond to discontinuous constituency trees where each node can dominate a non-contiguous sequence of words in the yield, as shown in Fig. 1 .\nWe focus on the LCFRS formalism as it has previously been successfully employed for supervised discontinuous constituency parsing (Levy, 2005; Maier, 2010; van Cranenburgh et al., 2016) . The complexity of parsing in a LCFRS is O(\u2113 3k |G|), where \u2113 is the sentence length, k is the fan-out (the maximum number of contiguous blocks of text that can be dominated by a nonterminal), and |G| is the grammar size. While polynomial, this is too high to be practical for unsupervised learning on real-world data. We thus restrict ourselves to LCFRS-2, i.e., binary LCFRS with fan-out two, which has been shown to have high coverage on discontinuous treebanks (Maier et al., 2012) . Even with this restriction LCFRS-2 remains difficult to induce from raw text due to the O(\u2113 6 |G|) dynamic program for parsing and marginalization. However Corro (2020) observe that a O(\u2113 5 |G|) variant of the grammar that discards certain rules can still recover 98% of real world treebank constituents. Our approach uses with this restricted variant of LCFRS-2 (see Sec 2.2). Finally, following recent work which finds that that overparameterizing deep latent variable models is beneficial for unsupervised learning (Buhai et al., 2020; Yang et al., 2021b; Chiu and Rush, 2020; Chiu et al., 2021) , we scale LCFRS-2 to a large number of nonterminals by adapting tensor-decomposition-based inference techniques-originally developed for PCFGs (Cohen et al., 2013; Yang et al., 2021b Yang et al., , 2022)) -to the LCFRS case.\nWe conduct experiments German and Dutchboth of which have frequent discontinuous and non-local language phenomena and have available discontinuous treebanks-and observe that our approach is able to induce grammars with nontrivial performance on discontinuous constituents. Rabanser et al., 2017) to decompose the 3D binary rule probability tensor T \u2208 R m\u00d7m\u00d7m as,\n\nApproach\nT = r q=1 u q \u2297 v q \u2297 w q ,\nwhere u q , v q , w q \u2208 R m , r is the tensor rank (a hyperparameter), and \u2297 is the outer product. Letting U, V, W \u2208 R r\u00d7m be the matrices resulting from stacking all u q , v q , w q , Cohen et al. ( 2013) give the following recursive formula for calculating the inside tensor \u03b1 \u2208 R (\u2113+1)\u00d7(\u2113+1)\u00d7m for a sentence of length \u2113:\n\u03b1 L i,j = V \u03b1 i,k , \u03b1 R j,k = W \u03b1 k,j , \u03b1 i,j = U T j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k .\nHere 2021b) and further pre-compute matrices J = V U T , K = W U T to rewrite the above recursive formula as:\n\u03b1 L , \u03b1 R \u2208 R (\u2113+1)\u00d7(\u2113+1\n\u03b1 L i,j = J\u03b1 \u2032 i,j ,\u03b1 R i,j = K\u03b1 \u2032 i,j \u03b1 \u2032 i,j = j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k\nwhere \u03b1 \u2032 \u2208 R (n+1)\u00d7(n+1)\u00d7r is an auxiliary inside score tensor. The resulting complexity of this approach is O(\u2113 3 r + \u2113 2 r 2 ), which is smaller than O(\u2113 3 r + \u2113 2 mr) when r \u226a m, i.e., in the setting with a large number of nonterminals whose probability tensor is of low rank. In this paper we adapt this low rank neural parameterization to the LCFRS case to scale to a large number of nonterminals.\n\nRestricted LCFRS\nIn an LCFRS, a single nonterminal node can dominate a tuple of strings that need not be adjacent in the yield. The tuple size is referred to as the fanout. We mark the fan-out of each non-leaf node in Fig. 1 . The fan-out of an LCFRS is defined as the maximal fan-out among all its nonterminals, and influences expressiveness and parsing complexity. For a binary LCFRS (i.e., LCFRS with derivation rules that have at most two nonterminals on the right hand side) with fan-out k, the parsing complexity for a sentence of length \u2113 is O(\u2113 3k ). 2 In this paper we work with binary LCFRS with fanout 2 (Stanojevi\u0107 and Steedman, 2020, LCFRS-2) , which is expressive enough to model discontinuous constituents but still efficient enough to enable practical grammar induction from natural language data. This choice is also motivated by Maier et al. (2012) who observe that restricting the fan-out to two suffices for capturing a large proportion of discontinuous constituents in standard treebanks. 3 However, LCFRS-2's inference complexity of O(\u2113 6 |G|) is still too expensive for practical unsupervised learning. We thus follow Corro (2020) and discard all rules that require O(\u2113 6 ) time to parse, which reduces parsing complexity to O(\u2113 5 |G|). 4 Formally, this restricted LCFRS-2 is a 6-tuple G = (S, N 1 , N 2 , P, \u03a3, R) where: S is the start symbol; N 1 , N 2 are a finite set of nonterminal symbols of fan-out one and two, respectively; P is a finite set of preterminal symbols; \u03a3 is a finite set of terminal symbols; and R is a set of rules of the following form (where M \u225c N 1 \u222a P):\nS(x) \u2192 A(x) A \u2208 N 1 A(xy) \u2192 B(x)C(y) A \u2208 N 1 , B, C \u2208 M A(yxz) \u2192 B(x)C(y, z) A \u2208 N 1 , B \u2208 M, C \u2208 N 2 A(x, y) \u2192 B(x)C(y) A \u2208 N 2 , B, C \u2208 M\n2 A binary CFG is thus a special case of a binary LCFRS with fan-out one, and parsing in this case reduces to the classic CKY algorithm.\n3 For instance, Stanojevi\u0107 and Steedman (2020) report that LCFRS-2 can cover up to 87% of the gold discontinuous constituents in the NEGRA treebank. We refer readers to Table 1 of Corro (2020) for more details. 4 These correspond to rules (d), (i), (j), (k), and (l) in Figure 3 of Corro (2020) .\n\nItem form:\n[A, i, j]: fan-out-1 node A spanning [i, j)\n[A, i, j, k, n]: fan-out-2 node A spanning [i, j), [k, n) Axioms: [A, i, i + 1], 0 \u2264 i < \u2113 + 1, A \u2208 N 1 Goals: [S, 0, n] Deductive rules: [B, i, k] [C, k, j] [A, i, j] A(xy) \u2192 B(x)C(y) i < k < j 1a [B, i, j] [C, m, n] [A, i, j, m, n] A(x, y) \u2192 B(x)C(y) i < j < m < n 1b [B, m, n] [C, i, m, n, j] [A, i, j] A(yxz) \u2192 B(x)C(y, z) i < m < n < j 2a [B, i, k] [C, k, j, m, n] [A, i, j, m, n] A(xy, z) \u2192 B(x)C(y, z) i < k < j < m < n 2b [B, k, j] [C, i, k, m, n] [A, i, j, m, n] A(yx, z) \u2192 B(x)C(y, z) i < k < j < m < n 2c [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, xz) \u2192 B(x)C(y, z) i < j < m < k < n 2d [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, zx) \u2192 B(x)C(y, z) i < j < m < k < n 2e\nTable 1 : Chart parsing algorithm described in the parsing-asdeduction framework. Here \u2113 is the sentence length and we use interstice indices (not word indices) as in Corro (2020) .\nA(xy, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(yx, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, xz) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, zx) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M T (w) \u2192 w, T \u2208 P, w \u2208 \u03a3.\nHere A(x) indicates that A has a fan-out 1; A(x, y) indicates that A has a fan-out 2 and x and y are nonadjacent contiguous strings in the yield of A. \n\u2192 B(x)C(y, z) above. B is a fan-out-1 node whose yield is x = w i \u2022 \u2022 \u2022 w k\u22121 and C is a fan-out-2 node whose first span is y = w k \u2022 \u2022 \u2022 w j\u22121 and whose second span is z = w m \u2022 \u2022 \u2022 w n\u22121 .\nA is the parent node of B, C, and inherits the yields of B and C, where x is concatenated with y to form a contiguous span and z is a standalone span.\nParsing. Table 1 gives the parsing-asdeduction (Pereira and Warren, 1983 ) description of the CKY-style chart parsing algorithm of our restricted LCFRS-2.\n\nTensor decomposition-based neural parameterization\nWe now describe a parameterization of LCFRS-2 that combines a neural parameterization with tensor decomposition, which makes it possible to scale LCFRS-2 to thousands of nonterminals.\nLet\nm 1 = |N 1 |, m 2 = |N 2 |, p = |P|, and m = m 1 + p.\nThe rules involving A \u2208 N 1 on the left hand side are 1a and 2a , whose probabilities can be represented by 3D tensors\nC 1 \u2208 R m 1 \u00d7m\u00d7m and D 1 \u2208 R m 1 \u00d7m\u00d7m 2 . For A \u2208 N 2 , the relevant rules are 1b , 2b , 2c , 2d , 2e , whose proba- bilities can be represented by 3D tensors C 2 \u2208 R m 2 \u00d7m\u00d7m and D 3 , D 4 , D 5 , D 6 \u2208 R m 2 \u00d7m\u00d7m 2 . We stack D 3 , D 4 , D 5 , D 6 into a single 4D tensor D 2 \u2208 R m 2 \u00d7m\u00d7m 2 \u00d74\nto leverage the structural similarity of these rules. Since these tensors are probabilities, we must have\nEQUATION\nEQUATION\nTensor decomposition. To scale up the LCFRS-2 to a large number of nonterminals, we first apply CPD on all the binary rule probability tensors,\nC 1 = r 1 \u22121 q=0 U 1 :,q \u2297 V 1 :,q \u2297 W 1 :,q C 2 = r 2 \u22121 q=0 U 2 :,q \u2297 V 2 :,q \u2297 W 2 :,q D 1 = r 3 \u22121 q=0 U 3 :,q \u2297 V 3 :,q \u2297 W 3 :,q D 2 = r 4 \u22121 q=0 U 4 :,q \u2297 V 4 :,q \u2297 W 4 :,q \u2297 P :,q\nwhere U :,q denotes the q-th column of U . The dimensions of these tensors are\nU 1 \u2208 R m 1 \u00d7r 1 , V 1 , W 1 \u2208 R m\u00d7r 1 , U 2 \u2208 R m 1 \u00d7r 2 , V 2 \u2208 R m\u00d7r 2 , W 2 \u2208 R m 2 \u00d7r 2 , U 3 , W 3 \u2208 R m 2 \u00d7r 3 , U 4 , W 4 \u2208 R m 2 \u00d7r 4 , V 3 \u2208 R m\u00d7r 3 , V 4 \u2208 R m\u00d7r 4\n, and P \u2208 R 4\u00d7r 4 . Here r 1 , r 2 , r 3 , r 4 are the ranks of the tensors that control inference complexity. To ensure these factorizations lead to valid probability tensors, 1), we additionally impose the following restrictions: (1) all decomposed matrices are non-negative; (2) P, V i , W i are column-wise normalized where i \u2208 {1, 2, 3, 4};\n(3) \u2200i, j U 1 ij + k U 2 ik = 1; and (4) \u2200i, j U 3 ij + k U 4 ik = 1.\nIt is easy to verify that Eq. 1 and 2 are satisfied if the above requirements are satisfied.\nRank-space dynamic programming. For unsupervised learning, we need to compute the marginal likelihood of a sentence p(w 1 w 2 \u2022 \u2022 \u2022 w \u2113 ). We give the rank-space dynamic program (i.e., the inside algorithm) for computing p(w\n1 w 2 \u2022 \u2022 \u2022 w \u2113 ) in this tensor decomposition-based LCFRS-2 in App. A.\nThe resulting complexity is dominated by O(\u2113 5 r 4 + \u2113 4 (r 3 +r 4 )(r 2 +r 4 )). We thus set r 4 to a very small value, which greatly improves runtime.\nParameterization. Following prior work on neural parameterizations of grammars (Jiang et al., 2016; Kim et al., 2019) , we parameterize the component matrices to be the output of neural networks over shared embeddings.\nThe symbol embeddings are given by: E 1 \u2208 R m\u00d7d where the first m 1 rows correspond to fanout-1 nonterminal embeddings and the last p rows are the preterminal embeddings; E 2 \u2208 R m 2 \u00d7d for the fan-out-2 nonterminal embedding matrix; r \u2208 R d for the start symbol embedding. We also have four sets of \"rank embeddings\"\nR 1 \u2208 R r 1 \u00d7d , R 2 \u2208 R r 2 \u00d7d , R 3 \u2208 R r 3 \u00d7d\n, and R 4 \u2208 R r 4 \u00d7d . Given this, the entries of the U, V, W matrices are given by,\nU o ij \u221d exp{(R o j ) \u22a4 f o U (E 1 i )}, o \u2208 {1, 2} U o ij \u221d exp{(R o j ) \u22a4 f o U (E 2 i )}, o \u2208 {3, 4} V o ij \u221d exp{(R o j ) \u22a4 f o V (E 1 i )}, o \u2208 {1, 2, 3, 4} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 1 i )}, o \u2208 {1, 2} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 2 i )}, o \u2208 {3, 4} where f o U , f o V , f o W are one-layer ReLU MLPs with output size d. U o , V o , W o\nare normalized according to the requirements described in the previous subsection. We share the parameters of the following MLP pairs:\n(f 1 U , f 2 U ), (f 3 U , f 4 U ), (f 1 V , f 3 V ), (f 2 V , f 4 V ), (f 1 W , f 3 W ), (f 2 W , f 4 W )\nas they play similar roles (e.g., f 1 V and f 3 V are both applied to left children). For the D 2 tensor we also require the matrix P \u2208 R 4\u00d7r 4 , and this is given by P \u22a4 = f P (R 4 ), where f P is a one-layer residual network with output size 4 that is normalized via a softmax across the last dimension.\nFinally, for the starting and the terminal distributions we have\ns = f s (r), Q = f Q (E 1 m 1 :\n), which results in s \u2208 R m 1 (i.e., the probability vector for rules of the form S \u2192 A) and Q \u2208 R p\u00d7v (i.e., probability matrix for rules of the form T (w) \u2192 w). Here E 1 m 1 : is the last p rows of E 1 , and f s and f Q are residual MLPs with softmax applied in the last layer to ensure that s and Q are valid probabilities.\nDecoding. While the rank-space inside algorithm enables efficient computation of sentence likelihoods, direct CKY-style argmax decoding in this grammar requires instantiating the full probability tensors and is thus computationally intractable. We follow Yang et al. (2021b) and use Minimal Bayes Risk (MBR) decoding (Goodman, 1996) . This procedure first obtains the posterior probability of each span's being a constituent via the inside-outside algorithm (which has the same complexity as the inside algorithm). Then, these posterior probabilities are used as input into CKY in a grammar that only has a single nonterminal. The complexity of this approach is thus independent of the number of nonterminals in the original grammar, and takes O(\u2113 5 ). This strategy can be seen as finding the tree that has the largest number of expected constituents (Smith and Eisner, 2006) . See App. A for details.\n\nEmpirical Study\nData. We conduct experiments with our Tensor decomposition-based Neural LCFRS (TN-LCFRS) on German and Dutch, where discontinuous phenomena are more common (than in English). For German we concatenate TIGER (Brants et al., 2001) and NEGRA (Skut et al., 1997) as our training set, while for Dutch we use the LASSY Small Corpus treebank (van Noord et al., 2013) . The data split can be found in App. B.1. For processing we use disco-dop 5 (van Cranenburgh et al., 2016) and discard all punctuation marks. We further take the most frequent 10,000 words for each language as the vocabulary, similar to the standard setup in unsupervised constituency parsing on PTB (Shen et al., 2018 (Shen et al., , 2019;; Kim et al., 2019) .\nGrammar size. To investigate the importance of using a large number of latent variables (which has previously been shown to be helpful for structure induction (Buhai et al., 2020; Yang et al., 2021b )), we train TN-LCFRSs of varying sizes. We first choose the number of preterminals |P| \u2208 {45, 450, 4500} and set the number of fan-out one and fan-out two nonterminals to be\n|N 1 | = |N 2 | = 1 3 |P|.\nThe rank of the probability tensors are set to r 1 = r 3 = 400, r 2 = r 4 = 4, and the dimensionality of the Baselines. Our baselines include: the neural PCFG (N-PCFG) and the compound PCFG (C-PCFG) (Kim et al., 2019) , which cannot directly predict discontinuous constituents 6 but still serve as strong baselines for overall F1 since the majority of spans in these treebanks are continuous; and their direct extensions, neural LCFRS (N-LCFRS) and compound LCFRS (C-LCFRS), which do not employ the tensor-based low-rank factorization. These non-low-rank models have high computational complexity and hence we set |P| = 45 for these models. When |P| = 4500, we also compare against the tensor decompositional-based neural PCFG (TN-PCFG) from Yang et al. (2021b) .\nEvaluation. We use unlabeled corpus-level F1 to evaluate unsupervised parsing performance, reporting both overall F1 and discontinuous F1 (DF1). For all experiments, we report the mean results and standard deviations over four runs with different random seeds. See App. B.2 for further details.\n\nMain results\nTable 2 shows the main results. With smaller grammars (|P| = 45), we find that both neural/compound LCFRSs have lower F1 than their PCFG counterparts, despite being able to predict discontinuous constituent spans. On the other hand, TN-LCFRS achieves better F1 than N-LCFRS even though it is a more restricted model (since it assumes that the rule probability tensors are of low rank), showing the benefits of parameter sharing through low rank factorizations. As we scale up TN-LCFRSs with |P| \u2208 {45, 450, 4500} we observe continuous improvements in performance, with TN-LCFRS 4500 achieving the best F1 and DF1 on all three datasets. These results all outperform trivial (left branching, right branching, and random tree) baselines.\nAs an upper bound we also train a supervised model with TN-LCFRS 4500 . 7 We also show the maximum possible performance with oracle binary trees with this optimal binarization. While the discontinuous F1 of our unsupervised parsers are nontrivial, there is still a large gap between the unsupervised and supervised scores (and also between the supervised and the oracle scores), indicating opportunities for further work in this area.\n\nAnalysis\nRecall by constituent label. Table 3 shows the recall by constituent tag for the different models averaged over four independent runs. Overall the unsupervised methods do well on noun phrases (NP), prepositional phrases (PP) and proper nouns (PN), with some of the models approach the supervised baselines. Verb phrases (VP) and adjective dynamic programming to sum out all possible nonterminals for each node, resulting in the joint log probability of unlabeled binarized tree and sentence. This was then maximized during training. As for the oracle bound, we emphasize that the gold trees are nonbinary while our model can only predict binary trees. Approximation error. Approximation error in the context of unsupervised learning arises due to the mismatch between the EM objective (i.e., log marginal likelihood) and structure recovery (i.e., F1), and is related to model misspecification (Liang and Klein, 2008) . Figure 2 (left column) plots the training/dev perplexity as well as the dev F1/DF1 as a function of the number of epochs. We find that larger grammars result in better performance in terms of both perplexity and structure recovery, which ostensibly indicates that the unsupervised objective is positively correlated with structure induction performance. However, when we first perform supervised learning on the log joint likelihood and then switch to unsupervised learning with log marginal likelihood (Figure 2 , right), we find that while perplexity improves when we switch to the unsupervised objective, structure induction performance deteriorates. 8 Still, the difference in F1 before and after switching to the unsupervised objective is less for larger models, confirming the benefits of using larger grammars.\n\nEven more restricted LCFRS formalisms.\nThere are even more restricted versions of LCFRSs which have faster parsing (e.g. O(\u2113 3 ), O(\u2113 4 )) but 8 It is worth noting that the phenomenon of mismatch between log marginal likelihood objective and parsing accuracy is quite common in unsupervised grammar induction (and latent variable modeling approaches to structured induction more generally). Many previous works have observed this phenomenon, e.g., Merialdo (1994) in the context of HMMs, and Johnson et al. (2007) and Liang and Klein (2008) in the context of PCFGs. This is partially attributed to the fact that generative grammars often make some unreasonable independence assumptions to make the training process tractable, which does not fully comply with the true generative process of human languages and their underlying structures. 45.4 0.9 44.5 0.5\n\nModel\nTable 5 : Ablation studies on the German (TIGER) treebank.\ncan still model discontinuous constituents. In the supervised case, these restricted variants have been shown to perform almost as well as the more expressive O(\u2113 5 ) and O(\u2113 6 ) variants (Corro, 2020) .\nIn the unsupervised case however, we observe in Table 5 that disallowing O(\u2113 5 ) rules ( 2b , 2c , 2d , 2e ) significantly degrades discontinuous F1 scores. We posit that this phenomena is again related to empirical benefits of latent variable overparameterization-while in theory it is possible to model most discontinuous phenomena with more restricted rules, making the generative model more expressive via \"overparameterizing\" in rule expressivity space (i.e., using more flexible rules than is necessariy) empirically leads to better performance.\nParameter sharing. As shown in Table 5 , it was important to share the symbol embeddings across the different rules. Sharing the parameters of the MLPs as described in Sec. 2.3 was also found to be helpful. This highlights the benefits of working with neural parameterizations of grammars which enable easy parameter sharing across rules that share symbols and/or have similar shapes.\nQualitative analysis. In Fig. 3 , we show some examples trees in German. For each sentence, we show the gold, TN-LCFRS 4500 , and TN-PCFG 4500 trees. In the first sentence, the crossing dependency occurs due to the initial adverb (\"So\")'s being analyzed as a dependent of the non-finite verb phrase at the end of the sentence which occurs due to German V2 word order. Our parser correctly predicts this dependency, although the subject NP (which itself is correctly identified) has the wrong internal structure. relative clause a part of the non-finite verb complex, which does not conform to the annotation guidelines but resembles an alternative analysis that has been proposed for extraposed relative clauses (Baltin, 1983) . Sentence initial adverbs in the context of auxiliary verb constructions and right-extraposed relative clauses describe two common instances of discontinuous phenomena in German. Wh-questions constitute another potential class of discontinuous phenomena; however, these are not treated as discontinuous in TIGER/NEGRA. See App. D for more examples trees (including on Dutch).\n\nRelated work\nMildly context-sensitive grammars. Given the evidence against the context-freeness of natural language (Shieber, 1985) , mildly context-sensitive grammars such as tree adjoining grammars were thought to be just flexible (but still constrained) enough to model natural language (Joshi, 1985) . Prior work on inducing mildly context-sensitive grammars has generally focused on combinatory categorial grammars (Bisk and Hockenmaier, 2012, 2013) , and we are unaware of any work on in-ducing LCFRSs from observed yields alone. Our work is also related to the rich line of work on supervised discontinuous parsing (Kallmeyer and Maier, 2010; Maier et al., 2012; Maier, 2015; Corro, 2020; Vilares and G\u00f3mez-Rodr\u00edguez, 2020; Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez, 2020 , 2021 , 2023) , though we are unaware of any prior work on unsupervised discontinuous parsing.\nNeural grammars. Early work on probabilistic approaches to grammar induction was largely negative (Lari and Young, 1990; Carroll and Charniak, 1992) . However, recent work has shown that neural parameterizations of classic grammars can greatly improve structure induction. Our work adds to the line of work on neural parameterizations of dependency models (Jiang et al., 2016; Han et al., 2017; He et al., 2018; Yang et al., 2020) , context-free grammars (Kim et al., 2019; Jin et al., 2019; Zhu et al., 2020; Yang et al., 2021a) , and synchronous grammars (Kim, 2021; Wang et al., 2022; Friedman et al., 2022) . Neural parameterizations make it easy to share parameters and condition on additional side information (images/audio/video) which has shown to be particularly useful for multimodal grammar induction (Zhao and Titov, 2020; Jin and Schuler, 2020; Su et al., 2021; Hong et al., 2021; Zhang et al., 2021) .\nScaling latent variable models. Buhai et al. (2020) study the empirical benefits of overparameterization in learning latent variable models. Other works have explored parameterizations of latent variable models that make it especially amenable to scaling (Chiu and Rush, 2020; Chiu et al., 2021; Yang et al., 2021b Yang et al., , 2022)) . Relatedly, Peharz et al. (2020) and Liu et al. (2022) show the benefits of scaling probabilistic circuits (Choi et al., 2020) .\n\nConclusion\nThis work studied unsupervised discontinuous constituency parsing with mildly context-sensitive grammars, focusing on the formalism of linear context-free rewriting systems. By using a tensor decomposition-based neural parameterization of linear context-free rewriting systems, our approach was able to induce grammars that had nontrivial discontinuous parsing performance on German and Dutch. Whether even more expressive grammars will eventually lead to models learn linguistically meaningful structures and are at the same time competitive with pure neural language models (as a language model) remains an open question.\n"}
{"question": "By introducing semantics, what performance do we achieve?", "evidence": " By introducing semantics, we improve existing speech encoder spoken language understanding (SLU) performance by over 5% on intent classification (IC), with modest gains in named entity resolution (NER) and slot filling (SF), and spoken question answering (SQA) FF1 score by over 2%.   ", "options": ["A.  We improve the performance by over 5% on named entity resolution(NER).", "B.  We improve existing speech encoder spoken language understanding (SLU) performance with modest gains.", "C.  We improve the performance by over 5% on slot filling (SF).", "D.  We improve spoken question answering (SQA) FF1 score by over 2%."], "answer": "D", "content": "\nIntroduction\nRealizing artificial intelligence (AI) that can understand and respond to spoken language is a north star for many speech and natural language processing (NLP) researchers. A particularly effective framework for this is the encoder-decoder architecture, where an encoder represents input audio signals as high-dimensional embeddings and a decoder converts said embeddings to outputs for different downstream tasks. Benchmarks for such systems include spoken language understanding, where intent, named entities, or slot values are predicted from input utterances (Yang et al., 2021; Bastianelli et al., 2020; Shon et al., 2022) , and spoken question answering, where the start and end frames of an input audio passage answering an input audio question are predicted (Lin et al., 2022a) .\nA particularly notable setup of the encoderdecoder framework is the universal representation setup (Yang et al., 2021) , where a shared selfsupervised speech encoder is pretrained upstream once and frozen for all downstream tasks, then a different lightweight decoder is fine-tuned on each downstream task. This setup is appealing for building speech systems as maintaining a separate large specialized model for every task is not computationally efficient. The universal representation setup has been widely adopted in other areas of research, such as computer vision (Goyal et al., 2019; Ericsson et al., 2021) and NLP (Rogers et al., 2020; Qiu et al., 2020) , and production when there are many downstream tasks or domains (Molino et al., 2019) . The current state-of-the-art speech encoders under this setup are W2V2 and HUBERT (Yang et al., 2021; Baevski et al., 2020; Hsu et al., 2021) , which are transformer-based models trained with self-supervised learning (SSL) on raw audio and have achieved impressive performance on various tasks.\nRecently, analytical works found SSL speech encoders capture primarily acoustic, not semantic, information (Pasad et al., 2021) . Thus, researchers proposed end-to-end systems (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) that introduce semantic information through large language models (LLMs), such as ROBERTA (Liu et al., 2019) or BART (Lewis et al., 2019) , which are pretrained to capture language semantics (Clark et al., 2019) . This is typically accomplished by the pipeline approach (Bastianelli et al., 2020) , which passes audio input through the SSL speech encoder, then bridge module, then LLM. The bridge module converts speech encoder embedding outputs into LLM token inputs (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) .\nUnsupervised ASR models (ASR-U) (Liu et al., 2020b; Baevski et al., 2021; Liu et al., 2022) have also seen recent success. The state-of-the-art ASR-U model uses generative adversarial networks (GANs) (Goodfellow et al., 2020) to generate text transcription from input audio (Liu et al., 2022) .\nCurrent works combining SSL speech encoders and LLMs do not satisfy the universal representation framework, as they either (1) rely on ASR data on the downstream task, which is expensive to collect and maintain, (2) are not lightweight, requiring training the whole system end-to-end, or (3) are not general, as they do not consider a wide variety of downstream tasks (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) . Similarly, ASR-U was proposed for speech recognition and the focus is not improving SSL speech encoders (Baevski et al., 2021; Liu et al., 2022) .\nWe propose introducing Semantics into Speech Encoders, SSE, a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. Concretely, SSE adopts the pipeline approach to obtain semantic embeddings, with an ASR-U bridge connector to extract information from LLMs. As ASR-U is inherently noisy, SSE introduces attention residual connection (He et al., 2016; Vaswani et al., 2017) between the speech encoder and LLM. SSE also efficiently aligns the LLM with the speech encoder through adapter modules (Houlsby et al., 2019) . SSE improves W2V2 (Baevski et al., 2020) and HUBERT (Hsu et al., 2021) on 3 SLU tasks across 3 datasets, all under the universal representation setup. SSE also outperforms state-of-the art no-ASR method, DUAL (Lin et al., 2022a) , in SQA.\nWhile recent works use ASR-U to augment existing speech encoders with phoneme-level LLMs (Feng et al., 2022; Meng et al., 2022; Shi et al., 2022; Hsu et al., 2022) , subword-level LLMs contain much more pertinent and measurable semantic information (Clark et al., 2019) . Other works in SQA rely on clustering to assign audio frames to frequent subword tokens, but this requires heavy finetuning on the downstream task (Lin et al., 2022a) .\nTo the best of our knowledge, we are the first to propose a task-agnostic SSL speech encoder which directly interfaces with subword-based LLMs, unblocking many other applications and future work in this domain. To this end, attention residual con-nections and adapters are essential to successfully extracting semantic information from noisy intermediary transcriptions. We summarize our contributions below:\n\u2022 We propose using ASR-U components to augment SSL speech encoders for generating subword tokens with semantic information.\n\u2022 The augmented SSL speech encoders can be connected with powerful LLMs seamlessly and yields state-of-the-art performance under the universal representation setup.\n\u2022 We show attention residual connections and adapters are essential to combining and aligning speech and text encoders.\n2 Related Works 2.1 Self-Supervised Speech Encoders SSL speech encoders (Liu et al., 2020a; Chung et al., 2020a; Ling and Liu, 2020; Liu et al., 2021 Liu et al., , 2020c;; Chung et al., 2019; Baevski et al., 2019; Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Qian et al., 2022; Zhang et al., 2022) are trained to learn and reconstruct pooled clustered representations of input audio from the original audio. The intuition for this objective comes from linguistics, where speech can be broken down into phoneme groups, where different chunks of input audio represent different phoneme groups.\nW2V (Schneider et al., 2019) trains a convolutional neural network model to reconstruct the quantized cluster representations. W2V2 (Baevski et al., 2020) uses transformers and a discrete codebook quantization module. HUBERT (Hsu et al., 2021) improves W2V2 by disentangling the clustering and SSL objectives and using a BERT-style encoder (Devlin et al., 2018) . The speech processing universal performance benchmark (SU-PERB) (Yang et al., 2021; Lin et al., 2022b; Tsai et al., 2022) shows SSL speech encoders are the most effective method for solving multiple downstream tasks with minimal fine-tuning. A recent analytical work finds SSL speech encoders successfully encode acoustic information, but lack semantic information (Pasad et al., 2021) . In response, CONTENTVEC (Qian et al., 2022) propose disentangling the speaker and semantic content of audio via an SSL objective. SPEECHLM (Zhang et al., 2022) propose training a multi-modal speech and text encoder.\n\nLarge Language Models\nIn contrast to speech encoders, pretrained LLMs are shown to capture rich semantic information (Clark et al., 2019) . These methods optimize variants of the masked language modeling (MLM) objective to train a large transformer model. BERT (Devlin et al., 2018) uses MLM to learn a transformer encoder. ROBERTA (Liu et al., 2019) introduces dynamic masking and a larger text corpus. BART (Lewis et al., 2019) supports generative modeling and adds a denoising objective, making it less susceptible to noisy text inputs. LONG-FORMER (Beltagy et al., 2020) is pretrained for long documents by increasing the document length limit during pretraining. LLMs have been successfully integrated with speech models for specific semantic tasks (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) , but not under the universal representation framework.\n\nTask-Specific Speech Models\nTask-specific SLU systems outperform generic SSL speech encoders typically by using a LLM. These systems rely on ASR data to reliably interface the LLM. LUGOSCH (Lugosch et al., 2019) trains a LSTM bridge module to convert audio features into phonemes then text. CTI's (Seo et al., 2022) bridge module uses ASR logits to compute a weighted average of token embeddings. In addition to improving the bridge module, other works attempt to also distill LLM embeddings into speech representations (Chung et al., 2020b; Cha et al., 2021; Kim et al., 2021; Agrawal et al., 2022) . For optimizing targeted metrics, researchers have also experimented with reinforcement learning (Rao et al., 2021) . While combinations of these methods achieve impressive performance, they do not satisfy the universal representation setup.\n\nUnsupervised ASR\nRecent work show the viability of unsupervised speech recognition. W2V2-U (Baevski et al., 2021) accomplished this by running Principal Component Analysis (PCA), k-means clustering, and mean pooling to convert W2V2 (Baevski et al., 2020) features into phoneme-granularity features, then trains a GAN model to output phoneme text from the post-processed model (Baevski et al., 2021) . The state-of-the-art method for phoneme-level unsupervised ASR is W2V2-U2.0 (Liu et al., 2022) which directly trains a CNN to output phonemes from W2V2 features and uses a reconstruction loss to tie the input audio with corresponding generated text. Both methods use WFSTs to decode the phonemes into raw text. While there have been preliminary attempts (Feng et al., 2022; Meng et al., 2022) to use W2V2-U2.0 with phoneme language models 1 , we are the first to combine it with semantically-rich subword-based LLMs.\n\nAdapters\nAdapters are intermediary layers added to a large pretrained encoder. Adapter weights are learned during fine-tuning while the rest of the pretrained model is frozen. Adapters serve the dual purpose of efficient fine-tuning and preventing overfitting. First used by computer vision researchers (Rebuffi et al., 2017) , adapters now enjoy much success in the natural language processing community by efficiently tuning LLMs (Houlsby et al., 2019) . In particular, the multilingual speech translation community found that adapters can effectively align SSL speech encoders and LLMs for spoken translation tasks (Li et al., 2020; Le et al., 2021) .\n\nProposed Method\nWe propose to introduce semantics into SSL speech encoders by using ASR-U to interface with LLMs. Section 3.2 describes how to use ASR-U to link a speech encoder with a LLM. Section 3.3 describes how to combine both acoustic and semantic information and deal with ASR transcriptions errors. Finally, Section 3.4 describes how to align LLMs with the speech encoder for downstream tasks.\n\nProblem Setting\nFollowing the universal representation framework (Yang et al., 2021) , our model consists of a large speech encoder, E : X \u2192 Z, mapping input audio, X \u2208 X , to embeddings, Z \u2208 Z, and a light-weight task decoder,\nD \u03c9 : Z \u2192 Y \u03c9 , mapping embeddings to downstream task outputs, Y \u03c9 \u2208 Y \u03c9 .\nThe speech encoder, E, is pretrained once, then shared on all downstream tasks. The task decoder, D \u03c9 , is fine-tuned on its respective task, \u03c9 \u2208 \u2126.\nDuring fine-tuning, the majority of model weights are frozen. This ensures the model can be efficiently stored and deployed.\nDuring pretraining, the speech encoder is trained on unlabelled audio, X \u2208 X , and unlabeled text, T u \u2208 T u . During finetuning, the model is trained on the labelled downstream dataset, (X, Y \u03c9 ) \u2208 X \u00d7 Y \u03c9 . Notice, costly labelled ASR data is not required during pretraining or finetuning.\n\nUnsupervised Semantic Representation as a Bridge\nTo incorporate semantic information into SSL speech encoders, E : X \u2192 Z, we wish to leverage subword-based LLMs, M : S \u2192 Z, that capture language semantics (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Beltagy et al., 2020) . The major challenge is the mismatch of input spaces. Speech encoders take raw audio as input, X \u2208 X . LLMs take subword tokens as input, S \u2208 S. SSE uses W2V2-U2.0 (Liu et al., 2022) as a bridge module (Seo et al., 2022) , B : Z \u2192 S, to convert speech encoder embedding output into LLM subword tokens in a pipelined approach,\nE SSE = E \u2022 B \u2022 M.\nFollowing W2V2-U2.0, the bridge module, B uses a GAN (Goodfellow et al., 2020) We also add an upsampling layer, U : Z \u2192 Z to make the sequence length of the LLM output match the speech encoder output, such that E and E SSE share the same output space.\nWe choose the 15th layer of the W2V2 (Baevski et al., 2020) as our speech encoder, as the last layers overfit the self-supervised training objective hence providing worse acoustic representations (Fan et al., 2020; Baevski et al., 2021; Pasad et al., 2021) . We choose BART (Lewis et al., 2019) as our LLM, as it is trained to denoise noisy input subword tokens, and we expect the bridge module to introduce some noise. We call this version of our model SSE-BASE. A depiction can be found in Figure 1a .\n\nCombining Semantics and Acoustics with Residual Attention\nWe hypothesize certain tasks may require more acoustic information than others. to implicitly transcribe parts of the input speech, a primarily acoustic task. Since the pipelined model may suffer from transcription errors introduced by ASR-U, naively using the pipelined approach introduces an information bottleneck at the bridge module. Hence, we propose adding a residual connection (He et al., 2016) between SSE-BASE and the speech encoder, E. This can be done in two ways: (1) upsampling semantic embeddings and concatenating with speech embeddings, Z = [Z E ||U(Z M )], or (2) using multihead attention (Vaswani et al., 2017) to merge the two embeddings, Z =\n[Z E ||MHA(Z E , Z M , Z M )],\nwhere Z E \u2208 Z is the output of the W2V2L15 (Baevski et al., 2020) and Z M \u2208 Z is the output of BART (Lewis et al., 2019) . The former is a simpler but more naive method. The latter is more effective as the attention layers are able to learn the alignment between speech and semantic embeddings. Notice, (2) introduces more learnable parameters to the finetuning-step, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder.\n\nAligning Pretrained Text Model with Adapters\nInspired by works from speech translation (Li et al., 2020; Le et al., 2021) , we hypothesize that the LLM can easily be adapted for speech tasks through the use of adapters. We adopt the general recipe for adapters, where an adapter (Houlsby et al., 2019) , composed of a LayerNorm and 2-layer ReLU neural network, is added to the end of each feed forward layer in the LLM and finetuned on downstream tasks. This introduces additional parameters to finetuning, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder. We call the model using both residual attention and adapters SSE-TUNE, and outline it in Figure 1b .\n\nExperiments 4.1 Dataset\nTo show the effectiveness of introducing semantics into speech encoders, we evaluate 3 SLU tasks, intent classification (IC), slot filling (SF), and named entity recognition (NER), and SQA \n\nSpoken Language Understanding\nTo show SSE improves SSL speech encoders, we augment two state-of-the art speech encoders under the universal representation setup: W2V2 and HUBERT. Following prior works that found intermediary layers of W2V2 contain better representations (Pasad et al., 2021; Baevski et al., 2021) , we consider the 15th layer and the last layer of W2V2, named W2V2L15 and W2V2L24 respectively. As mentioned in Section 3, we show 2 versions of our model, SSE-BASE and SSE-TUNE. The former uses the pipelined approach to connect W2V2L15 with BART (Lewis et al., 2019) with no additional modifications. The latter introduces an attention residual connection and learnable adapters to combine acoustics and semantics together and align the LLM with the speech encoder respectively. We either connect the residual connection to the output of W2V2L15, yielding SSE-TUNE (W2V2L15), or to the output of HU-BERT, yielding SSE-TUNE (HUBERT).\nTo show the importance of using LLMs, we compare against 2 very recent approaches for improving SSL speech encoders without LLMs, SPEECHLM (Zhang et al., 2022) and CON-TENTVEC (Qian et al., 2022) . As HUBERT-BASE was used as the base speech encoder by both baselines, we also provide results where SSE-TUNE is used to augment HUBERT-BASE.\n\nSpoken Question Answering\nTo show the effectiveness of SSE, we compare it against DUAL (Lin et al., 2022a) , the state-ofthe-art SQA model which does not use ASR data. While both SSE and DUAL obtain frame-level tokens from speech input, SSE uses ASR-U to obtain its tokens, whereas DUAL uses clustering. As a result, SSE's output tokens exists in the LLM's existing vocabulary, whereas DUAL's output tokens does not. Hence, DUAL must retrain the LLM on its output tokens.\nWe compare DUAL to the closest analogous SSE model, which is SSE-BASE but with adapter layers, SSE-BASE (ADAP). Similar to DUAL, both methods modify the LLM weights. Unlike DUAL, SSE-BASE (ADAP) is lightweight, tuning only around 10% of the total parameters. To produces framelevel predictions, we remove the upsampling layer from SSE-BASE (ADAP). We choose W2V2L15 as our speech model and BART as our LLM, as it is robust to ASR errors.\nWe also show a PIPELINE model, which trains a W2V2 model on ASR data and a LONGFORMER LLM on text-only question answering data. It is worth noting that since evaluation is based on the frame-level, SSL speech encoders are not a baseline since they operate at the audio level.\n\nDecoder Setup\nTo satisfy the universal representation setup, we adopt lightweight SLU decoders from SU-PERB (Yang et al., 2021) is sum pooling followed by a multilayer perceptron classifier trained with cross entropy loss. For the SF and NER tasks, the decoder is recursive neural network (RNN) that transcribes input audio into text. The decoder identifies named entities or slot values by surrounding them with named special tokens and is trained with connectionist temporal classification loss. For SQA, we adopt the same decoder as DUAL (Lin et al., 2022a) , which is a linear layer classifying each subword embedding as the start or end or neither of an answer span.\n\nImproving SSL Speech Encoders\nAs seen in Table 2 , SSE significantly improves the SLU performance of both W2V2 and HU-BERT, confirming that including semantic information drastically improves existing SSL speech encoder performance. Specifically, SSE-TUNE (W2V2L15) improves W2V2L15 on all tasks. SSE-TUNE (HUBERT) improves HUBERT on 3 out of 4 tasks, and is the best performing model overall. Comparing SSE-TUNE with SSE-BASE shows residual attention and adapters effectively counteracts bridge module transcription errors. The relative performance gain for IC is more than SF or NER. Unlike IC, both SF and NER require the speech encoder to transcribe identified audio snippets, and transcription is a primarily acoustic task. Hence SF and NER require less semantic information than IC. Nevertheless, combining both acoustic and semantic information, as done by SSE-TUNE, provides the most consistent performance improvement, since the skip connection can learn which type of information is more needed.\n\nImportance of LLMs\nAs seen in Table 2 , SSE-TUNE (HUBERT-BASE) outperforms alternative approaches augmenting speech encoders, SPEECHLM (HUBERT-BASE) and CONTENTVEC (HUBERT-BASE). Unlike these alternative approaches, SSE-TUNE incorporate information from LLMs, which we found to be very beneficial for capturing semantic information as they are carefully pretrained objectives on large amounts of unlabelled text data.\nIt is noteworthy that SSE-TUNE is a general framework which can augment any speech encoder of our choice, including SPEECHLM and CON-TENTVEC. Similarly, SSE-TUNE can directly integrate new LLMs without costly pretraining. We leave incorporating such encoders into SSE-TUNE as future work.\n\nSpoken Question Answering\nAs seen in Table 3 , SSE outperforms recent unsupervised clustering-based approaches, DUAL. In contrast to DUAL's HUBERT cluster tokens, SSE's ASR-U tokens are better aligned with LLMs and share the same space. Thus, SSE can better utilizes pretrained LLMs. Furthermore, SSE does not require carefully tuning the number of HUBERT cluster counts, as the vocabulary size of the LLM is fixed and consistent with ASR-U.\n\nChoice of Language Model\nWe find subword-based LLMs contain more information than phoneme-based LLMs (Clark et al., 2019) . We empirically verify this by replacing our subword-based LLM, BART (Lewis et al., 2019) , with popular character-based LLM, ByT5 (Xue et al., 2022) , and phoneme-based LLM, T5lephone (Hsu et al., 2022) in SSE-BASE. As seen in Table 4 , the subword-based LLM perform the best as each subword token is more semantically meaningful than a phoneme or character. We believe T5lephone outperforms the Byt5 as it has better robustness to ASR-U errors. Overall, subword-based LLMs are the best choice for embedding semantic information in transcribed text.\n\nResidual Attention and Adapters\nTo more carefully analyze the affect of residual attention and adapters in SSE-TUNE, we run experiments on all SLU datasets with and without each component. We denote these two design choices as (ResAtt) and (Adap) respectively. As seen in Table 4, both components provide ample performance improvement over SSE-BASE.\nWe also try the naive residual connection approach described in Section 3.3 by directly concatenating the LLM upsampled semantic embeddings to the speech embeddings. We call this approach SSE-BASE (RES). This method is less effective than SSE-BASE (RESATT) as it does not learn how to align speech and semantic embeddings, but improves SSE-BASE, further validating our hypothesis that merging acoustic and semantic information is beneficial.\nAs seen in parameter breakdown for the SSE-TUNE (W2V2L15) model in Table 1 , the number of new learnable parameters introduced by (Re-sAtt) and (Adap) are unsubstantial compared to the size of the lightweight downstream decoder. Specifically, the downstream task decoder accounts for 9.60% of the total model parameters. SSE-TUNE introduces only 10.47% more parameters than SSE-BASE during fine-tuning and 0.91% to the total model parameter count, but often provides significant performance improvement.\n\nComparison with Supervised ASR Methods\nTo quantify the effect of transcription errors introduced by the bridge module, we compute the word error rate (WER) of the bridge connector in SSE-TUNE, and compare it against standard W2V2 supervised ASR models (Baevski et al., 2020) trained on 10 minutes, 100 hours, and 960 hours of labeled ASR data. indicating the effectiveness of the bridge module.\nOn SLURP and SLUE, the relative drop in WER (> 20%) is substantially more than the relative drop in downstream performance (< 5%), verifying SSE-TUNE's tolerance to noisy transcriptions. The robustness to ASR errors come from our choice of LLM, BART, which is trained to handle noisy inputs, residual connection to acoustic embeddings, and LLM alignment with adapters.\n\nComparison to Specialized SLU Models\nTo better quantify the performance improvement introduced by SSE, we compare against 2 specialized SLU models that do not abide by the universal representation framework: Kaldi+HerMiT, which is a pipelined Kaldi ASR (Povey et al., 2011) and HerMiT NLU (Vanzo et al., 2019) model reported in the SLURP paper (Bastianelli et al., 2020) , and CTI (Seo et al., 2022) , which is an end-to-end pipelined W2V2 (Baevski et al., 2020) ASR and ROBERTA (Liu et al., 2019) NLU model. To the best of our knowledge, CTI is the state-of-the-art SLU model. In addition to unlabelled text, unlabelled audio, and downstream data, both Kaldi+HerMiT and CTI require 40 hours of downstream SLURP ASR data (Bastianelli et al., 2020) . Kaldi+HerMiT requires an additional 24,000 hours of ASR data (Povey et al., 2016) . CTI requires an additional 960 hours of ASR data (Panayotov et al., 2015) . Neither use lightweight fine-tuning. Thus, such specialized SLU models are less general, more expensive, and require much more data. As seen in Table 6 , SSE helps bridge the gap between tailormade models and more practical SSL speech encoders. We believe ASR-U errors plays a major role in the remaining gap, as the ASR-supervised Kaldi+HerMiT and CTI models have WER of 16.20% and 16.67% respectively, compared to A mix-up is when the model either misclassifies label \"A\" as \"B\" or misclassifies label \"B\" as \"A\". For each mix-up, we compute the percentage of less mistakes made by SSE-TUNE (HUBERT) than HUBERT. For example, SSE-TUNE (HUBERT) misclassifies calendar_set as calendar_query or vice-versa 20% less frequently than HUBERT. The \"general-quirky\" label is assigned to Out-of-Distribution inputs.\nSSE's ASR-U bridge with a WER of 51.51%.\n\nError Analysis\nTo better understand the semantic information captured by SSE, we study predictions made by both HUBERT and SSE-TUNE (HUBERT) on SLURP-IC's test set. We find HUBERT errors are made primarily between intents within the same or similar domains (e.g. calendar_set vs calendar_query).\nThe performance bottleneck lies with distinguishing finer-grained in-domain intents. Table 7 shows that SSE-TUNE is better at differentiating finergrained intents. SSE-TUNE's misclassifications come primarily from errors made by its ASR-U bridge component. As seen in Table 8 , the ASR-U WER of incorrect predictions made by HUBERT is much lower than that of incorrect predictions made by SSE-TUNE. When ASR-U returns resonable transcriptions (typically <50% WER), SSE-TUNE can correctly classify inputs that HUBERT cannot. Hence, the effectiveness of SSE is tightly coupled with the effectiveness of ASR-U.\n\nRepresentation Visualization\nTo better see the impact of including semantic representations, we visualize the pooled audio snippet embedding for intent classification on SLURP-IC using t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton, 2008) or incorrectly (\u2717). We denote the number of pairs belonging to each subset, in the thousands, in parentheses. denote the ground truth label of each audio snippet by the color of its pooled embedding. As seen in Figure 2 , the clusters produced by semantic embeddings are more spread out and better separated than those produced by just acoustic speech embeddings, indicating that SSE introduces new semantic information that existing speech encoders lack.\n\nConclusion\nWe presented a compelling case for introducing semantics into SSL speech encoders and an effective method of doing so. Our approach boosts the performance of existing speech encoders on multiple SLU and SQA tasks and datasets. We provide reasoning for what tasks may benefit more or less from incorporating semantics. Furthermore, our approach is task agnostic and can augment any existing SSL speech encoder. With SSE-TUNE, we show merging acoustic and semantic information and effectively aligning LLMs to the speech encoder on downstream tasks can further boost performance with minimal parameter overhead. As it can generalize to many downstream tasks, SSE provides an important step towards AI that can understand and respond to spoken language.\n"}
{"question": "On which of the following properties does the model work better than the larger model used for the assessment", "evidence": "  Participants were asked to rank the systems based on Content (overlap with the reference), Readability (the readability of a summary), Grammaticality (avoiding grammar errors), and Non-Redundancy (avoiding repetitions). QAMDEN was favored in all cases except for grammatical errors and readability (which corresponds to the Reinforcement Learning from Human Feedback phase of the GPT models). ", "options": ["A. Grammaticality and readability", "B. Content and Non-Redundancy ", "C. Readability and Content", "D. Non-Redundancy and Grammaticality "], "answer": "B", "content": "\nIntroduction\nAmong recent NLP research, multi-document processing is gaining increasing attention, due to the need to handle and process an increasing amount of textual data and available documents online. A * Work partly done as an intern at AI2. 1 Our code is available at https://github.com/ aviclu/peekacross. which we split into context documents (2) and a held-out document (3), we select the most salient sentence (4) that is used for generating a question-answer pair (5).\nThen, we pre-train a model by generating the proper answer and the salient sentence, given the question and the context documents (6).\nnumber of prominent applications that are concerned with aggregating information from multiple texts are multi-document summarization (Fabbri et al., 2019; Zhao et al., 2020) , query-focused multidocument summarization (Xu and Lapata, 2020; Pasunuru et al., 2021a) , and multi-hop question answering (Yang et al., 2018; Welbl et al., 2018) . These tasks remain challenging mostly since existing NLP models are designed to handle single texts, rather than processing multiple documents at once (Caciularu et al., 2021) .\nEarly solutions for multi-text processing were task-specific and used complex architectures that were difficult to generalize across different multidocument tasks (Liu and Lapata, 2019; Wang et al., 2020; Ginzburg et al., 2021) . Efficient LMs (Tay et al., 2021; Beltagy et al., 2020) recently demonstrated that by simply concatenating multiple documents into a single sequence, the transformer can offload the goal of identifying and connecting relevant information between the documents. Recently, it was suggested that these long-context LMs can be equipped with new pre-training objectives to enable them to process multiple documents more effectively (Caciularu et al., 2021; Xiao et al., 2022; Yasunaga et al., 2022) .\nThese pre-trained models demonstrated state-ofthe-art performance on a variety of multi-document downstream tasks, and outperformed underlying LMs and task-specific architectures. Such models are often pre-trained using a dataset where each instance is a set of related documents (e.g., news articles all discussing a specific event), which facilitates modeling of cross-text relationships. Existing multi-document pre-training objectives involve unmasking tokens in a document (Caciularu et al., 2021) , or generating a salient masked sentence (Zhang et al., 2020; Xiao et al., 2022) , encouraging the model to recover missing information using other documents. While successful, these models are either limited to classification tasks (Caciularu et al., 2021) or primarily designed for summarization (Zhang et al., 2020; Xiao et al., 2022) .\nIn this work, we propose a novel pre-training objective that supports both short and long text generation, resulting in a versatile and general multidocument language model. In particular, we hypothesize that using questions and answers involving multiple documents can encourage the model to better learn and incorporate both fine-grained information (by asking questions about core information units in a specific sentence) as well as coarsegrained cross-document relationships required to generate a long text such as a summary. We show that this approach holds not only for summarization, but for other multi-document downstream tasks as well.\nDuring the pre-training of existing multidocument language models, the goal is to unmask spans (for encoder-only models) or generate masked textual spans (for encoder-decoder models) under a multi-document context. To that end, multiple concatenated sequences of related documents are fed during pre-training, thus requiring a large number of sets of related documents for an effective pre-training phase (Hoffmann et al., 2022) . In a variety of existing multi-document benchmarks, such as multi-document summarization, only small to medium-scale document clusters are readily available. These are acquired either automatically with lexical similarity and retrieval (Fabbri et al., 2019) or semi-automatically (Gu et al., 2020) , but generally, this process requires a substantial amount of human effort for filtering instances and generating high quality corpora.\nBy employing a novel multi-document question-answer generation procedure, we propose an effective method for expanding the multi-document pre-training corpora. Our approach allows us to provide multiple views for every single cluster of documents, thereby artificially increasing the pretraining data size (in terms of number of instances) via augmentation. To expose the model to a variety of contexts and diversify the pre-training data, we propose to generate multiple pairs of questions and answers and condition them on a subset of the documents' cluster. We select a salient sentence in one held-out document and then employ a recent parser to generate a high-quality question-answer pair about one predicate in the selected sentence, using a systematic semantically-oriented approach (Klein et al., 2022) . This new multi-document pre-training objective challenges the model to generate both the answer to the question as well as the salient sentence, while discarding the held-out document or parts of it (see Figures 1, 2 for illustration). This procedure exposes the model to a variety of contexts -a question and a different subset of the documents in the cluster per instance, in contrast to prior methods that provide only a single view of the cluster. Our contributions are summarized below:\n\u2022 A new pre-training approach for multidocument modeling, formulated as a crossdocument question answering task, further directing the LM to model cross-text relationships, focusing on both fine-and coarsegrained information. \n\nRelated Work\nLong-context efficient text generation transformers (Tay et al., 2021 (Tay et al., , 2022) ) extend earlier transformer models (Vaswani et al., 2017) for processing long sequences, often using a sparse self-attention architecture. Examples include the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) , and LongT5 (Guo et al., 2022) . These models demonstrated that single-text approaches be can adapted to multi-document tasks by concatenat-ing multiple documents into a single sequence and processing them using their sparse attention patterns. They sparsify the full self-attention matrix of transformers by using a combination of a localized sliding window (called local attention), as well as a global attention pattern on a few specific input locations. LED is build upon the BART model (Lewis et al., 2020) by using additional positional embeddings and global attention weights, and introduces the global attention mode that operates over pre-selected tokens. LongT5 extends the T5 model (Raffel et al., 2020 ) by using a similar technique introduced in the ETC and BIGBIRD models (Ainslie et al., 2020; Zaheer et al., 2020) , relieving the requirement to manually select global tokens by automatically globalizing the aggregated representations of groups of tokens.\nFurther strategies have been proposed for increasing these models' abilities in multi-document tasks. The Cross-Document Language Model (CDLM) (Caciularu et al., 2021) suggested pretraining a Longformer-encoder (Beltagy et al., 2020) over sets of related documents, and showed superior performance results over several multidocument tasks. Following this methodology, the authors of LinkBERT (Yasunaga et al., 2022 ) used a similar approach, but utilized Wikipedia's hyperlinks in order to curate informative pairs of linked documents for LM pre-training.\nIn order to adopt the multi-document pretraining approach for sequence-to-sequence tasks, PRIMERA (Xiao et al., 2022) , which is built on top of the Longformer encoder-decoder model (LED), selected salient sentences within clusters of related documents using a pyramid estimation approach, resembling the method presented for pre-training the single-document PEGASUS model (Zhang et al., 2020) . While this work is the closest to ours, it was pre-trained to generate masked salient sentences without any control, which makes the model potentially hallucinate while generating text, while our model uses a controlled QA-based objective. Furthermore, unlike these works, our method generates significantly more data then used to pre-train PRIMERA, which is possible to obtain by the singledocument QA generation approach. Our QA pretraining formulation allows us to generate multiple contexts per document cluster.\nAnother related line of work includes methods that incorporate large-scale QA-generated data for pre-training LMs (He et al., 2020; Jia et al., 2022 ;\n\n(a) The held-out document is discarded from the context (c) The held-out document is included in the context, but the answer in the anchor sentence is masked (b) The held-out document is included in the context, but the anchor sentence is masked\nFigure 2 : A schematic of our pretraining data modes. The salient sentence which is used for QA generation is colored in yellow. (a) The context does not include the held-out document, therefore this mode is the most challenging. (b) The held-out document is present in the context, but the salient sentence used for the QA generation is masked (red). (c) The held-out document is present in the context, but the answer span within the salient sentence is masked (red). Huber et al., 2022) . These works hypothesize and show that pre-training by utilizing generated QA data can encourage contextual representations to encode useful semantic information for other non-QA downstream tasks. Inspired by that, we conjecture that LMs can strongly benefit from infusing QA during pre-training in the multi-document setup, for adding an additional signal for modelling cross-text relationships.\n\nAugmenting the Multi-Document Pre-training objective\nIn this section, we provide the required steps for compiling the pre-training dataset for QAMDEN.\nWe next elaborate on the details of the data creation and provide analysis of the resulted corpus.\nRecent works have shown that for text summarization, pre-training LMs to generate a \"summarylike\" sequence, termed pseudo summary, inherently provides gains over general-purpose pre-trained LMs (PEGASUS, PRIMERA; Zhang et al., 2020; Xiao et al., 2022) . The data in which the PEGASUS and PRIMERA models were pre-trained on was constructed using the Gap Sentence Generation (GSG) method, which suggests masking highly-ranked salient sentences, where salience is pre-determined by a sentence-scoring method of interest. Particularly, in PEGASUS, GSG has been adopted as its pre-training objective, where some sentences in a single document are masked in the input and the model is tasked to generate them.\nFormally, for each sentence s i in a given input document D, PEGASUS computes its salience score based on its ROUGE score (Lin, 2004) w.r.t the rest of the sentences within the document (D/{s i }), i.e. Score(s i ) = ROUGE(s i , D/{s i }). Intuitively, \u2026Pokemon Sword and Shield might have already been announced, but we now know there's another new Pokemon game on the way from DeNA\u2026 QASem QA generation (Klein et al., 2022) Q1: What might been announced? A1: Pokemon Sword and Shield. (Answer length: 4) Q3: Who knows something? A: We. (Answer length: 1) Q2: Where does someone know something? A: On the way from DeNA. (Answer length: 5) Contextualization (Pyatkin et al., 2022) Q: Where did we know there's another new Pokemon game? A: On the way from DeNA.\n\nSelected\nFigure 3 : A schematic of the process of QA generation using QASEM (Klein et al., 2022) and the contextualization model from Pyatkin et al. (2021) . This is an actual sample that was created and used for pre-training QAMDEN, where the document is taken from New-SHead (Gu et al., 2020) .\nthis metric assigns a high score to the sentences that have a high overlap and share more lexical information with the rest of the sentences in the document, thus assigning high scores to prominent sentences. PRIMERA has generalized this notion to support the multi-document setup, by applying a GSG variant over a cluster of related documents.\nCross-Document GSG. We propose augmenting the GSG technique to formulate a cross-document question answering pre-training objective for multidocument tasks, instead of the existing pseudo summary generation methods. Our approach supports identification of both fine-and coarse-grained information as we describe below, and results in a substantially larger amount of pre-training examples compared to the preceding methods.\nFormally, we are given a cluster of related documents S = D 1 , D 2 , . . . , D |S| in a corpus C. Our cross-document (CD) GSG salience score for the i th sentence within the k th document in the set (s i k ), is defined by its ROUGE score w.r.t the rest of the sentences within the document (D k /{s i k }) as well as the other documents (S/D k ), i.e. CD-GSG-Score(s i k ) = ROUGE(s i k , S/{s i k }). Then, for every document k, following Zhang et al. (2020) ; Xiao et al. (2022) we select the top-scored sentence s * k , and then we use this sentence to generate a pair of a question and an answer.\nGenerating Cross-Document QAs. For generating the cross-document questions and their answers, we employ QASEM, a recent semantic parsing framework for question generation (Klein et \nfor k \u2190 1 to |Sn| do 4 s * k \u2190 arg max i CD-GSG-Score(s i k ); 5 (q * k , a * k ) \u2190 QASEM(s * k ); 6 t * k = [a * k , s * k ] # target text; 7 D \u2190 D \u222a {([Sn/D k , q * k ] , t * k )} # (a); 8 D \u2190 D \u222a {([Sn/ {s * k } , q * k ] , t * k )} # (b); 9 D \u2190 D \u222a {([Sn/ {a * k } , q * k ] , t * k )} # (c); 10 Return D;\n2022). 2 QASEM intended soliciting a manageable, discrete account of information in a text for the sake of building natural language semantic representations. It automatically labels each verbal predicate-argument relation with a questionanswer pair, where a natural language question represents a semantic role, while the answers correspond to the arguments that appear in the input text. QASEM is thus an appealing approach since it is capable of generating multiple high-quality questions given a sentence. We apply QASEM over the sentences withing the pre-training data in order to generate question-answer pairs, and then apply the model from Pyatkin et al. (2021) which transforms the question into a more natural and clear form, with contextualized arguments (see example in Figure 3 ). In order to resemble a summarization task where the generated text is typically long, we select the question-answer pair with the longest argument produced by QASEM. Formally, QASEM(\u2022) receives a sentence s * k as an input, and produces question-answer pair (q * k , a * k ), where a * k is the longest among the generated answers. See a detailed example and full description in App. A.1.\nConsidering the question-answer pair, our goal is to encourage the LM to generate the correct answer as well as the salient sentence in a multi-document context in order to learn cross-text relationships.\nData Generation Process. In order to facilitate the construction of a multi-document context, we propose three different modes, each one is responsible for uncovering information by using different contexts. For all the modes, we first generate a QA pair out of the most salient sentence in the held-out document.\n(a) Excluding the source document. In this mode we disregard the held-out document D k from the context S n given to the model, i.e, S n /D k . Hence, the model is tasked to predict the answer without having access to the source document at all, and is restricted to observe only the other documents in the set. Thus, this mode is considered as the most challenging one.\n(b) Masking the salient sentence. In this mode, the source salient sentence is masked, i.e, S n / {s * k }. The model has access to the surrounding context of the masked sentence in the held-out document, as well as the other documents in the set.\n(c) Masking the answer. In this mode, only the answer span within the salient sentence is masked, i.e, S n / {a * k }. The model has access to the surrounding salient sentence, as well as all the documents in the set.\nAs part of the new pre-training process of our novel multi-document model, we append the question after the context and instruct the model to generate an answer followed by its salient sentence, i.e., output = \u27e8answer\u27e9, \u27e8sentence\u27e9, inspired by Bohnet et al. (2022) . Generating the salient sentence introduces a copying mechanism (allows the model to also learn to copy information from the source directly) as well as allowing longtext generation, which is crucial for summarization downstream tasks (Zhang et al., 2020) , as well as outperforming a model which was pre-trained for generating the answer solely -according to the ablations study, this setup yields the best performance results ( \u00a74.4). In the pre-training evaluation phase, the held-out set was split and the loss was measured separately for each mode of the data. As expected, we observed that the loss for (a) was significantly higher than those for the other modes, with (a)\u227b(b)\u227b(c) ranking highest. The procedure for generating the pre-training data is summarized in Algorithm 1 and Figure 2 .\nThe resulted pre-training corpus. We applied our procedure over the NewSHead corpus (Gu et al., 2020) , which consists of a set of related documents per instance. This is the exact same pre-training corpus used also by our main baseline PRIMERA (Xiao et al., 2022) \n\nExperimental Setup and Results\nThis section presents experiments conducted to evaluate QAMDEN, as well as the the ablations and baselines we used. For the intrinsic evaluation we evaluated the models over multi-document QA tasks. For extrinsic evaluations we considered the multi-document abstractive summarization task.\nModel Implementation Details Following Xiao et al. ( 2022), we use the large-sized Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) for our model initialization. The length limits of input and output are 4096 and 1024, respectively. 3 Following the Huggingface implementation (Wolf et al., 2020) , we set the sliding window size to 1024 for local attention in the encoder part.\nSimilar to the PRIMERA model (Xiao et al., 2022) , when concatenating the documents and the question, we add a special document separator token (<doc-sep>) between the documents to signal to the model to be aware of the document boundaries. We also assign the global attention mode to these tokens which enables the model to share information across documents (Caciularu et al., 2021) . For further hyperparameter and pre-training execution details, see App. B.\n\nMulti-Document Question Answering\nMulti-document QA is the task of generating the correct answer, given a set of related multiple documents. For several multi-document QA benchmarks, models are often tasked to implicitly solve multiple sub-tasks or follow intermediate steps, such as comprehending the question, filtering out distracting documents in the context, and stitching pieces of information across the relevant documents (Geva et al., 2021; Caciularu et al., 2022) . Recall that QAMDEN was pre-trained over a automatically generated multi-document QA dataset. Hence, as a preliminary assessment, we first investigate QAMDEN's performance over two multi-document QA benchmarks, HopotQAdistractor (Yang et al., 2018) and WikiHop (Welbl et al., 2018) (see more details of the datasets in App. C.1), and compare to other models that were pre-trained using underling un-masking objectives.\nFine-Tuning Format. To follow our pre-training scheme, we append the question to the context and fine-tune the model to generate the correct answer. We use the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) and PRIMERA (Xiao et al., 2022) as the baselines, for assesing the contribution of our pre-trainig format. Confirmed by Beltagy et al. (2020) , we found out that appending the question: and context: prefixes before the question and the context tokens, respectively, resulted in better performance.\nBaselines. We compare QAMDEN (447M parameters) against a set of strong long-context transformer baselines, including LED (447M parameters) (Beltagy et al., 2020) , PRIMERA (447M parameters) (Xiao et al., 2022) , 4 and LongT5-xl (3B parameters) 5 (Guo et al., 2022 ) (see \u00a72). 6 Results. The results on multi-document QA are shown in Table 2 . We adopted the F1 and Exact Match (EM) evaluation metrics corresponding to the original works. Our QAMDEN outperforms both PRIMERA, LED, and LongT5, confirming that our pre-training data and input format are beneficial for both capturing cross-document relationships (QAMDEN\u227bLED) as well as exploiting both context and question (QAMDEN\u227bPRIMERA).\n\nMulti-Document Summarization (MDS)\nThis task aims at generating a summary for a given set of topically-related documents. Inherently, end-Model F1 EM HotpotQA LED (Beltagy et al., 2020) 65.8 50.6 LongT5-xl (Guo et al., 2022) 66.1 50.9 PRIMERA (Xiao et al., 2022) 65 Results. Tables 3 and 4 present the evaluation results over the Multi-News and Multi-XScience datasets, respectively. Following previous MDS works, we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.2 for details). For a fair comparison, we include the results of PRIMERA as well as the results of the previous state-of-the-art methods (Pasunuru et al. (2021b) and Lu et al. (2020) , for Multi-News and for Multi-XScience, respectively), and LED (Beltagy et al., 2020) . As shown in the results tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, especially on the Multi-News dataset, clearly demonstrating its consistent advan- (Beltagy et al., 2020) 47.4 20.7 23.7 LongT5-xl (Guo et al., 2022) 47.4 20.7 23.7 PRIMERA (Xiao et al., 2022) 49.9 21.1 25.9 QAMDEN 50.9 23.1 27.2 tage. This excludes the results for Multi-XScience where QAMDEN slightly underperforms the prior work and LongT5. An explanation which Xiao et al. (2022) points refers to the fact that the clusters in Multi-XScience have less overlapping information compared to the corpus we used, attributed to the use of abstracts as the input documents in Multi-XScience. In addition, LongT5 advantage over QAMDEN is attributed to significantly larger number of parameters of LongT5-xl.\n\nQuery-Focused Multi-Document Abstractive Summarization\nThe task of Query-focused Multi-Document Summarization (QMDS) aims at generating a summary from a set of documents, that answers a specific given query. Unlike MDS, QMDS tries to solve more realistic query-based scenarios, since it suggests summarizing only predefined salient information of interest that best answers the query. Since we proposed pre-trainng under the multi-document question answering setup, we posit that QAMDEN might be effective for QMDS.\nWe consider the datasets constructed by Pasunuru et al. (2021a), QMDSCNN and QMDSIR (see more details of the datasets in App. C.3) as well as their strong baseline, and include also the results of PRIMERA and LED.\nBaselines. Similar to the previous experiments, we compare QAMDEN against LED, PRIMERA, LongT5-xl. In addition, we consider also the baseline from Pasunuru et al. (2021a) . 37.9 16.4 35.2 LED (Beltagy et al., 2020) 32.3 14.3 30.9 LongT5-xl (Guo et al., 2022) 35.5 15.9 34.3 PRIMERA (Xiao et al., 2022) 36 Results. Tables 5 and 6 present the evaluation results over the QMDSCNN and QMDSIR datasets, respectively. Following MDS tasks and Pasunuru et al. (2021a) , we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.3 for details). As shown in the tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, clearly demonstrating its consistent advantage over the baselines.\n\nAblation Study\nData Generation. We next turn to a broad ablation study, for assessing our configuration and design choices across our suggested pipeline. First, we show the advantage of combining the three proposed data modes, rather than using a subset of them. We evaluate all the resulted models by fine-tuning them over HopotQA-distractor ( \u00a74.1), Multi-XScience ( \u00a74.2), and QMDSIR ( \u00a74.3). For HopotQA-distractor we report the Exact Match (EM) score, and for the summarization tasks we report the ROUGE-1 (R-1) score.\nBaselines. We pre-train QAMDEN for 100k steps, for using every subset of the set of the set (superset) of modes {(a), (b), (c)} (all its possible combinations) of the generated pre-training data modes presented in \u00a73. Note that our QAMDEN model is referred to as using all the modes, i.e., For QA we used the EM score, and for MDS and QMDS we used the ROUGE-1 score.\nResults. Figure 4 shows the ablation results. In all tasks, pre-training using all modes yields the best results. Among all modes, mode (c) appears to be the most effective for QA, since this is an extractive QA task, and mode (c) provides data in this format. Mode (a) excels at the summarization tasks, attributed to their abstractive nature as well as the requirement of all the documents for generating appropriate summaries.\nInput Format We repeat the previous experiment and ablate the pre-training input format according to the multiple different formats, and compare to the model pre-training format described in \u00a73 (with the same pre-training data): without questions, with random question, with random context document, with prefixes, placing the question before the context, with question filtering, and without generating the salient sentence. Additionally, we assess the choice of QASEM as our questionanswer generation module by using the generators from Jia et al. ( 2022) and Khashabi et al. (2022) . Finally, we also include the results of PRIMERA, which was further pre-trained for additional 300k steps (fine-tuning LED for 400k steps in total), for a fair comparison to QAMDEN ablated models. See full details regarding all the ablations in App. D.\nResults. Overall, our QAMDEN model outperforms the ablation models on most of the tasks, which a significant margin.\nPre-training the model without any questions during or using random questions, negatively impacts the results of downstream tasks. An impor- tant function of the question is to facilitate the model's ability to generate the appropriate answer and the source sentence. This aligns with the findings from Caciularu et al. (2021) , who showed that pre-training with random documents rather than related ones is sub-optimal. The use of question and context prefixes for positioning input appears to be helpful for QA, but is inferior when applied to summarization tasks due to its unique format, which is well suited for QA but seems to generalize harder for other setups. When the question is placed before the context, performance slightly decreases over query-based tasks, while maintaining the same results for summarization (where the question location is irrelevant).\nUsing question filtering is found to harm the downstream results of QAMDEN, in accordance to other QA-based pre-training prior works (Jia et al., 2022) .\nPre-training without generating the attributed source sentence introduces a significant flow to the model, particularly for the summarization downstream tasks. As mentioned before, generating longer sequences, as well as teaching the model to copy text, is beneficial for summarization tasks.\nApplying a different question generator rather then QASEM yields inferior results overall, since the other generators produce open-ended questions and answers which are more prone to errors, while QASEM utilizes an existing span in the context as the answer. In addition, QASEM generated local questions, which allows QAMDEN to focus on the fine-grained details, and not only the coarsegrained information in the multi-document context.\nWhen PRIMERA is pre-trained with 400k steps (to match QAMDEN's number of further pretraining steps), it underperforms QAMDEN and even fails to add any significant improvements over its 100K checkpoint, possibly due to the small amount of pre-training data it contains. \n\nComparison with Large Language Models\nIn order to get insights into how QAMDEN compares with state-of-the-art Generalist Large Language Models (LLMs), we provide a small comparison with two capable models, GPT-3.5 turbo (Ouyang et al., 2022) and GPT-4 8 (OpenAI, 2023) (including the 8k input length version) evaluated on the zero-shot setting.\nFor a fair comparison, we used the same context window size of 4K tokens for all models (and up to 8k for GPT-4 8k). Due to the fact that multidocument tasks involve processing long sequences, the cost of API calls is significant for a comprehensive evaluation across all datasets. Therefore, we only evaluate on a sample of 200 instances from the multi-news dataset (see prompting details in App. E). Table 8 depicts the results. We observe that QAMDEN significantly outperforms both GPT-3.5 and GPT-4 models, though the performance of GPT-4 and GPT-3.5 is comparable. We leave more comprehensive comparisons with LLMs to future work.\nWe further assessed QAMDEN through manual comparison against PRIMERA, GPT-3.5, and GPT-4 8k. NLP graduate students were shown summaries for a given topic from the three systems and QAMDEN in arbitrary order, along with a corresponding reference summary. Following (Ernst et al., 2022) , participants were asked to rank the systems based on Content (overlap with the reference), Readability (the readability of a summary), Grammaticality (avoiding grammar errors), and Non-Redundancy (avoiding repetitions), and we extract the pairwise results out of the rankings (see (Ernst et al., 2022) for further details). In App. F, we provide several examples to system summaries and their corresponding reference summaries.\nThe results of this study are presented in Table 9 . Under each evaluation criterion, it indicates the percentage of cases where QAMDEN was preferred over both baselines. QAMDEN was favored in all cases except for grammatical errors and readability (which corresponds to the Reinforcement Learning from Human Feedback phase of the GPT models).\n\nConclusions\nIn this work, we present a novel pre-training scheme for multi-document tasks. First, our approach suggests to augment the existing multidocument pre-training objectives into a crossdocument question answering task. Second, we generate high-quality large-scale QA pre-training data using a controlled generation approach, in which each QA pair originates from a salient sentence in one of the documents in the set.\nDuring pre-training, we task the the Longformer Encoder-Decoder (LED) model to generate the answer and the salient sentence on the basis of the remaining context. This objective encourages the LED model to elicit cross-document relationships, and stitch pieces of information across the input documents, which are relevant for performing multi-document tasks. The resulted model QAMDEN shows significant performance improvements compared to prior models under extensive experimentation over multiple challenging multidocument summarization and QA datasets.\nFuture work can extend the ideas in this work for equipping decoder-only large LMs with crossdocument modeling using our proposed method, also in the setup of in-context learning and prompt tuning. We foresee that our method should be significant specifically for retrieval-augmented language modeling setups (Izacard et al., 2022) , where there is a use of related documents as an outsourced external non-parametric knowledge source. Finally, the use of a single document in order to trigger cross-document relationships, as firstly introduced in this work, might be further investigated.\n"}
{"question": "What type of neural parameterization is used in the paper for the LCFRS?", "evidence": "  We now describe a parameterization of LCFRS-2 that combines a neural parameterization with tensor decomposition, which makes it possible to scale LCFRS-2 to thousands of nonterminals. ", "options": ["A. Recurrent neural networks (RNN)", "B. Convolutional neural networks (CNN)", "C. Transformer-based neural networks", "D. Tensor decomposition-based neural parameterization "], "answer": "D", "content": "\nIntroduction\nUnsupervised parsing aims to induce hierarchical linguistic structures given only the strings in a language. A classic approach to unsupervised parsing is through probabilistic grammar induction (Lari and Young, 1990) , which learns a probabilistic grammar (i.e., a set of rewrite rules and their probabilities) from raw text. Recent work has shown that neural parameterizations of probabilistic contextfree grammars (PCFG), wherein the grammar's rule probabilities are given by a neural network over shared symbol embeddings, can achieve promising results on unsupervised constituency parsing (Kim et al., 2019; Jin et al., 2019 Jin et al., , 2021;; Yang et al., 2021b Yang et al., , 2022)) .\nHowever, context-free rules are not natural for modeling discontinuous language phenomena such as extrapositions, cross-serial dependencies, and Code: https://github.com/sustcsonglin/TN-LCFRS. wh-movements. Mildly context-sensitive grammars (Joshi, 1985) , which sit between context-free and context-sensitive grammars in the classic Chomsky-Sch\u00fctzenberger hierarchy (Chomsky, 1959; Chomsky and Sch\u00fctzenberger, 1963 ), 1 are powerful enough to model richer aspects of natural language including discontinuous and non-local phenomena. And despite their expressivity they enjoy polynomial-time inference algorithms, making them attractive both as cognitively plausible models of human language processing and as targets for unsupervised learning.\nThere are several weakly equivalent formalisms for generating the mildly context-sensitive languages which might serve as potential targets for grammar induction: tree adjoining grammars (Joshi, 1975) , head grammars (Pollard, 1985) , combinatory categorial grammars (Steedman, 1987) , and linear indexed grammars (Gazdar, 1988) . In this paper we work with linear context-free rewriting systems (LCFGS, Vijay-Shanker et al., 1987) , which generalize the above formalisms and are weakly equivalent to multiple context-free grammars (Seki et al., 1991) . Derivation trees in an LCFRS directly correspond to discontinuous constituency trees where each node can dominate a non-contiguous sequence of words in the yield, as shown in Fig. 1 .\nWe focus on the LCFRS formalism as it has previously been successfully employed for supervised discontinuous constituency parsing (Levy, 2005; Maier, 2010; van Cranenburgh et al., 2016) . The complexity of parsing in a LCFRS is O(\u2113 3k |G|), where \u2113 is the sentence length, k is the fan-out (the maximum number of contiguous blocks of text that can be dominated by a nonterminal), and |G| is the grammar size. While polynomial, this is too high to be practical for unsupervised learning on real-world data. We thus restrict ourselves to LCFRS-2, i.e., binary LCFRS with fan-out two, which has been shown to have high coverage on discontinuous treebanks (Maier et al., 2012) . Even with this restriction LCFRS-2 remains difficult to induce from raw text due to the O(\u2113 6 |G|) dynamic program for parsing and marginalization. However Corro (2020) observe that a O(\u2113 5 |G|) variant of the grammar that discards certain rules can still recover 98% of real world treebank constituents. Our approach uses with this restricted variant of LCFRS-2 (see Sec 2.2). Finally, following recent work which finds that that overparameterizing deep latent variable models is beneficial for unsupervised learning (Buhai et al., 2020; Yang et al., 2021b; Chiu and Rush, 2020; Chiu et al., 2021) , we scale LCFRS-2 to a large number of nonterminals by adapting tensor-decomposition-based inference techniques-originally developed for PCFGs (Cohen et al., 2013; Yang et al., 2021b Yang et al., , 2022)) -to the LCFRS case.\nWe conduct experiments German and Dutchboth of which have frequent discontinuous and non-local language phenomena and have available discontinuous treebanks-and observe that our approach is able to induce grammars with nontrivial performance on discontinuous constituents. Rabanser et al., 2017) to decompose the 3D binary rule probability tensor T \u2208 R m\u00d7m\u00d7m as,\n\nApproach\nT = r q=1 u q \u2297 v q \u2297 w q ,\nwhere u q , v q , w q \u2208 R m , r is the tensor rank (a hyperparameter), and \u2297 is the outer product. Letting U, V, W \u2208 R r\u00d7m be the matrices resulting from stacking all u q , v q , w q , Cohen et al. ( 2013) give the following recursive formula for calculating the inside tensor \u03b1 \u2208 R (\u2113+1)\u00d7(\u2113+1)\u00d7m for a sentence of length \u2113:\n\u03b1 L i,j = V \u03b1 i,k , \u03b1 R j,k = W \u03b1 k,j , \u03b1 i,j = U T j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k .\nHere 2021b) and further pre-compute matrices J = V U T , K = W U T to rewrite the above recursive formula as:\n\u03b1 L , \u03b1 R \u2208 R (\u2113+1)\u00d7(\u2113+1\n\u03b1 L i,j = J\u03b1 \u2032 i,j ,\u03b1 R i,j = K\u03b1 \u2032 i,j \u03b1 \u2032 i,j = j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k\nwhere \u03b1 \u2032 \u2208 R (n+1)\u00d7(n+1)\u00d7r is an auxiliary inside score tensor. The resulting complexity of this approach is O(\u2113 3 r + \u2113 2 r 2 ), which is smaller than O(\u2113 3 r + \u2113 2 mr) when r \u226a m, i.e., in the setting with a large number of nonterminals whose probability tensor is of low rank. In this paper we adapt this low rank neural parameterization to the LCFRS case to scale to a large number of nonterminals.\n\nRestricted LCFRS\nIn an LCFRS, a single nonterminal node can dominate a tuple of strings that need not be adjacent in the yield. The tuple size is referred to as the fanout. We mark the fan-out of each non-leaf node in Fig. 1 . The fan-out of an LCFRS is defined as the maximal fan-out among all its nonterminals, and influences expressiveness and parsing complexity. For a binary LCFRS (i.e., LCFRS with derivation rules that have at most two nonterminals on the right hand side) with fan-out k, the parsing complexity for a sentence of length \u2113 is O(\u2113 3k ). 2 In this paper we work with binary LCFRS with fanout 2 (Stanojevi\u0107 and Steedman, 2020, LCFRS-2) , which is expressive enough to model discontinuous constituents but still efficient enough to enable practical grammar induction from natural language data. This choice is also motivated by Maier et al. (2012) who observe that restricting the fan-out to two suffices for capturing a large proportion of discontinuous constituents in standard treebanks. 3 However, LCFRS-2's inference complexity of O(\u2113 6 |G|) is still too expensive for practical unsupervised learning. We thus follow Corro (2020) and discard all rules that require O(\u2113 6 ) time to parse, which reduces parsing complexity to O(\u2113 5 |G|). 4 Formally, this restricted LCFRS-2 is a 6-tuple G = (S, N 1 , N 2 , P, \u03a3, R) where: S is the start symbol; N 1 , N 2 are a finite set of nonterminal symbols of fan-out one and two, respectively; P is a finite set of preterminal symbols; \u03a3 is a finite set of terminal symbols; and R is a set of rules of the following form (where M \u225c N 1 \u222a P):\nS(x) \u2192 A(x) A \u2208 N 1 A(xy) \u2192 B(x)C(y) A \u2208 N 1 , B, C \u2208 M A(yxz) \u2192 B(x)C(y, z) A \u2208 N 1 , B \u2208 M, C \u2208 N 2 A(x, y) \u2192 B(x)C(y) A \u2208 N 2 , B, C \u2208 M\n2 A binary CFG is thus a special case of a binary LCFRS with fan-out one, and parsing in this case reduces to the classic CKY algorithm.\n3 For instance, Stanojevi\u0107 and Steedman (2020) report that LCFRS-2 can cover up to 87% of the gold discontinuous constituents in the NEGRA treebank. We refer readers to Table 1 of Corro (2020) for more details. 4 These correspond to rules (d), (i), (j), (k), and (l) in Figure 3 of Corro (2020) .\n\nItem form:\n[A, i, j]: fan-out-1 node A spanning [i, j)\n[A, i, j, k, n]: fan-out-2 node A spanning [i, j), [k, n) Axioms: [A, i, i + 1], 0 \u2264 i < \u2113 + 1, A \u2208 N 1 Goals: [S, 0, n] Deductive rules: [B, i, k] [C, k, j] [A, i, j] A(xy) \u2192 B(x)C(y) i < k < j 1a [B, i, j] [C, m, n] [A, i, j, m, n] A(x, y) \u2192 B(x)C(y) i < j < m < n 1b [B, m, n] [C, i, m, n, j] [A, i, j] A(yxz) \u2192 B(x)C(y, z) i < m < n < j 2a [B, i, k] [C, k, j, m, n] [A, i, j, m, n] A(xy, z) \u2192 B(x)C(y, z) i < k < j < m < n 2b [B, k, j] [C, i, k, m, n] [A, i, j, m, n] A(yx, z) \u2192 B(x)C(y, z) i < k < j < m < n 2c [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, xz) \u2192 B(x)C(y, z) i < j < m < k < n 2d [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, zx) \u2192 B(x)C(y, z) i < j < m < k < n 2e\nTable 1 : Chart parsing algorithm described in the parsing-asdeduction framework. Here \u2113 is the sentence length and we use interstice indices (not word indices) as in Corro (2020) .\nA(xy, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(yx, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, xz) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, zx) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M T (w) \u2192 w, T \u2208 P, w \u2208 \u03a3.\nHere A(x) indicates that A has a fan-out 1; A(x, y) indicates that A has a fan-out 2 and x and y are nonadjacent contiguous strings in the yield of A. \n\u2192 B(x)C(y, z) above. B is a fan-out-1 node whose yield is x = w i \u2022 \u2022 \u2022 w k\u22121 and C is a fan-out-2 node whose first span is y = w k \u2022 \u2022 \u2022 w j\u22121 and whose second span is z = w m \u2022 \u2022 \u2022 w n\u22121 .\nA is the parent node of B, C, and inherits the yields of B and C, where x is concatenated with y to form a contiguous span and z is a standalone span.\nParsing. Table 1 gives the parsing-asdeduction (Pereira and Warren, 1983 ) description of the CKY-style chart parsing algorithm of our restricted LCFRS-2.\n\nTensor decomposition-based neural parameterization\nWe now describe a parameterization of LCFRS-2 that combines a neural parameterization with tensor decomposition, which makes it possible to scale LCFRS-2 to thousands of nonterminals.\nLet\nm 1 = |N 1 |, m 2 = |N 2 |, p = |P|, and m = m 1 + p.\nThe rules involving A \u2208 N 1 on the left hand side are 1a and 2a , whose probabilities can be represented by 3D tensors\nC 1 \u2208 R m 1 \u00d7m\u00d7m and D 1 \u2208 R m 1 \u00d7m\u00d7m 2 . For A \u2208 N 2 , the relevant rules are 1b , 2b , 2c , 2d , 2e , whose proba- bilities can be represented by 3D tensors C 2 \u2208 R m 2 \u00d7m\u00d7m and D 3 , D 4 , D 5 , D 6 \u2208 R m 2 \u00d7m\u00d7m 2 . We stack D 3 , D 4 , D 5 , D 6 into a single 4D tensor D 2 \u2208 R m 2 \u00d7m\u00d7m 2 \u00d74\nto leverage the structural similarity of these rules. Since these tensors are probabilities, we must have\nEQUATION\nEQUATION\nTensor decomposition. To scale up the LCFRS-2 to a large number of nonterminals, we first apply CPD on all the binary rule probability tensors,\nC 1 = r 1 \u22121 q=0 U 1 :,q \u2297 V 1 :,q \u2297 W 1 :,q C 2 = r 2 \u22121 q=0 U 2 :,q \u2297 V 2 :,q \u2297 W 2 :,q D 1 = r 3 \u22121 q=0 U 3 :,q \u2297 V 3 :,q \u2297 W 3 :,q D 2 = r 4 \u22121 q=0 U 4 :,q \u2297 V 4 :,q \u2297 W 4 :,q \u2297 P :,q\nwhere U :,q denotes the q-th column of U . The dimensions of these tensors are\nU 1 \u2208 R m 1 \u00d7r 1 , V 1 , W 1 \u2208 R m\u00d7r 1 , U 2 \u2208 R m 1 \u00d7r 2 , V 2 \u2208 R m\u00d7r 2 , W 2 \u2208 R m 2 \u00d7r 2 , U 3 , W 3 \u2208 R m 2 \u00d7r 3 , U 4 , W 4 \u2208 R m 2 \u00d7r 4 , V 3 \u2208 R m\u00d7r 3 , V 4 \u2208 R m\u00d7r 4\n, and P \u2208 R 4\u00d7r 4 . Here r 1 , r 2 , r 3 , r 4 are the ranks of the tensors that control inference complexity. To ensure these factorizations lead to valid probability tensors, 1), we additionally impose the following restrictions: (1) all decomposed matrices are non-negative; (2) P, V i , W i are column-wise normalized where i \u2208 {1, 2, 3, 4};\n(3) \u2200i, j U 1 ij + k U 2 ik = 1; and (4) \u2200i, j U 3 ij + k U 4 ik = 1.\nIt is easy to verify that Eq. 1 and 2 are satisfied if the above requirements are satisfied.\nRank-space dynamic programming. For unsupervised learning, we need to compute the marginal likelihood of a sentence p(w 1 w 2 \u2022 \u2022 \u2022 w \u2113 ). We give the rank-space dynamic program (i.e., the inside algorithm) for computing p(w\n1 w 2 \u2022 \u2022 \u2022 w \u2113 ) in this tensor decomposition-based LCFRS-2 in App. A.\nThe resulting complexity is dominated by O(\u2113 5 r 4 + \u2113 4 (r 3 +r 4 )(r 2 +r 4 )). We thus set r 4 to a very small value, which greatly improves runtime.\nParameterization. Following prior work on neural parameterizations of grammars (Jiang et al., 2016; Kim et al., 2019) , we parameterize the component matrices to be the output of neural networks over shared embeddings.\nThe symbol embeddings are given by: E 1 \u2208 R m\u00d7d where the first m 1 rows correspond to fanout-1 nonterminal embeddings and the last p rows are the preterminal embeddings; E 2 \u2208 R m 2 \u00d7d for the fan-out-2 nonterminal embedding matrix; r \u2208 R d for the start symbol embedding. We also have four sets of \"rank embeddings\"\nR 1 \u2208 R r 1 \u00d7d , R 2 \u2208 R r 2 \u00d7d , R 3 \u2208 R r 3 \u00d7d\n, and R 4 \u2208 R r 4 \u00d7d . Given this, the entries of the U, V, W matrices are given by,\nU o ij \u221d exp{(R o j ) \u22a4 f o U (E 1 i )}, o \u2208 {1, 2} U o ij \u221d exp{(R o j ) \u22a4 f o U (E 2 i )}, o \u2208 {3, 4} V o ij \u221d exp{(R o j ) \u22a4 f o V (E 1 i )}, o \u2208 {1, 2, 3, 4} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 1 i )}, o \u2208 {1, 2} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 2 i )}, o \u2208 {3, 4} where f o U , f o V , f o W are one-layer ReLU MLPs with output size d. U o , V o , W o\nare normalized according to the requirements described in the previous subsection. We share the parameters of the following MLP pairs:\n(f 1 U , f 2 U ), (f 3 U , f 4 U ), (f 1 V , f 3 V ), (f 2 V , f 4 V ), (f 1 W , f 3 W ), (f 2 W , f 4 W )\nas they play similar roles (e.g., f 1 V and f 3 V are both applied to left children). For the D 2 tensor we also require the matrix P \u2208 R 4\u00d7r 4 , and this is given by P \u22a4 = f P (R 4 ), where f P is a one-layer residual network with output size 4 that is normalized via a softmax across the last dimension.\nFinally, for the starting and the terminal distributions we have\ns = f s (r), Q = f Q (E 1 m 1 :\n), which results in s \u2208 R m 1 (i.e., the probability vector for rules of the form S \u2192 A) and Q \u2208 R p\u00d7v (i.e., probability matrix for rules of the form T (w) \u2192 w). Here E 1 m 1 : is the last p rows of E 1 , and f s and f Q are residual MLPs with softmax applied in the last layer to ensure that s and Q are valid probabilities.\nDecoding. While the rank-space inside algorithm enables efficient computation of sentence likelihoods, direct CKY-style argmax decoding in this grammar requires instantiating the full probability tensors and is thus computationally intractable. We follow Yang et al. (2021b) and use Minimal Bayes Risk (MBR) decoding (Goodman, 1996) . This procedure first obtains the posterior probability of each span's being a constituent via the inside-outside algorithm (which has the same complexity as the inside algorithm). Then, these posterior probabilities are used as input into CKY in a grammar that only has a single nonterminal. The complexity of this approach is thus independent of the number of nonterminals in the original grammar, and takes O(\u2113 5 ). This strategy can be seen as finding the tree that has the largest number of expected constituents (Smith and Eisner, 2006) . See App. A for details.\n\nEmpirical Study\nData. We conduct experiments with our Tensor decomposition-based Neural LCFRS (TN-LCFRS) on German and Dutch, where discontinuous phenomena are more common (than in English). For German we concatenate TIGER (Brants et al., 2001) and NEGRA (Skut et al., 1997) as our training set, while for Dutch we use the LASSY Small Corpus treebank (van Noord et al., 2013) . The data split can be found in App. B.1. For processing we use disco-dop 5 (van Cranenburgh et al., 2016) and discard all punctuation marks. We further take the most frequent 10,000 words for each language as the vocabulary, similar to the standard setup in unsupervised constituency parsing on PTB (Shen et al., 2018 (Shen et al., , 2019;; Kim et al., 2019) .\nGrammar size. To investigate the importance of using a large number of latent variables (which has previously been shown to be helpful for structure induction (Buhai et al., 2020; Yang et al., 2021b )), we train TN-LCFRSs of varying sizes. We first choose the number of preterminals |P| \u2208 {45, 450, 4500} and set the number of fan-out one and fan-out two nonterminals to be\n|N 1 | = |N 2 | = 1 3 |P|.\nThe rank of the probability tensors are set to r 1 = r 3 = 400, r 2 = r 4 = 4, and the dimensionality of the Baselines. Our baselines include: the neural PCFG (N-PCFG) and the compound PCFG (C-PCFG) (Kim et al., 2019) , which cannot directly predict discontinuous constituents 6 but still serve as strong baselines for overall F1 since the majority of spans in these treebanks are continuous; and their direct extensions, neural LCFRS (N-LCFRS) and compound LCFRS (C-LCFRS), which do not employ the tensor-based low-rank factorization. These non-low-rank models have high computational complexity and hence we set |P| = 45 for these models. When |P| = 4500, we also compare against the tensor decompositional-based neural PCFG (TN-PCFG) from Yang et al. (2021b) .\nEvaluation. We use unlabeled corpus-level F1 to evaluate unsupervised parsing performance, reporting both overall F1 and discontinuous F1 (DF1). For all experiments, we report the mean results and standard deviations over four runs with different random seeds. See App. B.2 for further details.\n\nMain results\nTable 2 shows the main results. With smaller grammars (|P| = 45), we find that both neural/compound LCFRSs have lower F1 than their PCFG counterparts, despite being able to predict discontinuous constituent spans. On the other hand, TN-LCFRS achieves better F1 than N-LCFRS even though it is a more restricted model (since it assumes that the rule probability tensors are of low rank), showing the benefits of parameter sharing through low rank factorizations. As we scale up TN-LCFRSs with |P| \u2208 {45, 450, 4500} we observe continuous improvements in performance, with TN-LCFRS 4500 achieving the best F1 and DF1 on all three datasets. These results all outperform trivial (left branching, right branching, and random tree) baselines.\nAs an upper bound we also train a supervised model with TN-LCFRS 4500 . 7 We also show the maximum possible performance with oracle binary trees with this optimal binarization. While the discontinuous F1 of our unsupervised parsers are nontrivial, there is still a large gap between the unsupervised and supervised scores (and also between the supervised and the oracle scores), indicating opportunities for further work in this area.\n\nAnalysis\nRecall by constituent label. Table 3 shows the recall by constituent tag for the different models averaged over four independent runs. Overall the unsupervised methods do well on noun phrases (NP), prepositional phrases (PP) and proper nouns (PN), with some of the models approach the supervised baselines. Verb phrases (VP) and adjective dynamic programming to sum out all possible nonterminals for each node, resulting in the joint log probability of unlabeled binarized tree and sentence. This was then maximized during training. As for the oracle bound, we emphasize that the gold trees are nonbinary while our model can only predict binary trees. Approximation error. Approximation error in the context of unsupervised learning arises due to the mismatch between the EM objective (i.e., log marginal likelihood) and structure recovery (i.e., F1), and is related to model misspecification (Liang and Klein, 2008) . Figure 2 (left column) plots the training/dev perplexity as well as the dev F1/DF1 as a function of the number of epochs. We find that larger grammars result in better performance in terms of both perplexity and structure recovery, which ostensibly indicates that the unsupervised objective is positively correlated with structure induction performance. However, when we first perform supervised learning on the log joint likelihood and then switch to unsupervised learning with log marginal likelihood (Figure 2 , right), we find that while perplexity improves when we switch to the unsupervised objective, structure induction performance deteriorates. 8 Still, the difference in F1 before and after switching to the unsupervised objective is less for larger models, confirming the benefits of using larger grammars.\n\nEven more restricted LCFRS formalisms.\nThere are even more restricted versions of LCFRSs which have faster parsing (e.g. O(\u2113 3 ), O(\u2113 4 )) but 8 It is worth noting that the phenomenon of mismatch between log marginal likelihood objective and parsing accuracy is quite common in unsupervised grammar induction (and latent variable modeling approaches to structured induction more generally). Many previous works have observed this phenomenon, e.g., Merialdo (1994) in the context of HMMs, and Johnson et al. (2007) and Liang and Klein (2008) in the context of PCFGs. This is partially attributed to the fact that generative grammars often make some unreasonable independence assumptions to make the training process tractable, which does not fully comply with the true generative process of human languages and their underlying structures. 45.4 0.9 44.5 0.5\n\nModel\nTable 5 : Ablation studies on the German (TIGER) treebank.\ncan still model discontinuous constituents. In the supervised case, these restricted variants have been shown to perform almost as well as the more expressive O(\u2113 5 ) and O(\u2113 6 ) variants (Corro, 2020) .\nIn the unsupervised case however, we observe in Table 5 that disallowing O(\u2113 5 ) rules ( 2b , 2c , 2d , 2e ) significantly degrades discontinuous F1 scores. We posit that this phenomena is again related to empirical benefits of latent variable overparameterization-while in theory it is possible to model most discontinuous phenomena with more restricted rules, making the generative model more expressive via \"overparameterizing\" in rule expressivity space (i.e., using more flexible rules than is necessariy) empirically leads to better performance.\nParameter sharing. As shown in Table 5 , it was important to share the symbol embeddings across the different rules. Sharing the parameters of the MLPs as described in Sec. 2.3 was also found to be helpful. This highlights the benefits of working with neural parameterizations of grammars which enable easy parameter sharing across rules that share symbols and/or have similar shapes.\nQualitative analysis. In Fig. 3 , we show some examples trees in German. For each sentence, we show the gold, TN-LCFRS 4500 , and TN-PCFG 4500 trees. In the first sentence, the crossing dependency occurs due to the initial adverb (\"So\")'s being analyzed as a dependent of the non-finite verb phrase at the end of the sentence which occurs due to German V2 word order. Our parser correctly predicts this dependency, although the subject NP (which itself is correctly identified) has the wrong internal structure. relative clause a part of the non-finite verb complex, which does not conform to the annotation guidelines but resembles an alternative analysis that has been proposed for extraposed relative clauses (Baltin, 1983) . Sentence initial adverbs in the context of auxiliary verb constructions and right-extraposed relative clauses describe two common instances of discontinuous phenomena in German. Wh-questions constitute another potential class of discontinuous phenomena; however, these are not treated as discontinuous in TIGER/NEGRA. See App. D for more examples trees (including on Dutch).\n\nRelated work\nMildly context-sensitive grammars. Given the evidence against the context-freeness of natural language (Shieber, 1985) , mildly context-sensitive grammars such as tree adjoining grammars were thought to be just flexible (but still constrained) enough to model natural language (Joshi, 1985) . Prior work on inducing mildly context-sensitive grammars has generally focused on combinatory categorial grammars (Bisk and Hockenmaier, 2012, 2013) , and we are unaware of any work on in-ducing LCFRSs from observed yields alone. Our work is also related to the rich line of work on supervised discontinuous parsing (Kallmeyer and Maier, 2010; Maier et al., 2012; Maier, 2015; Corro, 2020; Vilares and G\u00f3mez-Rodr\u00edguez, 2020; Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez, 2020 , 2021 , 2023) , though we are unaware of any prior work on unsupervised discontinuous parsing.\nNeural grammars. Early work on probabilistic approaches to grammar induction was largely negative (Lari and Young, 1990; Carroll and Charniak, 1992) . However, recent work has shown that neural parameterizations of classic grammars can greatly improve structure induction. Our work adds to the line of work on neural parameterizations of dependency models (Jiang et al., 2016; Han et al., 2017; He et al., 2018; Yang et al., 2020) , context-free grammars (Kim et al., 2019; Jin et al., 2019; Zhu et al., 2020; Yang et al., 2021a) , and synchronous grammars (Kim, 2021; Wang et al., 2022; Friedman et al., 2022) . Neural parameterizations make it easy to share parameters and condition on additional side information (images/audio/video) which has shown to be particularly useful for multimodal grammar induction (Zhao and Titov, 2020; Jin and Schuler, 2020; Su et al., 2021; Hong et al., 2021; Zhang et al., 2021) .\nScaling latent variable models. Buhai et al. (2020) study the empirical benefits of overparameterization in learning latent variable models. Other works have explored parameterizations of latent variable models that make it especially amenable to scaling (Chiu and Rush, 2020; Chiu et al., 2021; Yang et al., 2021b Yang et al., , 2022)) . Relatedly, Peharz et al. (2020) and Liu et al. (2022) show the benefits of scaling probabilistic circuits (Choi et al., 2020) .\n\nConclusion\nThis work studied unsupervised discontinuous constituency parsing with mildly context-sensitive grammars, focusing on the formalism of linear context-free rewriting systems. By using a tensor decomposition-based neural parameterization of linear context-free rewriting systems, our approach was able to induce grammars that had nontrivial discontinuous parsing performance on German and Dutch. Whether even more expressive grammars will eventually lead to models learn linguistically meaningful structures and are at the same time competitive with pure neural language models (as a language model) remains an open question.\n"}
{"question": "With respect to predictions made by both HUBERT and SSE-TUNE on SLURP-IC's test set, which of the following conclusion is false?", "evidence": " To better understand the semantic information captured by SSE, we study predictions made by both HUBERT and SSE-TUNE (HUBERT) on SLURP-IC's test set. We find HUBERT errors are made primarily between intents within the same or similar domains (e.g. calendar_set vs calendar_query).The performance bottleneck lies with distinguishing finer-grained in-domain intents. Table 7 shows that SSE-TUNE is better at differentiating finergrained intents. SSE-TUNE's misclassifications come primarily from errors made by its ASR-U bridge component. As seen in Table 8 , the ASR-U WER of incorrect predictions made by HUBERT is much lower than that of incorrect predictions made by SSE-TUNE.   ", "options": ["A. The performance bottleneck lies with distinguishing finer-grained in-domain intents.", "B. The ASR-U WER of incorrect predictions made by HUBERT is much higher than that of incorrect predictions made by SSE-TUNE. ", "C. SSE-TUNE is better at differentiating finergrained intents.", "D. HUBERT errors are made primarily between intents within the same or similar domains "], "answer": "B", "content": "\nIntroduction\nRealizing artificial intelligence (AI) that can understand and respond to spoken language is a north star for many speech and natural language processing (NLP) researchers. A particularly effective framework for this is the encoder-decoder architecture, where an encoder represents input audio signals as high-dimensional embeddings and a decoder converts said embeddings to outputs for different downstream tasks. Benchmarks for such systems include spoken language understanding, where intent, named entities, or slot values are predicted from input utterances (Yang et al., 2021; Bastianelli et al., 2020; Shon et al., 2022) , and spoken question answering, where the start and end frames of an input audio passage answering an input audio question are predicted (Lin et al., 2022a) .\nA particularly notable setup of the encoderdecoder framework is the universal representation setup (Yang et al., 2021) , where a shared selfsupervised speech encoder is pretrained upstream once and frozen for all downstream tasks, then a different lightweight decoder is fine-tuned on each downstream task. This setup is appealing for building speech systems as maintaining a separate large specialized model for every task is not computationally efficient. The universal representation setup has been widely adopted in other areas of research, such as computer vision (Goyal et al., 2019; Ericsson et al., 2021) and NLP (Rogers et al., 2020; Qiu et al., 2020) , and production when there are many downstream tasks or domains (Molino et al., 2019) . The current state-of-the-art speech encoders under this setup are W2V2 and HUBERT (Yang et al., 2021; Baevski et al., 2020; Hsu et al., 2021) , which are transformer-based models trained with self-supervised learning (SSL) on raw audio and have achieved impressive performance on various tasks.\nRecently, analytical works found SSL speech encoders capture primarily acoustic, not semantic, information (Pasad et al., 2021) . Thus, researchers proposed end-to-end systems (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) that introduce semantic information through large language models (LLMs), such as ROBERTA (Liu et al., 2019) or BART (Lewis et al., 2019) , which are pretrained to capture language semantics (Clark et al., 2019) . This is typically accomplished by the pipeline approach (Bastianelli et al., 2020) , which passes audio input through the SSL speech encoder, then bridge module, then LLM. The bridge module converts speech encoder embedding outputs into LLM token inputs (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) .\nUnsupervised ASR models (ASR-U) (Liu et al., 2020b; Baevski et al., 2021; Liu et al., 2022) have also seen recent success. The state-of-the-art ASR-U model uses generative adversarial networks (GANs) (Goodfellow et al., 2020) to generate text transcription from input audio (Liu et al., 2022) .\nCurrent works combining SSL speech encoders and LLMs do not satisfy the universal representation framework, as they either (1) rely on ASR data on the downstream task, which is expensive to collect and maintain, (2) are not lightweight, requiring training the whole system end-to-end, or (3) are not general, as they do not consider a wide variety of downstream tasks (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) . Similarly, ASR-U was proposed for speech recognition and the focus is not improving SSL speech encoders (Baevski et al., 2021; Liu et al., 2022) .\nWe propose introducing Semantics into Speech Encoders, SSE, a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. Concretely, SSE adopts the pipeline approach to obtain semantic embeddings, with an ASR-U bridge connector to extract information from LLMs. As ASR-U is inherently noisy, SSE introduces attention residual connection (He et al., 2016; Vaswani et al., 2017) between the speech encoder and LLM. SSE also efficiently aligns the LLM with the speech encoder through adapter modules (Houlsby et al., 2019) . SSE improves W2V2 (Baevski et al., 2020) and HUBERT (Hsu et al., 2021) on 3 SLU tasks across 3 datasets, all under the universal representation setup. SSE also outperforms state-of-the art no-ASR method, DUAL (Lin et al., 2022a) , in SQA.\nWhile recent works use ASR-U to augment existing speech encoders with phoneme-level LLMs (Feng et al., 2022; Meng et al., 2022; Shi et al., 2022; Hsu et al., 2022) , subword-level LLMs contain much more pertinent and measurable semantic information (Clark et al., 2019) . Other works in SQA rely on clustering to assign audio frames to frequent subword tokens, but this requires heavy finetuning on the downstream task (Lin et al., 2022a) .\nTo the best of our knowledge, we are the first to propose a task-agnostic SSL speech encoder which directly interfaces with subword-based LLMs, unblocking many other applications and future work in this domain. To this end, attention residual con-nections and adapters are essential to successfully extracting semantic information from noisy intermediary transcriptions. We summarize our contributions below:\n\u2022 We propose using ASR-U components to augment SSL speech encoders for generating subword tokens with semantic information.\n\u2022 The augmented SSL speech encoders can be connected with powerful LLMs seamlessly and yields state-of-the-art performance under the universal representation setup.\n\u2022 We show attention residual connections and adapters are essential to combining and aligning speech and text encoders.\n2 Related Works 2.1 Self-Supervised Speech Encoders SSL speech encoders (Liu et al., 2020a; Chung et al., 2020a; Ling and Liu, 2020; Liu et al., 2021 Liu et al., , 2020c;; Chung et al., 2019; Baevski et al., 2019; Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Qian et al., 2022; Zhang et al., 2022) are trained to learn and reconstruct pooled clustered representations of input audio from the original audio. The intuition for this objective comes from linguistics, where speech can be broken down into phoneme groups, where different chunks of input audio represent different phoneme groups.\nW2V (Schneider et al., 2019) trains a convolutional neural network model to reconstruct the quantized cluster representations. W2V2 (Baevski et al., 2020) uses transformers and a discrete codebook quantization module. HUBERT (Hsu et al., 2021) improves W2V2 by disentangling the clustering and SSL objectives and using a BERT-style encoder (Devlin et al., 2018) . The speech processing universal performance benchmark (SU-PERB) (Yang et al., 2021; Lin et al., 2022b; Tsai et al., 2022) shows SSL speech encoders are the most effective method for solving multiple downstream tasks with minimal fine-tuning. A recent analytical work finds SSL speech encoders successfully encode acoustic information, but lack semantic information (Pasad et al., 2021) . In response, CONTENTVEC (Qian et al., 2022) propose disentangling the speaker and semantic content of audio via an SSL objective. SPEECHLM (Zhang et al., 2022) propose training a multi-modal speech and text encoder.\n\nLarge Language Models\nIn contrast to speech encoders, pretrained LLMs are shown to capture rich semantic information (Clark et al., 2019) . These methods optimize variants of the masked language modeling (MLM) objective to train a large transformer model. BERT (Devlin et al., 2018) uses MLM to learn a transformer encoder. ROBERTA (Liu et al., 2019) introduces dynamic masking and a larger text corpus. BART (Lewis et al., 2019) supports generative modeling and adds a denoising objective, making it less susceptible to noisy text inputs. LONG-FORMER (Beltagy et al., 2020) is pretrained for long documents by increasing the document length limit during pretraining. LLMs have been successfully integrated with speech models for specific semantic tasks (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) , but not under the universal representation framework.\n\nTask-Specific Speech Models\nTask-specific SLU systems outperform generic SSL speech encoders typically by using a LLM. These systems rely on ASR data to reliably interface the LLM. LUGOSCH (Lugosch et al., 2019) trains a LSTM bridge module to convert audio features into phonemes then text. CTI's (Seo et al., 2022) bridge module uses ASR logits to compute a weighted average of token embeddings. In addition to improving the bridge module, other works attempt to also distill LLM embeddings into speech representations (Chung et al., 2020b; Cha et al., 2021; Kim et al., 2021; Agrawal et al., 2022) . For optimizing targeted metrics, researchers have also experimented with reinforcement learning (Rao et al., 2021) . While combinations of these methods achieve impressive performance, they do not satisfy the universal representation setup.\n\nUnsupervised ASR\nRecent work show the viability of unsupervised speech recognition. W2V2-U (Baevski et al., 2021) accomplished this by running Principal Component Analysis (PCA), k-means clustering, and mean pooling to convert W2V2 (Baevski et al., 2020) features into phoneme-granularity features, then trains a GAN model to output phoneme text from the post-processed model (Baevski et al., 2021) . The state-of-the-art method for phoneme-level unsupervised ASR is W2V2-U2.0 (Liu et al., 2022) which directly trains a CNN to output phonemes from W2V2 features and uses a reconstruction loss to tie the input audio with corresponding generated text. Both methods use WFSTs to decode the phonemes into raw text. While there have been preliminary attempts (Feng et al., 2022; Meng et al., 2022) to use W2V2-U2.0 with phoneme language models 1 , we are the first to combine it with semantically-rich subword-based LLMs.\n\nAdapters\nAdapters are intermediary layers added to a large pretrained encoder. Adapter weights are learned during fine-tuning while the rest of the pretrained model is frozen. Adapters serve the dual purpose of efficient fine-tuning and preventing overfitting. First used by computer vision researchers (Rebuffi et al., 2017) , adapters now enjoy much success in the natural language processing community by efficiently tuning LLMs (Houlsby et al., 2019) . In particular, the multilingual speech translation community found that adapters can effectively align SSL speech encoders and LLMs for spoken translation tasks (Li et al., 2020; Le et al., 2021) .\n\nProposed Method\nWe propose to introduce semantics into SSL speech encoders by using ASR-U to interface with LLMs. Section 3.2 describes how to use ASR-U to link a speech encoder with a LLM. Section 3.3 describes how to combine both acoustic and semantic information and deal with ASR transcriptions errors. Finally, Section 3.4 describes how to align LLMs with the speech encoder for downstream tasks.\n\nProblem Setting\nFollowing the universal representation framework (Yang et al., 2021) , our model consists of a large speech encoder, E : X \u2192 Z, mapping input audio, X \u2208 X , to embeddings, Z \u2208 Z, and a light-weight task decoder,\nD \u03c9 : Z \u2192 Y \u03c9 , mapping embeddings to downstream task outputs, Y \u03c9 \u2208 Y \u03c9 .\nThe speech encoder, E, is pretrained once, then shared on all downstream tasks. The task decoder, D \u03c9 , is fine-tuned on its respective task, \u03c9 \u2208 \u2126.\nDuring fine-tuning, the majority of model weights are frozen. This ensures the model can be efficiently stored and deployed.\nDuring pretraining, the speech encoder is trained on unlabelled audio, X \u2208 X , and unlabeled text, T u \u2208 T u . During finetuning, the model is trained on the labelled downstream dataset, (X, Y \u03c9 ) \u2208 X \u00d7 Y \u03c9 . Notice, costly labelled ASR data is not required during pretraining or finetuning.\n\nUnsupervised Semantic Representation as a Bridge\nTo incorporate semantic information into SSL speech encoders, E : X \u2192 Z, we wish to leverage subword-based LLMs, M : S \u2192 Z, that capture language semantics (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Beltagy et al., 2020) . The major challenge is the mismatch of input spaces. Speech encoders take raw audio as input, X \u2208 X . LLMs take subword tokens as input, S \u2208 S. SSE uses W2V2-U2.0 (Liu et al., 2022) as a bridge module (Seo et al., 2022) , B : Z \u2192 S, to convert speech encoder embedding output into LLM subword tokens in a pipelined approach,\nE SSE = E \u2022 B \u2022 M.\nFollowing W2V2-U2.0, the bridge module, B uses a GAN (Goodfellow et al., 2020) We also add an upsampling layer, U : Z \u2192 Z to make the sequence length of the LLM output match the speech encoder output, such that E and E SSE share the same output space.\nWe choose the 15th layer of the W2V2 (Baevski et al., 2020) as our speech encoder, as the last layers overfit the self-supervised training objective hence providing worse acoustic representations (Fan et al., 2020; Baevski et al., 2021; Pasad et al., 2021) . We choose BART (Lewis et al., 2019) as our LLM, as it is trained to denoise noisy input subword tokens, and we expect the bridge module to introduce some noise. We call this version of our model SSE-BASE. A depiction can be found in Figure 1a .\n\nCombining Semantics and Acoustics with Residual Attention\nWe hypothesize certain tasks may require more acoustic information than others. to implicitly transcribe parts of the input speech, a primarily acoustic task. Since the pipelined model may suffer from transcription errors introduced by ASR-U, naively using the pipelined approach introduces an information bottleneck at the bridge module. Hence, we propose adding a residual connection (He et al., 2016) between SSE-BASE and the speech encoder, E. This can be done in two ways: (1) upsampling semantic embeddings and concatenating with speech embeddings, Z = [Z E ||U(Z M )], or (2) using multihead attention (Vaswani et al., 2017) to merge the two embeddings, Z =\n[Z E ||MHA(Z E , Z M , Z M )],\nwhere Z E \u2208 Z is the output of the W2V2L15 (Baevski et al., 2020) and Z M \u2208 Z is the output of BART (Lewis et al., 2019) . The former is a simpler but more naive method. The latter is more effective as the attention layers are able to learn the alignment between speech and semantic embeddings. Notice, (2) introduces more learnable parameters to the finetuning-step, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder.\n\nAligning Pretrained Text Model with Adapters\nInspired by works from speech translation (Li et al., 2020; Le et al., 2021) , we hypothesize that the LLM can easily be adapted for speech tasks through the use of adapters. We adopt the general recipe for adapters, where an adapter (Houlsby et al., 2019) , composed of a LayerNorm and 2-layer ReLU neural network, is added to the end of each feed forward layer in the LLM and finetuned on downstream tasks. This introduces additional parameters to finetuning, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder. We call the model using both residual attention and adapters SSE-TUNE, and outline it in Figure 1b .\n\nExperiments 4.1 Dataset\nTo show the effectiveness of introducing semantics into speech encoders, we evaluate 3 SLU tasks, intent classification (IC), slot filling (SF), and named entity recognition (NER), and SQA \n\nSpoken Language Understanding\nTo show SSE improves SSL speech encoders, we augment two state-of-the art speech encoders under the universal representation setup: W2V2 and HUBERT. Following prior works that found intermediary layers of W2V2 contain better representations (Pasad et al., 2021; Baevski et al., 2021) , we consider the 15th layer and the last layer of W2V2, named W2V2L15 and W2V2L24 respectively. As mentioned in Section 3, we show 2 versions of our model, SSE-BASE and SSE-TUNE. The former uses the pipelined approach to connect W2V2L15 with BART (Lewis et al., 2019) with no additional modifications. The latter introduces an attention residual connection and learnable adapters to combine acoustics and semantics together and align the LLM with the speech encoder respectively. We either connect the residual connection to the output of W2V2L15, yielding SSE-TUNE (W2V2L15), or to the output of HU-BERT, yielding SSE-TUNE (HUBERT).\nTo show the importance of using LLMs, we compare against 2 very recent approaches for improving SSL speech encoders without LLMs, SPEECHLM (Zhang et al., 2022) and CON-TENTVEC (Qian et al., 2022) . As HUBERT-BASE was used as the base speech encoder by both baselines, we also provide results where SSE-TUNE is used to augment HUBERT-BASE.\n\nSpoken Question Answering\nTo show the effectiveness of SSE, we compare it against DUAL (Lin et al., 2022a) , the state-ofthe-art SQA model which does not use ASR data. While both SSE and DUAL obtain frame-level tokens from speech input, SSE uses ASR-U to obtain its tokens, whereas DUAL uses clustering. As a result, SSE's output tokens exists in the LLM's existing vocabulary, whereas DUAL's output tokens does not. Hence, DUAL must retrain the LLM on its output tokens.\nWe compare DUAL to the closest analogous SSE model, which is SSE-BASE but with adapter layers, SSE-BASE (ADAP). Similar to DUAL, both methods modify the LLM weights. Unlike DUAL, SSE-BASE (ADAP) is lightweight, tuning only around 10% of the total parameters. To produces framelevel predictions, we remove the upsampling layer from SSE-BASE (ADAP). We choose W2V2L15 as our speech model and BART as our LLM, as it is robust to ASR errors.\nWe also show a PIPELINE model, which trains a W2V2 model on ASR data and a LONGFORMER LLM on text-only question answering data. It is worth noting that since evaluation is based on the frame-level, SSL speech encoders are not a baseline since they operate at the audio level.\n\nDecoder Setup\nTo satisfy the universal representation setup, we adopt lightweight SLU decoders from SU-PERB (Yang et al., 2021) is sum pooling followed by a multilayer perceptron classifier trained with cross entropy loss. For the SF and NER tasks, the decoder is recursive neural network (RNN) that transcribes input audio into text. The decoder identifies named entities or slot values by surrounding them with named special tokens and is trained with connectionist temporal classification loss. For SQA, we adopt the same decoder as DUAL (Lin et al., 2022a) , which is a linear layer classifying each subword embedding as the start or end or neither of an answer span.\n\nImproving SSL Speech Encoders\nAs seen in Table 2 , SSE significantly improves the SLU performance of both W2V2 and HU-BERT, confirming that including semantic information drastically improves existing SSL speech encoder performance. Specifically, SSE-TUNE (W2V2L15) improves W2V2L15 on all tasks. SSE-TUNE (HUBERT) improves HUBERT on 3 out of 4 tasks, and is the best performing model overall. Comparing SSE-TUNE with SSE-BASE shows residual attention and adapters effectively counteracts bridge module transcription errors. The relative performance gain for IC is more than SF or NER. Unlike IC, both SF and NER require the speech encoder to transcribe identified audio snippets, and transcription is a primarily acoustic task. Hence SF and NER require less semantic information than IC. Nevertheless, combining both acoustic and semantic information, as done by SSE-TUNE, provides the most consistent performance improvement, since the skip connection can learn which type of information is more needed.\n\nImportance of LLMs\nAs seen in Table 2 , SSE-TUNE (HUBERT-BASE) outperforms alternative approaches augmenting speech encoders, SPEECHLM (HUBERT-BASE) and CONTENTVEC (HUBERT-BASE). Unlike these alternative approaches, SSE-TUNE incorporate information from LLMs, which we found to be very beneficial for capturing semantic information as they are carefully pretrained objectives on large amounts of unlabelled text data.\nIt is noteworthy that SSE-TUNE is a general framework which can augment any speech encoder of our choice, including SPEECHLM and CON-TENTVEC. Similarly, SSE-TUNE can directly integrate new LLMs without costly pretraining. We leave incorporating such encoders into SSE-TUNE as future work.\n\nSpoken Question Answering\nAs seen in Table 3 , SSE outperforms recent unsupervised clustering-based approaches, DUAL. In contrast to DUAL's HUBERT cluster tokens, SSE's ASR-U tokens are better aligned with LLMs and share the same space. Thus, SSE can better utilizes pretrained LLMs. Furthermore, SSE does not require carefully tuning the number of HUBERT cluster counts, as the vocabulary size of the LLM is fixed and consistent with ASR-U.\n\nChoice of Language Model\nWe find subword-based LLMs contain more information than phoneme-based LLMs (Clark et al., 2019) . We empirically verify this by replacing our subword-based LLM, BART (Lewis et al., 2019) , with popular character-based LLM, ByT5 (Xue et al., 2022) , and phoneme-based LLM, T5lephone (Hsu et al., 2022) in SSE-BASE. As seen in Table 4 , the subword-based LLM perform the best as each subword token is more semantically meaningful than a phoneme or character. We believe T5lephone outperforms the Byt5 as it has better robustness to ASR-U errors. Overall, subword-based LLMs are the best choice for embedding semantic information in transcribed text.\n\nResidual Attention and Adapters\nTo more carefully analyze the affect of residual attention and adapters in SSE-TUNE, we run experiments on all SLU datasets with and without each component. We denote these two design choices as (ResAtt) and (Adap) respectively. As seen in Table 4, both components provide ample performance improvement over SSE-BASE.\nWe also try the naive residual connection approach described in Section 3.3 by directly concatenating the LLM upsampled semantic embeddings to the speech embeddings. We call this approach SSE-BASE (RES). This method is less effective than SSE-BASE (RESATT) as it does not learn how to align speech and semantic embeddings, but improves SSE-BASE, further validating our hypothesis that merging acoustic and semantic information is beneficial.\nAs seen in parameter breakdown for the SSE-TUNE (W2V2L15) model in Table 1 , the number of new learnable parameters introduced by (Re-sAtt) and (Adap) are unsubstantial compared to the size of the lightweight downstream decoder. Specifically, the downstream task decoder accounts for 9.60% of the total model parameters. SSE-TUNE introduces only 10.47% more parameters than SSE-BASE during fine-tuning and 0.91% to the total model parameter count, but often provides significant performance improvement.\n\nComparison with Supervised ASR Methods\nTo quantify the effect of transcription errors introduced by the bridge module, we compute the word error rate (WER) of the bridge connector in SSE-TUNE, and compare it against standard W2V2 supervised ASR models (Baevski et al., 2020) trained on 10 minutes, 100 hours, and 960 hours of labeled ASR data. indicating the effectiveness of the bridge module.\nOn SLURP and SLUE, the relative drop in WER (> 20%) is substantially more than the relative drop in downstream performance (< 5%), verifying SSE-TUNE's tolerance to noisy transcriptions. The robustness to ASR errors come from our choice of LLM, BART, which is trained to handle noisy inputs, residual connection to acoustic embeddings, and LLM alignment with adapters.\n\nComparison to Specialized SLU Models\nTo better quantify the performance improvement introduced by SSE, we compare against 2 specialized SLU models that do not abide by the universal representation framework: Kaldi+HerMiT, which is a pipelined Kaldi ASR (Povey et al., 2011) and HerMiT NLU (Vanzo et al., 2019) model reported in the SLURP paper (Bastianelli et al., 2020) , and CTI (Seo et al., 2022) , which is an end-to-end pipelined W2V2 (Baevski et al., 2020) ASR and ROBERTA (Liu et al., 2019) NLU model. To the best of our knowledge, CTI is the state-of-the-art SLU model. In addition to unlabelled text, unlabelled audio, and downstream data, both Kaldi+HerMiT and CTI require 40 hours of downstream SLURP ASR data (Bastianelli et al., 2020) . Kaldi+HerMiT requires an additional 24,000 hours of ASR data (Povey et al., 2016) . CTI requires an additional 960 hours of ASR data (Panayotov et al., 2015) . Neither use lightweight fine-tuning. Thus, such specialized SLU models are less general, more expensive, and require much more data. As seen in Table 6 , SSE helps bridge the gap between tailormade models and more practical SSL speech encoders. We believe ASR-U errors plays a major role in the remaining gap, as the ASR-supervised Kaldi+HerMiT and CTI models have WER of 16.20% and 16.67% respectively, compared to A mix-up is when the model either misclassifies label \"A\" as \"B\" or misclassifies label \"B\" as \"A\". For each mix-up, we compute the percentage of less mistakes made by SSE-TUNE (HUBERT) than HUBERT. For example, SSE-TUNE (HUBERT) misclassifies calendar_set as calendar_query or vice-versa 20% less frequently than HUBERT. The \"general-quirky\" label is assigned to Out-of-Distribution inputs.\nSSE's ASR-U bridge with a WER of 51.51%.\n\nError Analysis\nTo better understand the semantic information captured by SSE, we study predictions made by both HUBERT and SSE-TUNE (HUBERT) on SLURP-IC's test set. We find HUBERT errors are made primarily between intents within the same or similar domains (e.g. calendar_set vs calendar_query).\nThe performance bottleneck lies with distinguishing finer-grained in-domain intents. Table 7 shows that SSE-TUNE is better at differentiating finergrained intents. SSE-TUNE's misclassifications come primarily from errors made by its ASR-U bridge component. As seen in Table 8 , the ASR-U WER of incorrect predictions made by HUBERT is much lower than that of incorrect predictions made by SSE-TUNE. When ASR-U returns resonable transcriptions (typically <50% WER), SSE-TUNE can correctly classify inputs that HUBERT cannot. Hence, the effectiveness of SSE is tightly coupled with the effectiveness of ASR-U.\n\nRepresentation Visualization\nTo better see the impact of including semantic representations, we visualize the pooled audio snippet embedding for intent classification on SLURP-IC using t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton, 2008) or incorrectly (\u2717). We denote the number of pairs belonging to each subset, in the thousands, in parentheses. denote the ground truth label of each audio snippet by the color of its pooled embedding. As seen in Figure 2 , the clusters produced by semantic embeddings are more spread out and better separated than those produced by just acoustic speech embeddings, indicating that SSE introduces new semantic information that existing speech encoders lack.\n\nConclusion\nWe presented a compelling case for introducing semantics into SSL speech encoders and an effective method of doing so. Our approach boosts the performance of existing speech encoders on multiple SLU and SQA tasks and datasets. We provide reasoning for what tasks may benefit more or less from incorporating semantics. Furthermore, our approach is task agnostic and can augment any existing SSL speech encoder. With SSE-TUNE, we show merging acoustic and semantic information and effectively aligning LLMs to the speech encoder on downstream tasks can further boost performance with minimal parameter overhead. As it can generalize to many downstream tasks, SSE provides an important step towards AI that can understand and respond to spoken language.\n"}
{"question": "Which is our approach\u2019s contribution?", "evidence": "  To this end, in this paper, we propose a novel TwO-stage Model-based rEtrieval approach, TOME (as illustrated in Figure 1 ), which makes two major technical contributions.\n\u2022 Firstly, we suggest using tokenized URLs (or URIs) as text identifiers, which are widely available for web pages or Wikipedia pages 1 . By using URL-based identifiers, the tokenized symbols are well aligned with the vocabulary of the generative PLM, thereby enhancing the generative capacity of the PLM. URLs are typically comprised of normal text, as opposed to manually or randomly constructed identifiers. As a result, such an identifier design can be used to help alleviate the gap between pre-training and fine-tuning.  there is a discrepancy between training and inference in the single-model generative architecture. While most existing studies incorporate multi-task learning (Tay et al., 2022) and auxiliary pre-training tasks (Zhou et al., 2022b) to model both documents and queries during training, the model only processes queries dur- ing inference, resulting in a gap between the training and inference stages.\n \n ", "options": ["A. Employ manually and randomly constructed identifiers as generation targets. ", "B. Incorporate multi-task learning and auxiliary pre-training tasks to model both documents and queries during training.", "C. Processes queries dur- ing inference", "D. Design an identifier that can be used to help alleviate the gap between pre-training and fine-tuning."], "answer": "D", "content": "\nIntroduction\nInformation retrieval systems have undergone continuous development over the past few decades, with the aim of obtaining relevant resources, such as documents, in response to a user query from a vast collection. With the recent success of Pretrained Language Models (PLMs) (Devlin et al., 2019; Raffel et al., 2020; Zhao et al., 2023) , researchers have developed PLM-based dense retrievers (Lin et al., 2021; Zhao et al., 2022) , which utilize dual-encoders and nearest neighbor search index for retrieval and achieve significant improvements over sparse retrievers.\nMore recently, a new retrieval paradigm, known as model-based retrieval (Tay et al., 2022; Zhou et al., 2022c) , has been introduced by developing an alternative architecture for retrieval. In contrast to traditional retrieval methods, it does not explicitly maintain a corpus index, thereby simplifying the classic index-retrieve-rerank process. Typically, a model-based retrieval system is built based on a sequence-to-sequence generation model with an encoder-decoder architecture, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) . It accepts a query as input and directly generates the corresponding document identifier via the generation model.\nDespite its attractive benefits in simplifying the retrieval pipeline, model-based retrieval still faces following major challenges.\n\u2022 Firstly, since the retrieval task is framed as a prediction task of document identifiers, making it crucial to design document identifiers that are well-suited to the underlying generative PLM. However, this issue is rarely discussed in prior research, and most existing approaches employ manually or randomly constructed identifiers (i.e., docids) as generation targets. Such docids are not adequately captured in the pretraining stage of the generative PLM, thus limiting PLM's capabilities for generative prediction (e.g., unseen docids during pre-training). This creates a discrepancy between the pre-training and fine-tuning phases.\n\u2022 Secondly, there is a discrepancy between training and inference in the single-model generative architecture. While most existing studies incorporate multi-task learning (Tay et al., 2022) and auxiliary pre-training tasks (Zhou et al., 2022b) to model both documents and queries during training, the model only processes queries dur- ing inference, resulting in a gap between the training and inference stages.\nTo this end, in this paper, we propose a novel TwO-stage Model-based rEtrieval approach, TOME (as illustrated in Figure 1 ), which makes two major technical contributions.\n\u2022 Firstly, we suggest using tokenized URLs (or URIs) as text identifiers, which are widely available for web pages or Wikipedia pages 1 . By using URL-based identifiers, the tokenized symbols are well aligned with the vocabulary of the generative PLM, thereby enhancing the generative capacity of the PLM. URLs are typically comprised of normal text, as opposed to manually or randomly constructed identifiers. As a result, such an identifier design can be used to help alleviate the gap between pre-training and fine-tuning.\n\u2022 Secondly, our approach decomposes the prediction task into two consecutive stages, namely passage generation and URL generation, which are fulfilled by two separate T5-based generation models, respectively. The first stage aims to generate a relevant passage in the corpus based on the query, while the second stage aims to generate the corresponding URL of the generated passage from the first stage. This two-stage architecture can reduce the discrepancy between training and inference. In addition, the entire generation process is progressive. Consequently, the second stage is capable of tolerating errors that may be introduced by the preceding stage and generates correct URLs.\nMoreover, we discover that optimizing modelbased retrieval becomes a challenging task when dealing with a vast corpus. As a result, we propose a number of improved training strategies to optimize the generation models, including query augmentation, passage length reduction, and model scaling.\nTo verify the effectiveness of TOME, we conduct extensive experiments on the publicly available MS MARCO and NQ datasets. Experimental results demonstrate the effectiveness of the proposed method, including the URL identifier design and the two-stage generation process. Additionally, case studies indicate that the second stage can tolerate errors induced by the first stage. Furthermore, we investigate the scaling laws of TOME by examining different model sizes, corpus sizes, and text lengths. We anticipate that these experimental results will facilitate further research on model-based retrieval.\n\nRelated Works\nText Retrieval. Text retrieval endeavors to find textual information related to a query from a large candidate corpus. Early studies on sparse retrieval focused on term matching by utilizing sparse representations and inverted indices, such as BM25 (Robertson et al., 2009) . In recent years, with the resurgence of neural networks and the emergence of pre-trained language models (PLMs) (Devlin et al., 2019; Raffel et al., 2020) , dense retrieval achieves better performance beyond traditional sparse retrieval on multiple tasks (Khattab and Zaharia, 2020; Karpukhin et al., 2020; Xiong et al., 2021; Qu et al., 2021) . The dense retrieval and the technique of approximate nearest neighbor search have been widely adopted in various applications (Oguz et al., 2020; Ren et al., 2021a,b; Asai et al., 2021; Ren et al., 2022; Zhou et al., 2022a) . Recently, Zhao et al. (2022) have made a very comprehensive survey about the recent progress of dense retrieval based on PLMs, and we refer the readers to this survey paper for more details.\nModel-based Retrieval. Both sparse retrieval and dense retrieval rely on explicit indices. Recently, researchers have proposed model-based retrieval (a.k.a., generative retrieval) models (Metzler et al., 2021; Tay et al., 2022) . These methods consider model parameters as retrieval indices and directly generate the identifiers of related documents. Such an idea is initially proposed for entity retrieval (Cao et al., 2021) , which autoregressively generates unique entity identifiers. Following this approach, researchers have introduced sequenceto-sequence encoder-decoder architecture for document retrieval (Zhou et al., 2022c; Bevilacqua et al., 2022; Zhuang et al., 2022; Wang et al., 2022; Lee et al., 2022; Chen et al., 2022; Zhou et al., 2022b) . As discussed in the previous section, there still remain issues with model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. Our work tries to deal with these issues with a two-stage generation architecture with URL identifiers.\n\nApproach\nIn this section, we first introduce the task formulation, followed by the description of the proposed two-stage generation approach TOME.\n\nTask Formulation\nIn this work, we consider the task of text retrieval, which aims to find relevant text resources (e.g., documents) related to a query from a large corpus. We further assume that these texts can be accessed by an associated URL 2 (or URI).\nTo develop our approach, we adopt the recently proposed model-based paradigm for text retrieval (Tay et al., 2022; Zhuang et al., 2022) . For retrieval, a model-based retrieval model takes a query q as input and uses the text-to-text model to generate the identifier y (length n) of the relevant document in an autoregressive manner, with the conditional probability:\nEQUATION\nwhere y i denotes the i-th output token in the identifier y, y <i denotes the previous tokens y 1 , . . . , y i\u22121 , and M represents the PLM. The identifier can be an atomic token or a string (Tay et al., 2022) . In our setting, it is assigned to an associated URL of a text (refer to Section 3.2.1). Typically, a generative pre-trained language model (PLM) with an encoder-decoder architecture is employed to implement the text-to-text model (e.g., T5), which is typically optimized by a cross-entropy loss as follows:\nL(M) = \u2212 log Pr M (y|q) = \u2212 n i=1 log Pr M (y i |y <i , q) . (2)\nThe key to model-based retrieval is to design a generative architecture that employs suitable document identifiers, and to develop effective training methods that can effectively associate queries to the identifiers of documents. Next, we expound our approach in detail.\n\nModel Architecture\nIn this section, we first introduce the design of document identifiers, and then present the two-stage generation architecture.\n\nIdentifier Design\nExisting studies typically use docids to represent a document (Tay et al., 2022; Zhuang et al., 2022) . These docids are often randomly generated or manually constructed, which may not exist in realworld text corpora. However, the generative PLM is pre-trained based on large-scale text corpora, leading to a discrepancy between pre-training and fine-tuning.\nDifferent from previous approaches, we consider a tokenized form of URLs as the docids. We directly treat the URL as a text string and tokenize it into a sequence of tokens using a T5 tokenizer. For instance, a sample URL 'https://en.wikipedia.org/wiki/Nevada' can be tokenized to {'https', '://', 'en', '.', 'wikipedia', '.', 'org', '/', 'wiki', '/', 'N', 'e', 'vada'}. We use the token sequence as the prediction target of the generative PLM, following the generation formula of Equation (1). It is worth noting that Ultron (Zhou et al., 2022b ) also uses URLs as identifiers, where a URL is reversed and only used as part of an identifier (also involving titles and domains). As a comparison, we solely utilize tokenized URLs as the identifier, without any additional processing.\nCompared to non-linguistic docids, URLs typically contain more meaningful tokens in the form normal text and widely exist in real-world text corpora, making them more suitable to modeling and prediction using generative PLMs. During decoding, we can directly adopt the general text decoding method to generate the URL, without resorting to limited search strategies such as constrained beam search (Tay et al., 2022; Bevilacqua et al., 2022) . Since these tokenized symbols often overlap among different URLs (e.g., web pages from the same domains), they naturally derives semantic strings as the clustering method in DSI (Tay et al., 2022) .\n\nTwo-stage Generation Architecture\nThe objective of the generative model for retrieval is to establish a correlation between a query and its corresponding docid (i.e., URL). However, owing to the scarcity of annotated data, various improved strategies such as multi-task learning (Tay et al., 2022) or pre-training (Zhou et al., 2022b) have been proposed. Typically, a model processes both documents and queries during training, while it processes only queries during inference, resulting in the discrepancy between training and inference. To tackle this issue, we propose a two-stage generation approach with two different generation models: one for passage generation and the other for URL generation, as shown in Figure 1 .\nPassage Generation. In the first stage, we employ a T5-based passage generation model to map an input query to the passage content according to Equation (1). The generated passage is anticipated as a relevant passage in the corpus that can provide an answer to the query. The objective of the passage generation model is to memorize the passages in the corpus, so as to generate the passages with utmost precision. It is trained with query-passage pairs, where each pair comprises a query and a passage from the document, along with the corresponding labeled URL. Different from existing methods (Tay et al., 2022; Bevilacqua et al., 2022) , we do not utilize any data structure to restrict the decoding process and simply use greedy search to generate an individual result for a query in an autoregressive manner, which has a high decoding efficiency. By incorporating the intermediate passage generation, our approach can mitigate the training-inference discrepancy that the query encoder also needs to process documents (Tay et al., 2022) . URL Generation. In the second stage, another T5-based PLM is employed to predict the corresponding URL as the retrieval result, utilizing the passage generated by the passage generation model as input. The URL is generated by means of greedy search decoding in a similar manner as in Equa-tion (1). The URL generation model is trained with passage-URL pairs, where each pair comprises a passage and its corresponding URL. The objective of the URL generation model is to memorize all the URLs in the corpus, so as to map a generated passage related to a query to a corresponding URL. Meanwhile, even if the generated passages contain some irrelevant content or noise, this stage can still make reliable predictions since it can employ long passages as the context, rather than short queries.\nOverall, such a two-stage generation approach can more effectively capture the semantic relatedness between queries and identifiers by both reducing the training-inference discrepancy and enriching the generation context, which is specifically tailored for model-based retrieval.\n\nTraining\nFor both the passage generation model and the URL generation model, we optimize them independently by utilizing the cross-entropy loss for optimizing standard T5 models, as shown in Equation (2). Nevertheless, optimizing model-based retrieval approaches (Zhuang et al., 2022; Wang et al., 2022 ) is a challenging task as they essentially require memorizing the corpus information, and generating long text also poses challenges in model convergence. In this part, we further propose several strategies for improving the training of our approach.\nQuery Augmentation. Generating pseudo queries is proven to be effective in improving the performance of model-based retrieval (Wang et al., 2022; Zhuang et al., 2022) . Here, we utilize query generation for constructing the training data for passage generation. Specifically, we take the passage collection as the corpus, and use an existing query generation model (i.e., DocT5query (Nogueira et al., 2019) ) trained on the labeled dataset to generate multiple pseudo queries for each passage in the corpus. Following DSI-QG (Zhuang et al., 2022) , we use the top-k sampling strategy for query generation, and set k up to 20. The generated pseudo queries and their corresponding passages are then used to construct query-passage pairs as the training data for the passage generation model. Such a query augmentation method can significantly increase the availability of training data, and also enhance the generalization capability of the model for different queries.\nReducing the Passage Length. Since passages are much longer than URLs, passage generation is more complicated than URL generation. In the generation task, a more extensive generation target results in larger search space, which typically leads to a decrease in efficiency and effectiveness. While, in our approach, passage generation serves as an indirect step for predicting the URL, so that we consider reducing the passage length for improving the training efficiency. For this purpose, we shorten the maximum truncation length of the passage, from 128 to 32. However, reducing the passage length will probably results in a information loss, thus hurting the generation performance. As the solution, we concatenate the title (a short text) and the shortened passage for enhancing the contained semantics. We also add prompts before titles and passage contents like \"title:\" or \"passage:\" for better generation performance.\nIncreasing Model Scale. Model-based retrieval requires a strong memorization capacity from the generative PLM, especially for our approach that involves a passage generation stage. Besides, scaling up the text corpus will significantly increase the difficulty of corpus memorization, and the PLM with a small parameter scale will have a limited memorization capacity when the data scale reaches a certain level. Considering the two aspects, we scale the model size accordingly and employ a larger PLM when necessary. Specifically, we use T5-large (the first stage is more difficult) and T5base for the two stages of our approach on a small corpus (e.g., subsets of MS MARCO), respectively. Further, we increase them to T5-3B and T5-large accordingly on a large corpus (e.g., the full set of MS MARCO). Besides the improved capacity, we find that using a larger model size is also useful in improving the convergence rate (as detailed in Section 5.4).\n\nExperimental Settings\nThis section describes the major experimental settings, including datasets, evaluation metrics, baselines and implementation details.\n\nDatasets and Evaluation Metrics\nDatasets. We conduct experiments on two public available datasets, namely MS MARCO (Nguyen et al., 2016) Passage Ranking and Natural Questions (NQ) (Kwiatkowski et al., 2019) . (1) MS MARCO contains Bing search queries as well as passages from web documents, making it one of the largest web search datasets to date, with a full corpus of over 8.8 million passages. In addition, we also consider two subsets, each containing 100K and 1M passages, by following (Tay et al., 2022; Zhuang et al., 2022) . Based on the MS MARCO Question Answering dataset, we extract the URLs associated with the passages, selecting a random URL if a passage contains multiple URLs (2) The NQ dataset is a question answering dataset where the query data is collected from Google search logs, and the document data is from Wikipedia. We use the NQ320K version by following NCI (Wang et al., 2022) , which contains 320K labeled querydocument pairs and 100K documents. We collect abstracts of documents as intermediate-generated passages.\nEvaluation Metric. Following previous works, we adopt Hits@1 as the evaluation metric. This metric is calculated as the percentage of queries to which the top-1 generation result is positive. Since the outputs of models at different stages are either passage texts or URL texts, unlike the conventional MS MARCO evaluation by determining whether the retrieved identifiers are in the identifier label list, we evaluate the results by determining whether it is an exact match to the label text.\n\nBaselines\nFor comparison, we chose the following baselines including sparse retrieval, dense retrieval, and model-based retrieval.\nBM25 (Robertson et al., 2009 ) is a classical sparse retriever that uses the inverted index to find relevant passages by term overlap. DPR (Karpukhin et al., 2020) and ANCE (Xiong et al., 2021) are two representative dense retrievers that adopts dual-encoder architecture. For modelbased retrievers, DSI (Tay et al., 2022 ) is a pioneer work for model-based retrieval that uses a sequence-to-sequence model to map the input query to the relevant docid. We use the open-source code released by DSI-QG for reproducing DSI baseline on MS MARCO. SEAL (Bevilacqua et al., 2022) is proposed to generate multiple ngrams for a query with an auxiliary Ferragina Manzini index. DSI-QG (Zhuang et al., 2022) proposes to improve DSI with augmented data constructed by query generation. NCI (Wang et al., 2022 ) also utilizes pseudo queries for improving model-based retrieval with tailored architecture. Due to the different experimental settings of different methods, we copy the performance values for some baselines on NQ in NCI and reproduce all of the baselines on MS MARCO under the same evaluation strategy. All the model-based retrieval baselines adopt the \"large\" version of PLMs.\n\nImplementation Details\nWe conduct our experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) and natural language processing toolkit PaddleNLP (Contributors, 2021) on up to 32 NVIDIA Tesla A100 GPUs (with up to 80G RAM).\nPLM. The generation models adopted in our work are initialized with different parameter scales of T5 (Raffel et al., 2020) . In the passage generation model, we use T5-3B for initialization on MS MARCO Full, and other models are initialized with T5-large. In the URL generation model, we use T5large for initialization on MS MARCO Full, and other models are initialized with T5-base.\nHyper-parameters. We adopt Adam optimizer with a learning rate of 5e-5, and train the models for a maximum of 3M steps with bf16 mixed precision strategy. The batchsize is set up to 128, 384 and 80 for T5-base, T5-large and T5-3B, respectively. The maximal length of queries, passages and URLs are set as 32, 32 and 80, respectively. The warm-up step is set as 100K and 10K for passage and URL generation task, respectively. Query Augmentation. We adopt the existing docT5query-large (Nogueira et al., 2019) model that trained on MS MARCO training set, and generate 20 and 15 queries per passage for MS MARCO and NQ, respectively. For training data, we only use pseudo-labeled data constructed by query generation on MS MARCO, and use both pseudolabeled data and labeled data on NQ.\n\nExperimental Results and Analysis\nIn this section, we report the experimental results of our proposed approach and conduct comprehensive empirical analysis. (Karpukhin et al., 2020) 71.84 52.52 29.54 DSI (Tay et al., 2022) 11.75 --DSI-QG (Zhuang et al., 2022) Table 1 : The Hits@1 results of different methods on variant corpus scales of MSMARCO.\n\nMethods\nHits@1 BM25 (Yang et al., 2017) 15.11 ANCE (Xiong et al., 2021) 52.63 DSI (Tay et al., 2022) 35.60 SEAL (Bevilacqua et al., 2022) 59.93 NCI (Wang et al., 2022) 66.23 DSI-QG (Zhuang et al., 2022) 61.34 Comparison with Model-based Retrievers. We observe that TOME consistently outperforms model-based retrievers on three subsets of MS MARCO and NQ320K datasets, thereby demonstrating the effectiveness of the proposed method. Moreover, NCI is a competitive baseline on NQ320K, which uses tailored decoder architecture, preprocessed semantic docid, and regularization on top of DSI-QG, while our method is simply trained with the standard T5 configuration without any additional processing. We also discover that DSI-QG is unable to effectively converge when trained on the MS MARCO Full. We speculate that random non-linguistic docids become a bottleneck as the corpus scales up, while the loss can normally converge when using normal text (e.g., URL) as a generation target.\nEffect of Two-stage Generation Architecture. By simply substituting the generation target of DSI-QG from random string docids to URLs (singlestage of our method), the performance has been improved (refer to DSI-QG and TOME single-stage in Table 1 and 2 ), indicating that natural language identifiers are more suitable for model-based retrieval tasks than non-linguistic docids. Furthermore, if we employ the two-stage generation that includes an intermediate step to generate passages before generating URLs, the performance will be further improved (refer to TOME single-stage and TOME two-stage in Table 1 and 2 tion demonstrates that integrating passage generation in the process of model-based retrieval leads to better performance.\nComparison with Dense Retrievers. By adopting a series of training strategies, we successfully train TOME on large-scale corpora. However, although TOME outperforms dense retrieval methods on MS MARCO 100K and NQ320K, there still remains a performance gap when compared to DPR on larger corpora such as MS MARCO 1M and Full. This indicates that our method still has gaps compared to advanced dense retrieval methods when the corpus scales up. Since the model-based method necessitates complete memorization of the entire corpus, it inherently possesses a disadvantage in larger-scale corpora when compared to dense retrievers, which needs to be further explored.\n\nAblation Study\nIn this section, we conduct an ablation study to examine the effectiveness of strategies in TOME.\nWe report the results on MS MARCO 100K and NQ320K. Here, we consider three variants based on TOME for comparison: (a) w/o prompt removes the prompts before titles and passages; (b) w/ increased maxlen increases the maximum truncated length of passage from 32 to 128; (c) w/ reduced pseudo query reduces the amount of pseudo query to 10 per passage. Table 3 presents the results for variants of TOME. We can observe the following findings: (a) The performance drops in w/o prompt, demonstrating that adding prompts for identifying the title and passage is helpful for generating better results. (b) The performance drops in w/ increased maxlen, demonstrating that due to various training strategies, shortening the maximum truncated passage length does not bring performance loss but reduces the difficulty of training. (c) The performance drops in w/ reduced pseudo query, demonstrating the effectiveness of generating a large number of pseudo queries for data augmentation.\n\nAnalysis on Two-stage Generation\nIn this section, we investigate the generation results of the passage generation model quantitatively and qualitatively to showcase the superiority of the proposed two-stage generation approach.\n\nQuantitative Analysis\nWe quantitatively analyze the generation results on MSMARCO dev set with the passage generation models trained on MS MARCO 100K.\nFirst, we are surprised to find that on the entire dev set, the proportion of generated passages are the passages exist in the corpus is about 95%. In cases where the model failed to generate labels correctly, about 85% of the generated passages still exist in the corpus. This result indicates that the model is capable of memorizing the corpus precisely and is able to generate a retrieval-like result. Moreover, previous studies of dense retrieval reveal that there are a lot of false negatives in MS-MARCO (Qu et al., 2021) . We also observe that approximately 80% of the generation results that are not labeled as positives but appear in the corpus are false negatives, showing that model-based retrieval suffers from the same issue of false negatives as dense retrieval. Despite this, the passage generation model actually has strong generation capability.\n\nQualitative Analysis\nTo explore the generative capabilities of TOME, we conduct a case study on MSMARCO 100K, utilizing a maximum truncation length of 128 for better illustration.\nTable 4 gives two sampled queries, along with their corresponding label passages, evidence passages (if available) and generated passages. With respect to the first query, the generated passage is not exactly the same as the labeled passage. In comparison with the labeled positive passage, the second half of the generated passage is altered. Despite the alteration in the generation passage, the URL generation model is still able to accurately map it to the correct URL, indicating that the URL generation model can tolerate changes introduced by the passage generation model. In the second example, the model extracts relevant content from both the label passage and the evidence passage, and then combines the contents to create the generated passage. It is interesting Ginger is an analgesic (a pain-killer) that may alleviate the pain associated with a sore throat. It is also a good antibacterial and antifungal and can help fight the infection causing your sore throat. . . . Table 4 : The comparison of the labeled passages and generated passages. The evidence passages are not manually labeled but contain relevant content. The italic words with underline represents the different parts of two passages, the ::::::::::::::::::::::::::\nitalic words with wavy underline and bold words with underline in different passages represent the reference parts. to observe that the passage generation model is capable of summarizing multiple passages.\n\nAnalysis on Scaling\nWe observe that long text generation poses a challenge to the convergence of loss, so we investigate the training efficiency and capability of the model under varying conditions. In particular, we use the same computing resource and conduct training on the passage generation stage (i.e., the first stage) of TOME. Considering that the trend is similar in the second stage, it has been omitted here due to limited space.\nEffect on Data Scale. We investigate the impact of expanding the corpus on model training and examine whether the model capacity is insufficient when dealing with a large corpus. We fix the T5-large model and conduct training on MSMARCO 100K, 1M and Full datasets, respectively, without shortening the length of passages. We use perplexity (PPL) to estimate the model capacity and monitor how perplexity changes as training steps increase. The results are shown in Figure 2 (a). It can be observed that the perplexity of the T5-large model fails to converge to a lower level after corpus scale expansion, which illustrates that under this task, a certain amount of data will lead to the capacity bottleneck of the model. In addition, the decline rate of perplexity slows down on larger corpora, indicating that models with the same parameter size have low learning efficiency on a large-scale corpus.. Effect on Passage Length. In order to investi-gate the effect of reducing the length of generated passages, we fixed the model as T5-large, and conducted experiments on passages with different maximum truncated lengths as generation targets on MSMARCO 1M. Figure 2 shows that after reducing the maximum truncated length of the generated passage, the perplexity significantly decreases, indicating that such a strategy is beneficial to mitigate the difficulty of the passage generation task. Moreover, the model exhibited enhanced efficiency when generating shorter passages.\n\nConclusion\nIn this paper, we introduce TOME, a innovative two-stage model-based retrieval approach. To implement our approach, we make two major technical contributions in the design of the identifier and the architecture of two-stage generation. Moreover, we also employ a number of training strategies to better optimize our proposed architecture, especially on large-scale corpora. Extensive results demonstrate the effectiveness of TOME. Furthermore, we perform a thorough analysis and summarize the scaling law for the proposed method. We believe such an idea itself is worthwhile for exploring in designing new model-based retrieval architecture.\n"}
{"question": "Which is not the issue our system aiming to address?", "evidence": "  Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metrics, and whether the metrics are effective for comprehensive, efficient, and reliable SG evaluation. To address the issues, we establish HAUSER, a holistic and automatic evaluation system for the SG task, which consists of five criteria from three perspectives and automatic metrics for each criterion. ", "options": ["A. what criteria of Similes\u2019 evaluation metricshould be considered", "B. how to quantify each criterion into metrics", "C.  whether the metrics are effective in evaluating SG  comprehensively, efficiently, and reliably.", "D. Whether similes play an important role in creative writing such as story and dialogue generation. "], "answer": "D", "content": "\nIntroduction\nSimiles play a vital role in human expression, making literal sentences imaginative and graspable. For example, Robert Burns famously wrote \"My Luve is like a red, red rose\" to metaphorically depict the beloved as being beautiful. In this simile, \"Luve\" (a.k.a. topic) is compared with \"red rose\" (a.k.a. vehicle) via the implicit property \"beautiful\" and the event \"is\". Here, topic, vehicle, property, and event are four main simile components (Hanks, 2013) . As a figure of speech, similes have been widely used in literature and conversations (Zheng et al., 2019; Chakrabarty et al., 2022) .\nSimile generation (SG) is a crucial task in natural language processing (Chakrabarty et al., 2020; Zhang et al., 2021; Lai and Nissim, 2022) , with the aim of polishing literal sentences into similes. In Fig. 1 , the literal sentence \"He yelps and howls.\" is polished into a simile by inserting the phrase \"like a wolf \", resulting in \"He yelps and The commonly used automatic metric BLEU deems the second candidate as the most high-quality one among all the generated similes, while our proposed metrics HAUSER deem the first candidate as the best one regarding its quality, creativity and informativeness, which better correlates with human ratings and also provides more criteria for SG evaluation.\nhowls like a wolf \". The ability to generate similes can assist various downstream tasks, such as making the generations more imaginative in story or poet generation task (Tartakovsky and Shen, 2018; Chakrabarty et al., 2022) and the generated response more human-like in dialogue generation task (Zheng et al., 2019) .\nAutomatic evaluation is critical for the SG task since it enables efficient, systematic, and scalable comparisons between models in general (Celikyilmaz et al., 2020) . However, existing studies are inadequate for effective SG evaluation. Task-agnostic automatic metrics (Papineni et al., 2002; Zhang et al., 2019; Li et al., 2016) are widely adopted for SG evaluation (Zhang et al., 2021; Lai and Nissim, 2022) , which have several limitations: (1) The simile components should receive more attention than other words during SG evaluation (e.g. \"he\" and \"wolf \" in Fig. 1 ), while there are no automatic metrics that consider the key components. (2) The SG task is open-ended, allowing for multiple plausible generations for the same input (Chakrabarty et al., 2020 ) (e.g. the howling man can be compared to \"wolf \", \"buffalo\", or \"tiger\" in Fig. 1 ). Hence, the metrics based on word overlap with a few references are inadequate to accurately mea-\n\nCriterion Literal Sentence\nExample Simile Candidates Quality Relevance Some raindrops struck the roof, window and ran down its panes. Some raindrops struck the roof, window and ran down its panes (like tears | like arrows).\n\nLogical Consistency\nStefan moved, every movement easy and precisely controlled.\nStefan moved (like lightning | like a dancer), every movement easy and precisely controlled.\n\nSentiment Consistency\nThe idea resounded throughout the land. The idea resounded (like an earthquake | like a thunderous wave) throughout the land.\n\nCreativity\nHe possessed a power of sarcasm which could scorch.\nHe possessed a power of sarcasm which could scorch (like vitriol | like fire).\n\nInformativeness\nThey gleamed. They gleamed (like the eyes of a cat | like the eyes of an angry cat).\nTable 1 : Examples of our criteria for Simile Generation (SG) Evaluation. We design five criteria from three perspectives. The vehicles of the better simile candidates given by each criterion are highlighted in bold.\nsure the overall quality of generated similes. As shown in Fig. 1 , the commonly used metric BLEU deems the second candidate as the highest quality, as it has more overlapped words with the only referenced groundtruth, while human deems the first candidate as the most coherent one.\n(3) The existing metrics are inadequate to provide fine-grained and comprehensive SG evaluation, considering that the creative generation tasks have distinct criteria for desired generations (Celikyilmaz et al., 2020) , such as novelty and complexity for story generation (Chhun et al., 2022) and logical consistency for dialogue generation (Pang et al., 2020) . However, establishing a comprehensive, efficient, and reliable evaluation system for SG is nontrivial, which raises three main concerns: (1) What criteria should be adopted to evaluate the SG task in a comprehensive and non-redundant fashion? (2) How to quantify each criterion into a metric thus enabling efficient and objective SG evaluation, given that the human evaluation of creative generation task is not only time-consuming but also subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014; Celikyilmaz et al., 2020) ? (3) Whether the proposed metrics are effective in providing useful scores to guide actual improvements in the realworld application of the SG model?\nIn this paper, we establish HAUSER, a Holistic and AUtomatic evaluation system for Simile gEneRation task, consisting of five criteria (Tab. 1):\n(1) The relevance between topic and vehicle, as the foundation of a simile is to compare the two via their shared properties (Paul, 1970) . (2) The logical consistency between the literal sentence and generated simile, since the aim of SG task is to polish the original sentence without altering its semantics (Tversky, 1977) . (3) The sentiment consistency between the literal sentence and generated simile, since similes generally transmit certain sentiment polarity (Qadir et al., 2015) . (4,5) The creativity and informativeness of the simile, since novel similes or those with richer content can enhance the literary experience (Jones and Estes, 2006; Roncero and de Almeida, 2015; Addison, 2001) . Overall, these five criteria can be categorized into three perspectives: quality (which considers relevance, logical, and sentiment consistency jointly), creativity, and informativeness. We further quantify each criterion into automatic metrics (Fig. 2 ) and prove their effectiveness through extensive experiments.\nTo the best of our knowledge, we are the first to systematically investigate the automatic evaluation of the SG task. To summarize, our contributions are mainly three-fold: (1) We establish a holistic and automatic evaluation system for the SG task, consisting of five criteria based on linguistic theories, facilitating both human and automatic evaluation of this task. (2) We design automatic metrics for each criterion, facilitating efficient and objective comparisons between SG models. (3) We conduct extensive experiments to verify that our metrics are significantly more correlated with human ratings than prior metrics.\n\nSimile Generation Task\nThere are two primary forms of the simile generation (SG) task: simile triplet completion and literal sentence polishing. For simile triplet completion, a model receives simile components, topic and property, and is required to generate the vehicle (Roncero and de Almeida, 2015; Zheng et al., 2019; Chen et al., 2022; He et al., 2022) . For literal sentence polishing, a model receives a literal sentence and is expected to convert it into similes (Zhang et al., 2021; Stowe et al., 2020; Chakrabarty et al., 2020; Lai and Nissim, 2022) . We focus on the latter. However, prior works mainly adopt task-agnostic automatic metrics to evaluate the SG task, raising concern as to whether the claimed improvements are comprehensive and reliable.\n\nAutomatic Evaluation for NLG Systems\nExisting automatic metrics for Natural Language Generation (NLG) evaluation can be categorized into task-agnostic and task-specific metrics. Taskagnostic metrics can be applied to various NLG tasks, which generally focus on the coherence of generations (Papineni et al., 2002; Zhang et al., 2019) , including n-gram-based metrics (Papineni et al., 2002; Lin, 2004; Denkowski and Lavie, 2014) and embedding-based metrics (Zhang et al., 2019; Zhao et al., 2019) . There are also many metrics for evaluating the diversity of generations (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) . Task-specific metrics are proposed to evaluate NLG systems on specific tasks (Tao et al., 2018; Dhingra et al., 2019; Ren et al., 2020) . Specifically, various works systematically study the evaluation of the creative generation task (Pang et al., 2020; Tevet and Berant, 2021; Chhun et al., 2022) . Different from these works, we revisit SG evaluation, propose holistic criteria based on linguistic theories, and design effective automatic metrics for it.\n\nHAUSER for SG evaluation\nWe establish HAUSER, a holistic and automatic evaluation system for SG evaluation, containing five criteria from three perspectives, and further design automatic metrics for each criterion (Fig. 2 ).\n\nQuality\nWe measure the overall quality of generated similes using three criteria: relevance, logical consistency, sentiment consistency. The key simile components -topic and vehicle -should be relevant, as the foundation of a simile is to compare the two via their shared properties (relevance) (Paul, 1970) . In Tab. 1, comparing \"raindrops\" to \"tears\" is more coherent than to \"arrows\". Additionally, the generated simile should remain logically consistent with the original sentence (logical consistency), as the SG task aims to polish the plain text without changing its semantics (Tversky, 1977) . In Tab. 1, comparing \"Stefan\" to \"dancer\" better depicts his controlled and easy movement than to \"lightning\". Furthermore, as similes generally transmit certain sentiment polarity (Qadir et al., 2015) , the generated simile should enhance the sentiment polarity of the original sentence (sentiment consistency). In Tab. 1, the vehicle \"thunderous wave\" enhances the positive polarity of the original sentence, while the vehicle \"earthquake\" brings a negative sentiment polarity in opposition to the original sentence.\n\nRelevance\nFor the relevance score, if the components of one simile are relevant, they tend to co-occur in simile sentences (Xiao et al., 2016; He et al., 2022) and possess shared properties (Paul, 1970; Tversky, 1977) . Hence, obtaining the relevance score requires large-scale simile sentences as references, as well as knowledge about the properties (adjectives) of each simile component. For a simile s, the relevance score is defined as follows:\nEQUATION\nwhere there are m p topic-vehicle pairs extracted from simile s, each denoted as (t, v) 1 . \u0393(t, v) is the set of similes containing (t, v) as simile components, each denoted as e. P e (t, v) is the probability that the simile components (t, v) share properties in the context of the simile sentence e.\nAn effective way to obtain the frequency information \u0393(t, v) and property knowledge P e (t, v) is to utilize the large-scale probabilistic simile knowledge base MAPS-KB (He et al., 2022) , which contains millions of simile triplets in the form of (topic, property, vehicle), along with frequency and two probabilistic metrics to model each triplet 2 . Specifically, the probabilistic metric Plausibility is calculated based on the confidence score of the simile instance (topic, property, vehicle, simile sentence) supporting the triplet, indicating the probability that the topic and vehicle share the property. The relevance score r can be calculated as follow:\nEQUATION\nwhere G (t,v) is the set of triplets (t, p ,v) containing the (t, v) pair in MAPS-KB, with p referring to the property. n and P are the metrics provided by MAPS-KB, where n and P denote the frequency and the plausibility of the triplet respectively. It is noticed that the metric is not coupled with MAPS-KB, as the frequency information can be obtained by referencing a large set of simile sentences and the property knowledge can be contained via other knowledge bases. More methods are beyond the scope of this paper. However, we additionally provide a method to approximate the relevance score. If we assume the probability that the simile components (t, v) share properties in each sentence is 1, the relevance score can be approximated as:\nEQUATION\nwhere n(t, v) denotes the number of samples that contain the simile components (t, v) in large-scale simile sentences. We discuss the effects of the referenced dataset size in Sec. 4.2.1.\n\nLogical Consistency\nThe literal sentence and the generated simile that are logically inconsistent generally exhibit contra-1 All the simile components in our work are extracted and cleaned using rules from (He et al., 2022) which determines the optimal semantics a component should carry, e.g., \"a kid in a candy store\" instead of just \"a kid\".\n2 More details of MAPS-KB is provided in Appx. D dictory logic. Hence, for a generated simile, we input the <literal text(l), simile(s)> sentence pair into existing pre-trained Multi-Genre Natural Language Inference (MNLI) model 3 , which determines the relation between them is entailment, neutral, or contradiction. The logical consistency score c l of this simile is defined as follows (Pang et al., 2020) :\nEQUATION\nwhere P (h <l,s> = c) represents the probability that the model predicts the relation of the sentence pair < l, s > to be contradiction (denoted as c).\n\nSentiment Consistency\nBetter similes tend to enhance the sentiment polarity of the original sentence (Qadir et al., 2015) . Hence, we first apply the model fine-tuned on the GLUE SST-2 dataset 4 to classify each simile as being either positive or negative. Then, the sentiment consistency score c s is defined as follows:\nEQUATION\nwhere a is the sentiment polarity of the literal sentence (positive or negative) predicted by the model. P (h s = a) and P (h l = a) denote the probabilities that the model predicts the sentiment polarity of the simile s and the literal sentence l to be a, respectively. It is noticed that different <topic, vehicle> pairs within a sentence may have distinct sentiment polarities, such as <She, scared rabbit> and <I, bird> in the simile \"If she escapes like a scared rabbit, I will fly like a bird to catch her.\". Directly inputting text containing multiple topic-vehicle pairs into the sentiment classification model will result in inferior performance. Therefore, for each simile, only the text from the beginning up to the first vehicle is input into the model (i.e. \"If she escapes like a scared rabbit\" in the given example), and for each literal sentence, the text from the beginning up to the first event (i.e. \"If she escapes\" in the given example) is input into the model.\nSince the aim of the SG task is to polish the plain text, the quality of similes generated from different texts can not be compared. Therefore, the normalized score among the simile candidates for each original text is utilized. Suppose there are m simile candidates S = {s 1 , s 2 , ..., s m } for the literal text l, the original relevance scores of R is R = {r 1 , r 2 , ..., r m } respectively. The normalized relevance score r \u2032 i of s i is formulated as follows:\nEQUATION\nwhich ranges from 0 to 1. Then, the normalized logical and sentiment consistency score c \u2032 li , c \u2032 si for each simile s i are obtained in the same manner 5 .\nFinally, the quality for simile s i is defined as the weighted combination of three parts as follows:\nEQUATION\nwhere \u03b1, \u03b2, and \u03b3 are hyperparameters.\n\nCreativity\nCreative similes can provide a better literary experience (Jones and Estes, 2006). In Tab. 1, comparing \"sarcasm\" to \"vitriol\" is less common than to \"fire\", yet it better conveys the intensity of a person's sarcasm. Hence, we design creativity score. Previous studies mainly evaluate the creativity of text generation tasks via human evaluation (Sai et al., 2022) , since measuring the creativity of openended text is a relatively difficult task (Celikyilmaz et al., 2020) . Although there have been many works evaluating the diversity of open-ended text generation (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) , these metrics are not suitable for measuring the creativity of the text. Because the diversity metrics take a set of generated text as input and output one score, while a creativity metric is required to measure each text individually and output a set of corresponding scores.\nDifferent from other open-ended generation tasks, the components of the generated similes enable us to evaluate creativity automatically. According to linguists, the creativity of a simile is determined by vehicles (Pierce and Chiappe, 2008; Roncero and de Almeida, 2015) . Intuitively, the generated simile may be less creative if its extracted 5 If all the relevance scores ri in R are the same, the normalized relevance scores r \u2032 i in R \u2032 are set to 0.5 uniformly.\ntopic-vehicle pair co-occurs frequently, or if many topics are compared to its vehicle in the corpus. Therefore, we adopt large-scale corpora as references when designing our creativity metric. The creativity score of s is calculated as follows:\nEQUATION\nwhere there are m v vehicles extracted from the simile s, each denoted as v. N v denotes the frequency of the vehicles appearing in the similes in the corpora. The log transformation aims to reduce the influence of extreme values.\nAn effective way to obtain the adequate frequency information N v is to utilize the millionscale simile knowledge base MAPS-KB, where the N v can be defined as follows:\nEQUATION\nG v is the set of triplets containing the vehicle v in MAPS-KB, n denotes the frequency of the triplet.\nIt is noticed that the metric is not coupled with MAPS-KB, as N v can also be obtained by counting the samples containing the vehicle v in largescale simile sentences. The method of obtaining the simile sentences is beyond the scope of this paper. Nevertheless, we discuss the effects of the referenced dataset size in Sec. 4.2.2.\n\nInformativeness\nThe vehicle with richer content can create a more impact and vivid impression (Addison, 2001) . In the example from Tab. 1, the addition of the word \"angry\" makes the similes more expressive. Therefore, we design the metric informativeness to measure the content richness of the vehicles.\nIntuitively, the more words a vehicle contains, the richer its content will be. Hence, for a given simile s, we adopt the average length of the extracted vehicles to be the informativeness score 6 (Chakrabarty et al., 2020; Zhang et al., 2021) , defined as I i = 1 mv v\u2208s len(v), where there are m v vehicles extracted from simile s. Here, BLEU2, Rouge2, and BERTScorelarge are presented since they perform the best in their respective category. To avoid overlapping points, random jitters sampled from N (0, 0.05 2 ) were added to human ratings after fitting the regression.\n\nSimile Generation\nThe existing datasets for the SG task are either Chinese (Zhang et al., 2021) , limited to the simile triplet completion (Roncero and de Almeida, 2015; Chen et al., 2022) , or having all vehicles located at end of the sentence (Chakrabarty et al., 2022; Lai and Nissim, 2022) , which are not practical for English simile generation in a real-world application.\nTo bridge the gap, we construct a large-scale English dataset for SG task based on simile sentences from (He et al., 2022) , which contains 524k simile sentences labeled with topic and vehicle. The output decoder target is the simile sentence s and the input encoder source is s rewritten to drop the comparator \"like\" and the vehicle. For example, given s = \"The idea resounded like a thunderclap throughout the land.\", the encoder source would be \"The idea resounded throughout the land.\". In particular, we remove the simile sentences whose event is a linking verb (e.g. be, seem, turn) as they would be meaningless after the vehicle is removed. The final train, validation and test sets contain 139k, 2.5k, and 2.5k sentence pairs, respectively. Based on our constructed dataset, we finetune a pre-trained sequence-to-sequence model, BART (Lewis et al., 2020) , for the SG task, which has been demonstrated to be an effective framework for various figurative language generation (Zhang and Wan, 2021; Chakrabarty et al., 2022; He et al., 2022; Lai and Nissim, 2022) . The experiments are run on RTX3090 GPU and the implementation of BART is based on the HuggingFace Transformers 7 . The experiments are run with a batch size of 16, a max sequence length of 128, and a learning rate of 4e-5 for 10 epochs.\n\nEvaluation Dataset Construction\nFirstly, we randomly sample 50 literal sentences from the test set and adopt the trained SG model to generate five candidates for each one. Then, for each perspective, three raters are asked to rate each 7 https://github.com/huggingface/transformers/ simile from 1 to 5, where 1 denotes the worst and 5 denotes the best 8 . Since evaluating the quality of generated similes is subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014), we remove the simile-literal sentence pairs if (1) raters argue that the pairs lack context and are difficult to rate (e.g. \"Nobody can shoot.\") or (2) some raters rate them as low quality (quality score of 1-2), while others rate them as high quality (scores of 4-5) (Niculae and Danescu-Niculescu-Mizil, 2014) . Moreover, we measure the inter-rater agreement by holding out the ratings of one rater at a time, calculating the correlations with the average of the other rater's ratings, and finally calculating the average or maximum of all the held-out correlations (denoted as \"Mean\" and \"Max\", respectively). The inter-rater agreement before and after applying the filtering strategies is shown in Tab. 2. Overall, the final inter-rater agreement ensures the reliability of our evaluation of automatic metrics and the filtering strategies improve the inter-rater agreement generally. We finally get 150 simile candidates generated from 44 literal sentences.\n\nQuality\nWe compare our quality metric with the following automatic metrics 9 : (1) BLEU (Papineni et \n\n2002) calculates the precision of n-gram matches,\n(2) RougeL (Lin, 2004 ) is a recall-oriented metric, (3) METEOR (Denkowski and Lavie, 2014) proposes a set of linguistic rules to compare the hypothesis with the reference, ( 4) BERTScore (Zhang et al., 2019) calculates the cosine similarity between the BERT embeddings, ( 5) Perplexity (Pang et al., 2020) measures the proximity of a language model, the inverse of which is utilized.\nCorrelations with Human Ratings. Tab. 3 shows the correlation coefficients between automatic metrics and human ratings. Firstly, our metrics are significantly more correlated with human ratings than prior automatic metrics. Moreover, all the sentence-level metrics, which consider the semantics of the entire sentence, perform worse than almost all the n-gram-level metrics, which compare the n-grams between the hypothesis and the reference, which reveals that simile components need to be specifically considered during SG evaluation.\nAccording to the visualized correlation result in Fig. 3 , datapoints from prior automatic metrics tend to scatter at 0 or 1, while the datapoints from our metric are distributed closer to the fitter line, proving that our metric can better measure the quality.\nRecommendation Task. We compare the rankings given by automatic metrics with human rankings 10 . We adopt the following metrics: Hit Ratio at rank K (HR@K(K=1,3)), Norgenerated from different literal sentences can not be compared. Please refer to Appx. C for the implementation of them. 10 We remove the literal sentences with fewer than three valid simile candidates in this task, as they are too simple to rank. We finally get 134 sentences from 35 literal sentences.\n\nMetrics\nHR@1 HR@3 nDCG@1 nDCG@3 MRR malized Discounted Cumulative Gain at rank K (NDCG@K(K=1,3)) 11 , and Mean Reciprocal Rank (MRR). From Tab. 4, our metric achieves significant improvement compared to other metrics, indicating that our metric can yield more accurate rankings for quality. Also, the n-gram-level metrics generally outperform sentence-level metrics, which is consistent with the result in Tab. 3. Ablation Study. To investigate the importance of different sub-metrics in quality metric, we compare the correlation between quality metric and human ratings after removing each sub-metric individually. From Tab. 3, the removal of any sub-metric leads to a decline in performance, which proves the effectiveness of each sub-metric. Among three components, the removal of the relevance results in the largest performance drop, which reveals that relevance is the most important sub-metric.\nThe Effects of Hyperparameters. Since different sub-metrics have varying levels of importance, we study the correlation results when gradually increasing the weight of relevance component and decreasing the weight of sentiment consistency component (as in Tab. 5). From Fig. 4 (left), increasing the weight of the relevance component consistently results in improved performance, peaking at the combination [7](\u03b1, \u03b2, \u03b3 = 3/6, 2/6, 1/6), before eventually causing a decline in performance. This reveals that although relevance is the most important sub-metric, too much weight on it can be detrimental.\nThe Effects of Referenced Dataset Size. We sample different numbers of simile sentences from (He et al., 2022) as references for relevance 5/6, 1/12, 1/12\nTable 5 : The setting of each hyperparameters combination for the quality metric. The result is shown in Fig. 4 (left).\n[\n1] [2] [3] [4] [5] [6] [7] [8] [9]\nHyper-parameters Combination score and study the correlation between the quality metric and human ratings 12 . From Fig. 4 (right) 13 , correlations grow linearly with exponential growth in referenced dataset size, indicating that using datasets larger than 100k will improve the correlation coefficients. Moreover, the performance at the peak surpasses the prior automatic metrics, proving the effectiveness of our approximation method.\n\nCreativity\nWe compare our creativity metric with the following automatic metrics: (1) Perplexity which is often utilized to measure diversity as well (Tevet and Berant, 2021) , (2) Self-BLEU (Zhu et al., 2018) calculates the BLEU score of each generation against all other generations as references, (3) Distinct n-grams(Dist) (Tevet and Berant, 2021) , which is the fraction of distinct n-grams from all possible n-grams across all generations.\nCorrelations with Human Ratings. From Tab. 6, our metric creativity is significantly more correlated with human evaluation scores compared with prior diversity metrics. According to the visualized correlation result in Fig. 5 , the prior diversity metrics have either wide confidence intervals (Perplexity, Dist) or scattered datapoints (self-BLEU), whereas our creativity metrics exhibit stronger linear correlation and narrower confidence intervals (Creativty w/ Log), implying higher reliability.\nRecommendation Task. We compare the rankings given by automatic metrics with human rank- ings. According to Tab. 7, our creativity metric outperforms prior automatic metrics, which proves our metric can better measure the creativity of simile candidates given a literal sentence, which is consistent with the results in Tab. 6. Ablation Study. According to Tab. 6, removing the log transformation leads to significant performance drops. According to the visualized correlation result in Fig. 5 , the datapoints are distributed closer to the fitter line and exhibit narrower confidence intervals after applying the log transformation, which further proves that log transformation is essential for our creativity metric.\nThe Effects of Referenced Dataset Size. According to Fig. 6 (left), the correlation coefficients increase continuously and eventually converge as the number of referenced sentences increases. Moreover, the performance after convergence is comparable to that given by the creativity metric based on the simile KB. The trend reveals that our metric referencing 10k similes can achieve a promising correlation with human ratings.\n\nInformativeness\nThe Pearson and Spearman correlation coefficients between our informativeness metric and human ratings are 0.798 and 0.882, respectively. According to Fig. 6 (right), the strong linear correlation between the metric and human ratings proves that our informativeness metric is simple yet quite effective.\n\nRelation between Metrics\nWe present pair-wise correlations between the three automatic metrics in Tab. 8 and also visualize them in Fig. 7 . Among the three metrics, creativity correlates with informativeness moderately, mainly because shorter vehicles tend to be less creative than longer ones. The correlations of all other pairwise metrics are relatively weak. Thus, it is evident that the three metrics are independent of each other and it is necessary to measure each one of them to obtain a holistic view of SG evaluation. \n\nHAUSER Application\nWe perform a case study to prove that our designed automatic metrics are effective for various methods. Here, we apply our metrics to a retrieval method (Zhang et al., 2021) (denoted as BM25), which utilizes the 20 context words around the insertion position given by groundtruth to retrieve the 5 most similar samples based on the BM25 ranking score from the training set, and adopts the vehicles from these samples to be those of simile candidates. This method ensures the diversity of generated similes. The method introduced in Sec. 4.1 is denoted as Ours. Given the candidates generated by each method, we rerank them using a weighted combination of quality, creativity, and informativeness rankings obtained by HAUSER, with a ratio of 2:2:1. From Tab. 11 in Appendix, the candidates generated by various methods can be more correlated with human rankings after being ranked by our metrics, thus proving the generality of our metrics. It is noticed that the insertion position for BM25 is provided by the groundtruth, while the insertion position for Ours is predicted by the model, thus proving the effectiveness of our generation method.\n\nConclusion\nIn this work, we systematically investigate the evaluation of the Simile Generation (SG) task. We establish a holistic and automatic evaluation system for the SG task, containing five criteria from three perspectives, and propose holistic automatic metrics for each criterion. Extensive experiments verify the effectiveness of our metrics. more correlated with humans than prior referencebased metrics (e.g. BLEU, Rouge, BERTScore), our metrics are still reference-based and rely on the quality and scale of referenced data. We have discussed the effect of referenced dataset size in our paper and will design reference-free metrics to further complement our metrics in future work. Additionally, since our metrics utilize a million-scale simile knowledge base or large-scale simile sentences as references, the efficiency of our method is slightly lower than the automatic metrics based on a few references. Nevertheless, this limitation does not prevent our metrics from performing systematic and scalable comparisons between SG models.\n"}
{"question": "What is one of the key reasons for the popularity of stock price movement prediction, as mentioned in the passage?", "evidence": "  The reason for its popularity is self-evident: once a model is able to predict future movement with considerable accuracy, numerous trading strategies can be easily built around it.  ", "options": ["Among various tasks, one of the most prominent is stock price movement prediction (Bhardwaj, 2021) ."], "answer": "C", "content": "\nIntroduction\nFinancial services, known for their competitiveness, have always been at the forefront of adopting data science techniques to drive investment decisions. Quantitative trading, a specific field within it, has drawn immense interest from both academia and industry over the last few decades. With the rapid advancements in deep learning recently, computer scientists and quantitative researchers have joined forces to apply AI techniques to tackle the challenges within this domain.\nAmong various tasks, one of the most prominent is stock price movement prediction (Bhardwaj, 2021) . The reason for its popularity is selfevident: once a model is able to predict future movement with considerable accuracy, numerous trading strategies can be easily built around it.\nRecent studies have shown that deep neural networks are ideal candidates for such prediction models (Yoo et al., 2021; Gunduz, 2021) . Supporters of the efficient-market hypothesis (EMH), which posits that asset prices reflect all available information, tackle the task with price information alone (Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . However, an alternative perspective suggests that additional insights can be gained from analyzing news articles and social media posts, which may hold valuable clues about the future (Hu et al., 2018; Xu and Cohen, 2018; Wang et al., 2019b; Tang et al., 2020) .\nAnother intriguing approach analyzes the relationships between different stocks. Clearly positive and negative correlations, or even non-correlations can be immensely useful in constructing a diversified stock portfolio (Borodin et al., 2003) . Several studies even empirically demonstrate that exploiting correlations can improve the accuracy of stock price movement prediction (Long et al., 2020; Yoo et al., 2021) . However, their correlations are often realized by acquiring industry sector and calculating correlation matrices or attention scores, which are bidirectional and symmetrical, leading to excessive attention on spurious correlations. Due to the lag problem widely existed between two time series, we are more concerned about the dominance of information flow between stocks, specifically, the direction of causality.\nAdditionally, we have observed that the situation can significantly change when incorporating text information. Let's consider two highly correlated companies (A and B) and there is promising news specifically about company A. In such a scenario, it's fairly easy to infer that the current news might still have a substantial impact on company B, despite there being no direct connection between the two companies on paper. However, it's impossible to reach this conclusion by just examining the news about company A or the correlation between A and B alone, which highlights the limitations of relying solely on individual pieces of textual information or traditional correlations between stocks.\nInspired by observations above, we propose the Causality-guided Multi-memory Interaction Network (CMIN), a novel end-to-end deep neural network which captures both financial news as well as the causality-enhanced correlations between stocks for better stock price movement prediction.\nTo achieve this goal, CMIN incorporates two key components: the Text Memory Network and the Stock Correlation Memory Network. Both networks utilize a recurrent neural network with nonlinear combination of memory attentions to generate a global memory abstraction. And we introduce a global causality matrix according to the transfer entropy between stock price time series to guide the abstraction process, forming a Causal Attention mechanism to capture the asymmetric correlations. By considering causality, CMIN goes beyond traditional symmetric correlations and captures the true inter-dependencies between stocks. Furthermore, we employ an attention-based fusion mechanism between the two networks, introducing multi-directional interactions through which CMIN learns not only the self-influence within each network but also the interactive influence between them. It captures the interrelationship between textual information and correlations, enhancing the overall predictive power of CMIN.\nWe further demonstrate the effectiveness of CMIN with experiments conducted on 3 real-world datasets collected from both the U.S. and Chinese markets, where CMIN achieves state-of-the-art prediction accuracy, surpassing existing models in terms of performance.\nTo summarize, our main contributions are:\n\u2022 Proposal of a causality-guided multi-memory interaction network for stock movement prediction which is to our best knowledge the first attempt to simultaneously consider causalityenhanced correlations and textual information to achieve higher prediction accuracy;\n\u2022 Introduction of the attention-based multidirectional interactions, so that CMIN captures not only the self-influence of temporal movements and textual information but also the interactions between these two types of information flows;\n\u2022 Collection and release of two new datasets: one for the U.S. market and another for the Chinese market.\nBoth datasets include comprehensive financial texts and stock price time series data, which are publicly available at https://github.com/ BigRoddy/CMIN-Dataset, facilitating further research and benchmarking in the field.\n\nStock Movement Prediction\nIn traditional trading practices, two main frameworks are commonly used to make predictions on future stock prices (Ferreira et al., 2021) . The first is fundamental analysis, which aims to assess the intrinsic value of a stock by considering various factors related to it as a whole, such as financial statements, industry trends and economic conditions. The other is technical analysis, which operates under the assumption that the market is efficient (i.e., the Efficient Market Hypothesis holds true) and focuses on analyzing only historical and current price patterns in order to predict future movements.\nAlthough both frameworks have been widely adopted by top hedge funds and investment firms, technical analysis has gained more popularity among AI practitioners, many of whom focus on employing long short-term memory networks and other innovative architectures to model stock price history alongside technical analysis indicators (Nelson et al., 2017; Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . This is primarily because processing a single stream of price data is relatively simpler than analyzing and synthesizing a range of diverse data sources with varying frequencies and characteristics.\n\nPredicting with the Help of Text Data\nThe recent advancement of natural language processing (NLP) techniques has opened up new possibilities for analyzing large volumes of text data in the context of stock movement prediction. Many researchers have recognized the potential value of incorporating news articles, analysis, commentaries and even social media posts (Xu and Cohen, 2018) , which are believed to provide valuable insights about the future. Some studies focus solely on textual information. For example, (Hu et al., 2018) leverages attention mechanism at multiple levels within a deep structure to identify the most important news articles and predict price trends. Others adopt a two-step approach. First, they extract features (e.g. investor sentiment) from financial texts. Then they fuse these features with price information to make predictions such as (Li et al., 2017) and (Jin et al., 2020) . This integration of text analysis with quantitative techniques holds promise for enhancing the accuracy and effectiveness of stock movement prediction models.\n\nExploiting the Relations between Stocks\nAnother important trading framework takes advantage of the correlations between different stocks. Portfolio selection, particularly pairs trading, is a well-known and successful trading strategy that exploits the correlated nature of stocks, whether positive or negative. In fact, as early as (Borodin et al., 2003) pointed out that stock correlations based portfolio selection could beat any strategy that relied on predicting trends or specific targets.\nThe incorporation of correlations in stock movement prediction has gained attention in recent years, drawing inspiration from several existing works. For example, (Yoo et al., 2021) utilizes transformer to learn dynamic correlations between stocks in an end-to-end manner. (Long et al., 2020) employs knowledge graphs and graph embedding techniques to model the relationships between stocks. These studies have achieved admirable results, potentially due to effective feature engineering however, because the direct benefit of stock correlations in predicting future prices lacks fundamental logic.\nIn this paper, we propose constructing a single model to handle both textual data and stock correlations simultaneously, aiming to shed light on the success of correlation-based approaches with the help of financial texts. We also introduce a novel causal attention mechanism to interpret the underlying logic behind stock correlations, leveraging transfer entropy to provide insights. We further model the multi-directional interactions between texts and correlations so that we could uncover not only relevant texts for prediction through correlations, but also the hidden stock correlations through texts. By integrating text data and stock correla-tions within a unified model, we aim to provide a comprehensive understanding of the relationship between the two and discover valuable insights for stock movement prediction.\n\nProblem Formulation\nThis paper is dedicated to predict the price movement of a target stock. To this end, we leverage both the correlations between stocks and textual information to make prediction.\nConsider a target stock with numerical features denoted as P target \u2208 R k\u00d7d , where k represents the number of time steps in the monitoring window and d represents the dimension of price features, such as the highest and the closing prices. The prices of n other relevant stocks are denoted as:\nP = {P 1 , P 2 , \u2022 \u2022 \u2022 , P n } \u2208 R n\u00d7k\u00d7d .\nBesides, we have financial documents associated with the target stock, which are represented as\nM = {M 1 , M 2 , \u2022 \u2022 \u2022 , M k } \u2208 R k\u00d7l\u00d7w ,\nwhere l denotes the number of documents in a time step and w is the maximum number of words in a document. In cases where a specific stock has fewer than l documents at a given time step, zero padding values are added to align the lengths. Similarly, if a document contains fewer than w words, zero padding is applied to ensure uniform length across all documents (Ang and Lim, 2022) .\nWe formulate the task as a binary classification problem whose goal is to predict the movement of the target stock at the next time step, denoted as \u0177target . Here, \u0177target = 1 indicates a rise in the price while \u0177target = 0 indicates a fall. (1) The feature embedding module includes two encoders, one for embedding the textual information and another for embedding the price time series. Additionally, a global causality matrix is introduced to capture the asymmetric correlations using transfer entropy, which then guides the calculation of attention weights in the multi-memory networks.\n(2) The multi-memory networks consist of the Text Memory Network and Stock Correlation Memory Network, which are designed to select and re- tain the most relevant and influential information (textual and correlational) for the target stock.\n(3) The multi-directional interaction module facilitates the interaction between the textual and correlational information. This interaction allows the two types of information to reinforce each other and leverage the advantages of different information flows for better prediction performance, enhancing the predictive capabilities of the CMIN.\n\nFeature Embedding\nSelf-attention mechanisms have proven to be effective in capturing long-term dependencies and modeling complex sequential patterns, particularly in the Transformer architecture (Vaswani et al., 2017) . Given the significance of historical information in financial documents and stock prices for stock price movement prediction, we employ attention mechanisms to summarize this information.\n\nText Encoder\nThe Text Encoder focuses on processing the financial documents M to extract meaningful representations for stock movement prediction. We firstly use a popular word representation tool Glove (Li et al., 2018) to generate the word embedding tensor M word \u2208 R k\u00d7l\u00d7w\u00d7dw , where d w is the size of word embeddings. Each word in the financial documents is represented as a d w -dimensional vector.\nThen the word embeddings are passed through a text embedding layer. Here we adopt the bidirectional Gated Recurrent Unit (Bi-GRU) (Li et al., 2022) to capture both preceding and succeeding contexts within each document. The average of the last hidden vectors is taken as the text embeddings M text \u2208 R k\u00d7l\u00d7dm , or equivalently M text \u2208 R s\u00d7dm , where s is the total number of documents in the monitoring window.\nAfter that, the text attention mechanism is applied to summarize all historical documents across time steps. The text embedding of the last time step M text,\u22121 \u2208 R l\u00d7dm , serves as the query matrix, while the entire text embeddings M text \u2208 R s\u00d7dm acts as both the key and value matrices. Soft scaled dot-product attention is used to compute the attention weights, which are then applied to the text embedding to obtain a representation E text \u2208 R l\u00d7dm enhanced by the history state attention:\nE text = softmax( M text,\u22121 M T text \u221a d m )M text . (1)\nThe resulting E text is the textual embedding that contains highly concentrated information from the stock's related texts. This embedding serves as a summary of the historical text data and is used for further processing in the multi-memory networks and multi-directional interaction module of CMIN.\n\nPrice Encoder\nThe Price Encoder is introduced to utilize multivariate features from historical prices and capture their temporal interrelationships. Firstly we employ a feature mapping layer to project them into a latent space of dimension d p , aiming to improve the learning capacity (Yoo et al., 2021) . For target stock price P target \u2208 R k\u00d7d , the historical price embeddings Ptarget \u2208 R k\u00d7dp can be formulated as:\nEQUATION\nwhere\nW t \u2208 R d\u00d7dp , b t \u2208 R dp are parameters.\nMoreover, recognizing that historical patterns can repeat themselves sometimes, we incorporate a multi-head price attention layer to capture each stock's distinctive changing patterns. The price embedding of the target stock at the last time step is donated as P\u22121 target \u2208 R dp . Then we employ the multi-head attention mechanism with the query P\u22121 target and the key/value Ptarget as follows:\nv target = MultiheadAtt( Ptarget , P\u22121 target ) (3)\nv target is a key vector that serves as the initial hidden state for the two memory networks, playing a crucial role in the final prediction. Similarly, we process the remaining stocks and obtain the correlational embedding E corr \u2208 R n\u00d7dp . Notably, the shared parameters across all stocks ensure the stability and generality of the extracted features (Wang et al., 2019a) .\n\nCausality Matrix\nWhen it comes to detecting causal relationships and conducting predictive analysis, transfer entropy, a non-linear generalization of Granger causality (Seth, 2007) , serves as a conceptually neat and mathematically rigorous method. It has been considered as an important tool for causality analysis and successfully applied in diverse domains including financial markets (Sandoval Junior et al., 2015) .\nTransfer entropy is derived from Shannon Entropy: H = \u2212 N i=1 p i log p i . In this context, considering the time series of a stock, we can partition the possible values into different bins and calculate the probabilities at each time step. Transfer entropy from series X to another series Y can be defined as the average amount of information contained in the source X but not contained in Y's past:\nEQUATION\nBased on this principle, for each monitoring window, we calculate the transfer entropy between all stocks using their historical closing prices and generate a transfer entropy matrix, referred to as the Causality Matrix C \u2208 R n\u00d7n , which illustrates the asymmetric flow of information from one stock to another. Specifically, C[i, j] represents the transfer entropy from stock i to stock j, and C[i, j] > C [j, i] indicates that stock i provides more predictive information about the movement of stock j than j to i. This Causality Matrix will next serve as a guide for the memory networks, enabling the identification of causal dependencies between multivariate stocks.\n\nMulti-memory Networks\nWe introduce a Text Memory Network and a Stock Correlation Memory Network (Sukhbaatar et al., 2015) to manage the textual and correlational information separately. They each maintain a continuous representation and update it iteratively using multiple computational steps (hops), ultimately producing a global memory abstraction.\nAs shown in Figure 1 , each layer of the memory network comprises an attention unit and a GRU unit, which receive textual or correlational embeddings as inputs and are supervised by the continuous representation generated in the previous layer. To initialize the continuous representations of each network, we use the target stock vector v target (generated from Eq.3):\nv (0) text = v (0) corr = v target .\n(5)\n\nText Memory Network\nIn each layer h \u2208 [1, H] of the Text Memory Network, we input the textual embeddings E text (Eq.1) and the continuous representation from the previous layer v\n(h\u22121)\ntext . We utilize an attention unit (Eq.3) to identify important information within the textual embeddings. Subsequently, a non-linear GRU cell unit (Xu et al., 2019) acts as an information aggregator, determining the amount of text information to retain:\nv Att(h) text = MultiheadAtt(E text , v (h\u22121) text ), (6)\nwhere v (h\u22121) text is the query matrix and E text represents the raw form of the key and value matrices.\nThen the GRU cell unit updates the current hidden state into the next hidden state and outputs it to the next layer as the new continuous representation:\nv (h) text = GRU (v Att(h) text , v (h\u22121) text ).\n(7)\n\nStock Correlation Memory Network\nThe Stock Correlation Memory Network is employed to dynamically identify stock relationships and update the continuous representation of stock correlations in an intuitive and asymmetric manner. However, the use of unsupervised attention weights in previous models can be problematic as they may be inevitably misled by the dataset bias, resulting in excessive attention on spurious stock correlations. To address this, we introduce extra knowledge in the form of Transfer Entropybased causality to guide the attention weights and mitigate potential confounding effects.\nFor each target stock, we extract a causal vector v causal = C[:, target] from the pre-calculated causality matrix, which quantifies the degree of information flow from other stocks to it. Then we modify the traditional attention mechanism into Causal Attention by incorporating causal guidance:\nS = softmax( QK T \u221a d ), S = f (S, v causal ). (8)\nHere, f is a function that aggregates the attention weight S and the causal vector v causal to produce a causality-guided attention weight S. We use the average aggregation method for simplicity (i.e., f (S, v causal ) = (S + v causal )/2). To better balance them, one can introduce a hyperparameter \u03bb \u2208 [0, 1]. Then f() updates to f (S, v causal ) = \u03bbS + (1 \u2212 \u03bb)v causal . We believe that different degrees of causal attention can impact the model's performance, and leave it for future exploration. The continuous representation is gradually updated through the Causal Attention, indicating the influence of causal relationships on movement prediction and the self-influence on the flow of correlation information:\nEQUATION\nIt is important to note that although we design multiple layers within each memory network to learn deep representations, different layers of the same memory network share the same unit. This enables the network to focus on crucial information that affects the movement of the target stock, thereby enhancing the continuous representation.\n\nMulti-directional Interactions\nIn reality, textual information and correlations have an impact on each other when it comes to stock price movement prediction. For instance, news about a technological breakthrough in the new energy sector may uplift the prices of most stocks in that industry, thereby affecting the correlations among those stocks.\nTo simulate this phenomenon and enhance the synergy between textual and correlational information, we introduce a multi-directional interaction module. This module allows textual and correlational information to reinforce each other and amplify the advantages of different information flows for better prediction performance.\nTake the Text Memory Network as an example, in each layer we firstly calculate the self-influence by using v (h\u22121) text as the query:\nv Att(h) text\u2212>text = MultiheadAtt(E text , v (h\u22121) text ) (11)\nNext we consider the interactive influences from correlations to texts using v (h\u22121) corr as the query:\nv Att(h) corr\u2212>text = MultiheadAtt(E text , v (h\u22121) corr ) (12)\nFinally, we produce a new attentional continuous representation by averaging these two influences:\nEQUATION\nwhich means that we replace Eqs. 6 with Eqs. 11-13 to obtain the new attention-aggregated vector.\nThe workings of Stock Correlation Memory Network are quite similar.\nConsequently, the fusion of different information flows is promoted due to the multi-directional interaction mechanism in which CMIN learns not only the influences from text/correlation to movement prediction within each information flow but also the interactive influences between different information flows, representing the interrelationship between text and correlations.\n\nLearning Objective\nWith the continuous representations v (H) text and v (H) corr from the last layer of each memory network, along with the target stock representation v target , we concatenate them and apply a softmax function to generate the final prediction vector \u0177:\n\u0177 = softmax(W y [v (H) text , v target , v (H) corr ] + b y ). (14)\nThe objective is to minimize the cross entropy loss:\nL(y, \u0177) = \u2212 n i=1 (yi log (\u0177i) + (1\u2212yi) log (1\u2212 \u0177i)) (15)\nwhere n is the size of the training set.\n\nExperiments\nIn this section, we empirically evaluate our CMIN model with three real-world datasets collected from the U.S. and Chinese stock markets.\n\nDatasets\nIn our experiments we have used three datasets, namely ACL18, CMIN-US and CMIN-CN, spanning different time periods to evaluate our proposed model CMIN against other baselines.\nACL18 (Xu and Cohen, 2018 ) is a classic dataset with tweets from Twitter as financial texts in the task of text-enhanced stock movement prediction. As there are few existing high-quality datasets containing both texts and price, we are also making available two new benchmark datasets along with this paper from 2018-01-01 to 2021-12-31 in the U.S. and Chinese market named CMIN-US and CMIN-CN. These two datasets are available at https://github.com/BigRoddy/ CMIN-Dataset to facilitate further research and enable reproducibility. More details and statistics of those three datasets are in Appendix A.\n\nBaselines\nWe compare CMIN against the following four baselines, all of which are high-performing stock movement prediction models proposed by recent studies:\n\u2022ALSTM (Qin et al., 2017 ) is a dual-stage attention-based recurrent neural network, which selects relevant time series across all time steps.\n\u2022Adv-LSTM (Feng et al., 2019) uses adversarial training to improve the generalization of ALSTM.\n\u2022Stocknet (Xu and Cohen, 2018) introduces recurrent continuous latent variables and uses variational inference to address the posterior inference.\n\u2022DTML (Yoo et al., 2021) is a newly published attention-based model that exploits the correlations between stocks to improve the prediction accuracy.\n\nEvaluation metrics\nAs we have formulated stock price movement prediction as a classification problem, we choose two classic metrics: Accuracy (Acc.) and Matthews Correlation Coefficient (MCC), similar to the previous work (Xu and Cohen, 2018; Yoo et al., 2021) . \nEQUATION\n\nImplementation details\nWe set our model for daily price prediction, with a history market window size k = 5 and the number of price features d p = d = 3, namely the highest, the lowest and the closing prices. We limit the maximum number of financial texts in one single day to be l = 20 , and the maximum length of a text document w = 30. Within the Text Encoder, we set the size of word embedding vector d w = 50 and the hidden state of Bi-GRU network d m = 50.\nWe implement the CMIN with Pytorch on a NVIDIA Tesla V100 and train it with an Adam optimizer (Kingma and Ba, 2015) . All parameters of our model are initialized with Xavier Initialization (Glorot and Bengio, 2010) . We search the hyperparameters of CMIN as follows: number of layers of each memory network H in {1, 2, 3, 4, 5}, dropout rate in {0.1, 0.2, 0.3}, number of epochs in {10, 20, 50}, and size of the price hidden state d p in {3, 10, 50}. For baselines, we use their default parameters and fine-tune them to fit our data.\n\nPerformance Analysis\nThe results are summarized in Table 1 .\nAmong all models, ALSTM and Adv-LSTM performed poorly with little improvement over random prediction. This could be attributed to the fact that these models rely solely on stock prices as the basis for decision-making. The Stocknet and DTML incorporate additional information beyond stock prices, demonstrated significant improvements over ALSTM and Adv-LSTM, which highlights the importance of utilizing financial texts and stock correlations for this challenging task. CMIN outperformed all baselines and achieved state-of-the-art performance on both two metrics across all datasets, showing its excellent capabilities to leverage both financial texts and stock correlations, as well as capture their interrelationship. \n\nAblation Studies\nTo evaluate the contribution of CMIN's different components, we compare against several variants:\n\u2022CMIN-TE: CMIN without the Text (TE), which makes decisions just based on stock prices.\n\u2022CMIN-PR: CMIN without the Price (PR), which makes decisions just based on related texts.\n\u2022CMIN-CM: CMIN without the guide of causality matrix (CM).\n\u2022CMIN-MI: CMIN without multi-directional interactions (MI) between memory networks.\nThe results are summarized in Table 2 . CMIN-TE only achieves a level of prediction accuracy on par with ALSTM and Adv-LSTM, and is worst among all the variants, again indicating the importance of text data. Similar to the performance of Stocknet, CMIN-PR has a relatively high Acc. but a low MCC, suggesting texts are particularly helpful to predict on one side of the binary classification. By modeling both text data and stock relationships, CMIN-CM reaches a good result. Finally, better performance achieved when causality matrix and multi-directional interactions are introduced into the network. Overall, the ablation studies show that every component makes an important contribution to CMIN, and as a result the full model with all components achieves the best performance.\n\nAnalysis of Memory Network Depth\nAs introduced before, we propose two memory networks to retain vital features of texts and correlations with multiple computational layers. And we want to understand what would be the ideal number of depths to achieve the best prediction results.\nWe change the number of layers H of each memory network to find out how the performance fluctuates with it. The results are summarized in Figure 2 . When we only have one memory layer, there is no multi-directional information flows between the two memory networks and as a result they only try to identify the vital information in the embeddings related to or having an impact on the movement of the target stock under the supervision of v target . As the number of memory layers increases, the interactions between two memory networks also intensifies. It is intuitive that the performance of CMIN reaches its peak when it has three memory layers. With further increase the number of memory layers, CMIN is prone to overfit.\n\nCase Study\nHere we present an example to illustrate how CMIN considers both financial texts and stock correlations to avoid random noises in time series.\nWe visualized the causality matrix of ACL18 using a heat map as shown in Figure 3 . Stocks are sequenced by their industry sector. The black box on the left shows weak causality, representing weak information flow from Utilities to Materials. On the other hand, the yellow box on the right indicates the relative strong information flow from Materials to Finance and within the Finance industry.\nThe target stock is Bank Of America (BAC) with a monitor window spanning from 13/11/2015 to 19/11/2015. We employ CMIN to predict BAC's next movement direction on the day of 20/11/2015 and then output the attention scores of texts and causality-guided correlation. The most focused stock by CMIN is Berkshire Hathaway Inc. (BRK-A). It's interesting to note that both are in the same industry sector: Finance, and they do appear to follow a very similar movement pattern in the trading days leading to 20/11/2015, which demonstrates the ability of CMIN to find the dynamic stock correlations with the guidance of Causality Matrix.\nThe financial text of BAC that obtains the highest attention score is \"Beer, Credit Card Debt And Other Positives For Bank Of America\", the title of an news article 1 which reports the rapidlyimproving banking landscape in the U.S.. This text is clearly highly relevant to BAC's subsequent stock performance, which demonstrates that CMIN is able to identify highly relevant texts having a impact on the target stock movement.\nFurthermore, it also illustrates the underlying interrelationship between financial texts and stock correlations. Except expressing an optimistic sentiment towards BAC, the news also shows a rapidly improving state of affairs for the wider financial industry. Therefore, through the Multi-directional Interactions mechanism, the text strengthens the model's attention stocks in the same sector. These two aspects mutually reinforce and complement each other to help the model make the best judgment that BAC's stock price will rise on the next day.\n\nConclusions\nIn this paper, we proposed CMIN, a causalityguided multi-memory interaction network that simultaneously models financial documents, causality-enhanced stock correlations and the interactions between the two, and recurrently learns a global memory representation for movement prediction. This multi-modality network was designed to enable the concurrent discovery of texts and stock correlations relevant to future price change and we demonstrated, through experiments on three datasets across two distinct markets, that each component of the proposed architecture made significant contributions to the model, leading CMIN to achieve state-of-the-art accuracy.\n"}
{"question": "What was the main goal of the study regarding Greek papyri dating?", "evidence": "  Greek papyri, which mostly survive in fragments, are divided into two broad categories: books (literary and sub-literary papyri) and documents of all kinds (documentary papyri). The former ones never carry a date, whereas the latter often do, albeit not always unambiguously convertible by modern scholars. Its main contribution lies in providing an additional dating criterion for ancient Greek documents, in addition to the ones usually employed by papyrologists (palaeography, onomastics, prosopography, toponymy, archaeological evidence, etc.). It can predict a date for those papyri that do not include one, narrow down the possible time-span of doubtful dating, or contribute to deciding on one particular date when several alternatives seem possible. ", "options": ["A. To date literary papyri accurately.", "B. To estimate the date of production for nonliterary papyri.", "C. To improve the accuracy of carbon dating techniques.", "D. To analyze the linguistic evolution of Greek."], "answer": "B", "content": "\nIntroduction\nAncient textual artefacts are arguably the richest source of information on the ancient world. In the Graeco-Roman world and particularly in its Greekspeaking part, the most extensive coeval texts come from inscriptions and papyri. The latter is a collective term used for all ancient manuscripts, regardless of their writing material which, apart from papyrus, may be parchment, pottery, wood, and others. To correctly evaluate and make good use of these texts, we need to determine their date, provenance and historical context of their production and use. As far as dating is concerned, the value of the relevant evidence provided by the artefacts themselves varies considerably, ranging from a direct date in the text (following, of course, the calendar and dating system of the respective historical period) to no evidence at all. In between, there are texts containing references to known historical figures and events of a certain period, papyri which have been found next to other objects that can be dated, or other indirect evidence. The presence or absence of a date depends on the type of text preserved on the papyrus and its use through time, as well as on its state of conservation. Just like in modern times, it is much more likely to include a date in an official letter than in a page torn from a novel book. At the same time, it is more probable to find a date in a fully surviving letter than in a damaged one missing, for instance, the upper part of the first page.\nGreek papyri, which mostly survive in fragments, are divided into two broad categories: books (literary and sub-literary papyri) and documents of all kinds (documentary papyri). The former ones never carry a date, whereas the latter often do, albeit not always unambiguously convertible by modern scholars. Most importantly for our study, literary papyri contain copies of works authored many years (often centuries) before the production of the actual manuscripts. On the other hand, documentary texts were usually written down as they were composed or shortly after that, making the content of their texts contemporary to their writing style or script. Therefore, any temporal indication in the text is also dating evidence regarding the production of the document. Even when there is no direct date in the text (e.g. Figure 1 ), documentary papyri can be dated securely sometimes within a short time-frame, because they may refer to known historical events or concern people known through other sources to have lived at a particular time.\nWhen neither direct or indirect dating is possible, papyrologists resort to palaeography, the study of the script. In palaeography, particular writing styles are associated with certain chronological periods. Therefore, similar writing styles point to similar dates (Mazza, 2019) . Securely dated specimens are used as a guide to chronologically place the undated ones. Growing criticism on the subjectivity of palaeographical dating (Mazza, 2019; Choat, 2019; Nongbri, 2019 Nongbri, , 2014) ) highlights the need for more reliable methods. Recent efforts for computational dating of historical manuscripts are based on the script rather than the text and, although they consider various languages, they disregard Greek (Omayio et al., 2022) .\nIn this study we focus on computational dating of Greek documentary papyri based on their transcriptions, contributing in the following three ways:\n1. We present and publicly release a machineactionable dataset of 389 documentary Greek papyri, containing texts of various aspects of daily life (e.g. contracts, receipts, letters).\n2. We draw the baseline in text regression for the tasks of dating experimenting with Monte Carlo and leave one out cross validation.\n3. We apply a committee of regressors to three papyri, which present different types of dating challenges, and on 159 manuscripts for which only the upper date limit is known.\nThis approach does not apply to literary papyri and our research involves solely documents. Apart from their texts being contemporary with the actual manuscripts (by dating the text, we date the papyrus), nonliterary papyri also include vastly more numerous objectively dated specimens than literary ones. Specific dates on our training set also allow for more accurate (narrower date-spans) predictions by our models.\n\nRelated Work\nDating historical documents with computational means has been studied for many languages (Baledent et al., 2020; Dhali et al., 2020; Li et al., 2015; Hamid et al., 2019; Adam et al., 2018) . However, very limited work has been done for Greek and no published work at all has focused on Greek papyri. The only work to our knowledge is Ithaca, a Transformer trained on ancient Greek inscriptions performing text restoration, geographical attribution, and dating (Assael et al., 2022) . Ithaca has achieved an error of 0.29 centuries in dating epigraphs. This result is by far better than an onomastic baseline using the known distribution of Greek personal names to infer the date, which scored 1.44. Inscriptions differ from papyri in many aspects (such as the genre, the length, and their geographical distribution), but in principle, this system is applicable to our data and was therefore used as a baseline. Below, given the absence of dating studies for Greek, we summarise work for other languages.\nThe studied languages are Latin (Baledent et al., 2020; Wahlberg et al., 2016 Wahlberg et al., , 2015)) , Hebrew (Dhali et al., 2020) , Dutch (Hamid et al., 2019 (Hamid et al., , 2018;; He et al., 2014 He et al., , 2016b,a),a) , Arabic (Adam et al., 2018) , Swedish (Wahlberg et al., 2016 (Wahlberg et al., , 2015)) , French (Baledent et al., 2020) and English (Li et al., 2015; Rastas et al., 2022) . A collection of 595 Dead Sea Scrolls, in Aramaic script, was the dataset with the oldest manuscripts, dated from 250 to 135 BCE, and the only one so far concerning texts written on papyri (Dhali et al., 2020) . The rest of the datasets comprised more data, ranging from less than five (Adam et al., 2018) to more than ten thousand manuscripts (Wahlberg et al., 2015) or more (Rastas et al., 2022) , while the one with the most recent manuscripts comprises historical English-language documents (Li et al., 2015) , printed between the 15th and 19th CE.\nThe employed methods usually were standard machine learning methods, such as KNN (Adam et al., 2018) , decision trees (Baledent et al., 2020) , random forests (Baledent et al., 2020) and support vector machines (Hamid et al., 2019; Dhali et al., 2020; He et al., 2014 He et al., , 2016b,a),a) . Textural features, such as Gabor filters, Uniform Local Binary Patterns and Histogram of Local Binary Patterns are extracted and then fed to the classifiers (Hamid et al., 2018) . The writing style evolution, however, has also been used as an intermediate step (Dhali et al., 2020; Adam et al., 2018) . In this case, the periods are first aligned with specific writing styles. Then, any new manuscript is dated based on the detected style.\nPre-trained convolutional neural networks have been used to extract features, which are passed to a classifier or regressor (Hamid et al., 2019; Wahlberg et al., 2016) , or used in combination with text features extracted with optical character recognition methods (Li et al., 2015) . Transfer learning has been reported to lead to human performance (Wahlberg et al., 2016) . This was deemed to be the most promising direction for the present study on Greek manuscripts, and was, hence, employed.\n\nData\nOur dataset, which we release publicly, 1 comprises the transcriptions of 389 manuscripts, dated from the 3rd century BCE to the 7th century CE, originating from Greco-Roman Egypt (with a few exceptions from the Near-East).\n\nThe source\nThe dataset was compiled mainly from PA-PYRI. INFO. 2 The documents in its collections set a reliable point of reference for scholars who aspire to study the evolution of ancient manuscripts in time. These collections incorporate full transcriptions and references to scholarly editions of the papyri, as well as a set of metadata that can also assist in dating (e.g. provenance).\n\nThe scripts and the language\nNonliterary papyri in Greek from the 3rd c. BCE to the 7th c. CE are written in a great variety of cursive hands (Harrauer, 2010) , posing an extra challenge for image classification methods and calling for other approaches. The language of the papyri, Greek of the Ptolemaic, Roman and early Byzantine periods, reflects the diversity and the diachronic changes of the Greek-speaking communities in Egypt, which is the provenance of most of our specimens.\n\nThe ground truth\nThe date of a manuscript may be found in different forms. It can be an exact date, a range of years, a starting date (not before that date), or an ending date (not after that date), or two-three alternative dates. Our dataset has been curated so that dating applies at the level of the quarter of the century, by considering manuscripts dated exactly or with a period ranging within that quarter. We did not consider manuscripts that were dated only before or after a specific date.\n\nData collection\nOur first dataset comprised 400 manuscripts, 40 samples per century. Our initial pool consisted of 77,040 items and we opted for ones that satisfy the following conditions:\n\u2022 The transcriptions must be available in machine actionable form.\n\u2022 The papyri must contain documents (not works of literature) to ensure that text and papyrus are contemporary. 3\n\u2022 The papyri must be securely and accurately dated. Many papyri do not carry a date and are, therefore, dated with subjective criteria or with a large date span (e.g. 1st-2ndCE).\n\u2022 The image is available, to allow image-based dating and potentially jointly from different modalities: text and image.\nGiven these limitations, it was the 7thCE that dictated the size per century of a balanced dataset, since there are not more than 40 securely dated papyri from 7thCE. For each of these records, the text was retrieved afterwards from PAPYRI.INFO by parsing the respective XML files. We discarded records whose extracted text was less than ten characters, which resulted in our final 389 records. From these records, we extracted the entire text from one side of the papyrus (the side that had more text than the other). In the few cases of papyri with more than one fragment, we only included the first one. This decision was based on weighing the benefit of avoiding a considerable amount of noise during automatic parsing against eliminating a portion of text, in a dataset whose nature is by definition fragmentary.\n\nNormalisation\nThe transcribed text comprises a variety of characters and symbols. We preprocessed the data by lowercasing and normalising the text (see Table 1 ). We (o) '\u1f45', '\u1f43', '\u03cc', '\u03cc', '\u1f44', '\u1f41', '\u1f40', '\u1f78' (\u03b1) '\u1f02', '\u1fb4', '\u1f03', '\u1f85', '\u03ac', '\u1f01', '\u1f04', '\u1fb6', '\u1f00', '\u1fb7', '\u1f70', '\u03ac', '\u1f05' (\u03b7) '\u1f24', '\u1f23', '\u1fc3', '\u1f22', '\u1f20', '\u03ae', '\u1f26', '\u1f25', '\u1fc6', '\u1f27', '\u1f97', '\u1f94', '\u1fc7', '\u1f21', '\u1fc4', '\u1f91', '\u1f74', '\u03ae' (\u03b9) '\u03af', '\u1fd6', '\u1f37', '\u1f31', '\u1f36', '\u1fd2', '\u03af', '\u1f30', '\u1f76', '\u0390', '\u1f34', '\u03b9', '\u03ca', '\u1f33', '\u1f35' (\u03b5) '\u03ad', '\u1f72', '\u03ad', '\u1f14', '\u1f10', '\u1f15', '\u1f11', '\u03b5' (\u03c5) '\u1f57', '\u03cd', '\u1fe6', '\u03cd', '\u1f55', '\u1f53', '\u1f7a', '\u1f51', '\u1f54', '\u1f50', '\u1f56' (\u03c1) '\u1fe5', '\u1fe4' (\u03c9) '\u03ce', '\u1ff6', '\u1f66', '\u1fa4', '\u1f60', '\u1f67', '\u1ff3', '\u1ff7', '\u1f62', '\u1f65', '\u03ce', '\u1fa7', '\u03ce', '\u1fa6', '\u1f64', '\u1fa0', '\u1f7c', '\u1f61', '\u1ff4' (\u03c3) '\u03c3', '\u03c2' Table 1 : Normalisation rules of characters in the dataset, all characters on the right have been replaced by the character on the left. also discarded any character besides the 24 Greek letters, also removing white space and all punctuation marks. We did not eliminate the editors' corrections and supplements nor edit otherwise the data, which often led to duplicate words with alternative orthography (original and normalisation).\nThe transcriptions available are not diplomatic (reflecting exactly what is written) but normalised according to modern conventions, for example as far as punctuation and word separation (or sometimes spelling) are concerned. Therefore, we chose to disregard these conventions, because they do not represent data present in our sources, but normalisation on the papyrologists' part for the purpose of scholarly editions.\nTo provide some more concrete examples, there is no capitalization of proper names or initial words in sentences in papyri. Punctuation is very scarce and sometimes completely absent. Diacritics are not meaningless, but they are extremely rare in documentary papyri (i.e., except diaresis which is used in a different way than modern conventions, to mark iota and upsilon as the first letter of a word). Breathings and accents are marked inconsistently (if at all) by different scribes. Hence, removing diacritics leads to inclusion and can help avoid multiple variations of what is in fact the same word. Regarding spelling, we kept both the original and the corrected form (if provided by the editors), because spelling mistakes reflect language evolution.\n\nExploratory analysis\nThe overall text length per quarter of century varies over time, as can be seen in Figure 2 . Although we have selected an equal number of manuscripts per century ( \u00a73.4), the number of lines within each manuscript varies, and so does the line length. Furthermore, within a century, manuscripts of a spe- cific quarter of a century may be more frequent due to random discoveries, as is the case of 7thCE, where the first quarter holds most of the support, a discrepancy deriving from the reduced number of dated papyri in this century overall.\nThe most frequent character in our dataset is '\u03b1' (35,101 occurrences), followed by '\u03bf' (33,176), '\u03b9' (30,151), and '\u03b5' (25,116) . On the other hand, the least common are '\u03b2' (2520), '\u03be' (1210), '\u03b6' (379), and '\u03c8' (334). These figures are coherent with general frequencies of letters in Ancient and Modern Greek (Mikros et al., 2005) .\nIn order to assess the quality of the ground truth, we employed the Callimachus' Conservation number (CCN), 4 which provides an educated estimation of the preservation and legibility of a papyrus. The lowest score is 0 and the highest score (i.e., 1) indicates readability and 'perfect' conservation of the text. The status of the conservation of a papyrus affects the quality of the transcription, indicating the amount of text that has not been recorded in the transcriptions (or recorded with some level of uncertainty) because of the material state of preservation of the manuscripts. Damage in papyri could affect as little as one or two letters (or even none), to as much as several lines and whole parts of the \n\nMethodology\nTo estimate the date of production of manuscripts, we opted for text regression, taking advantage of the continuous target objective. Statistical validity was established with 5-fold Monte Carlo crossvalidation. The best regression method was used to form a committee of models, which were applied on unseen data in order to analyse the predictions.\n\nBenchmarking\nWe performed Monte Carlo cross-validation, by sampling 90% for training, 10% for validation, and then re-sampling with replacement five times. We report the mean absolute error (MAE), the mean squared error (MSE), and the explained variance (R 2 ). Besides the average results across folds, we also report the best score achieved per metric.\n\nRegression methods\nFern\u00e1ndez-Delgado et al. ( 2019) surveyed 77 regression methods and undertook an experimental analysis on 83 datasets. Regression with extremely randomised trees achieved the best R 2 in many datasets but gradient boosting and random forests were also found to have a promising performance. Following these findings, we opted for extremely randomised trees, random forests, gradient boosting, and linear regression for our experiments. 5 Extremely randomised trees (XTrees) is a treebased ensemble, created with the Extra-Trees algorithm (Geurts et al., 2006) . Although simple in nature, it is both accurate and efficient Fern\u00e1ndez-Delgado et al. (2019) . Compared to other ensembles that use decision trees, XTrees splits the nodes of the tree by choosing randomly cut-off points and the trees grow by using the whole sample to learn instead of bootstrapping.\n\nThe Committee\nUsing the best-performing regression method out of the ones examined, we performed leave one out cross-validation, which allowed an evaluation using the whole dataset. Furthermore, it yielded as many regressors as the data points, which in our case is 389. We used these models to form a committee and date unseen papyri (further discussed in \u00a76).\n\nEmpirical analysis\nThis section presents our experimental results using regression on textual features to date Greek manuscripts. First, we present preliminary experiments and then we analyse the experimental findings from our regression analysis.\n\nPreliminary experiments\nPreliminary experiments comprised image classification (Hamid et al., 2018 ), text classification with Transformers trained on another domain (Assael et al., 2022) , and transferring learning from large language models (Koutsikakis et al., 2020) . Image classification was used prior to using transcribed text as our input, experimenting with using the documents' images (Hamid et al., 2018; Wahlberg et al., 2016; Paparigopoulou et al., 2022) . Vanilla convolutional neural networks were outperformed by a pre-trained one (Tan and Le, 2019), fine-tuned for our dating task. Our estimated MAE, however, was consistently more than a hundred years (Table 2 ), hence we opted for textual input. Ithaca was presented by Assael et al. (2022) , consisting of a Transformer that is trained not only in dating but also in text restoration and geographical attribution. Ithaca has achieved an error of 0.29 centuries in dating inscriptions, which is by far better than an onomastics baseline (error of 144 years).\nBy using the open-access web interface, 6 we scored all our preprocessed texts, 7 registering a MAE of approx. one century by using the maximum decade predicted or the average of the distribution (Table 2). The difference from the published result possibly stems from the fact that this is a model trained and focused on inscriptions, not papyri. Transfer learning was used with GreekBERT, a Transformer that is pre-trained in masked language modelling, among other tasks, in modern Greek (Koutsikakis et al., 2020) . GreekBERT has been further pre-trained in ancient Greek (Singh et al., 2021) . We experimented with fine-tuning both variants in predicting the date, 8 but MAE was approx. one century (Table 2 ).\n\nRegression analysis\nExperiments were undertaken with Google Colaboratory, using a 12GB NVIDIA Tesla K80 GPU. We extracted term-frequency-inverse-documentfrequency features using lower-cased text and character n-grams (from 1-to 5-grams). 9 All other parameters were set to default values. 10\n\nMonte Carlo cross validation\nLinear regression achieved a MAE of 86 years on average (Table 2 ) and a MSE of 1.33. R 2 was similar across folds, around 83. A random forest had an even better MAE of 73 years on average but a worse MSE (1.58). Its average R 2 was lower than that of linear regression, but the maximum one achieved across folds was much better. Random forest also outperformed both gradient boosting methods in MAE but GBoost achieved a better MSE and R 2 on average. XTrees achieved the best results in all metrics, with a MAE of 54 years and the best R 2 climbing up to 95.43.\n\nLeave one out cross validation\nUsing the best performing XTrees, we performed leave one out cross validation, by hiding one instance, training the algorithm on the remaining instances, and then using the model to predict the hidden record. 11 The MAE was found to be 55 years, MSE was 1.11, and R 2 was 85.89, close to the Monte Carlo evaluation scores. In order to better understand the errors, we rounded the predictions and the ground truth, evaluating as if we would in a classification setting. Predictions most often fall on or close to the diagonal (Figure 4 ), which explains the low error. The best result is 8 We used white space, to allow subword computation. 9 Preliminary experiments with centroid or trainable word embeddings before recurrent or convolutional neural networks deteriorated performance.\n10 Manual hyper-parameter tuning per regressor yielded insignificant improvements.\n11 The experiment lasted 15 hours. 3 .\nachieved for the 1st and 2nd CE, followed by the 7th CE (see Table 3 ). The overall accuracy is 60%.\n\nError analysis\nIn very few cases, our leave-one-out regression fell considerably out of its predictions (Figure 4 ). Our analysis showed that these texts happen to contain specific words typical of another period, which confused the prediction. For instance among the highest prediction error were two late texts (6-7thCE) that exceptionally contain \u03a3\u03b5\u03c1\u03b1\u03c0\u03af\u03bf\u03c5 and \u0392\u03b1\u03c3\u03b9\u03bb\u03b5\u03af\u03bf\u03c5, usually found in Ptolemaic time (3rd-1stBCE). In another case, we provided experimentally the longer version of the text, initially parsed only partially ( \u00a73.4). Using the full text led to an accurate prediction, influenced by the word 'indiction' in the additional text ( \u00a77.1).\n\nUse cases\nWe applied our 389 regressors, produced upon leave-one-out cross-validation, to three use cases, which present different types of dating challenges.\n\nPSI 8 934\nThis document 12 preserves the ca. 15 last lines of a land lease. texts from the 6th and early 7thCE c., the Dioscorus archive (Fournet, 2008) , because, among other concordant elements, it contains microtoponyms from the respective village countryside. The notary who signed the contract, Abraam, is known from other documents, which is crucial evidence for the dating of the papyrus. This notary's period of activity has been proven to span at least between 524 and 545 (Fournet, 2003) . This papyrus, therefore, is securely dated by indirect evidence, but no date is explicitly mentioned in the text (Fournet, 2008) . Our average prediction is 310 CE, dated between 260 CE (min) and 352 CE (maximum prediction).\n\nP. Basel 2 15\nThis papyrus, also shown in Figure 1 , is a private letter dated indirectly from the 1st CE. The letter is almost complete, except for a damaged word at the end of line 5. Private letters usually do not bear a date. The dating, therefore, by the editor is done on palaeographical grounds as well as on the basis of scribal habits: \"the hand [...] is more at home in the first century CE than the second, a dating that is supported by the writer's use of iota adscript...\" (Huebner et al., 2020) . Iota adscript is an expected feature in the 3rd BCE, starting to be irregularly written between the 2nd BCE and the first CE to almost completely disappear from the 2nd CE onwards (Clarysse, 1976) . Onomastics strengthen the editor's dating hypothesis: of the three personal names mentioned in the letter (Pasis, Orsenouphis, and Tithoes), the first two are attested from ca. 250 BCE to 250 CE while the last one starts appearing in the papyri only in the 1st c. CE. 13 Our models date this to 140 BCE, from 165 BCE to 112 BCE.\n\nP. Petra 1 5\nThe last manuscript 14 contains a request for transfer of taxation from 538 CE. It is a geographical outsider since it does not come from Egypt but from Petra (Jordan). We tested this manuscript since many of the words found in the text are infrequent in Egyptian manuscripts, on which our models are trained. The date mentioned in the papyrus is \"second indiction\". This refers to the second year of a repeated fifteen-year cycle (indiction) and the year 538 is relative, since it could be the second year of the previous or the next indiction (523 or 553). 538 is logically deduced by the editors in view of the whole dossier of papyri from Petra. Our models date this manuscript to 555 CE (521-575 CE), overcoming the geographical variation.\n\nDiscussion\nThe computational, quantitative method suggested in this work is intended to complement human expertise. Its main contribution lies in providing an additional dating criterion for ancient Greek documents, in addition to the ones usually employed by papyrologists (palaeography, onomastics, prosopography, toponymy, archaeological evidence, etc.). It can predict a date for those papyri that do not include one, narrow down the possible time-span of doubtful dating, or contribute to deciding on one particular date when several alternatives seem possible. Despite the fact that limitations exist (discussed in \u00a77.3), compared to traditional approaches the models trained in this study are expected to reduce biases. Their value is not limited to predicting dates for individual manuscripts, but they can be applied to any attribute of a group of papyri, e.g. the place of provenance or the text's type. At the same time, easily accessible open-source metadata exist for most published papyri ( \u00a73.1).\n\nRationale generation\nThe use of supervised learning, such as the work of Assael et al. (2022) or ours, can yield accurate estimations, which can at least help the human expert. The assistance is greater, however, when explanations are provided for the models' decisions. In our case, we used a committee of hundreds of regressors in order to estimate the date of three use cases. Therefore, we sampled models per case and generated rationales regarding their predictions, by using their Shapley values (Lundberg and Lee, 2017).\nIn the case of PSI 8 934 ( \u00a76.1), our investigation showed that the mention of the name 'Aurelios Victor' ('\u0391\u1f50\u03c1\u03ae\u03bb\u03b9\u03bf\u03c2 \u0392\u03af\u03ba\u03c4\u03c9\u03c1') influenced the decision, resulting to a more recent date than what would have been predicted otherwise. Similarly, in the case of P. Petra 1 5 ( \u00a76.3), the decision was influenced by a reference to 'indiction' ('\u1f30\u03bd\u03b4\u03b9\u03ba\u03c4\u03af\u03c9\u03bd\u03bf\u03c2'), a word that refers to a periodic reassessment of taxation in the Late Roman Empire.\n\nIn the wild\nComputational dating can facilitate a macroscopic analysis of vaguely dated or undated manuscripts. By generating estimated dates for hundreds of such manuscripts, the expert can view from distance the collection, potentially drawing useful conclu-sions or making significant remarks. To test this hypothesis, we collected 220 manuscripts dated with an upper CE date limit (i.e., not after that date). We formed a committee of regressors, 15 and we estimated the minimum, the maximum, and the average chronology of each manuscript. In 28% of them, the maximum prediction exceeded the upper threshold and was discarded to avoid doubting the expert. This process led to the date estimation for 159 manuscripts, which we release publicly in our repository to assist other researchers. As can be seen in Figure 5 , some of our estimations fall far away from the upper limit (in red) while others fall close. The estimated date from our regressors' committee should be read along with other information, which is kept in the shared corpus, such as the place settlement (Figure 6 shows frequent places). We observe, for example, that in some places the estimated dates fall closer to the upper limit (e.g. in Oxyrhynchos and Tebtynis the distance is 132 years) compared to others (e.g. in Antinoopolis and Hermopolis the distance is 283 and 384 years).\n\nChallenges and limitations\nOur experimental analysis proved that text regression is a considerably reliable and accurate tool in dating nonliterary papyri. Limitations and challenges stem mainly from the composition of our dataset, which is balanced as far as the dates of the papyri included are concerned, both at the level of the century (approx. 40 records per century) and at the level of the quarter of the century (albeit less strictly and with the exception of the 7th CE). Furthermore, although we retained a substantial text sample of each papyrus, in approximately 1/4 of the records some text was eliminated.\n\nBiases\nDespite our effort to balance the dataset in terms of dates, biases are present. Since our main concern in collecting the data was for the date distribution, no deliberate selection was made on the basis of the document types. Some types are thus over or underrepresented (e.g. private letters that do not usually bear a date; \u00a76.2). Each type of document has however distinctive linguistic characteristics, such as the level of formality or unusual constructions (e.g. accounts). This uneven typological representation probably affects the performance of the models. Other possible biases in the dataset concern the provenance of papyri, the length of their text, and the state of conservation (sizeable portions of missing text or entirely missing parts of the documents).\n\nChronological analysis of words\nChronological analysis of word occurrence is possible if we detect and collect terms only attested in the papyrological material during a limited period. The word 'denarius' only appears after the 2nd CE and before the 5th CE, its presence in a text thus means that the text must have been written during this timespan. Likewise a text containing the word 'indiction' cannot have been written before the 4th CE. The investigation should also regard the possibility that the models make a prediction for a papyrus based on typical dating formulas present in the text like the name of the ruling emperor. Although our investigation of explanations did not yield any major concerns, a bigger sample of test cases should be created and more explainability methods should be employed (Ribeiro et al., 2016) to make conclusive remarks on this front.\n\nTranscription of papyri is not optional\nTranscription of the papyri is required (at least partial, but substantial) to reach this high degree of accuracy with our method. Thus, while there are transcriptions available for most already published papyri, it is less practical for dating unpublished papyri that have not been yet transcribed to a relatively high standard. In that case, image classification on the scripts can provide a less accurate prediction of the date as starting point.\n\nConclusion\nWe presented a machine-actionable dataset of 389 Greek documentary papyri of (mostly) Egyptian provenance, dated and balanced in terms of chronological quarter-century distribution. We trained extremely randomised trees on top of character n-gram-based features, reaching a mean absolute error of 54 years and 60% in century-level classification accuracy. We then formed a committee of regressors, which we applied to three use cases: a land lease, a private letter, and a geographical outsider (not from Egypt). To assist future research, our committee dated 159 manuscripts, for which only the upper limit is known. Future endeavours for this research extend far beyond the dating of individual manuscripts. It can produce valuable data for the study of the Greek language and its evolution through a millennium, help identify and trace linguistic habits and trends, as well as the history of document production, circulation, and use (e.g. which period produces what kind of texts, which administration relied on what type of documents, etc.). It can also produce further data and resources towards the typology of ancient Greek documents, completing with computational methods the work already underway and well-advanced of the grammateus project. Last, it can in the future fruitfully be combined with computational paleography to analyse the script and content of a given text.\n"}
{"question": "Which of the following is unrelated to our future work?", "evidence": "  For future work, we plan to extend our framework to faithfully correct misinformation in social media posts and news articles to inhibit the dissemination of false information. In addition, it may be meaningful to explore extending zero-shot factual error correction to multimedia task settings, such as identifying inconsistencies between chart and text   ", "options": ["A. We expand our framework to accurately rectify misinformation present in social media posts and news articles.", "B. Make contribution to the inhibition of  false information\u2019s dissemination.", "C. Investigate the extension of zero-shot factual error correction to multimedia task settings.", "D. Delete all the fake news on websites."], "answer": "D", "content": "\nIntroduction\nThe task of correcting factual errors is in high demand and requires a significant amount of human effort. The English Wikipedia serves as a notable case in point. It is continually updated by over 120K editors, with an average of around six factual edits made per minute 2 . Using machines to correct factual errors could allow the articles to be updated with the most current information automatically. This process, due to its high speed, can help retain the integrity of the content and prevent the spread of false or misleading information.\nIn addition, the hallucination issues have been shown to be a prime concern for neural models,\n\nEvidence\nThe novel COVID-19 is highly contagious and is transmitted mostly through respiratory droplets. But, whether its transmission can be forwarded by touching a surface (i.e., a fomite) is uncertain.... COVID-19 has a case fatality rate of below 2%.\n\nFinal Correction\nCOVID-19 is not infectious.\n\nInput Claim\nFigure 1 : An example of a factual but unfaithful correction leading to misleading information. While it is technically true that the majority of people infected with COVID-19 will recover, there is no information in the evidence that supports the final correction. Additionally, when this statement is taken out of context, it could mislead people to believe that COVID-19 is not dangerous and that there is no need for precautions, which is false. A factual and faithful correction is \"COVID-19 is highly contagious.\".\nwhere they are prone to generate content factually inconsistent with the input sources due to the unfaithful training samples (Maynez et al., 2020) and the implicit \"knowledge\" it learned during pre-training (Niven and Kao, 2019) . Factual error correction can be used in both pre-processing and post-processing steps to rectify the factual inconsistencies in training data and generated texts, respectively. This can help build trust and confidence in the reliability of language models.\nPrior work typically formulates factual error correction as a sequence-to-sequence task, either in a fully supervised or in a distantly supervised manner (Shah et al., 2020; Thorne and Vlachos, 2021) . While these approaches have made great strides in generating fluent and grammatically valid corrections, they only focus on the aspect of factuality: whether the outputs are aligned with facts. Little emphasis was placed on faithfulness: the factual consistency of the outputs with the evidence. Faithfulness is critical in this task as it indicates whether a generated correction reflects the information we intend to update. If faithfulness is not ensured, this could lead to the spread of misleading content, causing serious consequences. Figure 1 shows a concrete example. In the context of automatically updating textual knowledge bases, the topic of an unfaithful output would likely deviate much from that of the expected correction. Therefore, such an edit is not desirable, even if it is factual.\nIn this work, we present the first study on the faithfulness aspect of factual error correction. To address faithfulness, we propose a zero-shot factual error correction framework (ZEROFEC), inspired by how humans verify and correct factual errors. When humans find a piece of information suspicious, they tend to first identify potentially false information units, such as noun phrases, then ask questions about each information unit, and finally look for the correct answers in trustworthy evidence (Saeed et al., 2022; Chen et al., 2022) . Following a similar procedure, ZEROFEC breaks the factual error correction task into five sub-tasks:\n(1) claim answer generation: extracting all information units, such as noun phrases and verb phrases, from the input claim; (2) question generation: generating question given each claim answer and the original claim such that each claim answer is the answer to each generated question; (3) question answering: answering each generated question using the evidence as context; (4) QA-to-claim: converting each pair of generated question and answer to a declarative statement; (5) correction scoring: evaluating corrections based on their faithfulness to the evidence, where faithfulness is approximated by the entailment score between the evidence and each candidate correction. The highest-scoring correction is selected as the final output. An overview of our framework is shown in Figure 2 . Our method ensures the corrected information units are derived from the evidence, which helps improve the faithfulness of the generated corrections. In addition, our approach is naturally interpretable since the questions and answers generated directly reflect which information units are being compared with the evidence.\nOur contributions can be summarized as follows:\n\u2022 We propose ZEROFEC, a factual error correction framework that effectively addresses faithfulness by asking questions about the input claim, seeking answers in the evidence, and scoring the outputs by faithfulness. \u2022 Our approach outperforms all prior methods, including fully-supervised approaches trained on 58K instances, in ensuring faithfulness on two factual error correction datasets, FEVER (Thorne et al., 2018) and SCIFACT (Wadden et al., 2020) . \u2022 We analyze the correlation of human judgments with automatic metrics to provide intuition for future research on evaluating the faithfulness, factuality, and intelligibility of factual error corrections.\n\nTask\nIn Thorne and Vlachos (2021)'s setting, retrieved evidence is used, which means the model may be able to correct factual errors, even though there is no supporting information in the evidence. In this case, although the prediction is considered correct, the model is hallucinating, which is not a desired property. Additionally, due to the way data was collected, they require systems to alter the input claim even if the input claim is already faithful to the evidence. We argue that no edit is needed for claims that are faithful to the evidence.\nTo address these shortcomings, our setup aims to edit a claim using a given piece of grounded evidence that supports or refutes the original claim (see Figure 2 ). Using gold-standard evidence avoids the issue where a system outputs the correct answer by chance due to hallucinations. In our setting, a system must be faithful to the evidence to correct factual errors, allowing us to evaluate system performance more fairly. Furthermore, we require the model not to edit the original claim if it is already factually consistent with the provided evidence.\nConcretely, the input to our task is a claim C and a piece of gold-standard evidence E that supports or refutes C. The goal of factual error correction is to produce a corrected claim \u0108 that fixes factual errors in C while being faithful to E. If C is already supported by E, models should output the original claim (i.e. \u0108 = C).\n\nProposed Methods\nOur framework, ZEROFEC, faithfully corrects factual errors using question-answering and entailment.\nSpecifically, we represent the input claim C as question-answer pairs \n{(Q 1 , A C 1 ), ..., (Q n , A C n )} such that each question Q i reflects the corresponding information unit A C i ,\n\nCandidate Corrections\nNight of the Living Dead is a horror film.\n\nFinal Correction\nFigure 2 : An overview of our framework. First, given an input claim, we generate the claim answers by enumerating all information units in the input claim. Second, conditioned on each extracted answer and the input claim, a question is generated. Third, each question is then fed to a question answering model to produce an evidence answer using the given evidence as context. Fourth, using a sequence-to-sequence approach, each evidence answer and the corresponding question are transformed into a statement, which serves as a candidate correction. Finally, the final correction is produced by scoring candidate corrections based on faithfulness.\nanswer A E i in the given evidence E using a learned QA model ( \u00a73.3). Each candidate correction S i is obtained by converting the corresponding pair of Q i and A E i into a declarative statement ( \u00a73.4). This guarantees that the corrected information units we replace factual errors with are derived from the evidence and thus ensures high faithfulness. The final output of ZEROFEC is the S i with the highest faithfulness score computed by an entailment model ( \u00a73.5). An overview of our framework is shown in Figure 2 .\nOne major challenge that makes our task more difficult than prior studies on faithfulness (Wang et al., 2020; Fabbri et al., 2022a ) is that we need to handle more diverse factual errors, such as negation errors and errors that can only be abstractively corrected. For instance, in the second example of in Table 2 , the QA model should output \"Yes\" as the answer, which cannot be produced by extractive QA systems. To address this issue, we adopt abstractive QG and QA models that can handle diverse question types and train our QA-to-claim model on multiple datasets to cover cases that cannot be handled by extractive systems. The following subsections illustrate the details of each component in our framework.\n\nClaim Answer Generation\nThe goal of claim answer generation is to identify information units in the input claim that may be unfaithful to E. We aim to maximize the recall in this step since the missed candidates cannot be recovered in later steps. Therefore, we extract all noun chunks and named entities using Spacy 3 and extract nouns, verbs, adjectives, adverbs, noun phrases, verb phrases using Stanza 4 . Additionally, we also extract negation terms, such as \"not\" and \"never\", from the input claim. We name the extracted information units claim answers, denoted as\nA C = {A C 1 , A C 2 , ..., A C n }.\n\nQuestion Generation\nUpon claim answers are produced, we generate questions that will be later used to look for correct information units in the evidence. Questions are generated conditioned on the claim answers using the input claim as context. We denote the question generator as G. Each claim answer\nA C i is concatenated with the input claim C to generate a question Q i = G(A C i , C).\nWe utilize MixQG (Murakhovs 'ka et al., 2022) as our question generator G to cover the wide diversity of factual errors and candidates extracted. MixQG was trained on nine question generation datasets with various answer types, including boolean, multiple-choice, extractive, and abstractive answers.\n\nQuestion Answering\nThe question answering step identifies the correct information units A E i corresponding to each question Q i in the given evidence E. Our QA module answers questions from the question generation steps with the given evidence as context. Let F denote our QA model. We feed the concatenation of a generated question and the evidence to the QA model to produce an evidence answer (Khashabi et al., 2022) is used as our question answering model. UnifiedQA-v2 is a T5-based (Raffel et al., 2020b) abstractive QA model trained on twenty QA datasets that can handle diverse question types.\nA E i = F(Q i , E). UnifiedQA-v2\n\nQA-to-Claim\nAfter questions and answers are generated, we transform each pair of question and answer into a declarative statement, which serves as a candidate correction that will be scored in the next step. Previous studies on converting QAs to claims focus on extractive answer types only (Pan et al., 2021) . To accommodate diverse types of questions and answers, we train a sequence-to-sequence model that generates a claim given a question-answer pair on three datasets: QA2D (Demszky et al., 2018) for extractive answers, BoolQ (Clark et al., 2019) for boolean answers, and SciTail (Khot et al., 2018) for covering scientific domain QAs. Note that samples in BoolQ do not contain converted declarative statements. Using Stanza's constituency parser, we apply heuristics to transform all QAs to their declarative forms in BoolQ. Our QA-to-claim model is a T5-base fine-tuned on these three datasets. Concretely, let M denote our QA-to-claim model. M takes in a generated question Q i and an evidence answer A E i as inputs and outputs a statement\nS i = M(Q i , A E i ).\n\nCorrection Scoring\nThe final correction is produced by scoring the faithfulness of each candidate correction from the previous steps w.r.t. the evidence. We use entailment score to approximate faithfulness. Here, DocNLI (Yin et al., 2021) is used to compute such document-sentence entailment relations. Doc-NLI is more generalizable than other documentsentence entailment models, such as FactCC (Kryscinski et al., 2020) , since it was trained on five datasets of various tasks and domains. Conventional NLI models trained on sentence-level NLI datasets, such as MNLI (Williams et al., 2018) , are not applicable since previous work has found that these models are ill-suited for measuring entailment beyond the sentence level (Falke et al., 2019) . In addition, to prevent the final correction from deviating too much from the original claim, we also consider ROUGE-1 scores, motivated by Wan and Bansal (2022) . The final metric used for scoring is the sum of ROUGE-1 score 5 and DocNLI entailment score. Formally,\nEQUATION\nEQUATION\nwhere C \u2032 is the final correction produced by our framework. Furthermore, to handle cases where the input claim is already faithful to the evidence, we include the input claim in the candidate correction list to be scored.\n\nDomain Adaptation\nDuring the early stage of our experiments, we found that our proposed framework did not perform well in correcting factual errors in biomedical claims. This results from the fact that our QA and entailment models were not fine-tuned on datasets in the biomedical domain. To address this issue, we adapt UNIFIEDQA-V2 and DOCNLI on two biomedical QA datasets, PUBMEDQA (Jin et al., 2019) and BIOASQ (Tsatsaronis et al., 2015) , by further fine-tuning them for a few thousand steps. We later show that this simple domain adaptation technique successfully improves our overall factual error correction performance on a biomedical dataset without decreasing performance in the Wikipedia domain (see \u00a75.1).\n4 Experimental Setup\n\nDatasets\nWe conduct experiments on two English datasets, FEVER and SCIFACT. FEVER (Thorne and Vla-chos, 2021 ) is repurposed from the corresponding fact-checking dataset (Thorne et al., 2018 ) that consists of evidence collected from Wikipedia and claims written by humans that are supported or refuted by the evidence. Similarly, SCIFACT is another fact-checking dataset in the biomedical domain (Wadden et al., 2020) . We repurpose it for the factual error correction task using the following steps. First, we form faithful claims by taking all claims supported by evidence. Then, unfaithful claims are generated by applying Knowledge Base Informed Negations (Wright et al., 2022) , a semantic altering transformation technique guided by knowledge base, to a subset of the faithful claims. Appendix A shows detailed statistics.\n\nEvaluation Metrics\nOur evaluation focuses on faithfulness. Therefore, we adopt some recently developed metrics that have been shown to correlate well with human judgments in terms of faithfulness. BARTScore (Yuan et al., 2021) computes the semantic overlap between the input claim and the evidence by calculating the logarithmic probability of generating the evidence conditioned on the claim. FactCC (Kryscinski et al., 2020) is an entailment-based metric that predicts the faithfulness probability of a claim w.r.t. the evidence. We report the average of the COR-RECT probability across all samples. In addition, we consider QAFACTEVAL (Fabbri et al., 2022a) , a recently released QA-based metric that achieves the highest performance on the SUMMAC factual consistency evaluation benchmark (Laban et al., 2022) . Furthermore, we also report performance on SARI (Xu et al., 2016) , a lexical-based metric that has been widely used in the factual error correction task (Thorne and Vlachos, 2021; Shah et al., 2020) .\n\nBaselines\nWe compare our framework with the following baseline systems. T5-FULL (Thorne and Vlachos, 2021) is a fully-supervised model based on T5-base (Raffel et al., 2020a) that generates the correction conditioned on the input claim and the given evidence. MASKCORRECT (Shah et al., 2020) and T5-DISTANT (Thorne and Vlachos, 2021) are both distantly-supervised methods that are composed of a masker and a sequence-to-sequence (seq2seq) corrector. The masker learns to mask out information units that are possibly false based on a learned fact verifier or an explanation model (Ribeiro et al., 2016) and the seq2seq corrector learns to fill in the masks with factual information. The biggest difference is in the choice of seq2seq corrector. T5-DISTANT uses T5-base, while MASKCOR-RECT utilizes a two-encoder pointer generator. For zero-shot baselines, we selected two post-hoc editing frameworks that are trained to remove hallucinations from summaries, REVISEREF (Adams et al., 2022) and COMPEDIT (Fabbri et al., 2022b) .\nREVISEREF is trained on synthetic data where hallucinating samples are created by entity swaps.\nCOMPEDIT learns to remove factual errors with sentence compression, where training data are generated with a separate perturber that inserts entities into faithful sentences.\n\nImplementation Details\nNo training is needed for ZEROFEC. As for ZEROFEC-DA, we fine-tune UNIFIEDQA-V2 and DOCNLI on the BIOASQ and PUBMEDQA datasets for a maximum of 5,000 steps using AdamW (Loshchilov and Hutter, 2019) with a learning rate of 3e-6 and a weight decay of 1e-6.\nDuring inference time, all generative components use beam search with a beam width of 4.\n\nMain Results\nTable 1 summarizes the main results on the FEVER and SCIFACT datasets. Both ZEROFEC and ZEROFEC-DA achieve significantly better performance than the distantly-supervised and zeroshot baselines. More impressively, they surpass the performance of the fully-supervised model on most metrics, even though the fully-supervised model is trained on 58K samples in the FEVER experiment.\nThe improvements demonstrate the effectiveness of our approach in producing faithful factual error correction by combining question answering and entailment predictions. In addition, even though our domain adaptation technique is simple, it successfully boosts the performance on the SCIFACT dataset while pertaining great performance on the FEVER dataset. The first example in It is true that ZEROFEC-DA requires additional training, which is different from typical zero-shot methods. However, the key point remains that our framework does not require any task-specific training data. Hence, our approach still offers the benefits of zero-shot learning by not requiring any additional training data beyond what was already available for the question answering task, a field with much richer resources compared to the factchecking field.\n\nQualitative Analysis\nTo provide intuition for our framework's ability to produce faithful factual error corrections, we manually examined 50 correct and 50 incorrect outputs made by ZEROFEC on the FEVER dataset. The interpretability of ZEROFEC allows for insightful examinations of the outputs. Among the correct samples, our framework produces faithful corrections because all intermediate outputs are accurately produced rather than \"being correct by chance\". For the incorrect outputs, we analyze the source of mistakes, as shown in Figure 3 . The vast majority of failed cases result from DocNLI's failure to score candidate corrections faithfully. In addition to the mediocre performance of DocNLI, one primary reason is that erroneous outputs from other compo-nents would not be considered mistakes so long as the correction scoring module determines the resulting candidate corrections unfaithful to the evidence. A possible solution to improve DocNLI is to further fine-tune it on synthetic data generated by perturbing samples in FEVER and SCIFACT. Examples of correct and incorrect outputs are presented in Table 7 and Table 8 \n\nHuman Evaluation\nTo further validate the effectiveness of our proposed method, we recruited three graduate students who are not authors to conduct human evaluations on 100 and 40 claims from FEVER and SCIFACT, respectively. For each claim, human judges are presented with the ground-truth correction, the goldstandard evidence, and output produced by a factual error correction system and tasked to assess the quality of the correction with respect to three dimensions. Intelligibility evaluates the fluency of the correction. An intelligible output is free of grammatical mistakes, and its meaning must be T5-DISTANT's output: Fuller House ( TV series ) isn't airing on HBO.\nTable 2 : Example outputs from different approaches. The outputs from our framework are directly interpretable, as the generated questions and answers reflect which information units in the input claim are erroneous and which information in the evidence supports the final correction. We only show the generated answers and questions directly related to the gold correction. In the first example, ZEROFEC-DA corrects a mistake made by ZEROFEC thanks to domain adaptation. In the second example, ZEROFEC successfully produces a faithful factual error correction, whereas the output of T5-DISTANT, the distantly-supervised baseline, is factual yet unfaithful to the evidence.\nunderstandable by humans without further explanation. Factuality considers whether the input claim is aligned with facts. Systems' output can be factual and semantically different from the gold correction as long as it is consistent with the world's knowledge. Faithfulness examines whether the input is factually consistent with the given evidence. Note that a faithful output must be factual since we assume all evidence is free of factual error. To evaluate the annotation quality, we compute the inter-annotator agreement. Krippendorff's Alpha (Krippendorff, 2011 ) is 68.85%, which indicates a moderate level of agreement. Details of our human evaluation can be found in Appendix B.\nThe human evaluation results are demonstrated in Table 3 . We observe that: (1) ZEROFEC and ZEROFEC-DA achieve the best overall performance in Factuality and Faithfulness on both datasets, even when compared to the fully-supervised method, suggesting that our approach is the best in ensuring faithfulness for factual error correction.\n(2) Our domain adaptation for the biomedical domain surprisingly improves faithfulness and factuality in the Wikipedia domain (i.e. FEVER). This suggests that fine-tuning the components of our framework on more datasets helps improve robustness in terms of faithfulness.\n(3) Factual output produced by ZEROFEC and ZEROFEC-DA are always faithful to the evidence, preventing the potential spread of misleading information caused by factual but unfaithful corrections. The second example in Table 2 demonstrates an instance of factual but unfaithful correction made by baseline models. Here, the output of T5-DISTANT is unfaithful since the evidence does not mention whether Fuller House airs on HBO. In fact, although Fuller House was not on HBO when it premiered, it was later accessible on HBO Max. Therefore, the correction produced by T5-DISTANT is misleading.\n\nCorrelation with Human Judgments\nRecent efforts on faithfulness metrics have been mostly focusing on the summarization task. No prior work has studied the transferability of these metrics to the factual error correction task. We seek to bridge this gap by showing the correlation between the automatic metrics used in measure, the results are summarized in Table 4 .\nWe have the following observations. (1) SARI is the most consistent and reliable metric for evaluating Factuality and Faithfulness across two datasets. Although the other three metrics developed more recently demonstrate high correlations with human judgments of faithfulness in multiple summarization datasets, their transferability to the factual error correction task is limited due to their incompatible design for this particular task. For example, QA-based metrics like QAFACTEVAL are less reliable for evaluating faithfulness in this task due to their inability to extract a sufficient number of answers from a single-sentence input claim. In contrast, summaries in summarization datasets generally consist of multiple sentences, enabling the extraction of a greater number of answers. To validate this, we analyzed the intermediate outputs of QAFACTEVAL. Our analysis confirms that it extracts an average of only 1.95 answers on the FEVER dataset, significantly lower than the more than 10 answers typically extracted for summaries. (2) Across the two datasets, the correlations between all automatic metrics and Intelligibility are low. The extremely high proportion of intelligible outputs may explain the low correlation. (3) The correlation for learning-based metrics, including QAFACTEVAL and FACTCC, drop significantly when applied to SCIFACT. This is likely caused by the lack of fine-tuning or pre-training with biomedical data.\n6 Related Work\n\nFactual Error Correction\nAn increasing number of work began to explore factual error correction in recent years, following the rise of fact-checking (Thorne et al., 2018; Wadden et al., 2020; Gupta and Srikumar, 2021; Huang et al., 2022b) and fake news detection (Shu et al., 2020; Fung et al., 2021; Wu et al., 2022; Huang et al., 2022a) . Shah et al. (2020) propose a distant supervision learning method based on a masker-corrector architecture, which assumes access to a learned fact verifier. Thorne and Vlachos (2021) created the first factual error correction dataset by repurposing the FEVER (Thorne et al., 2018) dataset, which allows for fully-supervised training of factual error correctors. They also extended Shah et al. (2020) 's method with more advanced pre-trained sequence-to-sequence models. Most recently, Schick et al. (2022) proposed PEER, a collaborative language model that demonstrates superior text editing capabilities due to its multiple text-infilling pre-training objectives, such as planning and realizing edits as well as explaining the intention behind each edit 6 .\n\nFaithfulness\nPrevious studies addressing faithfulness are mostly in the summarization field and can be roughly divided into two categories, evaluation and enhancement. Within faithfulness evaluation, one line of work developed entailment-based metrics by training document-sentence entailment models on synthetic data (Kryscinski et al., 2020; Yin et al., 2021) or human-annotated data (Ribeiro et al., 2022; Chan et al., 2023) , or applying conventional NLI models at the sentence level (Laban et al., 2022) . Another line of work evaluates faithfulness by comparing information units extracted from summaries and input sources using QA (Wang et al., 2020; Deutsch et al., 2021) . There is a recent study that integrates QA into entailment by feeding QA outputs as features to an entailment model (Fabbri et al., 2022a) . We combine QA and entailment by using entailment to score the correction candidates produced by QA. Within faithfulness enhancement, some work improves factual consistency by incorporating auxiliary losses into the training process (Nan et al., 2021; Cao and Wang, 2021; Tang et al., 2022; Huang et al., 2023) . Some other work devises factuality-aware pre-training and fine-tuning objectives to reduce hallucinations (Wan and Bansal, 2022) . The most similar to our work are studies that utilize a separate rewriting model to fix hallucinations in summaries. For example, Cao et al. (2020) present a post-hoc corrector trained on synthetic data, where negative samples are created via perturbations. Adams et al. (2022) fix factually inconsistent information in the reference summaries to prevent the summarization from learning hallucinating examples. Fabbri et al. (2022b) propose a compression-based post-editor to correct extrinsic errors in the generated summaries. By contrast, we leverage the power of QA and entailment together to address faithfulness.\n\nConclusions and Future Work\nWe have presented ZEROFEC, a zero-shot framework that asks questions about an input claim and seeks answers from the given evidence to correct factual errors faithfully. The experimental results demonstrate the superiority of our approach over prior methods, including fully-supervised methods, as indicated by both automatic metrics and human evaluations. More importantly, the decomposability of ZEROFEC naturally offers interpretability, as the questions and answers generated directly reflect which information units in the input claim are incorrect and why. Furthermore, we reveal the most suitable metric for assessing faithfulness of factual error correction by analyzing the correlation between the reported automatic metrics and human judgments. For future work, we plan to extend our framework to faithfully correct misinformation in social media posts and news articles to inhibit the dissemination of false information. In addition, it may be meaningful to explore extending zero-shot factual error correction to multimedia task settings, such as identifying inconsistencies between chart and text (Zhou et al., 2023) .\n"}
{"question": "Which of the following is not the quantitative model performance metric used to evaluate models?", "evidence": "  We evaluate our models using NLP and machine translation metrics, including BLUE (Papineni et al., 2002; Lin and Och, 2004) , Perplexity, Relation Generation (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD), and Translation Edit Rate (TER) (Snover et al., 2006; Post, 2018) .   ", "options": ["A. Perplexity", "B. Relation Generation", "C. factual precision", "D. BLEU"], "answer": "C", "content": "\nIntroduction\nStudies have shown that captions can improve the recall and comprehension of the data that charts depict (Hegarty and Just, 1993; Large et al., 1995) . For instance, when a caption emphasizes visually prominent features of a chart, like a peak or a sharply declining trend, readers treat this information as the key takeaway (Kim et al., 2021) . Moreover, for people with visual disabilities, captions (or equivalent descriptions such as alt text) are often the only means of accessing the presented data. However, as evidenced by numerous guidelines (Jung et al., 2021) , producing high-quality * Both authors contributed equally to this work. chart captions is a non-trivial and laborious manual process. Thus, despite these advantages, charts are only rarely captioned in practice (Lundgard and Satyanarayan, 2022) .\nTo bridge this gap, several research communities have begun to explore methods for automatically generating chart captions, including using templates and heuristics (Demir et al., 2008; Srinivasan et al., 2019) , adapting image captioning techniques (Balaji et al., 2018; Chen et al., 2019a) , or via data-to-text machine translation (Kantharaj et al., 2022; Obeid and Hoque, 2020) . While promising, these approaches have largely produced captions that either describe a chart's construction (e.g., \"The graph is plot between 'Number of people' x-axis over 'Movie Genres' y-axis\" (Balaji et al., 2018) ) or provide statistical summaries (e.g., \"Machinery and equipment was the most valuable commodity for Singapore in 2019\" (Kantharaj et al., 2022) ). However, these captions do not articulate the perceptual and cognitive features that make charts a distinctive and compelling medium for communicating data (e.g., \"Prices of Big Tech corporations seem to fluctuate but nevertheless increase over time\" (Lundgard and Satyanarayan, 2022) ). Indeed, as Lundgard and Satyanarayan (2022) find, both sighted and blind readers strongly prefer captions that express this type of content.\nTo automatically produce such semantically richer captions, we introduce VisText: a benchmark dataset of 12,441 pairs of charts and captions. VisText makes two key extensions over prior approaches. First, VisText offers three representations of charts: a rasterized image and backing data table, as in previous work; and a scene graph, a hierarchical representation akin to a web page's Document Object Model (DOM), that presents an attractive midpoint between the affordances of chart-as-image and chart-as-data-table. Second, for each chart, VisText provides a synthetically generated caption detailing its construction as well as a crowdsourced caption describing its statistical, perceptual, and cognitive features. These crowdsourced captions represent a substantial increase in data over prior comparable datasets (Mahinpei et al., 2022; Kantharaj et al., 2022) .\nTo demonstrate the possible uses of the VisText dataset, we train three classes of models -textbased caption models, image-guided captioning models, and semantic prefix-tuning. Text-based captioning models fine-tune large language models for VisText's chart captioning task, revealing that both data table and scene graph representations can produce compelling and semantically rich captions. Following recent advancements in image-guided translation (Sulubacak et al., 2020) , we leverage the additional visual affordances in chart images to develop image-guided chart captioning models. Finally, since users often have varying preferences about the type of semantic content in their captions (Lundgard and Satyanarayan, 2022) , we apply semantic prefix-tuning to each of our models, enabling them to output customizable captions.\nOur models generate coherent, semantically rich captions across the VisText charts. Evaluating against standard machine translation and text generation metrics reveals that our models consistently output captions that accurately describe the chart's construction, such as its chart type, title, and axis ranges. Through qualitative analysis of our model's captions, we find that our model competently outputs semantically rich captions that describe data trends and complex patterns. Further, we categorize six common captioning errors that can inform the future development of chart captioning models on the VisText dataset.\nThe VisText dataset and source code are available at: https://github.com/mitvis/ vistext.\n\nRelated work\nHeuristic-Based Chart Captioning. Automatically generating natural language descriptions of data tables dates back to Reiter and Dale (1997) . Demir et al. (2008 Demir et al. ( , 2010 Demir et al. ( , 2012) ) survey this early work and describe the process of extracting insights from a chart by evaluating a list of propositions and composing selected propositions together to produce a natural language summary. More recently, data visualization researchers have explored heuristics that calculate summary statistics and templates to assemble natural language \"data facts\" (Srini-vasan et al., 2019) or descriptions (Cui et al., 2019) . While useful, these approaches yield standardized descriptions that lack the variation and linguistic construction that characterize semantically rich captions (Lundgard and Satyanarayan, 2022) .\nChart Captioning as Image Captioning. With rapid advances of neural image captioning (Vinyals et al., 2015; Anderson et al., 2018) , researchers have begun to adapt these methods for captioning charts. For instance, Balaji et al. (2018) develop a deep learning pipeline that ingests a PNG chart image, classifies the chart type, detects and classifyies textual content present in the chart, and uses this information to generate a textual description. Chen et al. (2019a Chen et al. ( ,b, 2020) ) propose a simpler workflow using ResNet to encode the chart image and an LSTM with Attention to decode it into a natural language description. Both approaches share a pair of limitations. The captions they produce convey relatively simplistic information about the chart (e.g., title, axis labels, etc.) or articulate concepts in visual rather than data terms (e.g., \"Dark Magenta has the lowest value\"). While both approaches contribute associated datasets, their charts and captions are synthetically generated and may not represent real-world counterparts. SciCap (Hsu et al., 2021) addresses this limitation by scraping real-world charts from 290,000 arXiv papers; however, the baseline models trained on this dataset struggle to generate semantically rich captions.\nChart Captioning as Text Translation. Perhaps closest to our contribution is recent work modeling chart captioning as a data-to-text problem. For instance, Spreafico and Carenini (2020) train an encoder-decoder LSTM architecture to generate a natural language caption from time series data. Similarly, Obeid and Hoque (2020) and Kantharaj et al. (2022) explore how transformer architectures can translate tabular structures into captions. These data-to-text methods are more successful than chart-as-image captioning, yielding captions that better capture relevant information from the charts and have higher BLEU scores. Nevertheless, we observe two limitations with these data-to-text approaches that motivate our contribution. First, data-to-text methods are heavily reliant on access to a chart's data table. In practice, data tables are only rarely published alongside charts and methods that recover equivalent information via OCR experience a significant drop in performance (Kantharaj et al., 2022) . Second, the associated datasets do not contain sufficient training examples of captions that express semantically rich insights about the depicted data (i.e., the perceptual and cognitive phenoma that distinguish charts as a medium as distinct from data tables (Lundgard and Satyanarayan, 2022) ). As a result, while the generated captions are compelling, they are largely limited to reporting statistics which sighted and blind readers prefer less than captions that convey complex trends and patterns (Lundgard and Satyanarayan, 2022) .\n\nThe VisText Dataset\nWe designed the VisText dataset in response to two limitations existing datasets present for generating semantically rich chart captions. First, existing datasets represent charts as either rasterized images or as data tables. While useful, these representations trade off perceptual fidelity and chart semantics in mutually exclusive ways -images capture the perceptual and cognitive phenomena that are distinctive to charts (e.g., trends or outliers) but pixels cannot express the rich semantic relationships between chart elements (e.g., estimating plotted data values using axis labels). While the vice-versa is true (Lundgard and Satyanarayan, 2022) , tables also present additional caveats. There is not always a one-to-one relationship between the semantics of a data table and chart (i.e., one data table may be the source for several distinctly different charts). Moreover, data tables are rarely published alongside charts; and, automatic data table extraction is error-prone due to the diversity of chart types and visual styles as well as the difficulty of reasoning about visual occlusion (Kantharaj et al., 2022; Luo et al., 2021; Jung et al., 2017) ).\nSecond, if existing datasets provide captions that describe perceptual or cognitive features, these captions comprise only a small portion of the dataset. At best, LineCap (Mahinpei et al., 2022) offers 3,528 such captions for line charts only, while Chart-to-Text (Kantharaj et al., 2022) estimates that roughly 15% of the sentences in its captions across a variety of chart types express such content.\nIn contrast, VisText provides 12,441 crowdsourced English captions that articulate statistical, perceptual, and cognitive characteristics of bar, line, and area charts. In VisText, charts are available as not only data tables and rasterized images but also as scene graphs. Scene graphs are hierarchical representations that better preserve perceptual fidelity and chart semantics, are often the format for publishing web-based charts, and can be recovered from chart images (Poco and Heer, 2017) .\n\nData Table Collection\nThe data tables found in VisText are sourced from the Statista dataset of the Chart-to-Text benchmark (Kantharaj et al., 2022) . The tables were collected by crawling Statista.com in December 2020 and contain real-world data related to technology, trade, retail, and sports. We process these tables to make them amenable for chart generation, including stripping formatting symbols (e.g., $ and %), standardizing data strings, and identifying the measure type of each column (i.e., quantitative, categorical, or temporal). Data tables are discarded if they do not contain at least one quantitative field and one categorical or temporal field, or if other errors occur during the processing steps. We further down select to data tables containing between 2 to 20 columns and 10 to 500 rows. If a data table has over 500 rows, we randomly sample rows. In larger data tables, this step potentially affects how salient a trend is.\n\nChart Generation and Representation\nCharts in the Chart-to-Text Statista dataset all feature the same layout and visual appearance. In contrast, we aim for richer visual diversity by generating charts using the Vega-Lite visualization library (Satyanarayan et al., 2016) via the Python Altair package (VanderPlas et al., 2018) . To facilitate collecting high-quality captions, we focus on univariate charts: charts that depict one quantitative observation against a categorical or temporal variable. This focus is informed by recent work in the data visualization research community which has chosen single-series line charts as the target of study for natural language descriptions (Kim et al., 2021; Stokes et al., 2022) . VisText also includes single-series bar and area charts as they typically exhibit similar perceptual features to line charts.\nFor each data table, we iterate through pairs of univariate fields. If the pair contains a temporal field, we randomly generate an area or line chart; if the pair contains a categorical field, we randomly generate a horizontal or vertical bar chart. For diversity in layout and visual appearance, we randomly rotate axis labels and apply one of fourteen themes provided by the Vega-Lite library. These themes mimic the visual style of common chart platforms or publishers (e.g., ggplot2 or the LA Times). \n\nGenerated L1 Caption\nHere is a area chart is labeled Cumulative number of patients diagnosed with coronavirus (COVID-19) in Japan as of December 4, 2020, by place of infection. On the x-axis, Month is measured with a categorical scale starting with April and ending with October. There is a linear scale with a minimum of 0 and a maximum of 150,000 along the y-axis, labeled Patients within Japan.\n\nCrowdsourced L2/L3 Caption\nBy December 4th 2020, approximately 160,000 people in Japan had been diagnosed with COVID-19. The first person diagnosed with COVID-19 in Japan was diagnosed in March 2020. The greatest increase in cumulative number of patients in Japan diagnosed with COVID-19 occurred between November and December 2020. In VisText, each chart is represented as a rasterized image, stored as an RGBA-encoded PNG file, as well as a scene graph. A scene graph is a textual representation of the rendered chart similar to a web page's Document Object Model (DOM). Scene graphs encode the position, value or content, and semantic role of all visual elements within a chart, including the individual marks (i.e., bars or points along the line), titles, axes gridlines, etc. Thus, scene graphs express the perceptual features of rasterized images in a more computationallytractable form.\n\nCumulative number of patients diagnosed with coronavirus (COVID-19) in\nScene graphs are a standard data structure for representing vector-based graphics -the most common format for publishing visualizationsand, thus, can be trivially recovered (e.g., by traversing the SVG text string). We extract the scene graph directly from the rendered chart using the Vega-Lite API. As most text generation models expect a linear set of input tokens, we flatten the scene graph via a depth-first traversal. To scale to large language models, we need to further reduce the size of the scene graph. Thus, we preserve the following elements which we hypothesize as being most critical for generating semantically rich captions: title, title coordinates, axis labels, axis label coordinates, axis tick coordinates, mark coordinates, and mark sizes. VisText includes both the original (hierarchical) and reduced (linearized) scene graphs.\n\nCaption Generation and Collection\nOur captioning process is guided by the framework developed by Lundgard and Satyanarayan (2022) , which identifies four levels of semantic content: L1 content enumerates aspects of the chart's construction (e.g., axis ranges); L2 content reports summary statistics and relations (e.g., extrema); L3 content synthesizes perceptual and cognitive phenomena (e.g., complex trends); and, L4 content describes domain-specific insights (e.g., sociopolitical context). In subsequent studies, the authors find that while sighted readers typically prefer higher levels of semantic content, blind readers are split about the usefulness of L1 and L4 content. Thus, given these differing preferences, we define a single caption to express multiple levels of content separated across clauses or sentences. We only consider the first three levels of this model, and leave L4 content to future work. Following guidelines prescribed by the National Center for Accessible Media (NCAM), our captions begin with L1 content and then turn to L2 and L3 content (Gould et al., 2008) .\nWe algorithmically generate L1 content and use a crowdsourced protocol to collect L2 and L3 content. This approach follows (Lundgard and Satyanarayan, 2022)'s computational considerations as well as results from Morash et al. (2015) who find that, even with instructions and guidelines, crowd workers do not describe a chart's structural elements sufficiently for blind readers. Thus, synthetically generating L1 content allows us to ensure that captions convey complete descriptions of the chart's structural elements. L1 content comprises 1 sentence conveying the chart type and title, and then 1 -2 sentences describing the axes (including the titles, ranges, and scales). We use template randomization to generate a diverse range of L1 captions to mimic human variability and reduce the capacity of the model to overfit to a single L1 style. Three templates are defined for the first sentence and twenty-six template combinations for the subsequent sentences. During generation, we randomly select a pair of templates and fill in in- formation from the abstract chart specification. For additional diversity, we randomly drop scale information and swap template words with synonyms. Templates and synonym replacements are listed in Appendix E.2.\nTo crowdsource L2 and L3 content, we extend the protocol used by Lundgard and Satyanarayan (2022) . After soliciting consent, we introduce the task: participants are presented with a chart image and corresponding L1 description; they are asked to write a description about the trends and patterns they observe without drawing on background knowledge or repeating L1 information. The introduction provides examples and explanations of valid and invalid responses. After acknowledging these examples, participants are asked to complete 5 random iterations of the task. To maximize the quality of our crowdsourced captions, we manually curated the charts and L1 descriptions used in the study. We discarded any charts that were challenging to read (e.g., colors were too similar, marks were not easily readable, etc.). Participants were recruited on the Prolific.co platform, took approximately 14 minutes to complete the study, and were compensated $3.25 ($14/hour). Additional details on our crowdsourcing process are in Appendix E.3.\nWe manually verified charts where participants failed an attention check and discarded invalid descriptions. Additionally, we manually inspected captions for personally identifiable information or offensive content. Using heuristics, we removed captions where respondents described charts as unclear or illegible and replaced newline characters with spaces. Although we attempted to fix incorrect spelling and casing errors using a similar heuristic-based approach, we observed that this process could improperly affect axis and chart names. As a result, these errors remain in our dataset.\n\nDataset Analysis\nFigure 2 shows the distribution and means of the lengths of chart representations and captions. Synthetically generated L1 captions have roughly 1.5x more characters than crowdsourced L2/L3 captions (\u00b5 = 255 vs. \u00b5 = 177) but the average number of sentences are comparable (2.5 vs. 2). The VisText dataset consists of captions for 3,189 area charts, 6,238 bar charts, and 3,014 line charts -the roughly twice-as-many bar charts as area or line charts corresponds to the randomization of temporal fields during chart generation (Sec. 3.2). As some charts have multiple crowdsourced captions, we randomly split our dataset into training, validation, and test sets using the chart IDs to prevent data leakage across sets. This resulted in an approximate ratio of 80:10:10.\nFinally, to understand the distribution of semantic content, we manually coded 2% (230) of crowdsourced captions. We followed a protocol inspired by Lundgard and Satyanarayan (2022) by breaking sentences down into independent statements and mapping these statements to their semantic content level. We marked statements as not categorizable if they did not map to the framework -for instance, if captions expressed commentary from crowd workers such as \"this chart is hard to read.\" Our analysis revealed 11 L1 statements (2.4%), 180 L2 statements (39.7%), 253 L3 statments (55.7%), and 10 not categorizable statements (2.2%). While a handful express L1 content, the bulk of statements (95%) express L2 or L3 content, with approximately 1.4x L3 statements than L2.\n\nChart Captioning Models\nTo demonstrate the affordances of the VisText dataset, we train three classes of models. First, we fine-tune large language models to translate from textual chart representations to natural lan-guage captions. These models evaluate the feasibility and impact of scene-graph models compared to prior data-table approaches (Kantharaj et al., 2022) . Second, as VisText provides multiple chart representations, we adapt image-guided translation (Sulubacak et al., 2020; Cho et al., 2021) to develop two multimodal chart captioning models: image-scene-graph and image-data-table. Finally, since VisText offers captions at different semantic levels and prior work has shown significant differences in readers' preferences (Lundgard and Satyanarayan, 2022) , we explore prefix-tuned models that selectively output L1, L2/L3, or L1+L2/L3 captions. Training details are in Appendix D.\n\nText-Based Chart Captioning\nInformed by prior work (Kantharaj et al., 2022) , we investigate text translation models for generating chart captions. In particular, Kantharaj et al. found that models that translate data tables to chart captions significantly outperform image captioning models. However, when data tables were not available, the authors found a significant drop in their models' ability to extract relevant information from the chart -an effect that was only slightly ameliorated by using OCR methods to extract text from chart images. In contrast, VisText's scene graphs can be more readily recovered from charts when data tables are not available -for instance, by processing the SVG format of web-based visualizations. Moreover, scene graphs offer a potentially richer source of information than data tables as they encode visual properties of the chart (e.g., coordinates and colors) and are less noisy than tokens recovered via OCR. Thus, to evaluate the feasibility and efficacy of scene graphs, we train a scene-graph text translation model and a baseline data-table model for comparison.\nFor each model, we fine-tune a pretrained ByT5 transformer model (Xue et al., 2022) on the Vis-Text dataset. We choose ByT5 over T5 transformers (Raffel et al., 2020) because it uses a token-free, byte-encoding that eliminates the use of a tokenizer. As a result, it is robust to noisy inputs, minimizes the need for text preprocessing, and eliminates the out-of-dictionary problem. This allows our model to handle common typographical and chart reading errors in the crowdsourced L2 and L3 captions and increases generalizability to previously-unseen words that could be present in chart and axes titles.\n\nImage-Guided Chart Captioning\nFollowing recent advancements in image-guided machine translation (Sulubacak et al., 2020) , we train image-guided captioning models using the VisText dataset. Images have improved text-based machine translation models by providing visual information complementary to natural language inputs. Similarly, chart images can contain visuals complementary to the textual specification. For instance, visual affordances that are important for perceiving a trend (e.g., gestalt relations, relative sizes/areas, etc.) may be obfuscated in the scene graph but better captured in the chart image.\nWe train three image-guided chart captioning models: image, image-scene-graph, and image-data-table. All models leverage the vision-language transformer model VL-T5 (Cho et al., 2021) . VL-T5 is pretrained on image captioning and visual grounding tasks and was successfully applied to machine translation, making it suitable for chart captioning. We extract visual features for each VisText chart image using a Bottom-Up Feature Extractor (Anderson et al., 2018) . To explore the value of images to chart captioning, our image model only takes in the image features, while image-scene-graph and image-data-table concatenate the image features with the chart's textual representations (scene graph or data table).\n\nSemantic Prefix-Tuning\nIn real-world chart captioning settings, users want to vary the level of semantic content in their captions. For instance, while some blind users want verbose captions that describe the chart visuals, sighted users may only want captions that help them expose data trends (Lundgard and Satyanarayan, 2022) . To develop models capable of such customization, we leverage prefix-tuning strategies alongside VisText's semantic caption breakdown. Prefix-tuning specifies a task alongside the input, permitting a single large language model to perform many different tasks. In our setting, we use prefix-tuning to specify the level of semantic content to include in the caption (Li and Liang, 2021) .\nWe train each of our models with and without semantic prefix-tuning. With semantic prefix-tuning, we treat chart captioning as a multi-task fine-tuning problem, where the model is trained to generate the L1 and L2/L3 captions separately. In every epoch, the model sees each VisText chart twice, once with the L1 prefix and caption and once with the L2/L3 prefix and caption.\n\nEvaluation and Results\nTo evaluate the VisText dataset and our chart captioning models, we measure the readability and accuracy of generated captions and their similarity to the VisText target caption. We also qualitatively analyze the descriptiveness of generated L2/L3 captions and categorize common errors.\n\nQuantitative Model Performance\nWe evaluate the results of our text-based and imageguided captioning models with and without prefixtuning. We also compare to a current state-of-theart chart captioning model that uses data table chart representations and a T5 generation model (Kantharaj et al., 2022) . To measure the quality of output captions, we evaluate each model on machine translation and language generation metrics (Table 1 ).\nChart images do not support captioning. The image model performs the worst of all the chart captioning models. Its low perplexity and high error rates indicate it is highly confident in its inaccurate captions. While chart images contain the same information encoded in the chart's textual representations, it is presumably not adequately extracted by the model. Both the image model backbone (Cho et al., 2021) and the visual feature extractor (Anderson et al., 2018) are trained on natural images, making chart images out-of-distribution inputs that are likely to be poorly represented by these vision models. As the chart captioning task grows, model backbones, architectures, and feature extractors could be customized to chart images, which may improve image-based chart captioning.\nAll models produce high quality L1 captions. In our chart captioning setting, relation generation (Wiseman et al., 2017) measures how often the chart title, axis names, and axis scales in the input appear in the caption. Every model (except image) achieves a similarly-high relation generation score, indicating that every model can generate detailed L1 captions.\nScene graphs perform as well as data tables. Models trained on scene graph representations achieve similar performance across the evaluative metrics to models trained on data tables. As scene graphs can be more easily extracted from web-based charts images, they may be the preferred representation for future chart captioning models.\nImage-guiding does not improve captioning. Our image-guided captioning models do not experience the significant increase in performance other image-guided translation tasks report. While in image-guided translation, images contain substantial additional information beyond the text, the image and textual representations in chart captioning often contain highly similar information. The small amount of additional information in images might benefit complex captioning tasks on multivariate charts or infographics; however, the current VisText captions rarely reference visual information not present in the scene graph or data table.\nPrefix-tuning is free. Adding semantic prefixtuning to our models does not significantly change their performance. Models trained with and without prefix-tuning are exposed to the same set of charts, so it is consistent that prefix-tuning would not impact the quality of output captions. Given prefix-tuned models are able to output L1, L2/L3, and L1+L2/L3 captions, prefix-tuning may be preferred if users require semantic customization.\n\nQualitative Caption Evaluation\nTo augment our quantitative evaluation, we qualitatively assess the descriptiveness and accuracy of the generated chart captions. Since L1 caption accuracy can be measured at scale via relation generation, we focus our evaluation on L2/L3 predictions.\nPrior analysis tasked annotators with comparing the accuracy, coherence, and fluency of generated captions compared to a target caption (Kantharaj et al., 2022) . Instead, our approach follows an inductive qualitative data analysis approach: iteratively analyzing captions in a \"bottom-up\" fashion to identify emergent patterns in how generated captions compare to the ground truth (Bingham and Witkowsky, 2021). We randomly sample 176 generated captions from the scene-graph model with prefix-tuning and break them into their independent L2 and L3 statements, resulting in 181 (48.27%) L2 statements and 194 (51.73%) L3 statements.\nApproximately half (241 / 512) of the L2 and L3 statements made in the generated captions are factually accurate. Moreover, many of the full sentences are written in a natural, human-like manner and generated captions frequently include both compound and complex sentences. On average, every generated caption has one L3 statement and zero to et al., 2022) . We evaluate each model using machine translation and text generation metrics, including BLEU (Papineni et al., 2002) , Perplexity, Relation Generation (RG) (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD) (Kusner et al., 2015) , and Translational Error Rate (TER) (Snover et al., 2006) . We report the mean and standard deviation of three independent models. Darker colors indicate better scores.\nInput PT BLEU \u2191 Perplexity \u2193 RG \u2191 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193 Kantharaj et\ntwo L2 statements. Often this takes the form of a L3 general trend statement (e.g., \"The median annual family income in Canada has increased from 2000 to 2018\") accompanied by an L2 minimum and maximum statement (\"The highest was in 2015 at 80k and the lowest was in 2000\"). For the remaining half of analyzed captions, we identified the following recurring types of errors:\nIdentity Errors. We identify 86 identity errors (22.93% of analyzed statements). An identity error occurs when an L2 or L3 statement incorrectly reports the independent variable for a given (often correctly identified) trend. For bar charts, this error means incorrectly reporting the categorical label associated with a bar (e.g., in Appendix Figure 5c : \"The most popular music activity is vinyl albums and vinyl singles\" should be \"The most popular music activity is tickets for festivals\"). For area and line charts, this error means incorrectly identifying the temporal point or range of the trend. With bar charts, in particular, we observed that the identities were often \"off-by-one\" (i.e., identifying a minimum or maximum value, but attributing it to the second-highest or second-lowest category).\nValue Errors. A value error occurs when the quantitative data value of a statement is incorrect.\nOf the captions we analyzed, 3.20% (12) of statements contained a value error. For instance, as shown in Appendix Figure 4c , for the caption \"The total gate revenue from sporting events worldwide by region from 2006 to 2015 has increased from around 15 billion dollars to around 15 billion dollars\", the value should be around 18 billion dollars.\nIf it is ambiguous whether an error is an Identity or Value Error, we classify it as the former.\nDirection Errors. A direction error occurs when the direction (which can be increasing, decreasing, or stable) of a trend in an L3 statement is incorrect. We uncovered 32 direction errors (8.53% of analyzed statements). For instance, in the caption \"The per capita consumption of sweet corn in the US has increased from 2000 to 2019\" (Appendix Figure 3c ), the trend is actually decreased. In most direction errors, the identity (i.e., temporal range) is correct.\nStability Errors. A stability error occurs when the magnitude of a direction or the variance in a trend is incorrect. This can often refer to how much a trend is increasing or decreasing, such as rapidly or slowly, as well as whether it's a steady change or highly-fluctuating change. Looking ahead, while accessibility remains a key domain that would benefit from automated chart captioning, and deploying automated chart captioning models into the field is an exciting prospect, we believe the most promising approach for future work lies in \"mixed-initiative\" (i.e., human + AI) chart authoring systems. In particular, as we describe in our Ethics Statement below, chart captioning models are currently prone to make a number of factual inaccuracies which can have severe harmful consequences. On the other hand, by integrating these models into chart authoring systems (e.g., Tableau, Charticulator, Data Illustrator, or Lyra), chart authors can intervene and make any necessary corrections. Indeed, such integration offers exciting opportunities to develop novel interactive methods for verifying generated captions. For instance, models like ours could generate an initial caption (or set of captions) based on the chart currently being authored; as the system has access to all three representations of the chart (the back-ing data table, chart image, and structured scene graph), it might automatically segment the caption into independent \"data segments\" and interactively link and map them to rows in the table or regions on the chart, akin to Kori (Latif et al., 2021) .\n\nLimitations\nComputational Constraints. Despite using modern GPUs, with large amounts of memory, we were forced to use the smallest-parameter variants of T5 and ByT5 as we encountered out-of-memory errors with the larger alternatives. More problematically, the quadratic relationship between sequence length and time/space complexity of transformer architectures (Vaswani et al., 2017) , especially when using byte-level sequences (Xue et al., 2022), has had a significant impact on our model performance.\nIn particular, to be computationally tractable, we were forced us to truncate our input and output sequences to, at most, 1,024 and 512 characters respectively (1,024 coming from the underlying ByT5 architecture (Xue et al., 2022)).\nThese character thresholds have likely had an outsized effect on scene-graph models. For instance, due to these character limits, we reduced scene graph sequences to only a minimal set of visual characteristics; VisText also includes the raw, unprocessed scene graphs which offer a richer source of information about the visual features that are important to how people decode charts (e.g., bounding boxes, color) but were unavailable to our models. Moreover, as Figure 2 shows, even with this reduced representation, the mean length of scene graph sequences is 948 characters (cf. 426 characters for data tables) with a wide distribution. Thus, despite scene-graph models achieving comparable performance to data-table models, the former saw a much smaller proportion of complete sequences as compared to the latter. This truncation step additionally negatively impacts charts with long titles or axis names -in such cases, we observed that the L2 or L3 caption would be altogether truncated before generation.\n\nChart Types and the Visualization Design Space.\nVisText is scoped to only univariate bar, area, and line charts. We chose to begin with these chart types informed by data visualization research that has focused on studying natural language descriptions of single-series line charts -a basic, but commonly occurring chart type that offers a compelling target of study as it most visibly surfaces any poten-tial trends in the data (Kim et al., 2021; Stokes et al., 2022) . Future work can now begin to consider more complex chart forms in a step-by-step manner. For instance, moving from univariate bar, area, and line charts to multivariate versions of these chart types (i.e., stacked bars and areas, grouped bars, and multi-series line charts). From there, work can also consider chart types that surface perceptual and cognitive phenomena in visually distinct ways (e.g., scatterplots, where trends appear as clusters of points; heatmaps, where color saturation often encodes a trend; or maps, where color or other layered elements such as symbols are used to represent data values). Finally, automated methods for captioning visualizations may eschew chart typologies altogether in favor of visualization grammars -by offering a more composable and combinatorial approach to the design space (Wilkinson, 2012) , learning over visualization grammars may offer a more robust approach to captioning highly customized or unique visual forms.\nFor each future work direction, we anticipate scene graph representations to prove more fruitful than the data table. As the complexity of the visualization increases, its relationship to the data table only grows more ambiguous; the scene graph, on the other hand, directly encodes the visual form and thus remains faithful to it. As a result, to support such future work, VisText provides the raw specifications used to produce our charts (via the Vega-Lite visualization grammar (Satyanarayan et al., 2016) ) as well as the raw, hierarchical scene graphs prior to our linearization and reduction step.\n\nEthics Statement\nThe Consequences of Incorrect Captions. Weidinger et al. ( 2021) comprehensively survey the risks associated with the large language models (LLMs) that underlie our contribution. Of the six categories of risk they identify, harms stemming from models producing factually incorrect statements are not only most pertinent to our work, but are likely heighted as compared to general uses of LLMs given the context we are addressing: automatically captioning charts. In particular, people most often consume charts and visualizations in order to make data-driven decisions (Keim et al., 2008) -for instance, about whether to evacuate ahead of a hurricane (Padilla et al., 2018) , or health & safety during the pandemic (Shneiderman, 2020) . Moreover, recent results have shown that readers not only fixate for longer and are more likely to recall the textual content of and around visualizations (Borkin et al., 2015) but this textual content can strongly influence the takeaway message readers leave with even when it is at odds with the depicted data (Kong et al., 2018 (Kong et al., , 2019)) . Finally, these issues are exacerbated by the persuasive and rhetorical force of data and charts (Kennedy et al., 2016; Hullman and Diakopoulos, 2011) , that often project a sense of authority and certainty (Correll, 2019) . As a result, readers may not think to double check the accuracy of chart captions, and inaccurate statements that models may produce could lead to harmful downstream decisions.\nTo proceed ethically with this line of research, we believe that advances in data and modeling need to be closely followed by attention devoted to mitigating the risks of incorrect statements. At base, automatically generated captions should be identified as such at the forefront to raise readers' awareness about the potential for incorrect statements. And, interactive visual linking strategies (such as those explored by Kong and Agrawala (2012) ; Kim et al. ( 2018)) could be deployed to help readers manually verify the constituent statements of a caption against the chart. These strategies, however, place the burden of harm mitigation on readers. Thus, an alternate approach might never surface automatically generated captions to readers directly but instead use them as part of mixed-initiative systems for jointly authoring visualization and text, such as Kori (Latif et al., 2021) . In such systems, automated chart captioning models would help to accelerate the authoring process -combatting the blank slate problem by providing an initial summary of the chart -and chart authors would make any necessary corrections prior to publication.\nBesides these human-computer interaction (HCI) approaches for mitigating harm, an equally important direction for future work should leverage interpretability techniques to more deeply study what the models are learning. To what degree are chart captioning models stochastic parrots (Bender et al., 2021) , and how much do they understand the information charts depict? Automated Captioning for Accessibility. Although accessibility is a guiding motivation for the bulk of work in automated captioning (be it image captioning or, as in our case, chart captioning), studies find mixed reactions, at best, about these approaches among people with disabilities (PWDs).\nFor instance, accessibility educator and researcher Chancey Fleet described Facebook's automatic image descriptions as \"famously useless in the Blind community\" despite \"garner[ing] a ton of glowing reviews from mainstream outlets\" (Fleet, 2021; Hanley et al., 2021) . This disconnect appears to stem from a more fundamental mismatch between what PWDs describe as their captioning needs, and what the research community -particularly through its automatic, quantitative evaluationsprioritizes (Jandrey et al., 2021) . In particular, surveys with PWDs repeatedly surface the contextual nature of captions. Bennett et al. (2021) find that the context of use shapes the degree to which PWD are comfortable with captions describing people's race, gender, and disabilities -for instance, changing their preferences if they were in a white, cisgender, nondisabled, and professional company versus their own community. Similarly, Jung et al. (2022) find shifting preferences for the content image descriptions should convey across different photo activites -for example, when viewing or taking photos, participants wished for descriptions that conveyed spatial cues whereas when searching or reminiscing about photos, participants hoped for descriptions to connect to personal data or differentiating details.\nIn contrast, quantitative metrics of model performance compare generated captions to a single \"ground truth\" caption. This framing of success not only makes it difficult to develop contextuallyvarying caption generation but can actively penalize such investigations. For instance, with our work, we explored how prefix-tuning can be used to develop models that are responsive to users' preferences about semantic content. However, as described in Sec. 5.1, existing quantitative metrics of model performance (e.g., BLEU, ROUGE, WMD, and TER) show a drop in model performance despite our qualitative analysis indicating that these captions are indeed high quality.\nFinally, our exploration of semantic prefixtuning represents only a very preliminary step towards addressing the contextual captioning needs of PWDs. In particular, the semantic labels Vis-Text assigns to captions were derived from prior work (Lundgard and Satyanarayan, 2022) that only explored natural language descriptions when consuming presentations of visualizations -one task from a broader palette (Brehmer and Munzner, 2013) . Future work might instead extend the Vis-Text dataset -and corresponding models -to consider captions for a broader range of tasks including consuming visualizations for scientific discovery, enjoyment or, producing, searching, or querying visualizations (Brehmer and Munzner, 2013) . \n\nModel Generated L1 Caption\nAverage spending per consumer on selected music activities in the United States as of July 2018 is a bar graph. The x-axis measures Response while the y-axis measures $40 to $99.99.\n\nModel Generated L2/L3 Caption\nThe most popular music activity is vinyl albums and vinyl singles.\nThe least popular music activity is vinyl albums. (b) Model results using the L2/L3 captions.\nTable 2 : We separately evaluate our L1 and L2L3 captions on all the same metrics except for Relation Generation.\nIn general, we observe that L1 captions perform better than the L2/L3 captions. Our models generate verbose L1 captions that are similar to the structure of our L1 templates, while the L2/L3 captions are human-generated and contain more variability. Darker colors indicate better scores.\n\nB Additional Evaluations\nB.1 Independent L1 and L2/L3 Caption Evaluation\nTo better understand how our models generate varying levels of semantic content, we separately evaluate our prefix-tuned models on L1 captioning and L2/L3 captioning tasks. Each prefix-tuned model can output an L1 or an L2/L3 caption for each chart. We evaluate these captions to their respective L1 or L2/L3 ground truth captions and report the results in Table 2 . Since we compute Relation Generation using only the L1 chart fields (e.g., chart title, axis scale, etc.), we do not report the results separately for L1 versus L2/L3 captioning. There is no direct Relation Generation analog for L2/L3 captions, since they are human-generated and do not follow a specific template. The Relation Generation for L1 captions is identical to the Relation Generation for L1/L2/L3 captions reported in Table 1 .\n\nB.2 Evaluation Details\nQuantitative Model Performance Metrics. We evaluate our models using NLP and machine translation metrics, including BLUE (Papineni et al., 2002; Lin and Och, 2004) , Perplexity, Relation Generation (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD), and Translation Edit Rate (TER) (Snover et al., 2006; Post, 2018) . We implement Relation Generation per Wiseman et al. (2017) , use the Gensim implementation of WMD, and use the Hugging Face implementation (Wolf et al., 2019) for the remaining metrics.\n\u2022 BLEU: BLEU requires several gold standard references. In our evaluation setup, we use the test set caption as a single reference.\n\u2022 Perplexity: We use a pretrained GPT-2 Medium model to compute Perplexity.\n\u2022 Relation Generation: The fields we evaluate on are the chart title, axis names, and axis scales (if any).\n\u2022 Translation Edit Rate (TER): Edits consist of deletions, additions, and substitutions, as present in SacreBLEU.\nQualitative Caption Evaluation. To produce our qualitative evaluation results (Sec. 5.2), we iteratively evaluated randomly sampled captions until there was no more marginal information about they types of errors to be gained from evaluating more captions. For each L2/L3 caption, we assess the number of independent, mutually-exclusive L2 and L3 claims/statements that are being made. In comparison to evaluating at a sentence-level, this allows us to take a more nuanced approach that isn't limited by where the model has generated a full-stop. This approach allows us to more-accurately evaluate factual precision without overly-penalizing for a single mistake. An example might take the form of \"The lowest value is X (claim 1), the highest value is Y (claim 2), and the second highest is Z (claim 3). Overall, it is increasing over time (claim 4).\" We observe that the first sentence is a compound sentence that consists of three independent clauses, each with a single factual L2 claim, while second sentence is a single factual L3 claim. Let us assume that claim 1 was factually incorrect. If we evaluate at a sentence-level, then the entire first sentence comprising of claim 1, claim 2, and claim 3 would be incorrect. However, by breaking this caption into independent, mutually-exclusive claims, we can more precisely calculate the factual precision of our text generation. \n\u2193 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193\nTransformer Backbone T5-small L2/L3 0.06 \u00b1 2.67e\u22123 35.81 \u00b1 4.13e+0 0.25 \u00b1 6.43e\u22123 0.09 \u00b1 3.43e\u22123 0.22 \u00b1 5.73e\u22123 0.22 \u00b1 5.60e\u22123 0.99 \u00b1 8.70e\u22123 113.33 \u00b1 2.94e+0 Ours (ByT5-small) L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\nL1 Generation new-seed L2/L3 0.08 \u00b1 5.93e\u22123 20.96 \u00b1 2.71e+0 0.29 \u00b1 5.77e\u22123 0.11 \u00b1 2.33e\u22123 0.25 \u00b1 5.30e\u22123 0.25 \u00b1 5.27e\u22123 0.91 \u00b1 1.83e\u22123 116.36 \u00b1 1.11e+1 original-seed L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\n(c) Ablation study results using the L2/L3 captions. \n\nC Ablation Studies\nTo evaluate our modeling and dataset design choices, we run ablation studies measuring the impact of our transformer model backbones and stochastic data generation pipeline. We report the results in Table 3 .\nTransformer Backbone. To understand the impact of our token-free, byte-to-byte architecture ByT5 model backbone, we explore other large language models. Specifically, we compare our 300M parameter ByT5-small model (Xue et al., 2022) with a 60M parameter T5-small (Raffel et al., 2020) and 140M parameter BART-base model (Lewis et al., 2020) . We also apply prefix-tuning to the ByT5 and T5 models. We cannot apply prefix-uning to BART because BART does not support multi-task learning. Quantitatively, using ByT5 does not appear to significantly improve upon T5. However, we theorize that ByT5's token-free paradigm increases the input sequence length by compressing more input text into fewer input tokens.\nL1 Caption Generation. Since we generate L1 captions stochastically, we evaluate whether our initial randomization impacted the model's results. We compare generate a second set of L1 captions using a different random seed. We see the results are nearly identical across all metrics, indicating our dataset captures a diverse set of L1 captions. We estimate that we trained each model between 5 to 10 times to achieved our final results.\n\nD.3 Ablation Models\nWe train our ablation models using the same parameters as our default models, only varying the parameter of interest. We train them on 16 virtual CPU cores on Xeon E5 hypervisors with 128GB of memory and PCI pass-through access to eight NVidia Titan XP GPUs with 12GB of memory.\n\nD.4 Notable Package Versions\nPackage versions are listed in Table 5 .\n\nE Additional VisText Dataset Details E.1 Licensing\nOur use of the raw Statista data from Kantharaj et al. ( 2022) is consistent with its intended use case. The data was licensed under the GNU General Public License v3.0. We release our data and code under GNU General Public License v3.0.\n\nE.2 L1 Caption Generation Process\nThe Level 1 captions are generated from a random process that chooses from 3 title templates and 6 axis templates. The title templates we use are:\n\u2022 This is a [chart-type] titled [chart-title]\n\u2022 This [chart-type] is titled [chart-title]\n\u2022 [chart-title] is a [chart-type]\nThe axis templates we use for each axis are:\n\u2022 For each axis template, we randomly choose whether to include the axis scale. Furthermore, within each template, we further randomly swap words with synonyms. A list of words and their possible synonym substitutions are:\n\u2022 this: here, a\n\u2022 chart: graph, diagram, plot\n\u2022 titled: called, named, labeled\n\u2022 on: along\n\u2022 plotted: defined, measured, drawn, shown\n\u2022 plots: measures, shows\n\u2022 with: using, on, along, as\n\u2022 found: seen\n\u2022 labeled: marked\n\nE.3 Crowdsourced Study Protocol\nFigures 6-10 screenshot the introduction, eligibility and consent statements, instructions, and a task from our crowdsourced study. We recruited participants on the Prolific.co crowdsourcing platform, following conventions in the data visualization research community 3 and recent research results (Tang et al., 2022) that suggest Prolific yields higher quality results than Amazon Mechanical Turk. We conducted multiple pilot runs to calibrate the amount of time it would take participants to complete the study, and found that most participants were able to successfully do so within 14 minutes. Following Silberman et al. (2018) , who advocate for paying workers at least minimum wage at your location, we choose to pay our participants $3.25 -a roughly $14/hour rate in line with the $14.25/hour minimum wage in Massachusetts at the time the study was conducted.\nOur study was determined to be exempt by MIT's institutional review board (IRB). Participants had to explicitly provide their consent in order to proceed with the study -if participants did not consent, they were redirected back to the Prolific platform. The consent statement (Fig. 8 ) reminded participants of their rights (including that their participation is voluntary and consent could be revoked at any time), and encouraged participants to contact either the study PI or IRB board directly should they have any concerns. We constrained our participant pool (and eligibility requirements) to people living within the United States or United Kingdom who self-reported as being sighted with no vision or color impairments. We did not collect any additional demographic data from participants as we did not determine this to bias or otherwise affect the content we hoped to collect. Each task (an example of which is shown in Fig. 10 ) included an attention check where participants were asked to correctly identify the chart type shown. If participants failed more than two attention checks, their submission was flagged for manual review -in practice, the bulk of participants who failed attention checks nevertheless produced valid captions and, thus, were paid fully. The task asked participants to complete a free response question to describe as completely as they could the trends and patterns observed, emphasizing that their response would be evaluated for correctness and completeness. Despite best practices suggesting a more structured, querying approach (called QID) can yield higher quality captions (Morash et al., 2015) , we opted for our free-response approach as the benefits of QID (namely, in expressing the chart type, title, and axes units) would already be captured by our synthetically generated L1 captions. Moreover, in contrast to the templatized output produced by QID, we hoped that our free-response responses would yield more \"natural\" articulations of perceptual and cognitive trends, following the Lundgard and Satyanarayan (2022) framework.\n\n\nhttps://github.com/mitvis/vistext 2 https://github.com/j-min/VL-T5\n"}
{"question": "Which is one of the advantages of the fine-tuned large language model over previous data table approaches?", "evidence": "  To maximize the quality of our crowdsourced captions, we manually curated the charts and L1 descriptions used in the study.  First, we fine-tune large language models to translate from textual chart representations to natural lan-guage captions. These models evaluate the feasibility and impact of scene-graph models compared to prior data-table approaches (Kantharaj et al., 2022)  We choose ByT5 over T5 transformers (Raffel et al., 2020) because it uses a token-free, byte-encoding that eliminates the use of a tokenizer. As a result, it is robust to noisy inputs, minimizes the need for text preprocessing, and eliminates the out-of-dictionary problem ", "options": ["A. The model is robust to noisy inputs", "B. Model eliminates missing dictionary problem", "C. Models can maximize the quality of crowdsourced headlines", "D. Models can assess the impact and feasibility of scenario diagram models"], "answer": "D", "content": "\nIntroduction\nStudies have shown that captions can improve the recall and comprehension of the data that charts depict (Hegarty and Just, 1993; Large et al., 1995) . For instance, when a caption emphasizes visually prominent features of a chart, like a peak or a sharply declining trend, readers treat this information as the key takeaway (Kim et al., 2021) . Moreover, for people with visual disabilities, captions (or equivalent descriptions such as alt text) are often the only means of accessing the presented data. However, as evidenced by numerous guidelines (Jung et al., 2021) , producing high-quality * Both authors contributed equally to this work. chart captions is a non-trivial and laborious manual process. Thus, despite these advantages, charts are only rarely captioned in practice (Lundgard and Satyanarayan, 2022) .\nTo bridge this gap, several research communities have begun to explore methods for automatically generating chart captions, including using templates and heuristics (Demir et al., 2008; Srinivasan et al., 2019) , adapting image captioning techniques (Balaji et al., 2018; Chen et al., 2019a) , or via data-to-text machine translation (Kantharaj et al., 2022; Obeid and Hoque, 2020) . While promising, these approaches have largely produced captions that either describe a chart's construction (e.g., \"The graph is plot between 'Number of people' x-axis over 'Movie Genres' y-axis\" (Balaji et al., 2018) ) or provide statistical summaries (e.g., \"Machinery and equipment was the most valuable commodity for Singapore in 2019\" (Kantharaj et al., 2022) ). However, these captions do not articulate the perceptual and cognitive features that make charts a distinctive and compelling medium for communicating data (e.g., \"Prices of Big Tech corporations seem to fluctuate but nevertheless increase over time\" (Lundgard and Satyanarayan, 2022) ). Indeed, as Lundgard and Satyanarayan (2022) find, both sighted and blind readers strongly prefer captions that express this type of content.\nTo automatically produce such semantically richer captions, we introduce VisText: a benchmark dataset of 12,441 pairs of charts and captions. VisText makes two key extensions over prior approaches. First, VisText offers three representations of charts: a rasterized image and backing data table, as in previous work; and a scene graph, a hierarchical representation akin to a web page's Document Object Model (DOM), that presents an attractive midpoint between the affordances of chart-as-image and chart-as-data-table. Second, for each chart, VisText provides a synthetically generated caption detailing its construction as well as a crowdsourced caption describing its statistical, perceptual, and cognitive features. These crowdsourced captions represent a substantial increase in data over prior comparable datasets (Mahinpei et al., 2022; Kantharaj et al., 2022) .\nTo demonstrate the possible uses of the VisText dataset, we train three classes of models -textbased caption models, image-guided captioning models, and semantic prefix-tuning. Text-based captioning models fine-tune large language models for VisText's chart captioning task, revealing that both data table and scene graph representations can produce compelling and semantically rich captions. Following recent advancements in image-guided translation (Sulubacak et al., 2020) , we leverage the additional visual affordances in chart images to develop image-guided chart captioning models. Finally, since users often have varying preferences about the type of semantic content in their captions (Lundgard and Satyanarayan, 2022) , we apply semantic prefix-tuning to each of our models, enabling them to output customizable captions.\nOur models generate coherent, semantically rich captions across the VisText charts. Evaluating against standard machine translation and text generation metrics reveals that our models consistently output captions that accurately describe the chart's construction, such as its chart type, title, and axis ranges. Through qualitative analysis of our model's captions, we find that our model competently outputs semantically rich captions that describe data trends and complex patterns. Further, we categorize six common captioning errors that can inform the future development of chart captioning models on the VisText dataset.\nThe VisText dataset and source code are available at: https://github.com/mitvis/ vistext.\n\nRelated work\nHeuristic-Based Chart Captioning. Automatically generating natural language descriptions of data tables dates back to Reiter and Dale (1997) . Demir et al. (2008 Demir et al. ( , 2010 Demir et al. ( , 2012) ) survey this early work and describe the process of extracting insights from a chart by evaluating a list of propositions and composing selected propositions together to produce a natural language summary. More recently, data visualization researchers have explored heuristics that calculate summary statistics and templates to assemble natural language \"data facts\" (Srini-vasan et al., 2019) or descriptions (Cui et al., 2019) . While useful, these approaches yield standardized descriptions that lack the variation and linguistic construction that characterize semantically rich captions (Lundgard and Satyanarayan, 2022) .\nChart Captioning as Image Captioning. With rapid advances of neural image captioning (Vinyals et al., 2015; Anderson et al., 2018) , researchers have begun to adapt these methods for captioning charts. For instance, Balaji et al. (2018) develop a deep learning pipeline that ingests a PNG chart image, classifies the chart type, detects and classifyies textual content present in the chart, and uses this information to generate a textual description. Chen et al. (2019a Chen et al. ( ,b, 2020) ) propose a simpler workflow using ResNet to encode the chart image and an LSTM with Attention to decode it into a natural language description. Both approaches share a pair of limitations. The captions they produce convey relatively simplistic information about the chart (e.g., title, axis labels, etc.) or articulate concepts in visual rather than data terms (e.g., \"Dark Magenta has the lowest value\"). While both approaches contribute associated datasets, their charts and captions are synthetically generated and may not represent real-world counterparts. SciCap (Hsu et al., 2021) addresses this limitation by scraping real-world charts from 290,000 arXiv papers; however, the baseline models trained on this dataset struggle to generate semantically rich captions.\nChart Captioning as Text Translation. Perhaps closest to our contribution is recent work modeling chart captioning as a data-to-text problem. For instance, Spreafico and Carenini (2020) train an encoder-decoder LSTM architecture to generate a natural language caption from time series data. Similarly, Obeid and Hoque (2020) and Kantharaj et al. (2022) explore how transformer architectures can translate tabular structures into captions. These data-to-text methods are more successful than chart-as-image captioning, yielding captions that better capture relevant information from the charts and have higher BLEU scores. Nevertheless, we observe two limitations with these data-to-text approaches that motivate our contribution. First, data-to-text methods are heavily reliant on access to a chart's data table. In practice, data tables are only rarely published alongside charts and methods that recover equivalent information via OCR experience a significant drop in performance (Kantharaj et al., 2022) . Second, the associated datasets do not contain sufficient training examples of captions that express semantically rich insights about the depicted data (i.e., the perceptual and cognitive phenoma that distinguish charts as a medium as distinct from data tables (Lundgard and Satyanarayan, 2022) ). As a result, while the generated captions are compelling, they are largely limited to reporting statistics which sighted and blind readers prefer less than captions that convey complex trends and patterns (Lundgard and Satyanarayan, 2022) .\n\nThe VisText Dataset\nWe designed the VisText dataset in response to two limitations existing datasets present for generating semantically rich chart captions. First, existing datasets represent charts as either rasterized images or as data tables. While useful, these representations trade off perceptual fidelity and chart semantics in mutually exclusive ways -images capture the perceptual and cognitive phenomena that are distinctive to charts (e.g., trends or outliers) but pixels cannot express the rich semantic relationships between chart elements (e.g., estimating plotted data values using axis labels). While the vice-versa is true (Lundgard and Satyanarayan, 2022) , tables also present additional caveats. There is not always a one-to-one relationship between the semantics of a data table and chart (i.e., one data table may be the source for several distinctly different charts). Moreover, data tables are rarely published alongside charts; and, automatic data table extraction is error-prone due to the diversity of chart types and visual styles as well as the difficulty of reasoning about visual occlusion (Kantharaj et al., 2022; Luo et al., 2021; Jung et al., 2017) ).\nSecond, if existing datasets provide captions that describe perceptual or cognitive features, these captions comprise only a small portion of the dataset. At best, LineCap (Mahinpei et al., 2022) offers 3,528 such captions for line charts only, while Chart-to-Text (Kantharaj et al., 2022) estimates that roughly 15% of the sentences in its captions across a variety of chart types express such content.\nIn contrast, VisText provides 12,441 crowdsourced English captions that articulate statistical, perceptual, and cognitive characteristics of bar, line, and area charts. In VisText, charts are available as not only data tables and rasterized images but also as scene graphs. Scene graphs are hierarchical representations that better preserve perceptual fidelity and chart semantics, are often the format for publishing web-based charts, and can be recovered from chart images (Poco and Heer, 2017) .\n\nData Table Collection\nThe data tables found in VisText are sourced from the Statista dataset of the Chart-to-Text benchmark (Kantharaj et al., 2022) . The tables were collected by crawling Statista.com in December 2020 and contain real-world data related to technology, trade, retail, and sports. We process these tables to make them amenable for chart generation, including stripping formatting symbols (e.g., $ and %), standardizing data strings, and identifying the measure type of each column (i.e., quantitative, categorical, or temporal). Data tables are discarded if they do not contain at least one quantitative field and one categorical or temporal field, or if other errors occur during the processing steps. We further down select to data tables containing between 2 to 20 columns and 10 to 500 rows. If a data table has over 500 rows, we randomly sample rows. In larger data tables, this step potentially affects how salient a trend is.\n\nChart Generation and Representation\nCharts in the Chart-to-Text Statista dataset all feature the same layout and visual appearance. In contrast, we aim for richer visual diversity by generating charts using the Vega-Lite visualization library (Satyanarayan et al., 2016) via the Python Altair package (VanderPlas et al., 2018) . To facilitate collecting high-quality captions, we focus on univariate charts: charts that depict one quantitative observation against a categorical or temporal variable. This focus is informed by recent work in the data visualization research community which has chosen single-series line charts as the target of study for natural language descriptions (Kim et al., 2021; Stokes et al., 2022) . VisText also includes single-series bar and area charts as they typically exhibit similar perceptual features to line charts.\nFor each data table, we iterate through pairs of univariate fields. If the pair contains a temporal field, we randomly generate an area or line chart; if the pair contains a categorical field, we randomly generate a horizontal or vertical bar chart. For diversity in layout and visual appearance, we randomly rotate axis labels and apply one of fourteen themes provided by the Vega-Lite library. These themes mimic the visual style of common chart platforms or publishers (e.g., ggplot2 or the LA Times). \n\nGenerated L1 Caption\nHere is a area chart is labeled Cumulative number of patients diagnosed with coronavirus (COVID-19) in Japan as of December 4, 2020, by place of infection. On the x-axis, Month is measured with a categorical scale starting with April and ending with October. There is a linear scale with a minimum of 0 and a maximum of 150,000 along the y-axis, labeled Patients within Japan.\n\nCrowdsourced L2/L3 Caption\nBy December 4th 2020, approximately 160,000 people in Japan had been diagnosed with COVID-19. The first person diagnosed with COVID-19 in Japan was diagnosed in March 2020. The greatest increase in cumulative number of patients in Japan diagnosed with COVID-19 occurred between November and December 2020. In VisText, each chart is represented as a rasterized image, stored as an RGBA-encoded PNG file, as well as a scene graph. A scene graph is a textual representation of the rendered chart similar to a web page's Document Object Model (DOM). Scene graphs encode the position, value or content, and semantic role of all visual elements within a chart, including the individual marks (i.e., bars or points along the line), titles, axes gridlines, etc. Thus, scene graphs express the perceptual features of rasterized images in a more computationallytractable form.\n\nCumulative number of patients diagnosed with coronavirus (COVID-19) in\nScene graphs are a standard data structure for representing vector-based graphics -the most common format for publishing visualizationsand, thus, can be trivially recovered (e.g., by traversing the SVG text string). We extract the scene graph directly from the rendered chart using the Vega-Lite API. As most text generation models expect a linear set of input tokens, we flatten the scene graph via a depth-first traversal. To scale to large language models, we need to further reduce the size of the scene graph. Thus, we preserve the following elements which we hypothesize as being most critical for generating semantically rich captions: title, title coordinates, axis labels, axis label coordinates, axis tick coordinates, mark coordinates, and mark sizes. VisText includes both the original (hierarchical) and reduced (linearized) scene graphs.\n\nCaption Generation and Collection\nOur captioning process is guided by the framework developed by Lundgard and Satyanarayan (2022) , which identifies four levels of semantic content: L1 content enumerates aspects of the chart's construction (e.g., axis ranges); L2 content reports summary statistics and relations (e.g., extrema); L3 content synthesizes perceptual and cognitive phenomena (e.g., complex trends); and, L4 content describes domain-specific insights (e.g., sociopolitical context). In subsequent studies, the authors find that while sighted readers typically prefer higher levels of semantic content, blind readers are split about the usefulness of L1 and L4 content. Thus, given these differing preferences, we define a single caption to express multiple levels of content separated across clauses or sentences. We only consider the first three levels of this model, and leave L4 content to future work. Following guidelines prescribed by the National Center for Accessible Media (NCAM), our captions begin with L1 content and then turn to L2 and L3 content (Gould et al., 2008) .\nWe algorithmically generate L1 content and use a crowdsourced protocol to collect L2 and L3 content. This approach follows (Lundgard and Satyanarayan, 2022)'s computational considerations as well as results from Morash et al. (2015) who find that, even with instructions and guidelines, crowd workers do not describe a chart's structural elements sufficiently for blind readers. Thus, synthetically generating L1 content allows us to ensure that captions convey complete descriptions of the chart's structural elements. L1 content comprises 1 sentence conveying the chart type and title, and then 1 -2 sentences describing the axes (including the titles, ranges, and scales). We use template randomization to generate a diverse range of L1 captions to mimic human variability and reduce the capacity of the model to overfit to a single L1 style. Three templates are defined for the first sentence and twenty-six template combinations for the subsequent sentences. During generation, we randomly select a pair of templates and fill in in- formation from the abstract chart specification. For additional diversity, we randomly drop scale information and swap template words with synonyms. Templates and synonym replacements are listed in Appendix E.2.\nTo crowdsource L2 and L3 content, we extend the protocol used by Lundgard and Satyanarayan (2022) . After soliciting consent, we introduce the task: participants are presented with a chart image and corresponding L1 description; they are asked to write a description about the trends and patterns they observe without drawing on background knowledge or repeating L1 information. The introduction provides examples and explanations of valid and invalid responses. After acknowledging these examples, participants are asked to complete 5 random iterations of the task. To maximize the quality of our crowdsourced captions, we manually curated the charts and L1 descriptions used in the study. We discarded any charts that were challenging to read (e.g., colors were too similar, marks were not easily readable, etc.). Participants were recruited on the Prolific.co platform, took approximately 14 minutes to complete the study, and were compensated $3.25 ($14/hour). Additional details on our crowdsourcing process are in Appendix E.3.\nWe manually verified charts where participants failed an attention check and discarded invalid descriptions. Additionally, we manually inspected captions for personally identifiable information or offensive content. Using heuristics, we removed captions where respondents described charts as unclear or illegible and replaced newline characters with spaces. Although we attempted to fix incorrect spelling and casing errors using a similar heuristic-based approach, we observed that this process could improperly affect axis and chart names. As a result, these errors remain in our dataset.\n\nDataset Analysis\nFigure 2 shows the distribution and means of the lengths of chart representations and captions. Synthetically generated L1 captions have roughly 1.5x more characters than crowdsourced L2/L3 captions (\u00b5 = 255 vs. \u00b5 = 177) but the average number of sentences are comparable (2.5 vs. 2). The VisText dataset consists of captions for 3,189 area charts, 6,238 bar charts, and 3,014 line charts -the roughly twice-as-many bar charts as area or line charts corresponds to the randomization of temporal fields during chart generation (Sec. 3.2). As some charts have multiple crowdsourced captions, we randomly split our dataset into training, validation, and test sets using the chart IDs to prevent data leakage across sets. This resulted in an approximate ratio of 80:10:10.\nFinally, to understand the distribution of semantic content, we manually coded 2% (230) of crowdsourced captions. We followed a protocol inspired by Lundgard and Satyanarayan (2022) by breaking sentences down into independent statements and mapping these statements to their semantic content level. We marked statements as not categorizable if they did not map to the framework -for instance, if captions expressed commentary from crowd workers such as \"this chart is hard to read.\" Our analysis revealed 11 L1 statements (2.4%), 180 L2 statements (39.7%), 253 L3 statments (55.7%), and 10 not categorizable statements (2.2%). While a handful express L1 content, the bulk of statements (95%) express L2 or L3 content, with approximately 1.4x L3 statements than L2.\n\nChart Captioning Models\nTo demonstrate the affordances of the VisText dataset, we train three classes of models. First, we fine-tune large language models to translate from textual chart representations to natural lan-guage captions. These models evaluate the feasibility and impact of scene-graph models compared to prior data-table approaches (Kantharaj et al., 2022) . Second, as VisText provides multiple chart representations, we adapt image-guided translation (Sulubacak et al., 2020; Cho et al., 2021) to develop two multimodal chart captioning models: image-scene-graph and image-data-table. Finally, since VisText offers captions at different semantic levels and prior work has shown significant differences in readers' preferences (Lundgard and Satyanarayan, 2022) , we explore prefix-tuned models that selectively output L1, L2/L3, or L1+L2/L3 captions. Training details are in Appendix D.\n\nText-Based Chart Captioning\nInformed by prior work (Kantharaj et al., 2022) , we investigate text translation models for generating chart captions. In particular, Kantharaj et al. found that models that translate data tables to chart captions significantly outperform image captioning models. However, when data tables were not available, the authors found a significant drop in their models' ability to extract relevant information from the chart -an effect that was only slightly ameliorated by using OCR methods to extract text from chart images. In contrast, VisText's scene graphs can be more readily recovered from charts when data tables are not available -for instance, by processing the SVG format of web-based visualizations. Moreover, scene graphs offer a potentially richer source of information than data tables as they encode visual properties of the chart (e.g., coordinates and colors) and are less noisy than tokens recovered via OCR. Thus, to evaluate the feasibility and efficacy of scene graphs, we train a scene-graph text translation model and a baseline data-table model for comparison.\nFor each model, we fine-tune a pretrained ByT5 transformer model (Xue et al., 2022) on the Vis-Text dataset. We choose ByT5 over T5 transformers (Raffel et al., 2020) because it uses a token-free, byte-encoding that eliminates the use of a tokenizer. As a result, it is robust to noisy inputs, minimizes the need for text preprocessing, and eliminates the out-of-dictionary problem. This allows our model to handle common typographical and chart reading errors in the crowdsourced L2 and L3 captions and increases generalizability to previously-unseen words that could be present in chart and axes titles.\n\nImage-Guided Chart Captioning\nFollowing recent advancements in image-guided machine translation (Sulubacak et al., 2020) , we train image-guided captioning models using the VisText dataset. Images have improved text-based machine translation models by providing visual information complementary to natural language inputs. Similarly, chart images can contain visuals complementary to the textual specification. For instance, visual affordances that are important for perceiving a trend (e.g., gestalt relations, relative sizes/areas, etc.) may be obfuscated in the scene graph but better captured in the chart image.\nWe train three image-guided chart captioning models: image, image-scene-graph, and image-data-table. All models leverage the vision-language transformer model VL-T5 (Cho et al., 2021) . VL-T5 is pretrained on image captioning and visual grounding tasks and was successfully applied to machine translation, making it suitable for chart captioning. We extract visual features for each VisText chart image using a Bottom-Up Feature Extractor (Anderson et al., 2018) . To explore the value of images to chart captioning, our image model only takes in the image features, while image-scene-graph and image-data-table concatenate the image features with the chart's textual representations (scene graph or data table).\n\nSemantic Prefix-Tuning\nIn real-world chart captioning settings, users want to vary the level of semantic content in their captions. For instance, while some blind users want verbose captions that describe the chart visuals, sighted users may only want captions that help them expose data trends (Lundgard and Satyanarayan, 2022) . To develop models capable of such customization, we leverage prefix-tuning strategies alongside VisText's semantic caption breakdown. Prefix-tuning specifies a task alongside the input, permitting a single large language model to perform many different tasks. In our setting, we use prefix-tuning to specify the level of semantic content to include in the caption (Li and Liang, 2021) .\nWe train each of our models with and without semantic prefix-tuning. With semantic prefix-tuning, we treat chart captioning as a multi-task fine-tuning problem, where the model is trained to generate the L1 and L2/L3 captions separately. In every epoch, the model sees each VisText chart twice, once with the L1 prefix and caption and once with the L2/L3 prefix and caption.\n\nEvaluation and Results\nTo evaluate the VisText dataset and our chart captioning models, we measure the readability and accuracy of generated captions and their similarity to the VisText target caption. We also qualitatively analyze the descriptiveness of generated L2/L3 captions and categorize common errors.\n\nQuantitative Model Performance\nWe evaluate the results of our text-based and imageguided captioning models with and without prefixtuning. We also compare to a current state-of-theart chart captioning model that uses data table chart representations and a T5 generation model (Kantharaj et al., 2022) . To measure the quality of output captions, we evaluate each model on machine translation and language generation metrics (Table 1 ).\nChart images do not support captioning. The image model performs the worst of all the chart captioning models. Its low perplexity and high error rates indicate it is highly confident in its inaccurate captions. While chart images contain the same information encoded in the chart's textual representations, it is presumably not adequately extracted by the model. Both the image model backbone (Cho et al., 2021) and the visual feature extractor (Anderson et al., 2018) are trained on natural images, making chart images out-of-distribution inputs that are likely to be poorly represented by these vision models. As the chart captioning task grows, model backbones, architectures, and feature extractors could be customized to chart images, which may improve image-based chart captioning.\nAll models produce high quality L1 captions. In our chart captioning setting, relation generation (Wiseman et al., 2017) measures how often the chart title, axis names, and axis scales in the input appear in the caption. Every model (except image) achieves a similarly-high relation generation score, indicating that every model can generate detailed L1 captions.\nScene graphs perform as well as data tables. Models trained on scene graph representations achieve similar performance across the evaluative metrics to models trained on data tables. As scene graphs can be more easily extracted from web-based charts images, they may be the preferred representation for future chart captioning models.\nImage-guiding does not improve captioning. Our image-guided captioning models do not experience the significant increase in performance other image-guided translation tasks report. While in image-guided translation, images contain substantial additional information beyond the text, the image and textual representations in chart captioning often contain highly similar information. The small amount of additional information in images might benefit complex captioning tasks on multivariate charts or infographics; however, the current VisText captions rarely reference visual information not present in the scene graph or data table.\nPrefix-tuning is free. Adding semantic prefixtuning to our models does not significantly change their performance. Models trained with and without prefix-tuning are exposed to the same set of charts, so it is consistent that prefix-tuning would not impact the quality of output captions. Given prefix-tuned models are able to output L1, L2/L3, and L1+L2/L3 captions, prefix-tuning may be preferred if users require semantic customization.\n\nQualitative Caption Evaluation\nTo augment our quantitative evaluation, we qualitatively assess the descriptiveness and accuracy of the generated chart captions. Since L1 caption accuracy can be measured at scale via relation generation, we focus our evaluation on L2/L3 predictions.\nPrior analysis tasked annotators with comparing the accuracy, coherence, and fluency of generated captions compared to a target caption (Kantharaj et al., 2022) . Instead, our approach follows an inductive qualitative data analysis approach: iteratively analyzing captions in a \"bottom-up\" fashion to identify emergent patterns in how generated captions compare to the ground truth (Bingham and Witkowsky, 2021). We randomly sample 176 generated captions from the scene-graph model with prefix-tuning and break them into their independent L2 and L3 statements, resulting in 181 (48.27%) L2 statements and 194 (51.73%) L3 statements.\nApproximately half (241 / 512) of the L2 and L3 statements made in the generated captions are factually accurate. Moreover, many of the full sentences are written in a natural, human-like manner and generated captions frequently include both compound and complex sentences. On average, every generated caption has one L3 statement and zero to et al., 2022) . We evaluate each model using machine translation and text generation metrics, including BLEU (Papineni et al., 2002) , Perplexity, Relation Generation (RG) (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD) (Kusner et al., 2015) , and Translational Error Rate (TER) (Snover et al., 2006) . We report the mean and standard deviation of three independent models. Darker colors indicate better scores.\nInput PT BLEU \u2191 Perplexity \u2193 RG \u2191 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193 Kantharaj et\ntwo L2 statements. Often this takes the form of a L3 general trend statement (e.g., \"The median annual family income in Canada has increased from 2000 to 2018\") accompanied by an L2 minimum and maximum statement (\"The highest was in 2015 at 80k and the lowest was in 2000\"). For the remaining half of analyzed captions, we identified the following recurring types of errors:\nIdentity Errors. We identify 86 identity errors (22.93% of analyzed statements). An identity error occurs when an L2 or L3 statement incorrectly reports the independent variable for a given (often correctly identified) trend. For bar charts, this error means incorrectly reporting the categorical label associated with a bar (e.g., in Appendix Figure 5c : \"The most popular music activity is vinyl albums and vinyl singles\" should be \"The most popular music activity is tickets for festivals\"). For area and line charts, this error means incorrectly identifying the temporal point or range of the trend. With bar charts, in particular, we observed that the identities were often \"off-by-one\" (i.e., identifying a minimum or maximum value, but attributing it to the second-highest or second-lowest category).\nValue Errors. A value error occurs when the quantitative data value of a statement is incorrect.\nOf the captions we analyzed, 3.20% (12) of statements contained a value error. For instance, as shown in Appendix Figure 4c , for the caption \"The total gate revenue from sporting events worldwide by region from 2006 to 2015 has increased from around 15 billion dollars to around 15 billion dollars\", the value should be around 18 billion dollars.\nIf it is ambiguous whether an error is an Identity or Value Error, we classify it as the former.\nDirection Errors. A direction error occurs when the direction (which can be increasing, decreasing, or stable) of a trend in an L3 statement is incorrect. We uncovered 32 direction errors (8.53% of analyzed statements). For instance, in the caption \"The per capita consumption of sweet corn in the US has increased from 2000 to 2019\" (Appendix Figure 3c ), the trend is actually decreased. In most direction errors, the identity (i.e., temporal range) is correct.\nStability Errors. A stability error occurs when the magnitude of a direction or the variance in a trend is incorrect. This can often refer to how much a trend is increasing or decreasing, such as rapidly or slowly, as well as whether it's a steady change or highly-fluctuating change. Looking ahead, while accessibility remains a key domain that would benefit from automated chart captioning, and deploying automated chart captioning models into the field is an exciting prospect, we believe the most promising approach for future work lies in \"mixed-initiative\" (i.e., human + AI) chart authoring systems. In particular, as we describe in our Ethics Statement below, chart captioning models are currently prone to make a number of factual inaccuracies which can have severe harmful consequences. On the other hand, by integrating these models into chart authoring systems (e.g., Tableau, Charticulator, Data Illustrator, or Lyra), chart authors can intervene and make any necessary corrections. Indeed, such integration offers exciting opportunities to develop novel interactive methods for verifying generated captions. For instance, models like ours could generate an initial caption (or set of captions) based on the chart currently being authored; as the system has access to all three representations of the chart (the back-ing data table, chart image, and structured scene graph), it might automatically segment the caption into independent \"data segments\" and interactively link and map them to rows in the table or regions on the chart, akin to Kori (Latif et al., 2021) .\n\nLimitations\nComputational Constraints. Despite using modern GPUs, with large amounts of memory, we were forced to use the smallest-parameter variants of T5 and ByT5 as we encountered out-of-memory errors with the larger alternatives. More problematically, the quadratic relationship between sequence length and time/space complexity of transformer architectures (Vaswani et al., 2017) , especially when using byte-level sequences (Xue et al., 2022), has had a significant impact on our model performance.\nIn particular, to be computationally tractable, we were forced us to truncate our input and output sequences to, at most, 1,024 and 512 characters respectively (1,024 coming from the underlying ByT5 architecture (Xue et al., 2022)).\nThese character thresholds have likely had an outsized effect on scene-graph models. For instance, due to these character limits, we reduced scene graph sequences to only a minimal set of visual characteristics; VisText also includes the raw, unprocessed scene graphs which offer a richer source of information about the visual features that are important to how people decode charts (e.g., bounding boxes, color) but were unavailable to our models. Moreover, as Figure 2 shows, even with this reduced representation, the mean length of scene graph sequences is 948 characters (cf. 426 characters for data tables) with a wide distribution. Thus, despite scene-graph models achieving comparable performance to data-table models, the former saw a much smaller proportion of complete sequences as compared to the latter. This truncation step additionally negatively impacts charts with long titles or axis names -in such cases, we observed that the L2 or L3 caption would be altogether truncated before generation.\n\nChart Types and the Visualization Design Space.\nVisText is scoped to only univariate bar, area, and line charts. We chose to begin with these chart types informed by data visualization research that has focused on studying natural language descriptions of single-series line charts -a basic, but commonly occurring chart type that offers a compelling target of study as it most visibly surfaces any poten-tial trends in the data (Kim et al., 2021; Stokes et al., 2022) . Future work can now begin to consider more complex chart forms in a step-by-step manner. For instance, moving from univariate bar, area, and line charts to multivariate versions of these chart types (i.e., stacked bars and areas, grouped bars, and multi-series line charts). From there, work can also consider chart types that surface perceptual and cognitive phenomena in visually distinct ways (e.g., scatterplots, where trends appear as clusters of points; heatmaps, where color saturation often encodes a trend; or maps, where color or other layered elements such as symbols are used to represent data values). Finally, automated methods for captioning visualizations may eschew chart typologies altogether in favor of visualization grammars -by offering a more composable and combinatorial approach to the design space (Wilkinson, 2012) , learning over visualization grammars may offer a more robust approach to captioning highly customized or unique visual forms.\nFor each future work direction, we anticipate scene graph representations to prove more fruitful than the data table. As the complexity of the visualization increases, its relationship to the data table only grows more ambiguous; the scene graph, on the other hand, directly encodes the visual form and thus remains faithful to it. As a result, to support such future work, VisText provides the raw specifications used to produce our charts (via the Vega-Lite visualization grammar (Satyanarayan et al., 2016) ) as well as the raw, hierarchical scene graphs prior to our linearization and reduction step.\n\nEthics Statement\nThe Consequences of Incorrect Captions. Weidinger et al. ( 2021) comprehensively survey the risks associated with the large language models (LLMs) that underlie our contribution. Of the six categories of risk they identify, harms stemming from models producing factually incorrect statements are not only most pertinent to our work, but are likely heighted as compared to general uses of LLMs given the context we are addressing: automatically captioning charts. In particular, people most often consume charts and visualizations in order to make data-driven decisions (Keim et al., 2008) -for instance, about whether to evacuate ahead of a hurricane (Padilla et al., 2018) , or health & safety during the pandemic (Shneiderman, 2020) . Moreover, recent results have shown that readers not only fixate for longer and are more likely to recall the textual content of and around visualizations (Borkin et al., 2015) but this textual content can strongly influence the takeaway message readers leave with even when it is at odds with the depicted data (Kong et al., 2018 (Kong et al., , 2019)) . Finally, these issues are exacerbated by the persuasive and rhetorical force of data and charts (Kennedy et al., 2016; Hullman and Diakopoulos, 2011) , that often project a sense of authority and certainty (Correll, 2019) . As a result, readers may not think to double check the accuracy of chart captions, and inaccurate statements that models may produce could lead to harmful downstream decisions.\nTo proceed ethically with this line of research, we believe that advances in data and modeling need to be closely followed by attention devoted to mitigating the risks of incorrect statements. At base, automatically generated captions should be identified as such at the forefront to raise readers' awareness about the potential for incorrect statements. And, interactive visual linking strategies (such as those explored by Kong and Agrawala (2012) ; Kim et al. ( 2018)) could be deployed to help readers manually verify the constituent statements of a caption against the chart. These strategies, however, place the burden of harm mitigation on readers. Thus, an alternate approach might never surface automatically generated captions to readers directly but instead use them as part of mixed-initiative systems for jointly authoring visualization and text, such as Kori (Latif et al., 2021) . In such systems, automated chart captioning models would help to accelerate the authoring process -combatting the blank slate problem by providing an initial summary of the chart -and chart authors would make any necessary corrections prior to publication.\nBesides these human-computer interaction (HCI) approaches for mitigating harm, an equally important direction for future work should leverage interpretability techniques to more deeply study what the models are learning. To what degree are chart captioning models stochastic parrots (Bender et al., 2021) , and how much do they understand the information charts depict? Automated Captioning for Accessibility. Although accessibility is a guiding motivation for the bulk of work in automated captioning (be it image captioning or, as in our case, chart captioning), studies find mixed reactions, at best, about these approaches among people with disabilities (PWDs).\nFor instance, accessibility educator and researcher Chancey Fleet described Facebook's automatic image descriptions as \"famously useless in the Blind community\" despite \"garner[ing] a ton of glowing reviews from mainstream outlets\" (Fleet, 2021; Hanley et al., 2021) . This disconnect appears to stem from a more fundamental mismatch between what PWDs describe as their captioning needs, and what the research community -particularly through its automatic, quantitative evaluationsprioritizes (Jandrey et al., 2021) . In particular, surveys with PWDs repeatedly surface the contextual nature of captions. Bennett et al. (2021) find that the context of use shapes the degree to which PWD are comfortable with captions describing people's race, gender, and disabilities -for instance, changing their preferences if they were in a white, cisgender, nondisabled, and professional company versus their own community. Similarly, Jung et al. (2022) find shifting preferences for the content image descriptions should convey across different photo activites -for example, when viewing or taking photos, participants wished for descriptions that conveyed spatial cues whereas when searching or reminiscing about photos, participants hoped for descriptions to connect to personal data or differentiating details.\nIn contrast, quantitative metrics of model performance compare generated captions to a single \"ground truth\" caption. This framing of success not only makes it difficult to develop contextuallyvarying caption generation but can actively penalize such investigations. For instance, with our work, we explored how prefix-tuning can be used to develop models that are responsive to users' preferences about semantic content. However, as described in Sec. 5.1, existing quantitative metrics of model performance (e.g., BLEU, ROUGE, WMD, and TER) show a drop in model performance despite our qualitative analysis indicating that these captions are indeed high quality.\nFinally, our exploration of semantic prefixtuning represents only a very preliminary step towards addressing the contextual captioning needs of PWDs. In particular, the semantic labels Vis-Text assigns to captions were derived from prior work (Lundgard and Satyanarayan, 2022) that only explored natural language descriptions when consuming presentations of visualizations -one task from a broader palette (Brehmer and Munzner, 2013) . Future work might instead extend the Vis-Text dataset -and corresponding models -to consider captions for a broader range of tasks including consuming visualizations for scientific discovery, enjoyment or, producing, searching, or querying visualizations (Brehmer and Munzner, 2013) . \n\nModel Generated L1 Caption\nAverage spending per consumer on selected music activities in the United States as of July 2018 is a bar graph. The x-axis measures Response while the y-axis measures $40 to $99.99.\n\nModel Generated L2/L3 Caption\nThe most popular music activity is vinyl albums and vinyl singles.\nThe least popular music activity is vinyl albums. (b) Model results using the L2/L3 captions.\nTable 2 : We separately evaluate our L1 and L2L3 captions on all the same metrics except for Relation Generation.\nIn general, we observe that L1 captions perform better than the L2/L3 captions. Our models generate verbose L1 captions that are similar to the structure of our L1 templates, while the L2/L3 captions are human-generated and contain more variability. Darker colors indicate better scores.\n\nB Additional Evaluations\nB.1 Independent L1 and L2/L3 Caption Evaluation\nTo better understand how our models generate varying levels of semantic content, we separately evaluate our prefix-tuned models on L1 captioning and L2/L3 captioning tasks. Each prefix-tuned model can output an L1 or an L2/L3 caption for each chart. We evaluate these captions to their respective L1 or L2/L3 ground truth captions and report the results in Table 2 . Since we compute Relation Generation using only the L1 chart fields (e.g., chart title, axis scale, etc.), we do not report the results separately for L1 versus L2/L3 captioning. There is no direct Relation Generation analog for L2/L3 captions, since they are human-generated and do not follow a specific template. The Relation Generation for L1 captions is identical to the Relation Generation for L1/L2/L3 captions reported in Table 1 .\n\nB.2 Evaluation Details\nQuantitative Model Performance Metrics. We evaluate our models using NLP and machine translation metrics, including BLUE (Papineni et al., 2002; Lin and Och, 2004) , Perplexity, Relation Generation (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD), and Translation Edit Rate (TER) (Snover et al., 2006; Post, 2018) . We implement Relation Generation per Wiseman et al. (2017) , use the Gensim implementation of WMD, and use the Hugging Face implementation (Wolf et al., 2019) for the remaining metrics.\n\u2022 BLEU: BLEU requires several gold standard references. In our evaluation setup, we use the test set caption as a single reference.\n\u2022 Perplexity: We use a pretrained GPT-2 Medium model to compute Perplexity.\n\u2022 Relation Generation: The fields we evaluate on are the chart title, axis names, and axis scales (if any).\n\u2022 Translation Edit Rate (TER): Edits consist of deletions, additions, and substitutions, as present in SacreBLEU.\nQualitative Caption Evaluation. To produce our qualitative evaluation results (Sec. 5.2), we iteratively evaluated randomly sampled captions until there was no more marginal information about they types of errors to be gained from evaluating more captions. For each L2/L3 caption, we assess the number of independent, mutually-exclusive L2 and L3 claims/statements that are being made. In comparison to evaluating at a sentence-level, this allows us to take a more nuanced approach that isn't limited by where the model has generated a full-stop. This approach allows us to more-accurately evaluate factual precision without overly-penalizing for a single mistake. An example might take the form of \"The lowest value is X (claim 1), the highest value is Y (claim 2), and the second highest is Z (claim 3). Overall, it is increasing over time (claim 4).\" We observe that the first sentence is a compound sentence that consists of three independent clauses, each with a single factual L2 claim, while second sentence is a single factual L3 claim. Let us assume that claim 1 was factually incorrect. If we evaluate at a sentence-level, then the entire first sentence comprising of claim 1, claim 2, and claim 3 would be incorrect. However, by breaking this caption into independent, mutually-exclusive claims, we can more precisely calculate the factual precision of our text generation. \n\u2193 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193\nTransformer Backbone T5-small L2/L3 0.06 \u00b1 2.67e\u22123 35.81 \u00b1 4.13e+0 0.25 \u00b1 6.43e\u22123 0.09 \u00b1 3.43e\u22123 0.22 \u00b1 5.73e\u22123 0.22 \u00b1 5.60e\u22123 0.99 \u00b1 8.70e\u22123 113.33 \u00b1 2.94e+0 Ours (ByT5-small) L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\nL1 Generation new-seed L2/L3 0.08 \u00b1 5.93e\u22123 20.96 \u00b1 2.71e+0 0.29 \u00b1 5.77e\u22123 0.11 \u00b1 2.33e\u22123 0.25 \u00b1 5.30e\u22123 0.25 \u00b1 5.27e\u22123 0.91 \u00b1 1.83e\u22123 116.36 \u00b1 1.11e+1 original-seed L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\n(c) Ablation study results using the L2/L3 captions. \n\nC Ablation Studies\nTo evaluate our modeling and dataset design choices, we run ablation studies measuring the impact of our transformer model backbones and stochastic data generation pipeline. We report the results in Table 3 .\nTransformer Backbone. To understand the impact of our token-free, byte-to-byte architecture ByT5 model backbone, we explore other large language models. Specifically, we compare our 300M parameter ByT5-small model (Xue et al., 2022) with a 60M parameter T5-small (Raffel et al., 2020) and 140M parameter BART-base model (Lewis et al., 2020) . We also apply prefix-tuning to the ByT5 and T5 models. We cannot apply prefix-uning to BART because BART does not support multi-task learning. Quantitatively, using ByT5 does not appear to significantly improve upon T5. However, we theorize that ByT5's token-free paradigm increases the input sequence length by compressing more input text into fewer input tokens.\nL1 Caption Generation. Since we generate L1 captions stochastically, we evaluate whether our initial randomization impacted the model's results. We compare generate a second set of L1 captions using a different random seed. We see the results are nearly identical across all metrics, indicating our dataset captures a diverse set of L1 captions. We estimate that we trained each model between 5 to 10 times to achieved our final results.\n\nD.3 Ablation Models\nWe train our ablation models using the same parameters as our default models, only varying the parameter of interest. We train them on 16 virtual CPU cores on Xeon E5 hypervisors with 128GB of memory and PCI pass-through access to eight NVidia Titan XP GPUs with 12GB of memory.\n\nD.4 Notable Package Versions\nPackage versions are listed in Table 5 .\n\nE Additional VisText Dataset Details E.1 Licensing\nOur use of the raw Statista data from Kantharaj et al. ( 2022) is consistent with its intended use case. The data was licensed under the GNU General Public License v3.0. We release our data and code under GNU General Public License v3.0.\n\nE.2 L1 Caption Generation Process\nThe Level 1 captions are generated from a random process that chooses from 3 title templates and 6 axis templates. The title templates we use are:\n\u2022 This is a [chart-type] titled [chart-title]\n\u2022 This [chart-type] is titled [chart-title]\n\u2022 [chart-title] is a [chart-type]\nThe axis templates we use for each axis are:\n\u2022 For each axis template, we randomly choose whether to include the axis scale. Furthermore, within each template, we further randomly swap words with synonyms. A list of words and their possible synonym substitutions are:\n\u2022 this: here, a\n\u2022 chart: graph, diagram, plot\n\u2022 titled: called, named, labeled\n\u2022 on: along\n\u2022 plotted: defined, measured, drawn, shown\n\u2022 plots: measures, shows\n\u2022 with: using, on, along, as\n\u2022 found: seen\n\u2022 labeled: marked\n\nE.3 Crowdsourced Study Protocol\nFigures 6-10 screenshot the introduction, eligibility and consent statements, instructions, and a task from our crowdsourced study. We recruited participants on the Prolific.co crowdsourcing platform, following conventions in the data visualization research community 3 and recent research results (Tang et al., 2022) that suggest Prolific yields higher quality results than Amazon Mechanical Turk. We conducted multiple pilot runs to calibrate the amount of time it would take participants to complete the study, and found that most participants were able to successfully do so within 14 minutes. Following Silberman et al. (2018) , who advocate for paying workers at least minimum wage at your location, we choose to pay our participants $3.25 -a roughly $14/hour rate in line with the $14.25/hour minimum wage in Massachusetts at the time the study was conducted.\nOur study was determined to be exempt by MIT's institutional review board (IRB). Participants had to explicitly provide their consent in order to proceed with the study -if participants did not consent, they were redirected back to the Prolific platform. The consent statement (Fig. 8 ) reminded participants of their rights (including that their participation is voluntary and consent could be revoked at any time), and encouraged participants to contact either the study PI or IRB board directly should they have any concerns. We constrained our participant pool (and eligibility requirements) to people living within the United States or United Kingdom who self-reported as being sighted with no vision or color impairments. We did not collect any additional demographic data from participants as we did not determine this to bias or otherwise affect the content we hoped to collect. Each task (an example of which is shown in Fig. 10 ) included an attention check where participants were asked to correctly identify the chart type shown. If participants failed more than two attention checks, their submission was flagged for manual review -in practice, the bulk of participants who failed attention checks nevertheless produced valid captions and, thus, were paid fully. The task asked participants to complete a free response question to describe as completely as they could the trends and patterns observed, emphasizing that their response would be evaluated for correctness and completeness. Despite best practices suggesting a more structured, querying approach (called QID) can yield higher quality captions (Morash et al., 2015) , we opted for our free-response approach as the benefits of QID (namely, in expressing the chart type, title, and axes units) would already be captured by our synthetically generated L1 captions. Moreover, in contrast to the templatized output produced by QID, we hoped that our free-response responses would yield more \"natural\" articulations of perceptual and cognitive trends, following the Lundgard and Satyanarayan (2022) framework.\n\n\nhttps://github.com/mitvis/vistext 2 https://github.com/j-min/VL-T5\n"}
{"question": "What is one of the criteria for including a question in ELQA-small?", "evidence": "  For ELQA-small, we applied further filtering to ensure that the data has the least amount of noise:  a) questions should have a score of at least 4 (ensuring questions are clear and coherent), b) question has an answer with a score higher than 3 and c) there are no hyperlinks in at least one of the high-rated answers.  ", "options": ["A. Questions with a score of at least 4.", "B. Questions with a score of at least 2.", "C. Questions with hyperlinks in the answers.", "D. Questions with a score of at least 3."], "answer": "A", "content": "\nIntroduction\nLanguage is so powerful that it can be reflected back on itself. Statements like \"In informal usage, a steep learning curve means something that is difficult (and takes much effort) to learn\" or \"In some cases, an adjective has both -ic and -ical forms, with no difference in meaning\" expressly concern linguistic inventories, structures, and behaviors. In other words, they are metalinguistic-they use language to discuss language (cf. Wilson, 2013) . They may concern a particular instance of language use, or properties of a language or speaker in general; either way, they are metalinguistic in making linguistic phenomena the subject matter of a linguistic utterance. For the rest of this paper, the term metalanguage is used for natural language text in which natural language is also the subject matter.\nWhile NLP models have become powerful at predicting text in many settings, it remains to be seen whether such capability extends to metalanguagewhere linguistic strings are not being deployed to contribute to the discourse with their normal denotations, but rather, are treated as entities with linguistic properties (e.g., grammar, meaning). One way this can be explored is in a question answering framework, which requires suitable datasets, ideally based on questions that are realistic and paired with high-quality answers.\nIn this paper, we present a corpus of metalinguistic questions and answers about English. The corpus is collected and carefully processed from two Stack Exchange forum sites: English Language & Usage (ENG) and English Language Learners (ELL). It covers more than 70k questions on numerous topics about English such as grammar, meaning, fluency, and etymology along with answers. Our corpus, ELQA (English Language Questions and Answers), can serve as a tool to facilitate metalinguistic studies. Moreover, since questions in ELQA cover a variety of topics in English, it can be used in the educational and English language learning domains.\nAs the first case study of ELQA, we investigate the performance of current state-of-the-art NLP technology on free-form question answering in the English language domain. Additionally, we explore the possibility of building NLP models that can directly answer questions from language learners. We process a subset of ELQA and make it appropriate for this task. Then, we report on the results of both automatic and human evaluations using different experimental settings of T5 1 and GPT-3 2 models. Although most of these models achieve high ratings for well-formedness, the validity of their answers is significantly lower than that of human-authored answers, indicating that this type of metalinguistic QA task is challenging even for large language models.\nOur main contributions are: 1) we release the first publicly available metalinguistic QA dataset, 3 focused on the English language; 2) we present a taxonomy of questions in the corpus along with analysis; and 3) we investigate to what extent LLMs are able to articulate appropriate generalizations about language in response to these questions.\n\nRelated Work\nStack Exchange is a network of numerous CQA sites (originally and most famously, Stack Over-3 https://github.com/shabnam-b/ELQA flow) built on a common platform. Stack Exchange forums have been featured in a number of previous datasets (Yao et al., 2013; Hoogeveen et al., 2015; Ahmad et al., 2018; Penha et al., 2019; Campos et al., 2020; Kumar and Black, 2020; Rogers et al., 2023) , including the English site (our ENG) along with others such as Ask Ubuntu, Android, Gaming and WordPress (dos Santos et al., 2015; Nakov et al., 2017) . We focus on ENG and ELL as they concern the English language itself; we show that these datasets cover a wide range of metalinguistic questions.\nOur use of these forums contrasts with previous work on metalanguage in corpora, which annotated and quantified mentions (Anderson et al., 2004; Wilson, 2010 Wilson, , 2011 Wilson, , 2012 Wilson, , 2017)) , but did not consider entire questions and answers about language. Taylor (2015) studied metalanguage in online forums, but with a focus on the usage of metalinguistic expressions of mock politeness. More recently, Bogetic (2021) published the first corpus of contemporary Slovene, Croatian and Serbian media metalanguage texts.\nSo far, metalanguage has not been a focus in the QA domain-ours is the first publicly available English metalinguistic QA dataset. Most QA tasks are set up to have a question and a reference document, where the objective is to find the answer based on the document (Fan et al., 2019; Kwiatkowski et al., 2019) . In this paper, we explored a type of \"closed-book\" question answering task (Roberts et al., 2020; Khashabi et al., 2021) . To the best of our knowledge, this task has not been explored to date within the realm of English language questions that require significant generalization and adaptation rather than looking up facts.\n\nConstructing the Dataset\nWe collect our data from two sites on Stack Exchange: English Language & Usage (ENG) 4 and English Language Learners (ELL). 5 Sample screenshots of the site are shown in Figure 1 . The Stack Exchange data is publicly released under the CC-BY-SA 3.0 license. We preprocessed the data until 2021-12-06 collected from the Internet Archive 6 to be suitable for NLP studies and release it as ELQA. Additionally, some cleanup (e.g., removing posts marked as \"spam\" or \"offensive\") was done. Fields for each entry (question) include the title, body, user bio (if available), score (which is calculated based on up-votes and down-votes by other users), tags (user-assigned, related to the area/topic of the question), favorite count, and a list of answers. Textual content (body and user bio) is provided in two formats: HTML and plain text without HTML tags. We release two versions of ELQA based on different preprocessing steps. In ELQA-large, we keep questions as long as they don't include any images (<img> HTML tag) and have an answer with a score of at least 2 (meaning at least two people other than the user posting the answer found it helpful). For ELQA-small, we applied further filtering to ensure that the data has the least amount of noise: a) questions should have a score of at least 4 https://english.stackexchange.com/ 5 https://ell.stackexchange.com/ 6 https://archive.org/ 2 (ensuring questions are clear and coherent), b) question has an answer with a score higher than 3 and c) there are no hyperlinks in at least one of the high-rated answers. The last step reduces noise and facilitates a fair comparison for the closed-book question-answering task ( \u00a74) with model-generated answers, as models cannot be expected to have access to the web to suggest valid URLs compared to humans who would search the web for appropriate resources to include in their answers.\nFor quality assurance, we also did a human annotation on ELQA-small. Two of the authors annotated 250 question and answer pairs for the following: 1) Is the question answerable? and 2) Does the answer fully address the question? We found 99.2% of the questions answerable and 91.8% of the answers acceptable.\nTable 1 contains overall statistics on both versions. Figure 2 shows the distribution of the 10 most common tags in each of the sites. Since users assign these tags to their questions (0 to multiple), similar or near-duplicate tags are common within the collection. Some form more general and more fine-grained variants, e.g. 'meaning' and 'meaningin-context'. In addition to available user-assigned tags, we manually inspected a large subset of the data to identify salient types of questions. These are defined below and illustrated in Table 2 . We then labeled 100 random questions to get a rough estimate of their frequencies (two annotators annotated these 100 samples and they agreed on 92% of cases in an overlapping subset).\n\u2022 Fluency (\u224838% of questions): Usually asking about a particular sentence, comparison of multiple sentences, and/or probing how an expression should be used in general. The user wants to know if X is correct, or to decide between multiple choices, which one is correct. \"Correct\" could mean grammatical, most natural/idiomatic, stylistically appropriate, conveying the intended meaning, etc. In Qs where options are provided by the user, there are cases in which 1) none of the choices are correct, 2) multiple choices are correct, and 3) only one is correct. \u2022 Form to Meaning (Interpretation) (\u224819% of questions): Questions such as \"What does X mean?\" (of an expression in general, or an encountered passage) or \"What's the difference in meaning between X and Y?\". \u2022 Meaning to Form (Encoding) (\u224820% of questions): In these questions, the user gives some As can be seen from the examples in Table 2 , it is common for questions and answers to contain example usages, often visually distinguished with Markdown formatting (such as blockquotes, bullets, and italics) which we retain in the processed corpus markup. Examples can be incorporated into a post in a variety of ways-e.g., asking for an interpretation of one usage, as in the Form to Meaning example in Table 2 , or contrasting multiple usages such as in the following question:\n\nDid VS Have done\nWhat is difference between the following statements: Did you tell your parents yet? Have you told your parents yet? Haven't you told your parents yet? Are these questions correct? why do we use one over another in some cases? What is the difference in meaning?\nUsage examples provided in a question may be instances that the author encountered \"in the wild\" (such as in a novel or film), or in a grammar book or dictionary, or they may have been constructed by the user. Answers sometimes include examples found through a corpus search.\n\nEnglish Language Question Answering\nLarge language models can produce output that is fluent and (at times) informationally adequate when presented with factual questions about entities in the world (Roberts et al., 2020) . But how do such models perform when asked questions about the language itself? In this section, we investigate the free-form English language question answering task.\nThis task has the potential to benefit educational applications for language learners. Research on NLP for educational purposes has investigated tasks such as automated grammatical error correction (Dale et al., 2012; Ng et al., 2014; Bryant et al., 2019; Wang et al., 2021, inter alia) , question and quiz generation for language learning (Sakaguchi et al., 2013; Chinkina and Meurers, 2017; Marrese-Taylor et al., 2018; Vachev et al., 2021) , and automated essay scoring (Burstein, 2003; Farag et al., 2018, inter alia) . Nevertheless, an application that has not been taken up by the educational NLP community is free-form question answering about language. Second language learners possess a degree of metalinguistic awareness about the language they are learning, and often turn to teachers or more advanced speakers with explicit questions about vocabulary, grammar, and usage. Community Question Answering (CQA) websites such as Stack Exchange have sites for language learners' questions and answers. These sites require consid- erable effort by volunteers, and learners may have to wait for an answer-if an answer is provided at all. In fact, looking at the data from 2021-12-06 for ENG and ELL, 9% of questions have no answers.\n\nData\nWe randomly divided ELQA-small into train/test/dev splits. This resulted in 21,175 Q&A pairs in the train split and 3,107 Q&A pairs in each of the dev and test splits. Answers in these splits have a score of at least 4. If there are multiple high-rated answers to a question, we include all of them for training. Some of these questions can be answered by looking at a dictionary or vocabulary list for descriptions. But many of them are explanations in relation to particular instances of language use and require significant reasoning rather than looking up facts. Thus in this setup, we do not have any external context/reference available at evaluation time, i.e. this is a closed-book QA task.\nThe input for the task is Title:\n[Q title] <sep> Body: [Q body].\nWe use the HTML version of ELQA for this task since metalinguistic mentions are usually distinguished via formatting (e.g., blockquotes, bullets) and the ultimate goal is a system that humans can easily use to get answers to their language-related questions.\n\nSetup\nWe use T5 (Raffel et al., 2020; Roberts et al., 2022) and GPT-3 (Brown et al., 2020) as our models since they have been shown to be strong baselines in other QA domains. We believe the questions in ELQA offer new challenges for the QA task since they require different types of knowledge/understanding to be able to generate answers. Addition-ally, these questions contain noise (grammatical errors) and cases of textual metalanguage which is likely harder to comprehend for a model.\nWe fine-tune T5-l and T5-xxl for this task. 7 We saved multiple checkpoints during fine-tuning and evaluated them with the interpolation of BLEU (Papineni et al., 2002) , BERTScore (Zhang et al., 2020) and ROUGE (Lin, 2004) on the dev set to choose the best-performing one (checkpoint at 75k updates, hyperparameters available in Table 8 in the Appendix).\nWith GPT-3 we used text-davinci-003 and experimented with both fine-tuning (FT) on 100 and 1000 samples and a few-shot (FS) setting in which the model is given a few demonstrations of the questions and answers at inference time as conditioning, but no weights are updated (Radford et al., 2019) . In the FS setting, we show the model four Q&A pairs since we wanted the model to see different question types but there were also limits on the input length. To select these 4 pairs, we randomly created 5 different sets of Q&A pairs, evaluated on a subset of dev, and chose the best-performing set for the experiments (dev results available in Appendix, Table 9 ).\n\nAutomatic Evaluation\nResults are shown in Table 3 . GPT-3 FS outperforms all other methods in all metrics with a large margin except for BLEU Score. We also observed that using GPT-3 in a few-shot setup worked much better than the fine-tuned version. Looking at some of the model-generated answers, we noticed that the fine-tuned model tends to generate longer an- swers containing redundant text. We observed improvements when we used 1000 samples instead of 100 for fine-tuning and hence, fine-tuning on larger data might result in better performance, however, we only experimented with 100 and 1000 samples in this paper due to having limited resources.\nBased on Table 3 , T5-xxl seems to perform similarly to GPT-3 FT-1000. However, a small manual evaluation showed otherwise (GPT-3 FT-1000 answers were slightly better). Furthermore, we observe that the scores for even the best system are very low, but manual evaluations showed that the GPT-3 FS generates fairly good answers in many cases. Due to these observations and also given the well-known limitations of automatic metrics for evaluating generation tasks (Kasai et al., 2022; Celikyilmaz et al., 2020; Bhakthavatsalam et al., 2021) , we believe conducting human evaluation for deeper analysis is necessary for this task.\nIn Table 4 , we show results for each site to see if one is more challenging than the other. Overall, models perform slightly better on ELL based on automatic metrics-but we see in the next section (Table 5 ) that there isn't really a meaningful difference between the sites when humans evaluate the answers.\n\nHuman Evaluation\nHuman evaluators were presented with the question title and body, and then asked to rate 5 answers: a top-rated human-provided answer, a low-rated human-provided answer, and answers generated by 3 of our best models: GPT-3 FS, GPT3 FT-1000, T5-xxl.\nThey were asked to give ratings (via a slider widget, on a 1-5 integer scale-the higher, the better) for two criteria (C1 & C2): 8 1. Does the answer look grammatically/ structurally like a good answer (ignoring whether it answers the question)? 2. Is the information in this answer a valid response to the question (ignoring formatting/ stylistic issues)?\nThe first criterion aims to get a score for fluency and coherence and the second one for correctness and completeness.\nWe collected ratings for a set of 75 questions (375 different answers). Each question with its set of answers was evaluated by at least 2 raters, and then the average score was calculated based on their responses. 9 We also report the average z-score which is calculated over each annotator's raw score distribution for each metric, intended to normalize interannotator variation in how the scale is interpreted for each of the two metrics (details in Appendix B).\nThe results of this study are shown in Table 5 . Overall, answers generated by GPT-3 FS have a small gap with human answers in both C1 and C2. 10 GPT-3 FT-1000 comes next, with less accurate answers containing redundant text and hallucinations. The smallest model, T5-xxl, ranks last.\nRankings based on human evaluations are available in Table 6 . These results are also indicating that model-generated answers are fluent in most cases, but they are not as good as human answers Title Question Explain the verb tense in \"All that glitters is not gold\" I am confused by the maxim All that glitters is not gold. May I know why the word is is used there? In my known according the English grammar glitters is plural, So after plural we add verb+s. As in above example use auxiliary verb so why not arewhy is? Source Answer Top-rated human Good question, but it's worth pointing out that glitters in this context is a verb, not a noun. The subject of the sentence is all. When all is used as the subject of a sentence, it functions as a pronoun, and it can function as a singular or a plural pronoun.\nFrom NOAD <blockquote> all (pronoun) [with clause ] the only thing (used for emphasis) : all I want is to be left alone.\n(used to refer to surroundings or a situation in general) everything : all was well | it was all very strange. </blockquote> So: <blockquote> All that glitters is not gold </blockquote> is roughly equivalent to <blockquote> Everything that glitters is not gold </blockquote> and the verb tense agrees. but: <blockquote> All who eat are not gluttons </blockquote> is roughly equivalent to: <blockquote> All the people who eat are not gluttons </blockquote> so there is no problem there, either.\n\nLow-rated human\nGrammatically is is used with third person singular and also s form of verb is used. \"All\" indicates a third person single group.\n\nGPT-3 FS\nThe phrase all that glitters is not gold is an idiom, which means that things that appear to be valuable may not actually be so. It is an old proverb, and the verb tense used in the phrase is an example of the so-called \"historic present\". The historic present is a literary device used to give a sense of immediacy to a narrative. It is used to describe events that happened in the past, but are being described as if they are happening in the present. In this case, the phrase is used to emphasize the timelessness of the proverb. So, while the verb glitters is indeed plural, the verb is is used in the historic present to give the phrase a sense of immediacy and timelessness. GPT-3 FT-1000\nThe maxim is in the present tense, so the verb is also in the present tense: Glitters. The past tense of glitter is glittered, and the past participle is glittered. So if the maxim was in the past tense, it would be: <blockquote> All that glittered was not gold. </blockquote> The present tense maxim is a generalisation about something that is true in the speaker's experience. The past tense maxim would be a historical statement about what was true in the past.\n\nT5-xxl\nThis is an interesting question, because it is not immediately obvious what the grammar rules are here. It is not a fixed phrase, and so subject-verb agreement must be based on the context. In this case, the that is a prepositional phrase acting as a complement to the preposition all. It could be rephrased as <blockquote> All that glitters is not gold </blockquote> It is true that all means \"everything that glitters\" -but it is equally true that not everything that glitters means \"not everything that glitters is gold\". It is much the same as <blockquote> Everything that glitters is not gold. </blockquote> The subject all is singular, so the verb must be singular as well -is. If it was twice plural, then are would be used, but it is not. It is interesting to note that Shakespeare used the same construction in Hamlet: <blockquote> Everything that glitters is not gold. </blockquote> The subject that is singular, and so the auxiliary verb is agrees with the singular noun. when correctness/completeness is considered (GPT-3 FS is ranked first or as good as a top-rated human answer in only 45% of cases).\nFor each criterion and Top-rated human, Lowrated human and GPT-3 FS, histograms of the average score of the two annotators are plotted in Figure 3 . We can observe that GPT-3 FS and Lowrated human have very similar numbers of highscoring answers (human evaluation scores), but the number of low-scoring human answers drops off gradually as quality decreases, while the distribution is more spread out for GPT-3 FS. I.e., the model has some moderately bad answers as well as some good ones, whereas Low-rated human answers cluster more on the upper end of the scale. C1 (fluency/coherence). All models generated fairly fluent and well-structured answers. We even notice that GPT-3 FS scores higher in wellformedness than human answers. We looked at those samples and we believe there are two main reasons for this: 1) Some human answers were very long, containing multiple different quotes from different sources. On average, our evaluators preferred the structure of answers from GPT-3 FS, which took the form of a short paragraph addressing the question. 2) Some human answers have a more casual/conversational nature. On the other hand, GPT-3 FS generated more authoritativesounding, teacher-like answers with complete sentences, which were preferred by our evaluators in some cases. C2 (correctness/completeness). On average, models are worse on this metric, though sometimes they did produce acceptable answers (perhaps because variants of the question are commonly discussed on the web).\nOne challenge in this domain is that questions, particularly from ELL, may not be fully fluent. In the Fluency example from Table 7 we see that there are some grammatical errors in the question that are unrelated to the topic of the question. In addition, the questioner uses incorrect terminology, mentioning verb tense in the post title even though the question is actually about subject-verb agreement with respect to number (as can be inferred from the examples). While the good human response correctly flags the incorrect terminology and answers the underlying question, GPT-3 models give irrelevant answers about tense.\nAnother correctness failure from GPT-3 FS can be seen in the following pair involving a Meaningto-Form question:\nWhat is the analog equivalent of 'digitally'? In electronics, we can solve a problem digitally or analoguely ? That doesn't sound right, but surely there must be a word I can use? Update: some definitions: digitally: Relating to or being a service that provides information expressed in discrete numerical form. analoguely: Relating to or being a service that provides information expressed in continuous, non-discrete form. (This is my made up definition of a word I made up). analogously: Similar or alike in such a way as to permit the drawing of an analogy. As you can see, the word analogously doesn't mean the opposite of digitally, it means something else. Answer: The analog equivalent of 'digitally' is 'analogously'. Analogously means \"in a similar way\" or \"by comparison\". It is used to describe a situation where two things are similar in some way, but not necessarily opposite. For example, you could say that a digital signal is analogous to an analog signal, because they are both signals, but they are different in terms of how they are represented.\nWe see that the model begins by ignoring the user's explanation that analogously does not have the desired meaning, and suggests it anyway. The rest of the model's answer gives a (valid) general definition of analogously and clumsily attempts to apply it to the two kinds of signals, which is not responsive to the question. It may be the case that models particularly struggle with questions for which there is no straightforward solution (in this case, no adverb derived from analogue has the user's intended meaning).\n\nMetrics\nOur human evaluations found that the high-rated human answers are the best, and GPT-3 FS is roughly on par with the low-rated human answer, while other models are worse.\nAs noted in \u00a74.3.1, we did not find the automatic metrics very informative. We believe this is due to the long and free-form nature of this task: concepts can be explained in different ways and answers can include different examples which make automatic evaluations very challenging. For example, for the question Why is this sentence: \"Additional nine features were added. . . \" incorrect? a human answer is:\nIt has to do with the order of the adjectives. For example, consider this sentence: <blockquote> Happy nine men walk into a bar. </blockquote> Both nine and happy are adjectives, but we are really intending nine to describe the happy men, not happy describing the nine men. I don't .... Model-generated answers mostly mirror the examples in the question. But answers given by humans mention new examples-which are not in the question (and therefore unlikely to come up in other answers). Thus a good answer may mention superficially off-topic terms metalinguistically, which presents evaluation challenges unlike those of summarization or MT, for example (Reiter, 2018; Celikyilmaz et al., 2020) . For QA tasks in metalinguistic domains such as ours, caution is warranted when applying automatic metrics that are unaware of the use/mention distinction. Human evaluation is likely to be more illuminating. We release our full human evaluation data to enable detailed comparisons in future research.\n\nQualitative Analysis\nWe examined behavioral patterns for the best model, GPT-3 FS, organized by question type, with observations as follows. Interpretation and Encoding: These are the categories where the model performs the best, especially if the question has a straightforward answer (for example, there aren't several possible answers based on context). Topics in these categories often center on definitions of vocabulary and idioms. These questions may involve less reasoning to answer, and models may be leveraging content from the online dictionaries and thesauri in their web training data.\nGrammatical Analysis: When asked about general concepts and rules of grammar, the modelgenerated answers are usually acceptable:\n-ed and -ing adjectives What is the difference? <blockquote> I am interesting in mathematics. </blockquote> versus <blockquote> I am interested in mathematics. </blockquote> Murphy intermediate grammar Unit 97 tells me that I cannot use the former. Extended and general explanations will be appreciated. Answer: The difference between -ed and -ing adjectives is that the -ed adjectives describe a feeling or emotion that is felt by the subject, while the -ing adjectives describe a quality of the subject itself. In the example sentence, interested is the correct word to use because it describes the feeling of interest that the subject has in mathematics. Interesting would be incorrect because it describes the quality of the subject itself, not the feeling of the subject. For example, if you were to say \"This book is interesting,\" you would be describing the book itself, not the feeling you have towards the book. On the other hand, if you were to say \"I am interested in this book,\" you would be describing the feeling you have towards the book.\nHowever, when questions become more specific to examples, sometimes the model struggles:\nWhat does \"in\" refer to? <blockquote> A symbiotic relationship is an interaction between two or more species in which one species lives in or on another species. Answer: It refers to the other species. The symbiotic species lives in the other species.\nThe example in the question uses the word in twice. Rather than addressing the first token, which the questioner has bolded, the answer addresses the second one (denoting the physical relationship between one species and another). Fluency: The model correctly answers many of the fluency questions. Others require reasoning and understanding of context, and the model struggles to generate a correct answer. For example:\nReturning for the nth time Suppose I went to the supermarket three times. Is my third trip considered my \"second time returning there\" or my \"third time returning there?\" Thanks. (I know this question sounds silly, but English is my second language...) Answer: Your third trip to the supermarket would be considered your third time returning there. The phrase returning for the nth time is used to refer to the nth time you have returned to a place, not the n-1th time.\nWe also observe that when the correct usage of a case is not mentioned in the question itself, the model sometimes suggests incorrect corrections and wrong explanations:\nWhy is this usage of \"to\" incorrect? I am in communication with a friend who is learn-ing English. I noticed her using \"to\" in the wrong context, but I'm unable to explain which rule she is breaking. <blockquote> I can't access to Gmail. </blockquote> Answer: In this sentence, the word to is incorrect because it is being used as a preposition. The correct word to use in this context is for. <block-quote> I can't access for Gmail. </blockquote> The word to is used as a preposition when it is followed by a verb, such as go, come, run, etc. For example: <blockquote> I am going to the store. </blockquote> However, when to is followed by a noun, it is usually incorrect. In this case, the correct word to use is for. <blockquote> I need a gift for my friend.</blockquote>\n\nOutlook\nOverall, we were surprised by the quality of many of the answers from GPT-3 FS: many would likely have received high user ratings if submitted as answers on the site. At the same time, the model is not to the point where we would want to trust its answers without human review. We believe that answer confidence estimation-so that users can be shown only the best model-generated answers-is an important direction for using learner QA models in practice (Jiang et al., 2021) .\n\nConclusion\nWe presented ELQA, a dataset containing metalinguistic questions and answers about the English language. We provided analysis and a taxonomy of the data, along with experiments on free-form answer generation and investigated the extent to which language models can articulate their generalizations about language. Since many of the questions in ELQA were asked by language learners, it forms a potentially useful and so far untapped resource for educational NLP purposes and metalinguistic question answering. We release the dataset to enable further studies of this task.\n"}
{"question": "Which type of political actors are included in the evaluation of the model's generalization capability in actor profiling tasks?", "evidence": "  This type of task can be formulated as a user-level classification task, where we aggregate multiple statements to predict the speaker's attribute.  Notably, datasets include not only congress legislators but also other political actors such as journalists, news media, and even anonymous users, to validate the model's generalization capability. ", "options": ["A. Only congress legislators", "B. Journalists and news media", "C. Celebrities and politicians", "D. Common users on Reddit", "CongS (Gentzkow et al., 2018) collects speeches from US congressional records. \u2022 celeb (Wojcieszak et al., 2022) contains tweets of celebrities (journalists, politicians and media)."], "answer": "C", "content": "\nIntroduction\nPolitical actors are shaping our attitudes, opinions, and decisions toward public issues. For instance, on social platforms, politicians can select and emphasize certain aspects of content to bias the discussion, through which they can derive an opinion climate from user engagement and acquire direct feedback from potential voters and opinion leaders (Bene, 2017; Heiss et al., 2019) . Political actor modeling is essential for quantitative political science and has applications in various downstream tasks such as roll call vote prediction (Yang et al., 2020) , frame detection (Johnson et al., 2017) and bias detection (Baly et al., 2020) .\nData-driven approaches utilize different kinds of information to profile political actors, including public statements, legislative behaviors and social Figure 1 : An illustration of political actors. They not only participate in legislative activities, but also form relationships with others, and convey opinions through tweets, speeches and etc. We propose to represent political actors based on their statements and learn the mapping from language to their representations using social networks and behaviors as self-constructed supervision.\nnetworks (Figure 1 ). Early research analyzes roll call data to estimate the ideology of political actors. Ideal point model (Clinton et al., 2004 ) is one of the most widely used approaches for votebased analysis that reveals how cleavages between legislators reflect partisan affiliation. Researchers further incorporate texts of bills to enhance the ideal point model (Gerrish and Blei, 2011, 2012; Kraft et al., 2016) and develop multidimensional vectors to replace one-dimension points. Recently, more abundant information has been considered to learn effective representations for political actors, such as co-sponsorship network (Yang et al., 2020) , relations of contributors (Davoodi et al., 2020) , stakeholders (Davoodi et al., 2022) , mention in documents (Pujari and Goldwasser, 2021) , and expert knowledge (Feng et al., 2021 (Feng et al., , 2022)) .\nGenerally speaking, previous research aims to learn representations for a certain group of political actors using supervision from specific downstream tasks as objectives. Although they report positive results on target tasks, their models lack generalization ability in two aspects. (1) Representations are learned on labeled data from specific tasks, e.g., state-level vote prediction, therefore they cannot be easily transferred to other tasks or scenarios. (2) The model is limited to the training setting and can not be adapted to dynamic social contexts. In other words, it's hard for the model to estimate new legislators, non-voting candidates and other political actors unseen.\nRecently, large-scale pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019b; Brown et al., 2020) have demonstrated a strong generalization ability and achieved excellent performance in many language modeling tasks. Motivated by PLMs, we explore representing political actors based on their statements and propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM) 1 . We employ a two-stage training procedure following the fashion of PLMs. Firstly, we pre-train our model to learn the mapping from statements to actor representation. We propose a multigranular method to represent political actors based on language, and information of political scenarios is further injected into our model via proposed structure-aware contrastive learning and behaviordriven contrastive learning tasks. Secondly, we fine-tune the model for downstream tasks using the corresponding supervised objectives.\nUPPAM is novel in three points. (1) We learn the mapping from statements to the representation of political actors, instead of directly learning actor representations. By doing so, the mapping parameters can be transferred to any downstream tasks easily, learning representations for unseen political actors based on their statements. (2) We propose several self-training tasks to inject general knowledge in the political scenarios into mapping parameters in the pre-training stage. (3) We propose a multigranular actor representation model, that can capture nuances of both general ideology and specific preferences between different political actors. We evaluate our approach on three types of tasks in quantitative political science, i.e., profile of actors, prediction of behaviors and analysis of languages. UPPAM outperforms general PLMs and other political domain-specific PLMs on these tasks. Our task-agnostic model also achieved competitive results compared to the task-specific models that employ architectures crafted for the 1 We have made our code publicly available at https:// github.com/xymou/UPPAM. vote prediction task. Further analysis shows the effectiveness and robustness of UPPAM in few-shot settings and different aggregation settings.\n\nMultigranular Actor Representation\nPolitical actors manifest themselves in political activities in multiple granularities. On the one hand, they hold a general ideology or bias, which is long-term and stable. On the other hand, when discussing or taking action on different issues, they hold specific positions (Gerrish and Blei, 2012) , which are the result of long-term bias and shorttime interests (Spell et al., 2020) . Based on this, we propose to represent political actors in two granularities to model both broad ideology and specific preferences for various downstream scenarios.\n\nGeneral and Specific Statements Collection\nIn practice, we use all statements a political actor has posted to get his general representation, characterizing the broad political leaning. Furthermore, issue-related content is adopted to help capture specific attitudes. Concretely, we use a handcrafted information retriever (see more details in Appendix A.2), to collect statements related to the queried policy area as input to encode the specific representation.\nStatements Aggregator Since a political actor can post thousands of statements, the first challenge is how to aggregate one's statements to get his representation. It is too expensive in time and computation cost to combine full sentences. Instead, we identify indicator words from statements for information aggregation. According to the framing theory (Entman, 1993) , entities and subjective content an author uses can implicitly reflect his political leaning. Following this, we identify entities, frame and sentiment words as indicators. We sort them by TFIDF (Jones, 1972) scores and keep indicators with the highest values to form an indicator sequence. In this case, for each political actor, we can get two kinds of indicator sequences, given a query about policy area j:\nS g i = w g 1 , w g 2 , ...w g N (1) S p j i = w p j 1 , w p j 2 , ...w p j M (2)\nwhere S g i is calculated from all the statements made by political actor i, S related to policy area j, and we reserve top N and M indicators with highest TFIDF value, where N and M are pre-defined hyper-parameters.\nIn subsequent pre-training and downstream tasks, we use general sequences as input when the goal is to profile the characters broadly, e.g., estimating ideology. And we input both sequences and average the representation when specific attitudes are required in tasks, as shown in Figure 2 . Note that even if the issue-related content can not be retrieved, we can use the general sequence as a substitute, to ensure input compatibility.\n\nMultidimensional Pre-training for Political Actor Modeling\nTo inject general knowledge of the political landscape into the mapping from statements to representation, we construct self-supervised tasks based on structural and behavioral information.\n\nStructure-aware Contrastive Learning (SCL)\nIn terms of structural information, we mainly focus on the relationship formed between political actors. Previous studies have revealed that homophily exists in political communities, where people with similar ideologies form a link with each other (Barber\u00e1, 2015) . We use two parts of links, namely party affiliation and co-sponsorship in voting. We treat party affiliation as a coarse relationship and cosponsorship as a fine relationship respectively. By doing this, the model can further capture nuances across parties as well as inside the same party.\nParty Affiliation Link We compare statements of legislators from different parties. We choose a legislator as the anchor, and then take another legislator with the same party affiliation as the positive sample, while those from the opposite party are regarded as negative samples. By comparing general statement sequences of legislators from different parties, the model can learn the differences in the languages of different ideologies.\nCo-sponsorship Link In the legislative process, a bill is initialized by a sponsor and several cosponsors. We assume that the more two legislators collaborate, the more they are alike since they reach agreements on many occasions (Yang et al., 2020; Mou et al., 2021) . Given an anchor legislator, other legislators are divided into three categories based on the number of times they co-sponsored with the anchor legislator: G 1 (the co-sponsorship times are above the average); G 2 (the co-sponsorship times are below the average); G 3 (they have never cosponsored). And we further sample positive and negative samples with the rule of\nG 1 < G 2 < G 3 .\nBased on the triplets constructed in the above two ways, the structure-aware contrastive objective is formulated as follows:\nLSCL = t\u2208T SCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4SCL + (3)\nwhere T SCL is the set of legislator triplets, t (a) , t (p) and t (n) are actor representation encoded by general sequences of anchor, positive and negative sample in triplet t, \u03b4 SCL is a hyperparameter and\n[\u2022] + is max(\u2022, 0).\nNotably, this task endows the model to capture general ideology of speakers from their languages.\n\nBehavior-driven Contrastive Learning (BCL)\nWhen it comes to behavioral information, we pay attention to the most common and important actions, i.e., voting. Specifically, we sample triplets consisting of an anchor bill and a pair of legislators, where the positive legislator p votes yea on the given bill and the negative one n votes nay.\nDifferent from the ideology cleavages modeled in Sec 2.2.1, the divergence of specific preferences is supposed to be reflected in the languages here. Thus, for each legislator, we extract statements about the policy area of the anchor bill as the specific sequence, input with the general sequence, as we mentioned in Sec 2.1. In this way, the behaviordriven contrastive objective is as follows:\nLBCL = t\u2208T BCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4BCL + (4)\nwhere T BCL contains all vote triplets, and \u03b4 BCL is a hyperparameter. t (a) is the bill representation, t (p) and t (n) are the average of representation of the general sequence and the specific sequence, for the positive and negative legislators respectively.\nIt's noticeable that this pattern is not limited to the roll-call vote scenarios, instead, it can be applied to model the preferences towards any bills, events, or targets with a text description.\n3 Pre-training Process\n\nLanguage Model Co-training\nAs mentioned in Sec 2.2.2, modeling political actors in political scenarios inevitably requires encoding textual information of the bills and issues they interact with, e.g., Equation 4. Meanwhile, it is important to understand their opinions in a single discourse without context. Thus, we incorporate additional modules to model political texts. Specifically, as shown in Figure 2 , we have two FFN layers in parallel in each transformer layer, to handle text and actor sequences separately. Given a sequence of input x = {x 1 , ..., x n }, the model first performs multi-head self-attention and then the corresponding module FNN k obtains the required representation:\nh k = FNN k ( Self-Attention ({x 1 , . . . , x n }))\n(5) where k \u2208 {0, 1} indicates the modules of actor and text respectively.\nWe adopt a masked language model objective to pre-train the language model. As mentioned before, political bias and framing effect are often reflected in the selection and mention of specific entities, subjective content, and emphasized frames. Thus, we take a masking strategy that upsamples entity tokens, sentiment words (Wilson et al., 2005) and frame indicators (Roy and Goldwasser, 2020) to be masked for the MLM objectives, with a 30% probability. More details can be found in Appendix B.\n\nOverall Pre-training\nSince the indicator sequence is not a normal sentence, we don't train the MLM task with contrastive learning together. Instead, the pre-training process is divided into two stages. In the first stage, we adopt the MLM task on the original statement sentences and activate text modules, to urge the model to understand the political text. Then, based on this checkpoint, we further conduct the multidimensional pre-training for political actor modeling by combining the objectives:\nEQUATION\n)\nwhere \u03b1 is hyperparameters.\n\nExperiment Setup\nWe fine-tune our model on different kinds of downstream tasks in quantitative political science. We then compare it with prior general PLMs and political domain-specific PLMs.\n\nPre-training Datasets\nCompared to other political actors, congress legislators are more typical and they generate massive content every day. Thus, we start with legislators to construct our pre-training datasets. Overall, we get 887 legislators and delete the meaningless tweets including self-promotion advertisements, notifications, etc., using regular expressions. Finally, the cleaned data contains 2,020,938 tweets, covering discussions of events in various areas. We keep 10K held-out tweets as the validation set.\n\nLegislative Context\nWe collect the party affiliation, sponsorship lists of bills, bills, and corresponding voting records from VoteView 2 and the website of U.S. Congress 3 . Each bill belongs to a specific policy area and has textual information of title and description. We get bills of 112th and 113th for pre-training and reserve those of 114th and 115th for the formulation of downstream tasks. In the pre-training stage, 1,045 bills and 375,440 voting records are involved.\nTo correlate legislators' votes to their statements in the related policy area, we filtered each legislator's tweets in each policy area by the handcrafted information retriever mentioned in Sec 2.1. We finally acquire 1,142,587 tweets, and the details can be found in Appendix A.2. The distribution of the policy agenda of bills and the percentage of legislators whose related tweets can be retrieved in each policy area are shown in Figure 3a and Figure 3b . Over 90% of legislators can be retrieved with relevant statements in most policy areas.\n\nImplementation Details\nUPPAM is produced via continued pre-training on RoBERTa-base model (Liu et al., 2019b) , where we add parallel FFN modules in each transformer layer with the same initialization as the original one. In the first stage, the model is trained on tweets, to minimize the MLM loss with AdamW (Loshchilov and Hutter, 2018) optimizer. In the second stage, the model is further trained on indicator sequences and bill texts, to minimize the L CL . We evaluate the model every 200 training steps on the validation set and keep the best checkpoint. The pre-training procedure takes around 96 hours on 4 Tesla V100-SXM2 GPUs. More details and hyperparameters can be found in Appendix B.\n\nDownstream Tasks and Datasets\nWe evaluate the models on three types of tasks, namely actor profiling, behavior prediction and language analysis. Notably, datasets include not only congress legislators but also other political actors such as journalists, news media, and even anonymous users, to validate the model's generalization capability.\n\nActor Profiling\nThis type of task can be formulated as a user-level classification task, where we aggregate multiple statements to predict the speaker's attribute.\nIdeology Detection is the main task to profile actors broadly, aiming to predict political leaning. Models are evaluated on the following datasets.\n\u2022 CongS (Gentzkow et al., 2018) collects speeches from US congressional records. \u2022 celeb (Wojcieszak et al., 2022) contains tweets of celebrities (journalists, politicians and media). We convert the ideology scores into labels according to the signs. \u2022 Reddit (Kitchener et al., 2022) collects comments of common users in non-political subreddits, and labels the users with ideology in the economic dimension. \u2022 PEM (Xiao et al., 2022) collects tweets of legislators, news outlets and cabinet of President Obama and President Trump. \u2022 TIMME (Xiao et al., 2020) includes Twitter accounts with location information and selfidentified political-polarity labels. These accounts are not run by politicians.\n\nBehavior Prediction\nThis type of task can be regarded as a relation prediction task, where we predict a political actor's attitude or action towards a given target with a piece of text description.\nVote Prediction tasks aim to predict votes of legislators towards bills with stances of yea or nay. We follow two configurations in (Mou et al., 2021) . \u2022 VoteIn refers to the in-session setup, where we randomly split the bills in the same congress session, i.e., the 114th session. \u2022 VoteOut refers to the more challenging outof-session setup, where we use data in the 114th session for training and validation while testing on the 115th session.\nGrade Prediction tasks are designed as classification tasks for ratings in a certain issue, given a politician's statements and background description of the given issue. We include datasets as follows:\n\u2022 NRA Grades (Pujari and Goldwasser, 2021) provides politicians' grades {A, B, C, D & F} assigned by National Rifle Association and their statements on guns, as well as background information of guns from ontheissues.org. \u2022 LCV Grades (Pujari and Goldwasser, 2021) is similar to NRA Grades, but it's about the scores in the environment area.\n\nLanguage Analysis\nIn addition to the overall characterization of political actors, we also test models' ability to understand individual discourses. We apply stance detection and frame detection as downstream tasks, which can be formulated as sentence-level classification tasks.\nStance detection tasks aim to predict one's stance towards a given target. The tasks take a 3-way label (favor, against, and neutral) or binary label (favor, against). We test on these datasets.\n\u2022 poldeb (Somasundaran and Wiebe, 2010) provides opinion-target pairs from several debating platforms covering different domains. \u2022 election (Kawintiranon and Singh, 2021) contains tweets related to the 2020 US presidential election, expressing stances towards President Trump and Biden.\n\u2022 SEval (Mohammad et al., 2016 ) is a shared task to detect stances in public tweets.\nFrame detection tasks aim to detect which frame dimensions are employed in a piece of text. It's a multi-label classification task with a pre-defined label set. We test on these datasets.\n\u2022 twitter (Johnson et al., 2017) annotates tweets of politicians with 17 general frames. \u2022 gvfc (Liu et al., 2019a) collects news headlines about gun violence, and annotates them with 9 issue-specific frame dimensions. \u2022 immi (Mendelsohn et al., 2021) collects immigration-related tweets posted by the public, annotated with 14 general frames.\n5 Experiment Results\n\nMain Results\nThe compared general PLMs include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) . We also compare our model with available PLMs for social science texts-SsciBERT (Shen et al., 2022) , and for the political domain: POLI-TICS (Liu et al., 2022) and PoliBERTweet (Kawintiranon and Singh, 2022). We fine-tune all the PLMs in the same settings, and we select the best fine-tuned model on validation sets using macro F1.\nThe implementation details and hyperparameters can be found in Appendix C.2. Table 1 presents macro F1 scores on the downstream tasks.\nActor Profiling Our model shows superior performance on various political actor modeling tasks.\nResults of ideology detection tasks indicate that our model can not only characterize the ideology of legislators but is also good at modeling other roles like journalists in the celeb dataset and cabinet in the PEM dataset, demonstrating the transferability of using languages to represent characters. The reason for not performing best on the Reddit dataset may be the gap between the expression habits of common users and that of politicians. Nevertheless, we still outperform the majority of baselines.\nBehavior Prediction All the models show excellent performance on vote prediction and grade prediction tasks, using languages to represent political actors. It indicates that it's a feasible scheme to infer political actors' behaviors from their languages. Among all the PLMs, our model is the best. We attribute the performance gain to our proposed behavior-driven pre-training task.\nLanguage Analysis Moreover, our model also achieves competitive performance on tasks of analyzing individual text including stance detection and frame detection, indicating that the ability to understand political languages is preserved while the model is learning to profile actors, benefiting from the co-training process in Sec 3.1.\n\nAblation Study\nTo explore the effects of different components, we conduct ablation studies and results are reported in Table 2 . Removing SCL or BCL mainly hurts the performance of actor profiling tasks. Removing the text modules results in the most loss in language analysis tasks, especially the frame detection task. This demonstrates the necessity of separate modules to guarantee the ability to model political text.\n\nFurther Analysis\nFew-shot Learning We fine-tune PLMs on different numbers of samples. Figure 4 tasks. Benefiting from the pre-training stages, our model can better capture ideology and preference differences, even when using only 16 samples.\nCompare with Task-specific Models Taking the vote prediction task as an example, we compare our model with previous task-specific models, where particular meta-data and structural information is crafted for the task. Table 3 shows that UPPAM achieves competitive results, indicating that we can deduce political actors' votes from languages. Additionally, our method can be used to analyze nonvoting actors, relieving the cold-start problem.\n\nMethods of Statements Aggregation\nWe show the impact of statements aggregation methods on ideology detection in fine-tuning. We mainly compare our method with concat (Table 4 ) and mean pooling (Table 5 ). concat means to concatenate each speaker's political statements into a flat sequence and then encode it. mean pooling encodes each sentence individually and uses the averaged representation as the final representation. We further discuss the impact of the number of aggregated sentences in Appendix C.2.2. Results illustrate that our model shows robustness in several settings and our aggregator is more effective and efficient.\n\nVisualization\nGeneral Ideology We perform Principle Component Analysis (PCA) on political actor representation generated by our model for the CongS dataset. As shown in Figure 5a , our method can well separate politicians of different ideologies.\nIndividual Specific Preferences We also visualize specific representation in different policy areas for individuals. Figure 5b shows the representation in several highly-discussed policy areas, learned by different models from the tweets of Rep. Rooney. We can observe that Rep. Rooney behaves conservatively in immigration, but expresses left-wing views on environment (Pujari and Goldwasser, 2021) . While most of our baselines fail to capture this nuance, UPPAM can well compare the relative polarity in each area.\n\nRelated Work\nPolitical Actor Modeling focuses on modeling attributes and behaviors of political actors, with special attention to estimating the ideology. Because of the publicity and typicality, politicians like legislators have been the research subject for most work.\nThe most widely used approach to estimate the ideology of legislators is ideal point model (Clinton et al., 2004 ) that represents legislators and bills as points in a one-dimension latent space from the rollcall data. After that, researchers further incorporate texts of bills (Gerrish and Blei, 2011; Gu et al., 2014) to enhance the model, solving the problem of prediction on new bills. Some embedding methods are also proposed to promote learning of legislators (Kraft et al., 2016; Kornilova et al., 2018) . More recently, external information including cosponsorship (Yang et al., 2020) , donors (Davoodi et al., 2020) , relevant stakeholders (Davoodi et al., 2022) and expert knowledge (Feng et al., 2021 (Feng et al., , 2022) ) is used to better learn legislator representation. They follow a mixed structure of textual encoder and graph encoder, to explicitly combine textual and structural information. Despite outstanding performance on target tasks, these methods are limited to certain settings or data, behaving inefficient in dynamic political scenarios. Thus they are hard to be transferred to all actors. By contrast, methods relying on texts (Vafa et al., 2020) provide more possibility for generalization.\n\nDomain-specific Pre-training\nBased on continued pre-training on domain-specific data, domainspecific Pre-trained Language Models have shown superiority on many NLP tasks. Domain-specific PLMs have been investigated in many areas including medical (Zhang et al., 2021) and financial (Araci, 2019) \n\nConclusion\nIn this paper, we propose to learn political actors from languages and inject multidimensional domain knowledge into the PLMs through structureaware contrastive learning and behavior-driven con-trastive learning. Experimental results validate the effectiveness and generalization capability of our approach.\n"}
{"question": "what method did the authors use to enhance the correlation between images and text?", "evidence": "  We also propose to better exploit the image by introducing guided self-attention. Guided self-attention consists in masking irrelevant connections between text and image representations; each text (resp. image) embedding can attend to itself and all other text (resp. image) positions, but can only attend to image (resp. text) positions conditioned on pre-extracted text-image alignments. ", "options": ["A.The authors used guided self-attention mechanisms.", "B. The authors used visual projection embeddings.", "C. The authors used local and global features of text inputs.", "D. The authors used random masked text inputs."], "answer": "A", "content": "\nIntroduction\nMultimodal machine translation (MMT) typically refers to the use of additional non-textual data in text-based machine translation (MT). Here, we focus on the case where source texts are accompanied by images, the idea being to exploit visual data to improve the translation of ambiguous sentences. For example, in Figure 1 , the English word glasses can either be translated as French verres 'drinking vessels' or lunettes 'spectacles', an ambiguity which is resolved using the image.\nA main research direction of MMT has been how to best exploit image representations and combine the image and text modalities (Yin et al., 2020; Caglayan et al., 2021; Calixto et al., 2017; Li et al., 2022) . It has typically been difficult to surpass strong text-only baselines, the image modality often being ignored (Wu et al., 2021) . A major issue holding back progress is that most current stateof-the-art MMT models (Yin et al., 2020; Elliott and K\u00e1d\u00e1r, 2017; Wu et al., 2021; Li et al., 2022) are trained solely on the \u223c30k examples of the Multi30k dataset (Elliott et al., 2016) , comprising image captions and their translations. This causes two issues: (i) the models do not exploit the large amount of text-only data available and therefore perform poorly in comparison to state-of-the-art text-only MT systems, and (ii) we show that very few examples require images to be correctly translated, which means that the datasets are ill-adapted to evaluating the use of the image modality.\nIn this article, we aim to overcome these problems by proposing (i) a new MMT approach that is able to exploit (text-only) monolingual and parallel data as well as (multimodal) captioning data, and that reaches a good balance between maintaining high MT quality and effectively exploiting images, and (ii) a test set, CoMMuTE, containing contrastive evaluation pairs, where images provide the necessary context to disambiguate between multiple meanings of the same source sentence.\nOur suggested model is inspired by work on adapting frozen language models (LMs) to multimodal inputs (Sung et al., 2022; Yang et al., 2022; Eichenberg et al., 2021; Pfeiffer et al., 2022) ; we propose to adapt a strong MT model to multimodal inputs with lightweight modules (Houlsby et al., 2019) to exploit the large amount of textual data it was trained on. We also propose to better exploit the image by introducing guided self-attention and by combining the standard MMT objective with a visually-conditioned masked language modelling (VMLM) objective (Li et al., 2019; Lu et al., 2019; Su et al., 2020) . Our model obtains competitive results compared to strong text-only baselines on standard En\u2192{Fr,De,Cs} MMT benchmarks (Elliott et al., 2016 (Elliott et al., , 2017;; Barrault et al., 2018) and outperforms them and state-of-the-art MMT models on our lexically ambiguous contrastive test set. 3 2 Related Work Multimodal MT data. The reference dataset to train and evaluate MMT models is Multi30k (Elliott et al., 2016) . However, recent work has shown that most MMT systems trained and evaluated on it do not effectively exploit the image information; Elliott (2018) showed that replacing the ground truth image with a random one does not lead to the drop in performance that would be expected, while Wu et al. (2021) argued that the observed gain in performance was due to a regularisation effect. It is also notoriously difficult to beat text-only baselines on this benchmark (Barrault et al., 2018) . This may be due to (i) some subsets of Multi30k having been translated independently from the images (Elliott et al., 2016) and (ii) most of the time, the source text being sufficient in theory to produce a perfect translation (i.e. the image is not necessary; see Section 5.2 for our own analysis).\nBased on this, alternative test sets and evaluation methods have been proposed. Caglayan et al. (2019) proposed to probe the use of images in MMT models, while Li et al. (2021) proposed another training corpus and evaluation benchmark to evaluate MMT systems, but their work is only based on gender ambiguity and requires specific training data to train MMT models. Lala and Specia (2018) released a lexically ambiguous MMT evaluation dataset to evaluate models ability to disambiguate source sentences, but we found that text context is generally sufficient to translate the evaluation dataset correctly.\nContrastive MT datasets. Another means of evaluating (and the one we adopt here) is to target specific phenomena through the use of contrastive test sets. They involve evaluating models based on their ability to rank pairs of translations, where one is correct and the other incorrect. They have been used for the evaluation of different linguistic phenomena, including grammaticality (Sennrich, 2017) , multi-sense word disambiguation (Rios Gonzales et al., 2017; Raganato et al., 2019) , pronoun translation (M\u00fcller et al., 2018; Bawden et al., 2018; Voita et al., 2019) and lexical coherence/consistency (Bawden et al., 2018; Voita et al., 2019) . Bawden et al. (2018) introduced the idea of conditioning which of the translations is correct depending on linguistic context, and we adopt the same strategy here with our CoMMuTE dataset, composed of lexically ambiguous sentences whose translations are determined by the visual context.\n\nAdapting pretrained LMs to multimodal inputs.\nA lot of progress has been made through the use of pretrained LMs (Devlin et al., 2019; Conneau and Lample, 2019; Liu et al., 2020) , often trained on raw text for text-only models or image captioning data for multimodal ones (Radford et al., 2021; Alayrac et al., 2022; Chen et al., 2022) . One of the most efficient ways to learn multimodal LMs is the visually-conditioned masked language modelling (VMLM) objective (Chen et al., 2020; Lu et al., 2019; Su et al., 2020; Li et al., 2020; Zhou et al., 2021; Huang et al., 2021a; Li et al., 2019) . Inspired by the masked language modelling (MLM) objective (Devlin et al., 2019) , it consists in randomly masking input text tokens and predicting them conditionally based on the visual features. A lot of interest has also been shown in lightweight modules such as adapters (Houlsby et al., 2019) to adapt large frozen LMs to multimodal tasks (Eichenberg et al., 2021; Yang et al., 2022; Pfeiffer et al., 2022; Tsimpoukelli et al., 2021; Sung et al., 2022) in order to avoid catastrophic forgetting (De Lange et al., 2021) . Based on these approaches, we propose to adapt a strong text-only MT model with lightweight modules in order to exploit the large amount of data it previously learned.\n\nWhich type of visual features in MMT systems?\nIn terms of how images are represented in multimodal models, different strategies exist. Many works first proposed to incorporate global visual features from object recognition models pretrained Frozen with trainable adapters\nv n t 1 t n <DE> CLIP emb.\nTwo ___ wearing ___ .\n\nVisual projection Embedding men hats\nVisually Conditioned MLM\nv 1 v n t 1 t n\n\n<EN>\nFigure 2 : Overview of our approach, multimodal MT (MMT) (left) and visually-conditioned masked language modeling (VMLM) (right) objectives. We train VGAMT on both objectives jointly.\non ImageNet (Deng et al., 2009) , such as ResNet50 (He et al., 2016) , either in the form of a single vector or a set of features (Calixto et al., 2017; Elliott and K\u00e1d\u00e1r, 2017; Calixto and Liu, 2017; Yao and Wan, 2020; Helcl et al., 2018) . More recent global features extractor such as CLIP (Radford et al., 2021) exist, but to our knowledge have not been used in MMT models. Extending this idea, other works focused on entities in the image and extracted bounding boxes using a pretrained Faster R-CNN (Ren et al., 2015) in order to introduce more semantic visual information into MT (Gr\u00f6nroos et al., 2018; Ive et al., 2019; Caglayan et al., 2021) . Recent efforts have been made to only select parts of the image that are relevant to the translation of the sentence. Some proposed to use a more selective attention mechanism between modalities (Liu et al., 2021; Ye et al., 2022) , while others suggested extracting other types of visual features (Huang et al., 2021b; Fang and Feng, 2022) . Based on this, Yin et al. (2020) decided to exploit local image-text correspondences in their model Graph-MMT. Similar to their approach, we use a simpler method to extract relevant visual features, using the output queries from a state-of-the-art free-form text object detector MDETR (Kamath et al., 2021) as our local visual features (in addition to global features from CLIP). \n\nOur approach: VGAMT\nThe two main aims of our approach are to (i) exploit a maximum available data (not just multimodal parallel text data) and to (ii) provide an effective way to combine image and text modalities. Our approach, shown in Figure 2 , consists in taking a strong text-only MT model 4 and adapting it to multimodal MT. To adapt this strong text-only model to multimodal inputs, we add several lightweight modules-bottleneck adapters (Houlsby et al., 2019) and linear visual projection layers-to the otherwise frozen initial model. The bottleneck adapters are lightweight linear layers introduced after each attention block and each feedforward layer to project embeddings down before projecting them up.\nIn terms of representing visual information, we choose to use two types of representation. We concatenate local (MDETR) features and global (CLIP) features to the text inputs. We choose to use global features too, since the source sentence can describe more general aspects of the image than mere objects (such as scenes). We jointly train the non-frozen parts of our model on two distinct objectives: multimodal MT (MMT) and visuallyconditioned masked language modelling (VMLM), as described in Section 3.1. We also introduce a guided self-attention to exploit image information in a straightforward manner (see Section 3.2) in the encoder (while the decoder uses regular self-and cross-attentions and can only attend to embeddings related to text positions). We call our approach Visually Guided and Adapted Machine Translation (VGAMT).\n\nCombining training objectives\nAs shown in Figure 2 , we jointly train VGAMT on two objectives: visual masked language modelling (VMLM) and multimodal MT (MMT). VMLM (resp. MMT) consists in predicting masked tokens (resp. translating the sentence) conditioned on the image. 5 The use of the VMLM objective in addition to MMT ensures that the model does not learn to ignore the visual inputs when translating (since Multi30k is mainly composed of very standard and unambiguous parallel sentences). We make sure to mask a high percentage (25%) of the text inputs so that the model is forced to attend to the image when producing translations.\n\nGuided self-attention\nThe backbone of VGAMT is an encoder-decoder MT model, in which image features are concatenated to textual input embeddings and shared selfattention is used over the two input modalities (see Figure 2 ). Instead of using full self-attention (Caglayan et al., 2021) (connections between all image parts and all text tokens), we introduce guided self-attention. Guided self-attention consists in masking irrelevant connections between text and image representations; each text (resp. image) embedding can attend to itself and all other text (resp. image) positions, but can only attend to image (resp. text) positions conditioned on pre-extracted textimage alignments. We obtain these alignments (in the form of a cross-modal correspondence matrix) using MDETR (Kamath et al., 2021) , which detects image regions and corresponding text spans based on a free-form text (see Figure 3 and Appendix B for more details).\nConcretely, let Q, K and V denote the learnable query, key and value parameters of a standard self-attention mechanism. Attention can be defined as Attention(Q, K, V ) = A \u2022 V , where the attention matrix A = (a ij ) is defined as\nA = softmax QK T / \u221a d k\n, where d k is the dimension of the key vector, i.e.:\na ij = e Q i K T j / \u221a d k l e Q i K T l / \u221a d k (1)\nThe idea behind our guided self-attention mechanism is that we want to allow subwords to attend to all subwords, all bounding boxes to attend to all bounding boxes, but to only allow cross-modal attention between a subword and bounding boxes that are linked by MDETR (see Figure 3 ). We therefore define a binary masking matrix C = (c ij ) where (i) c ij = 1 if indices i and j correspond to embeddings coming from the same modality, and (ii) c ij is provided by the MDETR matrix otherwise: it is 1 if MDETR has created a link between subword (resp. bounding box) i and bounding box (resp. subword) j. Once this guiding matrix C is defined, we can replace the standard attention (1) with our guided attention:\nEQUATION\n)\nThe main advantage of guided self-attention over full self-attention is that the model does not have to learn to ignore irrelevant text-image correspondences since alignments are introduced as a prior.\n\nContrastive Multilingual Multimodal Translation Evaluation (CoMMuTE)\nTo overcome the flaws of existing benchmarks (see Section 5.2), we introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation dataset 6 . It is composed of 155 lexically ambiguous sentences in English, each associated with two translations corresponding to two of the possible meanings of each sentence and two images that determine which of the translations is correct. It covers English\u2192French, English\u2192German and English\u2192Czech. An example is given in Figure 4 . 2018), and we created the remaining ones. 7 We collected two images for each sentence under Creative Commons license (either Google Images or our own photos), so that the image illustrates without ambiguity one of the two meanings of the sentence. We do not restrict the image-text relation to be strictly descriptive (as for image captions) in order to have a more general evaluation dataset. Each sentence was translated into two possible translations (each corresponding to one of the images) by a native speaker of the target language.\nAppendix A provides some basic statistics.\nThe idea of CoMMuTE is to use MMT models to rank each of the two translations based on image information. The perplexity of a sentence for a given model is defined as: P P L q (y) = N i=1 q(y i ) \u2212 1 N , where q is the probability distribution output by the model, N is the sequence length and y 1 , . . . , y N is the sequence of tokens. Now, let y 1 , . . . , y N 1 be the sequence of tokens of the correct translation and y \u2032 1 , . . . , y \u2032 N 2 the sequence of tokens of the incorrect translation, a model makes a correct prediction if P P L q (y) \u2264 P P L q (y \u2032 ). i.e. the model considers the correct translation more likely than the incorrect one. For each example, we rank each of the translations based on each of the images (2 comparisons per example), and report the accuracy over all the examples. As CoMMuTE is perfectly balanced, a text-only model will get exactly 50% accuracy on this task. 5 Experiments\n\nText-only data\nAll our experiments are based on the strong MT model mBART 8 (Liu et al., 2020) , which we finetune on parallel text (see Table 1 ). We use Open-\n1 2\nWe'll have to get rid of that mole.\nIl va falloir enlever ce grain de beaut\u00e9. Subtitles2018 9 (Lison et al., 2018) , Wikipedia (Wo\u0142k and Marasek, 2014) , Ted Talks (Reimers and Gurevych, 2020) and the Books datasets (Tiedemann, 2012). We preprocess the data using Moses scripts (Koehn et al., 2007 ). 10\n\nMultimodal data\nTest2016 Test2017 MSCOCO Ambiguous (%) 21 (2.1%) 20 (2%) 6 (1.3%) (2017) and Barrault et al. (2018) released two additional related test sets (Test2017 and Ambiguous Coco). However, on analysis of these sets and as shown in Table 2 , we found that very few examples are image-dependent (i.e. the source sentence is ambiguous and the image is required to solve the ambiguity in the target language), 11 meaning that an MMT system is unlikely to perform better than a text-only system. Moreover, most of these ambiguities are semantically similar and they only cover a few multi-sense words. Although Ambiguous Coco (Elliott et al., 2017) is designed to be an ambiguous test set as it is built around multi-sense verbs, it was automatically created from sentences from MSCOCO (Lin et al., 2014) for which the textual context is often sufficient for disambiguation. These benchmarks remain useful to make sure MMT systems do not perform worse than text-only MT models on examples where images are not necessary to translate correctly. However, we consider them insufficient to assess how well MMT systems exploit images to improve translation.\nMonolingual multimodal data. For the VMLM objective, we train our model on the Conceptual Captions (CC) dataset (Sharma et al., 2018) composed of 3.3M 12 images aligned with English text.\n\nImplementation details\nFor all our experiments, we use the mBART implementation from Hugging Face (Wolf et al., 2020) . Experiments with adapters used bottleneck adapters (Houlsby et al., 2019) with a reduction factor of 8 and ReLU activation (Agarap, 2018).\nWe use the implementation provided by adaptertransformers (Pfeiffer et al., 2020) . We use a batch size of 512, the Adam optimiser (Kingma and Ba, 2014) with \u03b2 1 = 0.9, \u03b2 2 = 0.99 and a learning rate of 10 \u22124 for En\u2192Fr and 10 \u22125 for En\u2192{De,Cs}.\nWe also applied 0.1 label smoothing (Szegedy et al., 2016) during training. We selected our final model according to the best BLEU score (Papineni et al., 2002) on the Multi30k dev set after at least one full pass over the Multi30k and Conceptual Captions training sets. We ran each experiment 3 times with different seeds and report the average BLEU 13 (Papineni et al., 2002) and COMET (Rei et al., 2020 ) scores 14 and the standard errors. We also report METEOR scores (Banerjee and Lavie, 2005) in Appendix E. All experiments were carried out on 8 NVIDIA V100 GPUs for \u223c15h.\n\nBaselines\nWe consider several text-only and multimodal baselines. All baselines except the MT models finetuned from mBART were trained from scratch with the original codebases and features released by the papers' authors. Models trained on the (multimodal) MT objective only where trained on Multi30k, while models jointly trained on the (multimodal) MT and (V)MLM objectives were trained on Multi30k and Conceptual Captions.\nText-only. We trained a text-only Seq2seq Transformer (Vaswani et al., 2017) from scratch and a text-only Seq2Seq Transformer initialised from TLM weights (Conneau and Lample, 2019) . We refer to these models as Vanilla MT and TLM + MT respectively. We also trained several MT models initialised from pretrained mBART (Liu et al., 2020 ) and which we fine-tuned on parallel data (Lison et al., 2018; Wo\u0142k and Marasek, 2014) . We refer to these models as mBART + MT. 'w/ adapters' specifies that the model's weights are frozen except bottleneck adapters (Houlsby et al., 2019) .\nMultimodal. We trained several state-of-the-art multimodal MT models: Graph-MMT (Yin et al., 2020) , Gated Fusion (Wu et al., 2021) and a Seq2Seq Transformer trained from VTLM weights (Caglayan et al., 2021) (hereafter VTLM + MMT).\n\nResults and Analysis\nQuatre cyclistes font une course sur un parcours avec une foule en arri\u00e8re-plan.\nQuatre motards font une course sur un parcours avec une foule en arri\u00e8re-plan. Tables 3 and 4 show BLEU, COMET and accuracy scores for all models compared on several En\u2192{Fr,De,Cs} test sets including CoMMuTE. An initial observation is that the text-only model is a strong baseline on the three standard benchmarks (Test2016, Test2017 and MSCOCO). As mentioned in Section 5.2, most of these evaluation datasets do not need visual context to be correctly translated. Our model VGAMT is on average on par with its counterpart text-only mBART+MT w/ adapters baseline for all Multi30k En\u2192Fr test sets, while being on average just below this baseline on En\u2192{De,Cs} Multi30k benchmarks. It outperforms other MMT models with a large margin due to both the effective use of textual knowledge from the frozen MT model but also guided self-attention. Note that the scores reported for the baselines are lower than the ones reported in the original papers of the models for several reasons. First, we computed the scores on fully detokenised data to have a uniform evaluation between all models. We also report the average score from three different runs using different seeds and not the best score obtained over a single run. More importantly, our VGAMT obtains strong improvements over both text-only baselines and state-of-the-art MMT systems on CoMMuTE; our model can use visual context to disambiguate sentences. This can be seen in Figure 5 (one of the \n\nAblation Study\nTo better understand the role of VGAMT's components, we carry out several ablations for En\u2192Fr and report all results in Table 5 .\nAdapters versus Fine-tuning. We compare the results of fine-tuning an unfrozen VGAMT model (w/o adapters) in comparison to our frozen model with adapters (VGAMT), all other things remaining equal. The unfrozen version faces a drop in scores on all test sets except Test2017. Notably, the unfrozen model's accuracy score of 60.5 on CoM-MuTE is 6.6 points lower than our final VGAMT model. As well as providing a more lightweight solution that does not involve fine-tuning all parameters, using neural adapters and freezing other weights is useful in terms of performance.\nImpact of the VMLM objective. VMLM sampling probability and degree of masking. We ran experiments to vary the VMLM sampling probability (see Section 3.1) and the percentage of masked text inputs (see Figure 7 for results on CoMMuTE). For the sampling between VMLM and MMT objectives, the maximum value is reached for p =50%, i.e. equal sampling between VMLM and MMT objectives (Figure 7a ). Similar results are obtained for p = 75%, i.e. 3 VMLM batches for 1 MMT batch, but the translation quality is lower. For the percentage of masking, there is a peak at 25% masked text inputs and a constant decrease for higher values (Figure 7b ).\n\nConclusion\nWe propose a new MMT approach (VGAMT) based on (i) adapting a strong text-only MT model with lightweight adapters and (ii) introducing better use of the text and image modalities through a novel guided self-attention mechanism and joint MMT and VMLM training. We also introduce \n"}
{"question": "In the paper, what is the purpose of the Causality Matrix (C) in the prediction process of stock price movement?", "evidence": "  Specifically, C[i, j] represents the transfer entropy from stock i to stock j, and C[i, j] > C [j, i] indicates that stock i provides more predictive information about the movement of stock j than j to i. This Causality Matrix will next serve as a guide for the memory networks, enabling the identification of causal dependencies between multivariate stocks The compared general PLMs include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) . We also compare our model with available PLMs for social science texts-SsciBERT (Shen et al., 2022) , and for the political domain: POLI-TICS (Liu et al., 2022) and PoliBERTweet (Kawintiranon and Singh, 2022).  ", "options": ["A. To calculate the average price changes between different stocks.", "B. To identify causal dependencies between multivariate stocks.", "C. To illustrate the symmetric flow of information between stocks.", "D. To determine the final prediction for the target stock.", "Based on this principle, for each monitoring window, we calculate the transfer entropy between all stocks using their historical closing prices and generate a transfer entropy matrix, referred to as the Causality Matrix C \u2208 R n\u00d7n , which illustrates the asymmetric flow of information from one stock to another."], "answer": "B", "content": "\nIntroduction\nFinancial services, known for their competitiveness, have always been at the forefront of adopting data science techniques to drive investment decisions. Quantitative trading, a specific field within it, has drawn immense interest from both academia and industry over the last few decades. With the rapid advancements in deep learning recently, computer scientists and quantitative researchers have joined forces to apply AI techniques to tackle the challenges within this domain.\nAmong various tasks, one of the most prominent is stock price movement prediction (Bhardwaj, 2021) . The reason for its popularity is selfevident: once a model is able to predict future movement with considerable accuracy, numerous trading strategies can be easily built around it.\nRecent studies have shown that deep neural networks are ideal candidates for such prediction models (Yoo et al., 2021; Gunduz, 2021) . Supporters of the efficient-market hypothesis (EMH), which posits that asset prices reflect all available information, tackle the task with price information alone (Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . However, an alternative perspective suggests that additional insights can be gained from analyzing news articles and social media posts, which may hold valuable clues about the future (Hu et al., 2018; Xu and Cohen, 2018; Wang et al., 2019b; Tang et al., 2020) .\nAnother intriguing approach analyzes the relationships between different stocks. Clearly positive and negative correlations, or even non-correlations can be immensely useful in constructing a diversified stock portfolio (Borodin et al., 2003) . Several studies even empirically demonstrate that exploiting correlations can improve the accuracy of stock price movement prediction (Long et al., 2020; Yoo et al., 2021) . However, their correlations are often realized by acquiring industry sector and calculating correlation matrices or attention scores, which are bidirectional and symmetrical, leading to excessive attention on spurious correlations. Due to the lag problem widely existed between two time series, we are more concerned about the dominance of information flow between stocks, specifically, the direction of causality.\nAdditionally, we have observed that the situation can significantly change when incorporating text information. Let's consider two highly correlated companies (A and B) and there is promising news specifically about company A. In such a scenario, it's fairly easy to infer that the current news might still have a substantial impact on company B, despite there being no direct connection between the two companies on paper. However, it's impossible to reach this conclusion by just examining the news about company A or the correlation between A and B alone, which highlights the limitations of relying solely on individual pieces of textual information or traditional correlations between stocks.\nInspired by observations above, we propose the Causality-guided Multi-memory Interaction Network (CMIN), a novel end-to-end deep neural network which captures both financial news as well as the causality-enhanced correlations between stocks for better stock price movement prediction.\nTo achieve this goal, CMIN incorporates two key components: the Text Memory Network and the Stock Correlation Memory Network. Both networks utilize a recurrent neural network with nonlinear combination of memory attentions to generate a global memory abstraction. And we introduce a global causality matrix according to the transfer entropy between stock price time series to guide the abstraction process, forming a Causal Attention mechanism to capture the asymmetric correlations. By considering causality, CMIN goes beyond traditional symmetric correlations and captures the true inter-dependencies between stocks. Furthermore, we employ an attention-based fusion mechanism between the two networks, introducing multi-directional interactions through which CMIN learns not only the self-influence within each network but also the interactive influence between them. It captures the interrelationship between textual information and correlations, enhancing the overall predictive power of CMIN.\nWe further demonstrate the effectiveness of CMIN with experiments conducted on 3 real-world datasets collected from both the U.S. and Chinese markets, where CMIN achieves state-of-the-art prediction accuracy, surpassing existing models in terms of performance.\nTo summarize, our main contributions are:\n\u2022 Proposal of a causality-guided multi-memory interaction network for stock movement prediction which is to our best knowledge the first attempt to simultaneously consider causalityenhanced correlations and textual information to achieve higher prediction accuracy;\n\u2022 Introduction of the attention-based multidirectional interactions, so that CMIN captures not only the self-influence of temporal movements and textual information but also the interactions between these two types of information flows;\n\u2022 Collection and release of two new datasets: one for the U.S. market and another for the Chinese market.\nBoth datasets include comprehensive financial texts and stock price time series data, which are publicly available at https://github.com/ BigRoddy/CMIN-Dataset, facilitating further research and benchmarking in the field.\n\nStock Movement Prediction\nIn traditional trading practices, two main frameworks are commonly used to make predictions on future stock prices (Ferreira et al., 2021) . The first is fundamental analysis, which aims to assess the intrinsic value of a stock by considering various factors related to it as a whole, such as financial statements, industry trends and economic conditions. The other is technical analysis, which operates under the assumption that the market is efficient (i.e., the Efficient Market Hypothesis holds true) and focuses on analyzing only historical and current price patterns in order to predict future movements.\nAlthough both frameworks have been widely adopted by top hedge funds and investment firms, technical analysis has gained more popularity among AI practitioners, many of whom focus on employing long short-term memory networks and other innovative architectures to model stock price history alongside technical analysis indicators (Nelson et al., 2017; Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . This is primarily because processing a single stream of price data is relatively simpler than analyzing and synthesizing a range of diverse data sources with varying frequencies and characteristics.\n\nPredicting with the Help of Text Data\nThe recent advancement of natural language processing (NLP) techniques has opened up new possibilities for analyzing large volumes of text data in the context of stock movement prediction. Many researchers have recognized the potential value of incorporating news articles, analysis, commentaries and even social media posts (Xu and Cohen, 2018) , which are believed to provide valuable insights about the future. Some studies focus solely on textual information. For example, (Hu et al., 2018) leverages attention mechanism at multiple levels within a deep structure to identify the most important news articles and predict price trends. Others adopt a two-step approach. First, they extract features (e.g. investor sentiment) from financial texts. Then they fuse these features with price information to make predictions such as (Li et al., 2017) and (Jin et al., 2020) . This integration of text analysis with quantitative techniques holds promise for enhancing the accuracy and effectiveness of stock movement prediction models.\n\nExploiting the Relations between Stocks\nAnother important trading framework takes advantage of the correlations between different stocks. Portfolio selection, particularly pairs trading, is a well-known and successful trading strategy that exploits the correlated nature of stocks, whether positive or negative. In fact, as early as (Borodin et al., 2003) pointed out that stock correlations based portfolio selection could beat any strategy that relied on predicting trends or specific targets.\nThe incorporation of correlations in stock movement prediction has gained attention in recent years, drawing inspiration from several existing works. For example, (Yoo et al., 2021) utilizes transformer to learn dynamic correlations between stocks in an end-to-end manner. (Long et al., 2020) employs knowledge graphs and graph embedding techniques to model the relationships between stocks. These studies have achieved admirable results, potentially due to effective feature engineering however, because the direct benefit of stock correlations in predicting future prices lacks fundamental logic.\nIn this paper, we propose constructing a single model to handle both textual data and stock correlations simultaneously, aiming to shed light on the success of correlation-based approaches with the help of financial texts. We also introduce a novel causal attention mechanism to interpret the underlying logic behind stock correlations, leveraging transfer entropy to provide insights. We further model the multi-directional interactions between texts and correlations so that we could uncover not only relevant texts for prediction through correlations, but also the hidden stock correlations through texts. By integrating text data and stock correla-tions within a unified model, we aim to provide a comprehensive understanding of the relationship between the two and discover valuable insights for stock movement prediction.\n\nProblem Formulation\nThis paper is dedicated to predict the price movement of a target stock. To this end, we leverage both the correlations between stocks and textual information to make prediction.\nConsider a target stock with numerical features denoted as P target \u2208 R k\u00d7d , where k represents the number of time steps in the monitoring window and d represents the dimension of price features, such as the highest and the closing prices. The prices of n other relevant stocks are denoted as:\nP = {P 1 , P 2 , \u2022 \u2022 \u2022 , P n } \u2208 R n\u00d7k\u00d7d .\nBesides, we have financial documents associated with the target stock, which are represented as\nM = {M 1 , M 2 , \u2022 \u2022 \u2022 , M k } \u2208 R k\u00d7l\u00d7w ,\nwhere l denotes the number of documents in a time step and w is the maximum number of words in a document. In cases where a specific stock has fewer than l documents at a given time step, zero padding values are added to align the lengths. Similarly, if a document contains fewer than w words, zero padding is applied to ensure uniform length across all documents (Ang and Lim, 2022) .\nWe formulate the task as a binary classification problem whose goal is to predict the movement of the target stock at the next time step, denoted as \u0177target . Here, \u0177target = 1 indicates a rise in the price while \u0177target = 0 indicates a fall. (1) The feature embedding module includes two encoders, one for embedding the textual information and another for embedding the price time series. Additionally, a global causality matrix is introduced to capture the asymmetric correlations using transfer entropy, which then guides the calculation of attention weights in the multi-memory networks.\n(2) The multi-memory networks consist of the Text Memory Network and Stock Correlation Memory Network, which are designed to select and re- tain the most relevant and influential information (textual and correlational) for the target stock.\n(3) The multi-directional interaction module facilitates the interaction between the textual and correlational information. This interaction allows the two types of information to reinforce each other and leverage the advantages of different information flows for better prediction performance, enhancing the predictive capabilities of the CMIN.\n\nFeature Embedding\nSelf-attention mechanisms have proven to be effective in capturing long-term dependencies and modeling complex sequential patterns, particularly in the Transformer architecture (Vaswani et al., 2017) . Given the significance of historical information in financial documents and stock prices for stock price movement prediction, we employ attention mechanisms to summarize this information.\n\nText Encoder\nThe Text Encoder focuses on processing the financial documents M to extract meaningful representations for stock movement prediction. We firstly use a popular word representation tool Glove (Li et al., 2018) to generate the word embedding tensor M word \u2208 R k\u00d7l\u00d7w\u00d7dw , where d w is the size of word embeddings. Each word in the financial documents is represented as a d w -dimensional vector.\nThen the word embeddings are passed through a text embedding layer. Here we adopt the bidirectional Gated Recurrent Unit (Bi-GRU) (Li et al., 2022) to capture both preceding and succeeding contexts within each document. The average of the last hidden vectors is taken as the text embeddings M text \u2208 R k\u00d7l\u00d7dm , or equivalently M text \u2208 R s\u00d7dm , where s is the total number of documents in the monitoring window.\nAfter that, the text attention mechanism is applied to summarize all historical documents across time steps. The text embedding of the last time step M text,\u22121 \u2208 R l\u00d7dm , serves as the query matrix, while the entire text embeddings M text \u2208 R s\u00d7dm acts as both the key and value matrices. Soft scaled dot-product attention is used to compute the attention weights, which are then applied to the text embedding to obtain a representation E text \u2208 R l\u00d7dm enhanced by the history state attention:\nE text = softmax( M text,\u22121 M T text \u221a d m )M text . (1)\nThe resulting E text is the textual embedding that contains highly concentrated information from the stock's related texts. This embedding serves as a summary of the historical text data and is used for further processing in the multi-memory networks and multi-directional interaction module of CMIN.\n\nPrice Encoder\nThe Price Encoder is introduced to utilize multivariate features from historical prices and capture their temporal interrelationships. Firstly we employ a feature mapping layer to project them into a latent space of dimension d p , aiming to improve the learning capacity (Yoo et al., 2021) . For target stock price P target \u2208 R k\u00d7d , the historical price embeddings Ptarget \u2208 R k\u00d7dp can be formulated as:\nEQUATION\nwhere\nW t \u2208 R d\u00d7dp , b t \u2208 R dp are parameters.\nMoreover, recognizing that historical patterns can repeat themselves sometimes, we incorporate a multi-head price attention layer to capture each stock's distinctive changing patterns. The price embedding of the target stock at the last time step is donated as P\u22121 target \u2208 R dp . Then we employ the multi-head attention mechanism with the query P\u22121 target and the key/value Ptarget as follows:\nv target = MultiheadAtt( Ptarget , P\u22121 target ) (3)\nv target is a key vector that serves as the initial hidden state for the two memory networks, playing a crucial role in the final prediction. Similarly, we process the remaining stocks and obtain the correlational embedding E corr \u2208 R n\u00d7dp . Notably, the shared parameters across all stocks ensure the stability and generality of the extracted features (Wang et al., 2019a) .\n\nCausality Matrix\nWhen it comes to detecting causal relationships and conducting predictive analysis, transfer entropy, a non-linear generalization of Granger causality (Seth, 2007) , serves as a conceptually neat and mathematically rigorous method. It has been considered as an important tool for causality analysis and successfully applied in diverse domains including financial markets (Sandoval Junior et al., 2015) .\nTransfer entropy is derived from Shannon Entropy: H = \u2212 N i=1 p i log p i . In this context, considering the time series of a stock, we can partition the possible values into different bins and calculate the probabilities at each time step. Transfer entropy from series X to another series Y can be defined as the average amount of information contained in the source X but not contained in Y's past:\nEQUATION\nBased on this principle, for each monitoring window, we calculate the transfer entropy between all stocks using their historical closing prices and generate a transfer entropy matrix, referred to as the Causality Matrix C \u2208 R n\u00d7n , which illustrates the asymmetric flow of information from one stock to another. Specifically, C[i, j] represents the transfer entropy from stock i to stock j, and C[i, j] > C [j, i] indicates that stock i provides more predictive information about the movement of stock j than j to i. This Causality Matrix will next serve as a guide for the memory networks, enabling the identification of causal dependencies between multivariate stocks.\n\nMulti-memory Networks\nWe introduce a Text Memory Network and a Stock Correlation Memory Network (Sukhbaatar et al., 2015) to manage the textual and correlational information separately. They each maintain a continuous representation and update it iteratively using multiple computational steps (hops), ultimately producing a global memory abstraction.\nAs shown in Figure 1 , each layer of the memory network comprises an attention unit and a GRU unit, which receive textual or correlational embeddings as inputs and are supervised by the continuous representation generated in the previous layer. To initialize the continuous representations of each network, we use the target stock vector v target (generated from Eq.3):\nv (0) text = v (0) corr = v target .\n(5)\n\nText Memory Network\nIn each layer h \u2208 [1, H] of the Text Memory Network, we input the textual embeddings E text (Eq.1) and the continuous representation from the previous layer v\n(h\u22121)\ntext . We utilize an attention unit (Eq.3) to identify important information within the textual embeddings. Subsequently, a non-linear GRU cell unit (Xu et al., 2019) acts as an information aggregator, determining the amount of text information to retain:\nv Att(h) text = MultiheadAtt(E text , v (h\u22121) text ), (6)\nwhere v (h\u22121) text is the query matrix and E text represents the raw form of the key and value matrices.\nThen the GRU cell unit updates the current hidden state into the next hidden state and outputs it to the next layer as the new continuous representation:\nv (h) text = GRU (v Att(h) text , v (h\u22121) text ).\n(7)\n\nStock Correlation Memory Network\nThe Stock Correlation Memory Network is employed to dynamically identify stock relationships and update the continuous representation of stock correlations in an intuitive and asymmetric manner. However, the use of unsupervised attention weights in previous models can be problematic as they may be inevitably misled by the dataset bias, resulting in excessive attention on spurious stock correlations. To address this, we introduce extra knowledge in the form of Transfer Entropybased causality to guide the attention weights and mitigate potential confounding effects.\nFor each target stock, we extract a causal vector v causal = C[:, target] from the pre-calculated causality matrix, which quantifies the degree of information flow from other stocks to it. Then we modify the traditional attention mechanism into Causal Attention by incorporating causal guidance:\nS = softmax( QK T \u221a d ), S = f (S, v causal ). (8)\nHere, f is a function that aggregates the attention weight S and the causal vector v causal to produce a causality-guided attention weight S. We use the average aggregation method for simplicity (i.e., f (S, v causal ) = (S + v causal )/2). To better balance them, one can introduce a hyperparameter \u03bb \u2208 [0, 1]. Then f() updates to f (S, v causal ) = \u03bbS + (1 \u2212 \u03bb)v causal . We believe that different degrees of causal attention can impact the model's performance, and leave it for future exploration. The continuous representation is gradually updated through the Causal Attention, indicating the influence of causal relationships on movement prediction and the self-influence on the flow of correlation information:\nEQUATION\nIt is important to note that although we design multiple layers within each memory network to learn deep representations, different layers of the same memory network share the same unit. This enables the network to focus on crucial information that affects the movement of the target stock, thereby enhancing the continuous representation.\n\nMulti-directional Interactions\nIn reality, textual information and correlations have an impact on each other when it comes to stock price movement prediction. For instance, news about a technological breakthrough in the new energy sector may uplift the prices of most stocks in that industry, thereby affecting the correlations among those stocks.\nTo simulate this phenomenon and enhance the synergy between textual and correlational information, we introduce a multi-directional interaction module. This module allows textual and correlational information to reinforce each other and amplify the advantages of different information flows for better prediction performance.\nTake the Text Memory Network as an example, in each layer we firstly calculate the self-influence by using v (h\u22121) text as the query:\nv Att(h) text\u2212>text = MultiheadAtt(E text , v (h\u22121) text ) (11)\nNext we consider the interactive influences from correlations to texts using v (h\u22121) corr as the query:\nv Att(h) corr\u2212>text = MultiheadAtt(E text , v (h\u22121) corr ) (12)\nFinally, we produce a new attentional continuous representation by averaging these two influences:\nEQUATION\nwhich means that we replace Eqs. 6 with Eqs. 11-13 to obtain the new attention-aggregated vector.\nThe workings of Stock Correlation Memory Network are quite similar.\nConsequently, the fusion of different information flows is promoted due to the multi-directional interaction mechanism in which CMIN learns not only the influences from text/correlation to movement prediction within each information flow but also the interactive influences between different information flows, representing the interrelationship between text and correlations.\n\nLearning Objective\nWith the continuous representations v (H) text and v (H) corr from the last layer of each memory network, along with the target stock representation v target , we concatenate them and apply a softmax function to generate the final prediction vector \u0177:\n\u0177 = softmax(W y [v (H) text , v target , v (H) corr ] + b y ). (14)\nThe objective is to minimize the cross entropy loss:\nL(y, \u0177) = \u2212 n i=1 (yi log (\u0177i) + (1\u2212yi) log (1\u2212 \u0177i)) (15)\nwhere n is the size of the training set.\n\nExperiments\nIn this section, we empirically evaluate our CMIN model with three real-world datasets collected from the U.S. and Chinese stock markets.\n\nDatasets\nIn our experiments we have used three datasets, namely ACL18, CMIN-US and CMIN-CN, spanning different time periods to evaluate our proposed model CMIN against other baselines.\nACL18 (Xu and Cohen, 2018 ) is a classic dataset with tweets from Twitter as financial texts in the task of text-enhanced stock movement prediction. As there are few existing high-quality datasets containing both texts and price, we are also making available two new benchmark datasets along with this paper from 2018-01-01 to 2021-12-31 in the U.S. and Chinese market named CMIN-US and CMIN-CN. These two datasets are available at https://github.com/BigRoddy/ CMIN-Dataset to facilitate further research and enable reproducibility. More details and statistics of those three datasets are in Appendix A.\n\nBaselines\nWe compare CMIN against the following four baselines, all of which are high-performing stock movement prediction models proposed by recent studies:\n\u2022ALSTM (Qin et al., 2017 ) is a dual-stage attention-based recurrent neural network, which selects relevant time series across all time steps.\n\u2022Adv-LSTM (Feng et al., 2019) uses adversarial training to improve the generalization of ALSTM.\n\u2022Stocknet (Xu and Cohen, 2018) introduces recurrent continuous latent variables and uses variational inference to address the posterior inference.\n\u2022DTML (Yoo et al., 2021) is a newly published attention-based model that exploits the correlations between stocks to improve the prediction accuracy.\n\nEvaluation metrics\nAs we have formulated stock price movement prediction as a classification problem, we choose two classic metrics: Accuracy (Acc.) and Matthews Correlation Coefficient (MCC), similar to the previous work (Xu and Cohen, 2018; Yoo et al., 2021) . \nEQUATION\n\nImplementation details\nWe set our model for daily price prediction, with a history market window size k = 5 and the number of price features d p = d = 3, namely the highest, the lowest and the closing prices. We limit the maximum number of financial texts in one single day to be l = 20 , and the maximum length of a text document w = 30. Within the Text Encoder, we set the size of word embedding vector d w = 50 and the hidden state of Bi-GRU network d m = 50.\nWe implement the CMIN with Pytorch on a NVIDIA Tesla V100 and train it with an Adam optimizer (Kingma and Ba, 2015) . All parameters of our model are initialized with Xavier Initialization (Glorot and Bengio, 2010) . We search the hyperparameters of CMIN as follows: number of layers of each memory network H in {1, 2, 3, 4, 5}, dropout rate in {0.1, 0.2, 0.3}, number of epochs in {10, 20, 50}, and size of the price hidden state d p in {3, 10, 50}. For baselines, we use their default parameters and fine-tune them to fit our data.\n\nPerformance Analysis\nThe results are summarized in Table 1 .\nAmong all models, ALSTM and Adv-LSTM performed poorly with little improvement over random prediction. This could be attributed to the fact that these models rely solely on stock prices as the basis for decision-making. The Stocknet and DTML incorporate additional information beyond stock prices, demonstrated significant improvements over ALSTM and Adv-LSTM, which highlights the importance of utilizing financial texts and stock correlations for this challenging task. CMIN outperformed all baselines and achieved state-of-the-art performance on both two metrics across all datasets, showing its excellent capabilities to leverage both financial texts and stock correlations, as well as capture their interrelationship. \n\nAblation Studies\nTo evaluate the contribution of CMIN's different components, we compare against several variants:\n\u2022CMIN-TE: CMIN without the Text (TE), which makes decisions just based on stock prices.\n\u2022CMIN-PR: CMIN without the Price (PR), which makes decisions just based on related texts.\n\u2022CMIN-CM: CMIN without the guide of causality matrix (CM).\n\u2022CMIN-MI: CMIN without multi-directional interactions (MI) between memory networks.\nThe results are summarized in Table 2 . CMIN-TE only achieves a level of prediction accuracy on par with ALSTM and Adv-LSTM, and is worst among all the variants, again indicating the importance of text data. Similar to the performance of Stocknet, CMIN-PR has a relatively high Acc. but a low MCC, suggesting texts are particularly helpful to predict on one side of the binary classification. By modeling both text data and stock relationships, CMIN-CM reaches a good result. Finally, better performance achieved when causality matrix and multi-directional interactions are introduced into the network. Overall, the ablation studies show that every component makes an important contribution to CMIN, and as a result the full model with all components achieves the best performance.\n\nAnalysis of Memory Network Depth\nAs introduced before, we propose two memory networks to retain vital features of texts and correlations with multiple computational layers. And we want to understand what would be the ideal number of depths to achieve the best prediction results.\nWe change the number of layers H of each memory network to find out how the performance fluctuates with it. The results are summarized in Figure 2 . When we only have one memory layer, there is no multi-directional information flows between the two memory networks and as a result they only try to identify the vital information in the embeddings related to or having an impact on the movement of the target stock under the supervision of v target . As the number of memory layers increases, the interactions between two memory networks also intensifies. It is intuitive that the performance of CMIN reaches its peak when it has three memory layers. With further increase the number of memory layers, CMIN is prone to overfit.\n\nCase Study\nHere we present an example to illustrate how CMIN considers both financial texts and stock correlations to avoid random noises in time series.\nWe visualized the causality matrix of ACL18 using a heat map as shown in Figure 3 . Stocks are sequenced by their industry sector. The black box on the left shows weak causality, representing weak information flow from Utilities to Materials. On the other hand, the yellow box on the right indicates the relative strong information flow from Materials to Finance and within the Finance industry.\nThe target stock is Bank Of America (BAC) with a monitor window spanning from 13/11/2015 to 19/11/2015. We employ CMIN to predict BAC's next movement direction on the day of 20/11/2015 and then output the attention scores of texts and causality-guided correlation. The most focused stock by CMIN is Berkshire Hathaway Inc. (BRK-A). It's interesting to note that both are in the same industry sector: Finance, and they do appear to follow a very similar movement pattern in the trading days leading to 20/11/2015, which demonstrates the ability of CMIN to find the dynamic stock correlations with the guidance of Causality Matrix.\nThe financial text of BAC that obtains the highest attention score is \"Beer, Credit Card Debt And Other Positives For Bank Of America\", the title of an news article 1 which reports the rapidlyimproving banking landscape in the U.S.. This text is clearly highly relevant to BAC's subsequent stock performance, which demonstrates that CMIN is able to identify highly relevant texts having a impact on the target stock movement.\nFurthermore, it also illustrates the underlying interrelationship between financial texts and stock correlations. Except expressing an optimistic sentiment towards BAC, the news also shows a rapidly improving state of affairs for the wider financial industry. Therefore, through the Multi-directional Interactions mechanism, the text strengthens the model's attention stocks in the same sector. These two aspects mutually reinforce and complement each other to help the model make the best judgment that BAC's stock price will rise on the next day.\n\nConclusions\nIn this paper, we proposed CMIN, a causalityguided multi-memory interaction network that simultaneously models financial documents, causality-enhanced stock correlations and the interactions between the two, and recurrently learns a global memory representation for movement prediction. This multi-modality network was designed to enable the concurrent discovery of texts and stock correlations relevant to future price change and we demonstrated, through experiments on three datasets across two distinct markets, that each component of the proposed architecture made significant contributions to the model, leading CMIN to achieve state-of-the-art accuracy.\n"}
{"question": "Which of the following is not a modality ?", "evidence": "  Multimodal deep learning involves interpreting and analyzing multimodal signals together, where each modality refers to a way in which something is experienced and felt, e.g., the visual, audio, or language modality. With the widespread popularity of online social media, such as Instagram, Tik-Tok, Facebook, etc., videos containing multiple modalities have become a major information carrier, which brings new challenges to content recommendation and classification, e.g., video question answering (Lei et al., 2021; Li et al., 2020) , video captioning (Ging et al., 2020; Li et al., 2020) , and video retrieval (Akbari et al., 2021; Lei et al., 2021) .\n ", "options": ["A. Audio  modality", "B.  language modality", "C. video captioning", "D. Thought"], "answer": "D", "content": "\nIntroduction\nMultimodal deep learning involves interpreting and analyzing multimodal signals together, where each modality refers to a way in which something is experienced and felt, e.g., the visual, audio, or language modality. With the widespread popularity of online social media, such as Instagram, Tik-Tok, Facebook, etc., videos containing multiple modalities have become a major information carrier, which brings new challenges to content recommendation and classification, e.g., video question answering (Lei et al., 2021; Li et al., 2020) , video captioning (Ging et al., 2020; Li et al., 2020) , and video retrieval (Akbari et al., 2021; Lei et al., 2021) .\nWhile traditional sentiment analysis is mainly based on language, multimodal sentiment analysis (MSA) predicts the human emotion by utilizing extra information available in visual and audio modalities of the content to assist with language-based prediction. Here, the text modality contains the semantic meaning of the spoken language. The visual modality extracts the facial characteristics (e.g., head orientation, facial expressions, and pose) of the speaker. The audio modality reflects the emphasis on the utterance (e.g., through pitch, bandwidth and intensity). MSA has recently gained much attention in research for several reasons. On one hand, because of the abundance of social media content, commercial interests are switching from gauging user opinions/emotions from text only to more thorough multimodal analysis based on videos. On the other hand, short video platforms (e.g., TikTok, Instagram) allow users to easily create multimodal content including visual information, audio, and inserted text, while these modalities are sometimes noisy or even contradicting each other in sentiments. Therefore, the presence of multimodal information in addition to the text or language itself is necessary to make a thorough conclusion about the overall sentiment of a video.\nMultimodal fusion has become essential to gaining a deeper understanding of these video scenes (Baltru\u0161aitis et al., 2018) and has proven to be helpful in many downstream tasks. Various multimodal fusion techniques have been proposed for MSA, among which a basic solution is concatenating the extracted feature of each modality before performing downstream regression or classification. Recent work has recognized the importance of identifying modality-invariant information across modalities and fuse them to strengthen sentiment prediction (Hazarika et al., 2020; Zadeh et al., 2018a; Rahman et al., 2020; Sun et al., 2020) .\nAlthough modality-invariant information helps reinforce the understanding of the content, there are also cases where sentiments of different modalities contradict each other. For example, when one thanks someone with phrases like \"Finally I can rest easy tonight\" or \"I can't thank you enough\", it is very hard to conclude whether the sentiment is positive or negative without looking at the nonverbal cues, such as tones, facial expressions, and gestures. In fact, many sarcastic opinions are expressed by non-linguistic markers. In these cases, the overall sentiment cannot simply be judged by a majority vote among all modalities. Thus, multimodal representation learning that respects both consistency and incongruity between modalities have recently shown great promise (Yu et al., 2020; Hazarika et al., 2020) .\nIn this paper, we propose ConFEDE, a Contrastive FEature DEcomposition framework, which integrates both modality decomposition within each sample and supervised contrastive learning across samples in a single unified contrastive learning framework. Our main contributions are summarized as follows: (1) We integrate inter-sample contrastive learning and intra-sample modality decomposition into a simple unified loss function, based on a customized data sampler that allows us to sample positive/negative data pairs to perform both learning tasks. (2) We propose to decompose each modality into a similarity feature and a dissimilarity feature, and use the similarity feature of the text as an anchor to build the contrastive relation among all decomposed features. This is due to the observation that sentiment analysis is still largely centered around text and spoken language, while other modalities can provide extra information to assist with prediction. (3) Based on multimodal representation learning proposed above, we further introduce a multi-task prediction loss that depends on each decomposed modality representation and enables the model to learn from both multimodal prediction and unimodal prediction.\nWe mainly evaluated ConFEDE on CH-SIMS (Yu et al., 2020) benchmark, which contains both unimodal and overall sentiment labels for each sample. The result shows that the proposed method significantly outperforms a wide range of stateof-the-art multimodal sentiment analysis methods. To test the capability when no unimodal labels are provided, we further conduct experiments on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , which contain only an overall sentiment label for each sample, which shows that our proposed method can also achieve better per-formance than state-of-the-art methods on a number of performance metrics without unimodal labels. We provide extensive ablation studies to show the effectiveness and necessity of each design component in ConFEDE. The code is released at https://github.com/XpastaX/ConFEDE/.\n\nRelated Work\nIn this section, we discuss the related work in MSA and contrastive representation learning.\n\nMultimodal Sentiment Analysis\nPrior works on multimodal sentiment analysis mostly focus on predicting sentiments based on text and vision (Zhu et al., 2022; Ji et al., 2019; Liu et al., 2019) .However, there is growing interest in analyzing sentiment using all three modalities: text, audio, and vision (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020) . Zadeh et al. (2016) were among the first to propose a multimodal dictionary that could learn the dynamic interactions between facial gestures and spoken words to model sentiments. They later introduced a Tensor Fusion Network (TFN) to learn the intra-modality and inter-modality dynamics of three modalities in an end-to-end way (Zadeh et al., 2017) . Furthermore, they presented a Memory Fusion Network (MFN) which is composed of Long Short Term Memories (LSTMs) to learn the view-specific and cross-view interactions of three views (text, video, and audio) to improve sentiment analysis performance. Rahman et al. (2020) proposed a Multimodal Adaptation Gate (MAG) to fine-tune BERT (Devlin et al., 2019) on multimodal data to improve sentiment analysis performance. However, these prior works do not consider modality-specific information.\nTo better study the impacts that modalityspecific information can bring to MSA, Yu et al. (2020) construct a new multimodal sentiment analysis dataset CH-SIMS, which contains a unimodal label for each modality of a sample. Experiments show a great improvement in overall sentiment prediction after simply integrating unimodal predictions as subtasks in the learning objective. Hazarika et al. (2020) further decompose each modality into a modality-invariant and a modalityspecific representation, and employ squared Frobenius norm loss as the regularizer. However, they treat all modalities equally while regularizing the prediction result, which ignores the different effectiveness of modalities. In real cases, the text is usually more effective on MSA tasks compared to vision and audio. In other words, it is less \"noisy\" than the other two modalities. Also, they employ Central Moment Discrepancy loss to push the modality-invariant representations close and a Frobenius norm to push modality-specific representations to be orthogonal, while in our method, we integrate the above mechanism into a single loss function. Moreover, they regularize the decomposed features by reconstructing the original features with the generated features. We, instead, avoid using such a method and regularize the decomposed features with unimodal prediction tasks. To improve the decomposition performance, we further aggregate the supervised contrastive learning between samples into our frameworks by a custom-designed sampling method.\nA concurrent work HyCon (Mai et al., 2021) introduces a contrastive learning method for MSA, taking both inter-sample and intra-sample contrasts into consideration. However, they ignore the regularization for each decomposed feature. In contrast, in ConFEDE, within-sample feature contrasts are constructed based on a specific pattern centered around text similarity features. Also, when performing inter-sample contrastive learning, Hy-Con samples positive and negative pairs randomly based on MSA labels. In contrast, we design a data sampler that considers both the labels and similarities between modalities to retrieve positive/negative pairs. Due to these reasons, our method beats Hy-Con on most metrics on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , and is able to utilize unimodal labels to further boost performance, e.g., on CH-SIMS (Yu et al., 2020) .\n\nContrastive Representation Learning\nContrastive learning has achieved great success in representation learning by contrasting positive pairs against negative pairs (Akbari et al., 2021; Hassani and Khasahmadi, 2020; Chen et al., 2020) . Akbari et al. (2021) train a Video-Audio-Text Transformer (VATT) using multimodal contrastive learning for the alignment of video-text and videoaudio pairs, and thus achieve state-of-the-art on various computer vision tasks (e.g., audio classification and video action recognition). Hassani and Khasahmadi (2020) propose to learn node and graph level representations by contrasting encodings obtained from different structural views of graphs and achieve the state-of-the-art on various graph classification benchmarks. Chen et al. (2020) present a self-supervised framework, SimCLR, to learn visual representations through a contrastive loss between augmented views of the same image sample. Khosla et al. (2020) extend self-supervised contrastive learning to the supervised setting, i.e., contrasting samples from different classes. They also claim that the supervised setting is more stable for hyperparameters. We design a novel contrastive learning framework that utilizes the contrasts of modalities both within a sample and between samples to enhance multimodal representation in a unified contrastive loss guided by a specific pairing pattern. Furthermore, we propose a data sampler to retrieve similar samples as positive pairs, which is in contrast to the above prior work that obtains positive pairs by data augmentation.\n\nMethodology\nIn this section, we introduce the overall architecture of ConFEDE followed by a detailed description of the contrastive feature decomposition process for learning multimodal representations.\n\nModel Architecture\nThe overall architecture of ConFEDE is shown in Figure 1 . Given a sample, we first encode each modality with corresponding feature extractors. Specifically, we use the [CLS] tag of BERT to encode text (i.e., T), and two separate transformer encoders to encode vision and audio modalities (i.e., V and A), respectively. After that, we decompose each encoded modality into a similarity feature (i.e., T s /V s /A s in Figure 1 ) and a dissimilarity feature (i.e., T d / V d /A d in Figure 1 ) with different projectors. Each projector is composed of layer normalization, a linear layer with the Tanh activation, and a dropout layer. Finally, we update the six decomposed features and fuse them to train the ConFEDE model with the following multi-task learning objective function:\nL all = L pred + \u00b4uni L uni + \u00b4cl L cl ,\nwhere L pred is the multimodal prediction loss, L uni represents the unimodal prediction loss and L cl represents the contrastive loss. \u00b4cl and \u00b4uni are hyper-parameters that balance the contribution of each regularization component to the overall loss L all . We describe each loss term as follows. L pred -Multimodal Prediction Loss. We use a multilayer perceptron (MLP) with the ReLU activation function as the classifier to get the final predictive result (i.e., \u0177 in Figure 1 ). We concatenate all 6 decomposed modality features to obtain the input to the classifier,\n[T i s ; T i d ; V i s ; V i d ; A i s ; A i d ]\n, where [\u2022; \u2022] denotes the concatenation of two vectors. Denote the set of samples in a batch as B. For a given sample i \u2208 B, let its prediction from the classifier be \u0177i m , we calculate the multimodal prediction loss by mean squared error:\n\u0177i m = MLP([T i s ; V i s ; A i s ; T i d ; V i d ; A i d ]), L pred = 1 n n i=1 (y i m \u2212 \u0177i m ) 2 ,\nwhere n is the number of samples in a batch and y i m is the multimodal label. L uni -Unimodal Prediction Loss. For each sample i, we also feed the 6 decomposed features\n[T i s , V i s , A i s , T i d , V i d , A i d ]\ninto a weight-shared MLP classifier separately to get the 6 predictions denoted by the vector \u00fbi . Specifically, we compute the unimodal prediction loss by:\n\u00fbi = MLP([T i s , V i s , A i s , T i d , V i d , A i d ]), u i = [y i m , y i m , y i m , y i t , y i v , y i a ], L uni = 1 n \u2225u i \u2212 \u00fbi \u2225 2 2 ,\nwhere the vector\nu i = [y i m , y i m , y i m , y i t , y i v , y i a ]\nrepresents the ground-truth labels for unimodal prediction. In other words, each decomposed feature is regularized to perform prediction individually.\nNote that the similarity features T i s , V i s , A i s are mapped through the MLP to predict the multimodal label y i m , whereas the dissimilarity features T i d , V i d , A i d are mapped through the MLP to predict modality-specific labels y i t , y i v , y i a (if available). When modality-specific labels are not available, the dissimilarity features T i d , V i d , A i d will also be used to predict multimodal label y i m . The rationale behind this design is that we let the similarity features capture the consistent information shared across different modalities via the overall multimodal label for the sample, while the dissimilarity features can retain modality-specific information represented by unimodal labels.\nL cl -Contrastive Loss. We further regularize the learning through Contrastive Feature Decomposition in one simple joint contrastive loss that contrasts (1) similar samples against dissimilar samples; (2) similarity features against dissimilarity features within a sample. The contrastive loss is denoted as:\nL cl = 1 n n i=1 \u2113 i cl ,\nwhere \u2113 i cl is the contrastive loss of sample i, the detailed derivation of which will be given in the following subsection. \nT V A T V A T V A Ts k Vs k As k T V A T V A T V A\n\nContrastive Feature Decomposition\nWe unify intra-sample and inter-sample contrastive learning into one simple NT-Xent contrastive loss framework (Chen et al., 2020) to conduct both modality representation learning and modality decomposition simultaneously. The loss for sample i is given by\n\u2113 i cl = (a,p)\u2208P i \u2212 log exp(sim(a, p)/\u00c4 ) (a,k)\u2208N i \u222aP i exp(sim(a, k)/\u00c4 ) ,\nwhere (a, p) and (a, k) denote a pair of decomposed feature vectors either within a sample, e.g., (T i s , V i s ), (T i s , A i d ), or across different samples, e.g., (T i s , T j s ). The sets P i and N i are given by\nP i = P i intra \u222a P i inter , N i = N i intra \u222a N i inter .\nHere P i is the positive pair set that includes both intra-sample positive pairs P i intra and inter-sample positive pairs P i inter , while N i is the negative pair set that consists of both intra-sample negative pairs N i intra and inter-sample negative pairs N i inter . Note that (a, p) is a positive pair in P i , and (a, k) is a pair in P i or N i . Specifically, we use the six decomposed features (T s , V s , A s , T d , V d , A d ) to form intra-sample positive/negative pairs, as shown in Figure 2 (a) , with P i intra and N i intra given by\nP i intra ={(T i s , V i s ), (T i s , A i s )} \u222a {(T j s , V j s ), (T j s , A j s ) |j \u2208 Neighbor i \u222a Outlier i }, N i intra ={(T i s , T i d ), (T i s , V i d ), (T i s , A i d )} \u222a {(T j s , T j d ), (T j s , V j d ), (T j s , A j d ) |j \u2208 Neighbor i \u222a Outlier i },\nwhere Neighbor i and Outlier i represent the similar samples and dissimilar samples for the sample i, respectively, to enlarge the scope of the contrast, the detail of which is given in Algorithm 1 that will be explained subsequently.\nNote that instead of treating all modalities equally as in other contrastive learning schemes, here we choose the text similarity feature T i s as an anchor, such that the visual and audio similarity features V i s and A i s are pushed closer to T i s , while in the meantime, the dissimilarity features in all modalities are pushed away from T i s . This is due to the observation that multimodal sentiment analysis is still largely centered around text information. Although other modalities can provide additional information to assist with sentiment prediction, they may also introduce more noise than text. Therefore, unlike other work, we avoid using visual/audio similarity features as anchors, which may bring noise into contrastive learning and confuse model training.\nWe now describe the data sampler shown in Algorithm 1 that retrieves similar samples for a given sample based on both multimodal features and multimodal labels to perform supervised contrastive learning across samples. Specifically, the sampling procedure can be divided into two steps.\nFirst, given the dataset D that contains |D| samples, for each sample pair (i, j) in D, we calculate the cosine similarity score between them:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]),\nwhere sim(w, v) = w T v/||w|| \u2022 ||v|| denotes the cosine similarity between two vectors w and v. And T, V, and A (in Figure 1 ) are the output of BERT, vision and audio encoders, respectively.\nSecond, we retrieve candidate similar/dissimilar sample sets for each sample. For each sample i, we sort samples that have the same multimodal label y i m according to the similarity scores in ascending order as a candidate similar sample set S i 0 . In contrast, we sort samples that have labels other than y i m as a candidate dissimilar sample set S i 1 . Two similar samples with high cosine similarity scores from S i 0 are randomly selected to form inter-sample positive pairs with sample i, which is denoted as Neighbor i . Four dissimilar samples from S i 1 are selected to form inter-sample negative pairs. We denote them as Outlier i in which two samples Outlier i 1 have low cosine similarity scores and the other two samples Outlier i 2 have high cosine similarity scores.\nUsually, we tend to select the samples in Neighbor i and Outlier i 1 to form positive and negative pairs with sample i, respectively. However, samples in Outlier i 2 have different labels but similar semantic information to sample i, making them hard to distinguish from sample i. Therefore, we additionally add these samples to Outlier i to specifically handle this issue by contrastive learning.\nBased on the samples retrieved by Algorithm 1 and the pairing strategy shown in Figure 2 (b), the inter-sample positive/negative pairs for sample i are given by:\nP i inter ={(T i s , T j s ), (V i s , V j s ), (A i s , A j s ) |j \u2208 Neighbor i } , N i inter ={(T i s , T k s ), (V i s , V k s ), (A i s , A k s ) |k \u2208 Outlier i }.\nNotably, our data sampler enables contrastive learning across samples through decomposed modality features without data augmentation. This contrasts original contrastive learning in image classification, which obtains positive pairs by augmentation applied to images. Moreover, we only use similarity features to obtain inter-sample pairing since the similarity features of similar samples in the same class should be close while the similarity features of samples in different classes should be far apart.\n\nExperiments\nWe mainly evaluate ConFEDE on CH-SIMS (Yu et al., 2020) , since it has unimodal labels, which can best meet the design of ConFEDE. To justify the effectiveness of ConFEDE when unimodal labels are unavailable, we further test ConFEDE on the MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018b) , which are two English MSA datasets. However, they can not best test the performance of ConFEDE.\nWe compare our methods with the state-of-theart baselines in Table 1 and 2: LF-DNN (Yu et al., 2020) , MFN (Zadeh et al., 2018a) , LMF (Liu et al., 2018) , TFN (Zadeh et al., 2017) , MulT (Tsai et al., 2019) , MISA (Hazarika et al., 2020) , MAG-BERT (Rahman et al., 2020) , HyCon (Mai et al., 2021) and Self-MM (Yu et al., 2021) . For a fair comparison, the methods which only report the results of a single run and have no valid official code released Algorithm 1: Data Sampling Algorithm Input: Dataset D with the corresponding features T , V , A and multimodal labels ym. Output: Neighbor i , Outlier i for every i \u2208 D Define: sim(w, v) = w T v/||w|| \u2022 ||v|| for every (i, j) \u2208 D do\nCompute the cosine similarity score:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]), end Define: argsort(X) = indices sort X ascendingly Let |D| = length of D, z = |D| 4 . for every sample i \u2208 D do\nRetrieve the similar sample set S i 0 :\nS i 0 = argsort({C i,j |j : y j m = y i m });\nRetrieve the dissimilar sample set S i 1 :\nS i 1 = argsort({C i,j |j : y j m \u0338 = y i m }),\nRandomly select two samples from the last z elements of S i 0 as Neighbor i ; Randomly select two samples from the first z elements of S i 1 as Outlier i 1 ; Randomly select two samples from the last z elements of S i 1 as Outlier i 2 ;\nOutlier i = Outlier i 1 \u222a Outlier i 2 .\nend for reproduction are not selected. A detailed introduction can be found in the supplementary material. The detailed experimental settings are introduced in Appendix C.\n\nEvaluation Metrics\nFollowing the previous works (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020; Hazarika et al., 2020) , we report our results in (multi-class) classification and regression with the average of 5 runs of different seeds. For classification, we report the multiclass accuracy and weighted F1 score. We calculate the accuracy of 2-class prediction (Acc-2), 3-class prediction (Acc-3), and 5-class (Acc-5) prediction for CH-SIMS and the accuracy of 2-class prediction and 7-class prediction (Acc-7) for MOSI and MOSEI. Besides, Acc-2 and F1-score of MOSI and MOSEI have two forms: negative/non-negative (non-exclude zero) (Zadeh et al., 2017; Yu et al., 2021) and negative/positive (exclude zero) (Tsai et al., 2019; Yu et al., 2021) \n\nResults\nThe performance comparison of all methods on CH-SIMS, MOSI, and MOSEI is summarized in Table 1 and Table 2 . The scores of the proposed method and its variations are the averages of 5 runs. The performances of all other baselines, except for MAG-BERT, have been sourced from published papers or official repositories 1 .\nOn the CH-SIMS dataset, our proposed method outperforms all baselines on all metrics. We achieve superior performance compared to the best baseline model, Self-MM, with an improvement of 2.19% on acc-2 and 1.64% on F1 scores. Additionally, the proposed model demonstrates exceptional ability in multi-class classification, outperforming the best baseline by 4.68% on acc-3 and 4.77% on acc-5.\nAs seen in the results, our proposed method, ConFEDE, consistently outperforms all other baselines on the CH-SIMS dataset. The superior classification performance demonstrates that our designed learning method is more effective than the compared methods. Our method, ConFEDE, effectively distinguishes similarity and dissimilarity information between modalities, providing clearer modality features to the downstream classifier for improved prediction. Additionally, the significant improvement in MAE and Corr further highlights the ability of our model to better understand the CH-SIMS dataset than the other baselines.\nTo further evaluate the effectiveness of our proposed method, ConFEDE, we trained our models on the MOSI and MOSEI datasets without uni-modal labels. Instead, we used their multimodal labels for compatibility. The results are presented in Table 2 . On the MOSI dataset, our method outperforms all other baselines in both the negative/nonnegative (NN) setting and negative/positive (NP) setting for acc-2 and F1 metrics. Additionally, our acc-7 and MAE metrics surpass most of the baselines. For the MOSEI dataset, our ConFEDE method outperforms all baselines in all metrics except for the NN Acc-2 and F1 score. Furthermore, our MAE is significantly lower than all baselines, reaching 0.522.\nIt is worth noting that our models perform much better in NP acc-2 than NN acc-2 for MOSEI, as shown in Table 2 . This is because the NN acc-2 setting is generally more challenging than the NP acc-2 setting, as it places more pressure on a model to classify data samples with a regression label of 0. Specifically, if there are two samples with a regression label of 0, when predicted by a regression model, the results might be -0.01 and 0.01. As the value range of \"Neutral\" is [-0.5,0.5) in MOSI and (-0.1,0.1] in SIMS, these two samples should be classified as \"Neutral\". However, in NN settings, they will be classified into two different classes, resulting in a worse acc-2. In contrast, with the NP setting, all \"Neutral\" samples are abandoned, resulting in a better acc-2.\nIn contrast, our method shows better performance in both NN and NP settings on MOSI when compared to other models. The Acc-7, MAE and Corr are also better or comparable to most baselines.\n\nAblation Study and Analysis\nTo evaluate the impact of our proposed structures, we conducted an ablation study on our proposed method by removing inter-sample contrastive learning and intra-sample contrastive learning. The results are shown in Table 1 . \"Plain\" represents the model without contrastive learning method, \"Inter\" represents the model with inter-sample contrastive learning only, and \"Intra\" represents the model with intra-sample contrastive learning and unimodal prediction as a sub-task.\nThe experiment shows that all three models perform worse than the original model. Among the three models, the plain setting has the lowest performance. Both intra-sample contrastive learning and inter-sample contrastive learning provide positive impacts on performance. Compared with Plain, by using text feature as the anchor, Intra filters out noise (useless information for sentiment analysis) in the vision and audio modality, leading to better prediction. This is also the reason why it reaches better acc-2 accuracy than both the other models. Since the acc-2 metric in CH-SIMS follows the negative/non-negative setting, a feature with lower noise helps the classifier make a more precise prediction value, making it easier to classify the 0-labeled samples. This also explains why we achieve better NN Acc-2 performance than all baselines on MOSI.\nFor the inter-sample contrastive learning method, by learning the common and different information between samples, \"Inter\" performs better on multiclass classification. The result of Inter on CH-SIMS shows great improvements on both acc-5 and MAE compared with the other two models, which proves that Acc-5 and regression performance benefits more from \"Inter\". This can also explain why we have a lower NN Acc-2 performance on MO-SEI. Since MOSEI is much larger than MOSI and CH-SIMS, it introduces more noise in each modality, the contrastive feature decomposition learning needs more epochs and a smaller learning rate to separate the useful information from noise. Meanwhile, inter-sample contrastive learning is more efficient on MOSEI. With the larger amount of samples, it is much easier for the sampler to find the most similar and dissimilar samples with the given sample, from which the model can understand the difference between samples better. Thus, ConFEDE can reach higher Acc-7 and regression performance than all other baselines on MOSEI.\nTo further evaluate the effectiveness of the contrastive feature decomposition method, we conducted an ablation study on Intra using the CH-SIMS dataset. As presented in Table 3 , we created three variations of Intra: 1) Intra with only multimodal labels for unimodal prediction (M-label); 2) Intra without the unimodal prediction component (-uni); 3) Intra without the similarity-dissimilarity learning method (-cl); and 4) Intra that uses all similarity features as anchors (+full), which utilizes T s , V s , and A s as anchors instead of T s only.\nThe results in the table demonstrate that all variations resulted in a decrease in performance compared to the original Intra in classification matrices. Both the intra-sample contrastive learning and the unimodal prediction task can regularize the learned representation, resulting in clearer information that aids the classifier in understanding the sample better. However, the \"+full\" setting introduces more noise by also using V s and A s as anchors, which confuses the model and diminishes the denoising ability of the contrastive feature decomposition learning.\n\nConclusion\nIn this paper, we propose a novel method for multimodal sentiment analysis (MSA) called ConFEDE. The ConFEDE framework is based on contrastive feature decomposition, which utilizes a unified contrastive training loss to capture the consistency and difference across modalities and samples. This approach allows for the simultaneous learning of modality decomposition within each sample and su-pervised contrastive learning across samples. Our proposed method is mainly evaluated on CH-SIMS. The result shows that the proposed method significantly outperforms many state-of-the-art multimodal sentiment analysis methods. We further conduct an extensive experiment on MOSI and MOSEI to test the capability of ConFEDE when no unimodal label is available, where our method achieves better performance than state-of-the-art methods on a number of performance metrics.\n"}
{"question": "Which approach is proposed in the text to address the challenge of generating diverse questions in conversational datasets, specifically boolean questions?", "evidence": "  To classify which signal should be sent to the QG model, we train a RoBERTa (Liu et al., 2019a) as our Question Type Classifier. Type Classifier (QTC) We design two control signals to guide the QG model: <BOOLEAN> is prepended to the textual input if we expect the model to generate a boolean question, and <NORMAL> otherwise.    ", "options": ["A. Using a pretrained language model for generating all questions.", "B. Filtering out questions with low fuzzy matching scores. ", "C. Rewriting the extracted answer spans.", "D. Training a RoBERTa-based Question Type Classifier (QTC)."], "answer": "D", "content": "\nIntroduction\nBuilding systems that can comprehend human speech and provide assistance to humans through conversations is one of the main objectives in AI. Asking questions during a conversation is a crucial conversational behavior that helps AI agents communicate with humans more effectively (Allen et al., 2007; Li et al., 2016b) . This line of research is known as Conversational Question Generation (CQG), which targets generating questions given the context and conversational history (Nakanishi et al., 2019; Pan et al., 2019a; Gu et al., 2021; Do et al., 2022) . Compared to traditional single-turn question generation (Pan et al., 2019b) , CQG is more challenging as the generated multi-turn questions in a conversation need not only to be coherent but also follow a naturally conversational flow.\nGenerally, there are two main settings for the CQG task: answer-aware and answer-unaware. In the answer-aware setting, the expected answers of the (to be) generated questions are exposed to the models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . In reality, however, the answers are only \"future\" information that are unknown beforehand. Thus, growing attention has been on the more realistic answer-unaware setting, in which the answers are unknown to the CQG model (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nPrior studies either attempt to ask the questions first, and compute the reward function to evaluate their answerability (Pan et al., 2019a) or informativeness (Qi et al., 2020) ; or they extract the answer spans from the context as the what-to-ask first, and generate the questions based on them (Nakanishi et al., 2019; Do et al., 2022) . However, it has been argued that the former approach tends to generate repetitive questions (Qi et al., 2020; Do et al., 2022) . For the latter approach, Do et al. (2022) recently proposed a selection module to shorten the context and history of the input and achieved stateof-the-art performance. Nonetheless, it simply employs a naive heuristic to select the earliest forward sentence (without traceback) in the context as the rationale to extract the answer span. Although such heuristics ensure the flow of the generated questions is aligned with the context, we argue that the resulting conversations may not be natural enough, because, in reality, the interlocutors often talk about the relevant parts that may not form a sequential context. Furthermore, previous studies (Gao et al., 2019; Do et al., 2022) trained the models to decide the type of the question (boolean/span-based) to be generated implicitly. We argue that modeling question type explicitly is critical since in this setting, the answer, which hints the models to generate a boolean or span-based question, is unavailable.\nTo address the above problems, we propose a two-stage CQG framework based on a semantic graph, SG-CQG, which consists of two main components: what-to-ask and how-to-ask. In particular, given the referential context and dialog history, the what-to-ask module (1) constructs a semantic graph, which integrates the information of coreference, co-occurrence, and named entities from the context to capture the keyword chains for the possible \"jumping\" purpose; (2) traverses the graph to retrieve a relevant sentence as the rationale; and\n(3) extracts the expected answer span from the selected rationale (Section 3.1). Next, the how-to-ask module decides the question type (boolean/spanbased) via two explicit control signals and conducts question generation and filtering (Section 3.2).\nIn order to exhaustively assess the quality of the generated question-answer pairs, we propose a set of metrics to measure the diversity, dialog entailment, relevance, flexibility, and context coverage through both standard and human evaluations. Compared with the existing answer-unaware CQG models, our proposed SG-CQG achieves state-ofthe-art performance on the standard benchmark, namely the CoQA dataset (Reddy et al., 2019) .\nOur contributions can be summarized as follows:\n(1) We propose SG-CQG, a two-stage framework, which consists of two novel modules: whatto-ask encourages the models to generate coherent conversations; and how-to-ask promotes generating naturally diverse questions. Our codes will be released at https://github.com/ dxlong2000/SG-CQG.\n(2) SG-CQG achieves state-of-the-art performance on answer-unaware CQG on CoQA.\n(3) To the best of our knowledge, we are the first to propose a set of criteria to comprehensively evaluate the generated conversations. Moreover, we propose Conv-Distinct to measure the diversity of the generated conversation from a context, which takes the context coverage into account.\n(4) We conduct thorough analysis and evaluation of the questions and answers of our generated conversations, which can bring some inspiration for future work on the answer-unaware CQG.\n\nRelated Work\nOur work is closely related to two lines of prior work. Extended related work is in Appendix A.1.\n\nConversational Question Generation\nQuestion Generation has gained much attention from the research community over the years (Pan et al., 2019b; Lu and Lu, 2021) . Despite such intensive exploration, much less attention has been drawn to Conversational QG or CQG. Generally, CQG has been considered in two main settings: answer-aware and answer-unaware. In the answeraware setting, the expected answers are revealed to models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . However, this is not always the case in reality, as the answers are \"future information\". The answer-unaware setting; therefore, receives growing interests recently (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nTo tackle the what-to-ask problem, prior studies (Pan et al., 2019a; Do et al., 2022) selected the next sentence in the context as the rationale. Do et al. (2022) extract the target answer span from the rationale, while Pan et al. (2019a) generate the question, and compute a reward function to fine-tune the model by reinforcement learning. The howto-ask challenge was simply formulated as that in the answer-aware setting. In contrast, we attempt to model the rationale selection in a more coherent way by constructing and traversing a semantic graph, which simulates the keyword chains. We further propose control signals to promote diversity and fluency in question generation.\n\nKnowledge-grounded Conversation Generation\nLeveraging graphs to enhance dialog response generation has received growing interest (Moghe et al., 2018; Liu et al., 2019b; Xu et al., 2020 Xu et al., , 2021)) .\nIn particular, Xu et al. (2020) proposed to extract event chains (Mostafazadeh et al., 2016) , and utilised them to help determine a sketch of a multi-turn dialog. Nonetheless, the situation differs significantly when it comes to the CQG task. The responses in the dialog response generation task are normally full sentences with enough relevant mentions. However, in CQG, the questions and answers are mostly short and lack clear keywords, which makes the existing keyword-graph not applicable. We thus present a semantic graph, which incorporates the coreference, co-occurrence, and named entities information from the context.\n\nSG-CQG\nWe formulate the answer-unaware conversational question generation (CQG) task as:\ngiven the referential context C = {s 1 , s 2 , ..., s m } with s i being the i-th sentence in context, and the conversational history H n = {(q 1 , a 1 ), (q 2 , a 2 ), ..., (q n\u22121 , a n\u22121 )} with (q i , a i ) being the i-th turn of the question-answer pairs, as input D n = {C, H n }, the model learns to generate the current question q n and answer a n .\nFigure 1 demonstrates an overview of our proposed framework. It consists of two main components: (1) A what-to-ask module aims to select a reasonable sentence in the referential context C as the current rationale r n and thereby a span in r n as the target answer a n , given D n . (2) A how-to-ask module aims to generate the question q n , guided by the rationale r n and target answer a n .\n\nWhat-to-ask Module (WTA)\nExisting answer-unaware CQG models (Pan et al., 2019a; Do et al., 2022) commonly utilize the next sentence of r n\u22121 in the context as the current rationale r n . Although such heuristics can guarantee that the flow of the generated questions is consistent with the narrative in context, the generated conversation may not always be as natural as in reality, since human speakers often jump back and forth across the relevant but not sequential contents in context. To facilitate the models in selecting the current rationale and target answer appropriately and further improve the semantic diversity of dialogue flow, we design a what-to-ask module, which consists of two components: semantic graph construction and graph traversal algorithm.\nSemantic Graph Construction (SGC) Figure 1 shows an example of our semantic graph. Each node is displayed as a textual span and the index of the sentence it belongs to. To construct the semantic graph G = {V, E}, we first obtain the corefer-ence clusters from the context C by AllenNLP (Shi and Lin, 2019) and build the set of initial nodes from phrases in the clusters. We then connect all the nodes in the same cluster as a chain: each node in the cluster (except the one that appears last in the context) is connected to the nearest forward one in the context. We denote this type of relation as Coreference. To enhance the connectedness of G, we extract all named entities by spaCy 1 and add them as additional nodes if they are not in any clusters. We then connect all the nodes in the same sentence in the context in the same chaining style and name those edges as Same Sentence. Finally, we add a type of Extra edges between all connected subgraphs to make G fully-connected. Since those Extra edges do not bring any semantic relation to the graph, our objective is to minimize the number of those edges. Specifically, we gradually select, and connect two sentences such that their nodes are in different connected components and have the smallest indexes with the smallest difference, until the graph is fully-connected. To connect two sentences, we add an Extra edge between the last phrase in the smaller-index sentence and the first phrase in the remaining sentence. The adding-Extra-edges algorithm is in Appendix A.4.\nGraph Traversal Algorithm (GTA) Given the conversational history H n and the semantic graph G, we create a queue q to store nodes for traversing. We first add the nodes that appear in any previous turn' rationale to q in the index order 2 . We then traverse G by popping the nodes in q until it becomes empty. For each node, we retrieve the sentence that contains it as the rationale r n . If the model can generate a valid question from r n and any answer span extracted from r n , we add all unvisited neighbors of the current node to the beginning of q. A question is considered being valid if it passes the QF module (Section 3.2). Prepending the neighbors to queue is to prioritize the nodes that are connected so that the generated conversation can be formed from a chain of relevant sentences, which consolidates the coherence of the conversation. If the model cannot generate any valid q n by the current node, we add its unvisited neighbors to the end of q. The pseudocode of our proposed Graph Traversal Algorithm is described in Appendix A.2. et al. (2022) to design the answer span extractor module. In particular, a T5 model is trained on SQuAD (Rajpurkar et al., 2016) to predict the target answer span (a), given its original sentence in context (r). We use this pretrained model to extract a n from r n . Note that we also deselect the answer spans that are the same as those of previous turns.\n\nHow-to-ask Module (HTA)\nA high ratio of boolean questions in conversational datasets such as CoQA (Reddy et al., 2019) (around 20%) is one of the main challenges for current CQG studies (Gao et al., 2019; Pan et al., 2019a; Gu et al., 2021) . To the best of our knowledge; however, there is no up-to-date work which attempts to tackle this challenge. This problem is even worse in the answer-unaware setting since there is no\nYes/No answer to be provided to guide the generation of the models. Previous studies (Pan et al., 2019a; Do et al., 2022) simply train the CQG models to let them implicitly decide when to generate the boolean and span-based questions without any explicit modeling of the question type. We argue that explicitly modeling the question type is critical, as the models will gain more control on generating diverse questions, thus making the conversation become more natural. To this end, we introduce two control signals as the additional input to the QG model, and develop a simple mechanism to select the signal for the current turn.\nQuestion Type Classifier (QTC) We design two control signals to guide the QG model: <BOOLEAN> is prepended to the textual input if we expect the model to generate a boolean question, and <NORMAL> otherwise. To classify which signal should be sent to the QG model, we train a RoBERTa (Liu et al., 2019a) as our Question Type Classifier. This binary clasifier takes the rationale r n and the answer span a n generated from what-toask module, the context and the shortened conversational history as the input, and generates the label 0/1 corresponding to <NORMAL>/<BOOLEAN>. We conduct additional experiments to discuss why the control_signals work in Section 6.3.\nRewriting and Filtering (RF) Our RF module serves two purposes. Firstly, following Do et al.\n(2022), we train a T5 model on CoQA (Reddy et al., 2019) as our CQA model to answer the generated questions. A question is passed this filtering step if the answer generated by the CQA model has a fuzzy matching score greater or equal to 0.8 with the input answer span. Secondly, when invigilating the generated conversations, we observe multiple other errors that the blackbox model encounters, as shown in Table 1 . We thus propose extra post-processing heuristics to filter out the gen-erated questions and try to avoid the following issues: (1) Wrong answer. Unlike Do et al. ( 2022) that took the extracted spans as the conversational answers, we rewrite the extracted answer spans for the boolean questions by selecting the answers generated from the CQA model;\n(2) Irrelevant. For each generated question, we remove stopwords and question marks only for filtering purpose, and we check if all the remaining tokens exist in the context C;\n(3) Uninformative. To remove the turns like (\"Who woke up?\", \"Justine\"), we check validity if no more than 50% of the tokens of r n exist in any previously generated QA pairs; (4) Redundant. Unlike previous studies (Qi et al., 2020; Do et al., 2022) which only considered the redundant information from the generated answers, for each generated question that has more than 3 tokens, we filter it out if it has a fuzzy matching score >= 0.8 with any of the previously generated questions.\nQuestion Generation (QG) We fine-tune a T5 model (Raffel et al., 2020) to generate conversational questions. We concatenate the input\nD a n = {C, H n , a n , r n , control_signal} in the for- mat: Signal: control_signal Answer: a n , r n Context: C [SEP] H sub , where H sub \u2208 H n .\nThe model then learns to generate the target question q n . In our experiments, H sub is the shortened H n , in which we keep at most three previous turns. It was shown to improve upon training with the whole H n significantly (Do et al., 2022) . The performance of the QG model is in Appendix A.3.\n\nExperimental Settings\nDataset We use CoQA (Reddy et al., 2019) , a large-scale CQA dataset, in our experiments. Each conversation includes a referential context and multiple question-answer pairs, resulting in a total of 127k question-answer pairs. Among them, around 20% of questions are boolean, which makes this dataset become challenging for the CQG task (Pan et al., 2019a; Gu et al., 2021) . Since the test set of CoQA is unavailable, we follow Do et al. (2022) to keep the original validation set as our test set and randomly sample 10% of the original training set as our new validation set.\nAutomatic Evaluation We utilise BERTScore (Zhang et al., 2020) as our dialog entailment metric (BERTScore-entailment), a generalization of Dziri et al. (2019) . It considers the generated re-sponse (question/answer) as the premise, and the utterances in the conversational history as the hypothesis, and measures their similarity score as the topic coherence score. This property is crucial as the questions/answers should focus on the same topic as the previous turn(s). In our experiment, we measure the dialog entailment score with 1, 2, and all previous turn(s). To measure the relevance between the generated conversation and the context, we concatenate the generated QA pairs and compute the BERTScore. It provides how the generated conversation is explicitly relevant to the context.\nWe observe short conversations with very few generated turns tend to yield very high scores on the available diversity measurement metrics such as Distinct (Li et al., 2016a) . Since the conversation is generated from a given context, we argue that how much information from the given context the generated conversation covers should be taken into account. To this end, we introduce Context Coverage (CC) to measure the percentage of the sentences in the context that are the rationales of generated QA pairs. Our proposed Conv-Distinct of a generated conversation is then computed by multiplying the Distinct score of the generated conversation with its CC score, to measure the diversity of the turns generated from a given context:\nConv-Distinct = CC * Distinct (1)\nWe further provide Jumping Score (JS) to measure the flexibility of the generated conversation. JS is defined as the percentage of turns in which the model jumps back to any previous content of their previous turn (i.e. trace-back). It is worth noting that we do not rank the models based on JS score. Details of proposed metrics are in Appendix A.7.\nHuman Evaluation Human evaluation is critical to evaluate the quality of the generated conversations since the CQG model may generate reasonable conversations but unmatched well with the provided ground-truth ones. We randomly select 25 contexts in our test set and take the first five generated turns from the output of each model to compare, resulting in 125 samples in total. We hire three annotators who are English native speakers. Each generated question is rated by annotators on a 1-3 scale (3 is the best). We follow Do et al. (2022) to utilize three criteria: (1) Factuality measures the factual correctness and meaning of generated questions, (2) Conversational Alignment measures how aligned the generated questions are with the history, (3) Answerability measures how answerable the generated questions are by the given context. Given the fact that LMs can generate fluent texts, we omit using Fluency and Grammaticality.\nWe measure the annotators' agreement by Krippendorff's alpha (Krippendorff, 2011) . Our human rating instructions are in Appendix A.9.\nImplementation Details We fine-tune a RoBERTa large (Liu et al., 2019a) as our binary Question Type Classifier with the pretrained checkpoints from fairseq (Ott et al., 2019) on CoQA. We use a learning rate of 1e-5, a window size of 512, a batch size of 4, and AdamW (Loshchilov and Hutter, 2019) as our optimizer.\nOur classifier achieves an accuracy of 95.6%. The model is finetuned on a P40 Colab GPU for 10 epochs. Details of the input format are in Appendix A.5. We initialise SG-CQG with pretrained checkpoints of T5 base model (Raffel et al., 2020) from Huggingface (Wolf et al., 2020) . We also use AdamW (Loshchilov and Hutter, 2019) as our optimizer with a warmup of 0.1 and an initial learning rate of 1e-4. We train the model for 100k iterations with a standard window size of 512, a batch size of 4, and use a Beam search decoding strategy with a beam size of 4. \n\nMain Results\nTo evaluate the performance of SG-CQG on the answer-unaware CQG task, we employ 4 baselines for comparison, as shown in Table 2 .\n(1) T5 base (Raffel et al., 2020) , (2) BART base (Lewis et al., 2020) , (3) GPT-2 (Radford et al., 2019) , which are fine-tuned to generate conversational questionanswer pairs end-to-end, and (4) CoHS-CQG (Do et al., 2022) which adopts a strategy to shorten the context and history of the input, achieves the SoTA performance on CoQA in answer-aware and answer-unaware CQG. Firstly, we observe that SG-CQG outperforms other methods on most of the metrics, except Distinct and BERTScore. The reason is that BART and T5 often generate short QA pairs (the CC scores are 8.62% and 23.33% on average, respectively), and copy more from the context, thus they get higher scores on Distinct and BERTScore. Secondly, the metric Conv-Distinct reasonably penalizes models that generate too short conversations, on which SG-CQG achieves the best results. Thirdly, by allowing the model to jump back and forth across the relevant contents in the context by the semantic graph, SG-CQG outperforms other methods significantly on BERTScore-entailment, which indicates that conversational coherence is indeed im-proved. Furthermore, SG-CQG achieves the highest JS score, which demonstrates that the whatto-ask module allows our model to be most flexible in selecting rationales compared to the baselines. SG-CQG also achieves a significantly higher Context Coverage (CC) score compared to CoHS-CQG. Finally, compared with the results of Oracle, which are from the human-generated conversations, SG-CQG achieves commensurate performance on BERTScore-entailment and BERTScore. It demonstrates that our generated conversations are as closely coherent as human-generated ones.\nQuestion Generation Evaluation We compare the generated conversational questions of our model with 4 baselines: (1) ReDR (Pan et al., 2019a) is an encoder-decoder framework which incorporates a reasoning procedure to better understand what has been asked and what to ask next about the passage; (2) T5 base (Raffel et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . For T5, GPT-2 and CoHS-CQG, we extract the generated questions from the generated conversations for comparison. We measure the diversity of the generated questions by Distinct (Li et al., 2016a) and our proposed Conv-Distinct. Table 3 shows evaluation results of the generated conversational questions. We observe that SG-CQG achieves the best performance on Conv-Distinct, which takes the context coverage into account.\n\nAnswer Span Extraction Evaluation\nWe further evaluate the generated conversational answers of our model with 4 baselines: (1) T5 base (Raffel et al., 2020) ; (2) BART base (Lewis et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . We extract the generated conversational answers from the generated conversations of the models for comparison. We train another T5 base model on CoQA for the CQA task (see Appendix A.6) and utilize it to generate the groundtruth answers for the generated questions of the models. We then evaluate the quality of the generated conversational answers by measuring the Exact Match (EM) and F1 scores with the groundtruth ones. Table 4 shows the evaluation results. We observe that the generated conversational answers extracted by SG-CQG achieve the best EM and F1 scores, which are significantly higher than the other baselines.\n\nHuman Evaluation\nThe results of the human evaluation are present in et al., 2022) . Compared to CoHS-CQG, it achieves higher scores on all metrics except the Context Coverage (CC), which reflects that the quality of the generated conversations is indeed improved. These improvements are expected as the model in this case gains more control over generating boolean questions and has a stricter filtering process. This stricter filtering process also explains why it gets a lower CC score compared to CoHS-CQG.\n\nAblation of Question Type Classifier (QTC)\nWe conduct an ablation study of the Question Type Classifier (QTC) module. We name this experiment SG-CQG + w/o QTC. Table 2 shows the evaluation results of generated question-answer pairs. Compared with SG-CQG, the performance of SG-CQG + w/o QTC drops slightly on nearly all metrics (except Distinct), which consolidates our hypothesis that explicitly modeling the question type improves the overall coherency of the conversation. Furthermore, Table 3 shows that QTC enhances the diversity of the generated questions, while Table 4 illustrates that QTC improves the quality of the 2 ) and questions (Table 3 ) significantly. Notably, without the RF module, the extracted answer spans by SG-CQG + w/o RF can be very different from the true conversational answers, resulting in very low F1 and EM scores (Table 4 ). Although the CC score is perfect, the generated question-answer pairs from this experiment are of bad-quality.\n\nCase Study\nWe present one conversation generated by our proposed SG-CQG in Table 6 . We observe that the rationale of Q2-A2 is the 3-rd sentence in the context, and the rationale of Q3-A3 is the 8-th sentence, which is a forward jump of the model. On the other hand, the rationale of the Q4-A4 is the 7-th sentence, which is a traceback. Such a traceback enhances reasonable coherence between Q3-A3 and Q4-A4. Furthermore, Q5-A5 to Q6-A6 is also a traceback, and especially, Q6 is a boolean question. More case studies are shown in Appendix A.10.\n\nWhy Do Control Signals Work?\nExperimental Settings We design the experiments to verify the helpfulness of our two proposed control_signals: <BOOLEAN> and <NORMAL>.\nIn particular, we train a T5 model (Raffel et al., 2020 ) in the answer-aware setting. Given the input D a n = {C, H n , a n , r n } with C, H n , a n , r n as the context, ground-truth conversational history, ground-truth answer, and round-truth rationale, respectively, we conduct three experiments in Table 9 : original input with Yes/No keyword (With Y/N ), original input without Yes/No keyword (W/o Y/N ), original input without Yes/No and with the ground-truth control_signal (W/o Y/N + control_signal). Note that we train the model with the whole context, and a maximum of three previous history turns, as discussed in Appendix A.3. We measure the performance of the answer-aware CQG model separately on two types of questions: boolean and span-based by ROUGE-L (Lin, 2004) and BERTScore (Zhang et al., 2020) .\nObservations Table 9 shows the experimental results. We derive two main observations. Firstly, without knowing the keyword Yes/No (W/o Y/N ) -this is the case in the answer-unaware setting, the model performs worse. This decrease shows that the Yes/No keyword is indeed helpful in hinting the model towards generating the correct questions. Secondly, by inputting the groundtruth control_signal into the model (W/o Y/N + control_signal), the performance is improved by a large margin compared to (W/o Y/N ). We obtain three implications from the above improvement. Firstly, it consolidates our hypothesis that inputting the ground-truth control_signal is truly helpful. Secondly, by training with the control_signal, the performance of the model is even higher than with Y/N in the span-based cases, which indicates that training the model with control_signal makes it more stable to generate the correct questions. Thirdly, the performance of (W/o Y/N + control_signal) is lower than (With Y/N ) in boolean cases. The reason is <BOOLEAN> only informs the model to generate a boolean question without informing to generate an Yes or No one.\n\nConclusion\nThis paper presents SG-CQG, a two-stage framework for the CQG task in the answer-unaware setting. Firstly, the what-to-ask module aims to select a sentence as the rationale by the proposed semantic graph and extract the answer span from it. The how-to-ask module classifies the type of the question before generating and filtering it. Additionally, we propose a set of automatic evaluation criteria for answer-unaware CQG, especially a novel metric, Conv-Distinct, to evaluate the generated conversation from a context. Extensive automatic evaluation and human evaluation show that our method achieves state-of-the-art performances in the answer-unaware setting on CoQA, with a significant improvement in the conversational alignment property compared to previous frameworks. In the future, we will focus on how to reason over our semantic graph to select the rationale, and further improve the performances of how-to-ask module.\n"}
{"question": "What is the role of Rule Encoder", "evidence": "  RBE is comprised of two neural networks, a rule encoder, and a text encoder, which jointly learn rich embedding representations for hateful content and the logical rules that govern them. Our architecture consists of a Rule Encoder \u0398 r and a Text Encoder \u0398 t . These are two Bert-like bidirectional transformer models (Devlin et al., 2018) each responsible for learning embedding representations of their respective inputs. ", "options": ["A. Responsible for embedding representations of learning rules", "B. Pairing hateful examples with clusters of rule exemplars that govern it", "C. Predicting through rules", "D. Not mentioned"], "answer": "A", "content": "\nIntroduction\nContent moderation is a major challenge confronting the safety of online social platforms such as Facebook, Twitter, YouTube, Twitch, etc. (Vaidya et al., 2021) . Major technology corporations are increasingly allocating valuable resources towards the development of automated systems for the detection and moderation of harmful content in addition to hiring and training expert human moderators to combat the growing menace of negativity and toxicity online (Wagner and Bloomberg, 2021; Liu et al., 2022) .\nDespite the popularity of deep learning approaches, many practical solutions used in products today are comprised of rule-based techniques based on expertly curated signals such as block lists, key phrases, and regular expressions (Gillespie, 2018; Zhang, 2019; Dada et al., 2019) . Such methods are widely used due to their transparency, ease of customization, and interpretability. However, they have the disadvantage of being difficult to maintain and scale, in addition to being inherently fragile and noisy (Zhang, 2019; Davidson et al., 2017; Lee, 2022; Lai et al., 2022) . Figure 1 shows an example where logical rules, while explainable in nature, face the problem of being inflexible to their context of use in natural language. While a given rule may be too specific and fail to capture different variations of usage commonly found in content online, rules can also be too broad and incorrectly block lexically similar content.\nIn contrast to the challenges faced by rule-based methods, data-driven deep learning approaches have shown great promise across a wide range of content moderation tasks and modalities (Malik et al., 2022; Shido et al., 2022; Lai et al., 2022) . Fueled by large amounts of data and deep neural networks, these complex models are capable of learning richer representations that better generalize to unseen data. The impressive performances of these models have resulted in significant industry investment in content moderation as-a-service. Several technology companies such as Google 1 , OpenAI 2 , and Microsoft 3 use these models to offer services to aid in content moderation. However, despite their significant investment, they face adoption challenges due to the inability of customers to understand how these complex models reason about their decisions (Tarasov, 2021; Haimson et al., 2021; Juneja et al., 2020) . Additionally, with the increasing attention around online content moderation and distrust amongst consumers, explainability and transparency are at the forefront of demands (Kemp and Ekins, 2021; Mukherjee et al., 2022) . This presents the challenging open question of how we can leverage the robustness and predictive performance of complex deep-learning models whilst allowing the transparency, customizability, and interpretability that rule-based approaches provide.\nPrior works such as Awasthi et al. (2020) ; Seo et al. (2021) ; Pryzant et al. (2022) have explored learning from rules for tasks such as controlling neural network learning, assisting in human annotation, and improving self-supervised learning in low data scenarios. Awasthi et al. (2020) propose a rule-exemplar training method for noisy supervision using rules. While performant in denoising over-generalized rules in the network via a soft implication loss, similar to other ML approaches, this method lacks the ability to interpret model predictions at inference time. Pryzant et al. (2022) propose a general-purpose framework for the automatic discovery and integration of symbolic rules into pre-trained models. However, these symbolic rules are derived from low-capacity ML models on a reduced feature space. While less complex than large deep neural networks, these low-capacity models are still not easily interpretable by humans. Therefore, the task of combining the explainability of rules and the predictive power of deep learning models remains an open problem.\nIn order to tackle this problem, we introduce Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is comprised of two neural networks, a rule encoder, and a text encoder, which jointly learn rich embedding representations for hateful content and the logical rules that govern them. Through the use of contrastive learning, our framework uses a semantic similarity objective that pairs hateful examples with clusters of rule exemplars that govern it. Through this approach, RBE is able to provide more explainable predictions by allowing for what we define as Rule-grounding. This means that our model is able to ground its predictions by showing the corresponding explainable logical rule and the exemplars that constitute that rule.\nWe evaluate RBE in both supervised and unsupervised settings using a suite of rulesets. Our results show that with as little as one exemplar per rule, RBE is capable of outperforming state-of-theart hateful text classifiers across three benchmark content moderation datasets in both settings. In summary, the contributions of this paper are:\n\u2022 Rule By Example (RBE): a novel exemplarbased contrastive learning approach to learn from logical rules for the task of textual content moderation. 4 \u2022 We demonstrate how RBE can be easily integrated to boost model F1-score by up to 4% on three popular hate speech classification datasets.\n\u2022 A detailed analysis and insights into the customizability and interpretability features of RBE to address the problem of emerging hateful content and model transparency.\n\nRule By Example Framework\nIn this section, we outline the Rule By Example framework, define its operational terms, and describe its end-to-end architecture. We first formally describe the two main operational terms used in our framework: 1) Ruleset -a ruleset is comprised of a series of executable functions that when given text as input \"fire\" if and only if all conditions defined in the rule are met by the input. Figure 1 shows an example of a simple rule that is triggered if a given text contains the keywords \"hate\" or \"loathe\" and contains \"women\". Rules can be any programmable function that acts on text such as regular expressions, blocklists, keywords, etc. In the scope of this work, we only consider simple rules that humans can easily interpret. As such an ML model cannot be considered a rule, given their black-box nature. 2) Exemplar -an exemplar is a given textual example that well-defines the type of content governed by a rule. For example, X 1 and X 2 in Figure 1 can be considered exemplars of rule R 1 since they correctly match the conditions of R 1 .\nConsider a ruleset of rule-exemplar pairs R\"tpr 1 , e 1 q, pr 2 , e 2 q, ..., pr n , e n qu where r i denotes a defined rule and e i denotes an exemplar for which r i correctly fires. For a given corpus X comprising labeled examples X\"tpx 1 , y 1 q, px 2 , y 2 q, ..., px m , y m qu, each rule r i can be used as a black-box function R i : x \u00d1 ty i , Hu to noisily label each instance x such that it assigns a label y or no label at all. An instance may be covered by more than one rule or no rule at all. Additionally, the cover set C denotes the set of instances in X where a rule r i fires. The generalization problem that arises when rules are applied noisily is two-fold. When rules are too broad the cover set C is large and incorrectly labels a large amount of non-hateful content. Likewise, when rules are too strict and fragile, the cover set C is too small, and lexically and semantically similar content that is hateful ends up being ignored. Our goal is to leverage these rules and their exemplars to facilitate explainable model learning. for each instance xi in X b do 5:\nGet exemplars ei \" doRulesetpR, xiq 6:\nConcatenate exemplars ei 7:\nend for 8:\nGet \u0398rpE b q and \u0398tpX b q 9: Compute L\" 1 2 pY b D 2 `p1 \u00b4Yb qmaxpmargin D, 0q 2 q 10: Update parameters of \u0398r and \u0398t 11: end while\n\nDual Encoder Architecture\nThe Dual-Encoder architecture, as illustrated in Figure 2 , is commonly used in dense retrieval systems and multi-modal applications (Clarke et al., 2022; Reimers and Gurevych, 2019; Xu et al., 2022) . Our architecture consists of a Rule Encoder \u0398 r and a Text Encoder \u0398 t . These are two Bert-like bidirectional transformer models (Devlin et al., 2018) each responsible for learning embedding representations of their respective inputs. This Dual Encoder architecture enables pre-indexing of exemplars allowing for faster inference at runtime after training.\nEncoding Pipeline Given an input text x t , we first extract the set of applicable rules and their respective exemplars from the ruleset R. We then concatenate each extracted exemplar to form x e . In the event that no rules are applicable to x t , we randomly sample exemplars from the en-tire ruleset to form x e . Using the form x e \" \u2423 rCLSs , e ( where e n k is the k-th token of the n-th exemplar and rSEP s and rCLSs are special tokens. Similarly, using the text encoder \u0398 t , we encode x t . In order to obtain a dense representation, we apply a mean pooling operation to the hidden states and derive a fixed-sized sentence embedding. After obtaining the representation for both the exemplars x e and the text x t , we use the cosine function to measure the similarity between them: simpx e , x t q \" \u0398 r px e q \u00a8\u0398t px t q }\u0398 r px e q} }\u0398 t px t q} (1)\nWe employ a contrastive loss (Hadsell et al., 2006) to learn the embedding representations for our rule and text encoder. Contrastive learning encourages the model to maximize the representation similarity between same-label examples and to minimize it for different-label examples. This enables the embedding representations of our encoded ruleset to match the representation of the text correctly covered by cover set C. Likewise, for benign examples that rules incorrectly cover, our contrastive learning objective increases the distance between those representations, thus restricting the over-generalization of certain rules in the ruleset. Let Y t be the correct label of the texts X t , D be the cosine distance of px e , x t q and m be the margin, our contrastive learning loss function is defined as follows:\nL \" 1 2 pYtD 2 `p1 \u00b4Ytqmaxpm \u00b4D, 0q 2 q (2)\nThe training loop, with the encoding pipeline and constrastive loss step, are detailed in Algorithm 1.\n\nRule-Grounding\nBy taking an embeddings-based approach to learning representations, RBE enables what we define as rule-grounding. Rule-grounding enables us to trace our model predictions back to the explainable ruleset accompanied by the exemplars that define each rule. For any input x t that has been marked as positive by our dual encoder, we perform a rules search to find which rules fire on that input as well as an embedding similarity search to find the nearest exemplars and the rules those exemplars belong to. Table 2 shows an example of this.\n\nExperimental Setup\nTraining We train all models with AdamW optimizer and weight decay of 0.01 on all data. We employ early stopping with a ceiling of 10 epochs, a learning rate of 2e-5, batch size of 8, and linear learning rate warmup over the first 10% steps with a cosine schedule. Our models are trained with NVIDIA Tesla V100 32GB GPUs using Azure Machine Learning Studio. We pre-process data and train all models with different random seeds over multiple runs. Our implementation of RBE is based on Huggingface Transformers (Wolf et al., 2020) and Sentence Transformers (Reimers and Gurevych, 2019) . RBE utilizes two Bert-based networks consisting of 110 million parameters each.\nApproximately 2,000 GPU hours were required to train all hyperparameter variations of RBE plus the Bert baseline across all 3 test sets.\nBaselines We evaluate our training algorithms in both supervised and unsupervised settings. We compare against the baselines of applying logical rules as is and the current SOTA approach of training transformer-based sequence classifiers (Mathew et al., 2020) .\n\nDatasets\nWe evaluate RBE across three datasets on the task of hate-speech classification. Across each dataset, we frame the problem as a binary classification task of detecting whether a given text is hateful or nonhateful. We augment each dataset with rulesets that we manually curate. More information on each dataset and ruleset is provided below.\nHateXplain (Mathew et al., 2020 ) is a large-scale benchmark dataset for explainable hate speech detection that covers multiple aspects of hate speech detection. It consists of \"20k samples across 3 labels \"hateful\", \"offensive\", and \"normal\". Additionally, each sample is accompanied by a corresponding target group and explainable rationales. In our experiments, we combine the output classes of hateful and offensive into one resulting in \"8k/1k/1k hateful samples and \"6k/781/782 non-hateful samples for train/validation/test respectively. Additionally, we utilize the accompanying rationales for ruleset construction.\nJigsaw 5 is a large-scale dataset of Wikipedia comments labeled by human raters for toxic behavior. The defined types of toxicity are \"toxic\", \"severe toxic\", \"obscene\", \"threat\", \"insult\", and \"identity hate\". Each comment can have any one or more of these labels. In total, it contains \"230k samples. In our experiments, we define examples of the \"identity hate\" class as hateful and the rest as non-hateful resulting in a dataset of 1405/100/712 hateful samples and \"158k/1k/63k non-hateful examples for train/validation/test respectively.\nContextual Abuse Dataset (CAD) (Vidgen et al., 2021) is annotated dataset of \"25k Reddit entries labeled across six conceptually distinct primary categories of \"Identity-directed\", \"Persondirected\", \"Affiliation directed\", \"Counter Speech\", \"Non-hateful Slurs\", and \"Neutral\". In our experiment, we define examples of the \"identity-directed\" class as hateful and treat the remaining examples as non-hateful resulting in a dataset of 1353/513/428 hateful samples and \"12k/4k/4k non-hateful samples for train/validation/test.\n\nRuleset Construction\nHate+Abuse List We utilize a ruleset targeting identity hate which we'll refer to as Hate+Abuse List. It consists of a list of n-grams representing harmful language such as slurs or hate verbs.\nHate+Abuse List is similar to the publically available bad word lists commonly found online. We treat each n-gram entry in Hate+Abuse List as its own rule that proposes a positive label if the ngram is in the input text. In total, Hate+Abuse List consists of 2957 distinct identity hate rules.\nHateXplain Rationale Ruleset Using the labeled annotator rationales included in the HateXplain dataset, we programmatically generate a Ruleset for HateXplain. To do so, we extract 1, 2, and 3-gram substrings from the annotator rationales and cluster them by annotator-identified target demographic groups. We then take the top N n-grams per each demographic group and automatically create rules for each of them. This results in rules similar in nature to our Hate+Abuse List. Using a default cluster size of 100 across the 25 target categories defined in HateXplain, we generated a total of 670 distinct rules for HateXplain.\nContextual Abuse Rationale Ruleset Similar to our derived HateXplain ruleset we programmatically generate a Ruleset for the Contextual Abuse Dataset using annotator-labeled rationales. Following the identical process outlined before, this results in a total of 2712 distinct rules for CAD.\nExemplar Selection For each dataset we complete our Ruleset construction by pairing each rule with accompanying exemplars. To achieve this, we first run our Ruleset on the dataset trainset and extract instances for which a rule correctly fires.\nFor each rule that correctly fires, we then randomly select N instances to act as the exemplars. Additionally, to restrict potentially overgeneralized rules we enforce the condition that no two rules can be mapped to the same exemplar. Unless stated otherwise, we report results using just one exemplar per rule in our experiments.\n\nUnsupervised Setting\nIn addition to evaluating RBE in supervised settings, we investigate the applicability of RBE in unsupervised settings where no labeled data is present.\nIn this setting, we are presented with a large unlabeled corpus T and a given ruleset R. This setting is particularly challenging due to the inherent generalization problem of rules. Loosely applying rules as is in this setting results in the model overfitting to the distribution of the ruleset as seen in Table 3 . To combat this issue, we design three different semantic clustering-based strategies for determining rule quality in an unsupervised setting: Mean, Concat, and Distance clustering. Given an unlabeled corpus T \" tt 1 , t 2 , ..., t n u, ruleset R \" tpr 1 , e 1 q, ..., pr n , e n qu, and a threshold k, we first encode the entire corpus T using a pre-trained sentence embedding model E \u0398 . In our case, we use a fine-tuned version of MPNet (Song et al., 2020) from the Sentence Transformers library. After receiving our encoded corpus E \u0398 pT q, for the Mean and Concat, we construct a rule embedding r i \u0398 for each rule r i in the ruleset. In the Mean strategy, this is obtained by taking the mean of all rule exemplars \u00b5pr i \u0398 q \" p 1 m \u0159 m i e i m q. For Concat, this is calculated by concatenating all rule exemplars \u00b5pr i q \" E \u0398 pe i 1 } ... } e i m q and encoding the concatenated representation. Once r i \u0398 is constructed, we then label each text in the corpus whose cosine similarity is within the threshold k: \nEQUATION\nIn contrast to the Mean and Concat strategies, the Distance strategy takes a rule elimination approach. Given an unlabeled corpus T \" tt 1 , t 2 , ..., t n u, ruleset R \" tpr 1 , e 1 q, ..., pr n , e n qu, and a threshold k, we first noisily label the entire corpus using the ruleset R i : x t \u00d1 t1, Hu such that each rule is paired with a cover set R \" tpr 1 , e 1 , c 1 q, ..., pr n , e n , c n qu where c i is the set of texts in covered by r i . Next, for each rule, we encode text in its cover set E \u0398 pc i q and calculate the average cosine distance between each embedding and its neighboring examples in c i .\navgDistpE \u0398 pc i qq \" 1 n n \u00ff i distpc i j , c i j\u00b41 q (4)\nLastly, once the average distance for each rule is calculated, using the defined threshold k, we flip any weakly labeled examples in the cover set if the average distance for that rule is above the threshold k:\nEQUATION\n4\n\nResults and Discussion\nWe analyze the results of our experiments, detail our insights, and discuss the implications of applying RBE for explainable hate speech detection.\nEvaluation Metrics: The precision, recall, and F1 score for each dataset in a supervised setting are reported in Table 1 . Due to the highly skewed class distribution, we favor macro F1 scores as our main evaluation metric. We also report accuracy scores (the fraction of entries for which the full set of labels matches) as another metric.\n\nSupervised Performance\nTable 1 reports our results on three hate speech classification datasets in the supervised setting. We observe that RBE is able to outperform SOTA transformer-based models BERT and MPNet by 1.3/1.4%, 4.1/2.3%, and 4.3/1.3% in F1-score on HateXplain, Jigsaw, and CAD respectively. This improvement highlights the impact of leveraging rules in the training process of our framework. Additionally, it is important to note that this increase was achieved using only 1 exemplar per rule in the ruleset. These exemplars were also used to train the comparative baseline models, ensuring that all approaches were trained on the same number of samples. This further showcases how lightweight and flexible RBE is to integrate into a content moderation workflow. For HateXplain, our experiments show that the combination of MPNet as the initialized encoder with both the HateXplain Rationale and Hate+Abuse Ruleset delivers the best performance. Upon deeper analysis, we find that this is due to two main factors: 1) Ruleset Size and Alignment -As explained in Section 3.2 the HateXplain Rationale Ruleset was automatically crafted using rationale labels from expert annotators. This results in a powerful ruleset capable of identifying a large amount of hateful content in the HateXplain dataset as shown 2) Embedding Initialization -Out of the box, pre-trained BERT does not produce meaningfully distinct sentence representations. In practice, the BERT [CLS] token as well as averaged BERT outputs can contain useful after downstream fine-tuning. This is shown by the BERT performance in Table 1 . However, when the pretrained model output is pooled across all dimensions and used for calculating semantic similarity, this results in similar representations even for completely different input text. As a result, if applied to the HateXplain dataset without any fine-tuning, BERT embeddings obtain a precision, recall, and F1-score of 59%, 100%, and 75% respectively, where every example is labeled as hateful. This lack of varied sentence representation coupled with a verbose ruleset such as the HateXplain Rationale Ruleset results in an initial biasing towards hateful examples as shown by the high recall scores. As such, utilizing a pre-trained sentence embedder, such as MPNet, with a pre-train task more optimized for semantic embeddings results in better performance. We observe a similar trend when utilizing our derived ruleset for CAD. Note: When trained longer, the bias of the BERT model decreases as more varied sentence representations are learned.\nOn Jigsaw and Contextual Abuse datasets using the Hate+Abuse List and derived CAD Ruleset, RBE outperforms SOTA by an increased margin of 4.1/2.3%, and 4.3/1.3% respectively. Contrary to HateXplain, these two datasets are more heavily imbalanced toward non-hateful examples and thus more representative of the real-world case of content moderation where most content is consid-ered benign. This increased performance highlights the power of incorporating logical rules to assist model learning and also the ability of RBE to better generalize rules. As seen in Table 1 , on its own the Hate+Abuse ruleset performs poorly on each dataset in both precision and recall. Despite RBE's reliance on this ruleset to guide model learning, when combined with labeled training data, RBE is capable of both restricting over-generalized rules and leveraging its understanding of semantic similarity to extend fragile rules regardless of the base model. Additionally, when using the CAD ruleset which is heavily overfitted to the CAD dataset, as shown by the skewed recall score, RBE is still capable of outperforming the baselines.\nOut-of-domain Rulesets Our Hate+Abuse ruleset is a generic ruleset unrelated to any of the datasets evaluated, and thereby an out-of-domain ruleset. This provides an example of out-of-domain performance using rules not derived from the target dataset. We observe that even when applying RBE with the Hate+Abuse ruleset we are able to outperform the baselines on each dataset. When applying RBE to new domain settings, all that is required is the authoring of additional rules for this new domain. This can be done manually, or more scalably by automatically deriving rules from the new domain data.\n\nInterpretability\nIn addition to its improved performance, another advantage of RBE lies in its ability to perform Rule-grounding. As explained in section 2.2, Rulegrounding enables us to trace our model predictions back to their respective rule accompanied by the exemplars that define that rule. Table 2 shows Rule-grounding examples extracted from each of our tested datasets. By nature, Rule-grounding enables two main features in RBE:\n1) Customizability/Ruleset Adaptation: Given the vast reach of online applications, content mod- 2) Prediction Transparency: By facilitating model interpretations via rule-grounding, users of online systems are offered tangible guidance should their content be flagged, potentially increasing user trust in the system. Additionally, this acts as a direct indicator of the type of content the rule authors want to moderate.\n\nUnsupervised Performance\nTable 3 reports our results in the unsupervised setting. We observe that RBE is able to outperform SOTA trained on noisy rules labeled samples for the HateXplain and Jigsaw dataset while also outperforming the ruleset as is on all three datasets. Across each dataset, we find that RBE's Distance based strategy produces the most consistent performance, outperforming SOTA on HateXplain and CAD while performing on par with SOTA on Jigsaw. We observe that this stability in performance is due to this strategy's rule elimination objective. As opposed to the Mean and Concat strategies which focus on deriving rule representations in a self-supervised manner, the Distance strategy instead focuses on eliminating over-generalized rules whose cover set of examples are semantically dissimilar. This is particularly useful in cases where precision scores are low due to a large number of false positives.\nFor Jigsaw, we observe a slight decrease in performance compared to SOTA. Upon further analysis, we posit that this is a result of RBE's overreliance on the ruleset in this setting, particularly for the Mean and Concat strategies. This is because the ruleset directly influences the derived rule embedding due to its labeling of the cover set C. As such when the ruleset is over-generalized, as is the case of Hate+Abuse rules on Jigsaw, RBE is likely to match the distribution of the ruleset. We find that performing self-supervised model pre-training (Gao et al., 2021) on the target corpus circumvents this trend for the Mean and Concat strategy. As such, with a more refined ruleset, a performance increase is expected as seen in HateXplain and CAD.\n\nRelated Work\nThere has been active work on detecting hate speech in language (Poletto et al., 2021; Al-Makhadmeh and Tolba, 2020; Schmidt and Wie-gand, 2017) . Hate Speech detection has proven to be a nuanced and difficult task, leading to the development of approaches and datasets targeted at various aspects of the problem (Vidgen et al., 2021; Mathew et al., 2020; Mody et al., 2023) . However, few attempts have been made to focus on the explainability of these models, which is an increasing area of concern surrounding their use online (Tarasov, 2021; Haimson et al., 2021) , thus leading to the continued utilization of less powerful but more explainable methods such as rules. Prior works have explored incorporating logical rules into model learning. Awasthi et al. (2020) proposed to weakly learn from rules by pairing them with exemplars and training a denoising model. However, this requires defining rules for all output classes, making it inapplicable to the task of hate speech detection. Additionally, this method only focuses on decreasing rule scope to solve the overgeneralization problem. It does not simultaneously tackle the over-specificity problem demonstrated in Figure 1 . Finally, this method does not provide a way for interpreting model predictions during inference. Seo et al. (2021) proposes a way to control neural network training and inference via rules, however, their framework represents rules as differentiable functions requiring complex perturbations to incorporate, making it more suitable to numerical rules such as those defined in healthcare and finance as opposed to the complex nuances of language. Pryzant et al. (2022) proposes a framework for the automatic induction of symbolic rules from a small set of labeled data. However, these rules are derived from low-capacity ML models and are as a result not human-readable or explainable.\n\nConclusion\nWe introduce Rule By Example, an exemplar-based contrastive learning framework that enables learning from logical rules for accurate and explainable hate speech detection. Specifically, we propose a novel dual-encoder model architecture designed to produce meaningful rule and text representations. RBE leverages a novel exemplar-based contrastive learning objective that converges the representations of rules and text inputs of similar classes. We share results on three public datasets for hate speech detection that validate the Rule By Example framework can not only vastly outperform the initial ruleset but also outperform baseline SOTA classification methods in both supervised and unsupervised settings. Moreover, RBE enables rule-grounding which allows for more explainable model prediction benefits not available in SOTA classification methods alongside additional flexibility via Ruleset Adaptation.\n"}
{"question": "What is the main objective of Stage 1 in the MixDA approach?", "evidence": "  In Stage 1 (Figure 1 (a)), we inject new feed-forward networks (FFNs) (namely domain-adapter) paralleled to the original pre-trained FFNs in some Transformer layers, acting as a key-value memory. The newly injected domain-adapter is trained on both domain-specific unlabeled data and original pre-training unlabeled data to store new factual associations. ", "options": ["A. Fine-tuning the task adapter on labeled data", "B. Injecting new factual associations into the domain adapter", "C. Training the MoA gate to select domain adapters", "D. Pre-training the model on a large generic corpus "], "answer": "B", "content": "\nIntroduction\nPre-trained language models (PLMs) have achieved a multitude of successful applications in natural language understanding (Devlin et al., 2018; Liu et al., 2019; He et al., 2021b) and generation (Lewis et al., 2019; Zhang et al., 2020; Yang et al., 2020; Brown et al., 2020) . The predominant methodology for domain adaptation is fine-tuning on labeled domain-specific data or continued pre-training (Gururangan et al., 2020) on unlabeled domain-specific data. Although effective, both fine-tuning and continued pre-training methods require tuning all the parameters of a PLM, raising high costs beyond many institutions' reach. To mitigate this, multiple parameter-efficient fine-tuning (PEFT) methods are proposed, including prompt-based tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . However, they are more concerned about task adaptation and it is still unclear how to regularly, and inexpensively inject domain knowledge into PLMs for different domain-specific tasks. Moreover, directly tuning PLMs on a domain-specific corpus with PEFT methods will lead to the catastrophic forgetting problem (Yogatama et al., 2019; Gururangan et al., 2020) . These limitations highlight an important research question: how to adapt PLMs with the new domain knowledge while keeping the old-domain knowledge unmodified?\nInspired by the recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022 ) that found knowledge is stored in feed-forward networks (FFNs), we decouple the FFNs into two parts: the original pre-trained FFNs to maintain the olddomain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Specifically, we propose Mixture-of-Domain-Adapters (MixDA), a mixture of several domain adapters to inject domain-specific knowledge without affecting the old-domain knowledge. Our model has two stages: piq domain-specific tuning multiple knowledge adapters on unlabeled data and then piiq task-specific tuning adapters on labeled data. In the first stage, we train several domain adapters on both domain-specific corpus and pre-training corpus simultaneously while keeping the original feed-forward networks unchanged. In the second stage, we train a mixture-of-adapters gate to dynamically select the desired knowledge adapter and a task-specific adapter for task adaptation.\nWe conduct experiments on a broad range of tasks, including 4 out-of-domain datasets, 9 in-domain datasets, and 2 knowledge-intensive datasets. Our experimental results demonstrate the effectiveness of MixDA on 15 datasets, spanning biomedical, computer science publications, news, and reviews. Further analysis displays three key properties of our proposed approach: piq Reliability: it shows superior performance on both in-domain and out-of-domain tasks. piiq Scalability: it scales well to the increasing number of domains. piiiq Efficiency: it adds only a small number of parameters per domain. We claim that these properties are helpful for language models as a service, where a frozen PLM is served, and multiple adapters are inserted to support different customized services.\n\nRelated Work\nIn this section, we will review four research lines related to injecting domain knowledge into pretrained language models: knowledge injection, domain adaptation, parameter-efficient fine-tuning, and mixture-of-adapters.\n\nKnowledge Injection\nKnowledge can be injected into PLMs by pretraining or fine-tuning, each corresponding to a separate research direction. During pre-training, the knowledge carried by knowledge graphs (Zhang et al., 2019; He et al., 2020) , entities (Sun et al., 2019; Xiong et al., 2020) , n-grams (Diao et al., 2020) , knowledge embedding (Wang et al., 2021b) , synonym and hyponym-hypernym relations in WordNet (Lauscher et al., 2019) , word-supersense knowledge (Levine et al., 2020) , and knowledge bases (Peters et al., 2019) can be injected into PLMs by feeding knowledge inputs and designing new objectives. However, pre-training-based methods are costly, making the application to huge PLMs (e.g., models with 175 Billion parameters) impossible. Fine-tuning-based methods only require an additional fine-tuning process. Some studies inject extra information into the input sentences, like knowledge triples from knowledge graphs (Liu et al., 2020) and knowledge context (Faldu et al., 2021) , while other studies explored specific model and training designs, like knowledge adapter networks (Wang et al., 2021a) , graph convolutional networks and LSTMs (Lin et al., 2019) , and metalearning (Sinitsin et al., 2020) . Zhu et al. (2020) formulated knowledge injection as a constrained optimization problem by adding a constraint on the loss on the unmodified facts. Recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) reveal that knowledge is stored in the feed-forward networks in PLMs. Inspired by these studies, we propose a new efficient tuning method to inject domain knowledge into feed-forward networks with minimal costs.\n\nDomain Adaptation\nPrevious studies have observed that language models suffer from a significant performance drop during the domain shift (Beltagy et al., 2019; Alsentzer et al., 2019; Huang et al., 2019; Lee et al., 2020; Ke et al., 2022b) . Effective strategies that can bridge the domain gap are introduced. Pre-training language models from scratch is effective but costly, like SciBERT (Beltagy et al., 2019) , BioBERT (Lee et al., 2020) , and ClinicalBERT (Alsentzer et al., 2019) . Recent studies explored continued pretraining (Gururangan et al., 2020) and adapter networks (Diao et al., 2021) to save time by training on unlabeled downstream task data. In this paper, we introduce plug-in domain adaptors for domain adaptation, which are effective and mitigate catastrophic forgetting issues because of the explicit learning strategy and efficient model architecture.\n\nParameter-Efficient Fine-tuning\nAnother relevant research direction is parameterefficient fine-tuning (PEFT), which only fine-tunes a small number of parameters. Existing works solve this problem from two perspectives: promptbased tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . Several works in adapter-based tuning are closely related to ours. AdapterFusion (Pfeiffer et al., 2021) aims to combine multiple task adapters but does not offer specific architecture or training strategies to learn external knowledge. DEMix (Gururangan et al., 2022) and MixDA both train adapters that specialize in domains and use mechanisms to route different adapters, but differ in routing methods, base models, and training strategies. K-Adapter (Wang et al., 2021a ) is re-stricted by its training on T-REx triples and lacks the flexibility to train on unstructured knowledge. Similar to MixDA, CPT (Ke et al., 2022a) integrates domain knowledge into LMs, but it employs a different approach. While MixDA uses domain adapters to substitute FFN layers and task adapters to perform end tasks, CPT adds CL-Plugins that learn domain knowledge. Recent work by He et al. (2021a) presents a unified framework that establishes connections across different PEFT methods. Our work can leverage any PEFT method and complement them.\n\nMixture-of-Experts\nMixture-of-Experts (MoE) (Shazeer et al., 2017) is introduced with several expert networks, gating networks, and load-balancing techniques. The following studies improve MoE on initialization and training schemes (Fedus et al., 2022) , routing mechanisms (Zuo et al., 2021; Yang et al., 2021) , and load-balancing issues (Lewis et al., 2021; Roller et al., 2021) . AdaMix (Wang et al., 2022) proposed a mixture of adapters to improve the downstream task performance. Instead of mixing different designs of adapters, our domain adapter is a feedforward network specifically designed for domain knowledge.\n\nApproach\nGiven a pre-trained language model M, the input is a sentence X \" t 1 t 2 \u00a8\u00a8\u00a8t i \u00a8\u00a8\u00a8t T (t i indicates the i-th token) and the output is the representation of each token. The overall architecture of our model is shown in Figure 1 . The training process is divided into two-stage. In Stage 1 (Figure 1 (a)), we inject new feed-forward networks (FFNs) (namely domain-adapter) paralleled to the original pre-trained FFNs in some Transformer layers, acting as a key-value memory. The newly injected domain-adapter is trained on both domain-specific unlabeled data and original pre-training unlabeled data to store new factual associations while keeping old-domain ones. All modules are frozen except domain-adapter in this stage. In Stage 2 (Figure 1 (b)), we train a mixture-of-adapters (MoA) gate and a task-adapter on downstream tasks with labeled data, and only these two new modules are updated. The MoA gate receives outputs from the old-domain FFNs and domain-adapter, then outputs a weighted sum of them. An additional taskadapter is inserted in each Transformer block to facilitate downstream tasks. Figure 1 (c) shows the structures of the domain-adapter and the MoA gate.\nIn this section, we first introduce domainadapter, which learns and stores domain-specific knowledge, and then describe task-adapters that perform the downstream task. Finally, we discuss how the MoA gate integrates the outputs from the FFN and the domain-adapter.\n\nDomain-Adapter\nPrevious studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) suggest that factual associations are stored in the FFNs of some Transformer layers. To help models learn domain-specific knowledge, we propose a lightweight domain-adapter that works parallel to the FFNs, and a training method to learn domain-specific knowledge alongside keeping old-domain ones. Domain-adapter has a simple bottleneck architecture consisting of a down projection layer, a nonlinearity (such as ReLU (Agarap, 2018)), and an up projection layer. This helps keep the parameter size low (Houlsby et al., 2019) with competitive performance.\nIn Stage 1, the domain-adapter is trained with the domain-specific and old-domain datasets in one batch. Note that all other parameters are frozen except the domain-adapter at this stage. Let L K denote the knowledge loss related to domain-specific knowledge, and L S denote the sampling loss related to old-domain knowledge. The knowledge loss is a cross-entropy loss on predicting masked tokens, and the sampling loss is designed to align the latent spaces of the old-domain knowledge and new domain-specific knowledge. The total loss L is given by a weighted sum of the two, that is:\nEQUATION\nwhere \u03bb is a weight for the knowledge loss.\nThe knowledge loss is implemented by using cross-entropy loss. Given a sentence with M mask tokens whose answers are m 1 , m 2 , \u00a8\u00a8\u00a8, m M , respectively, the knowledge loss L K is given by\nEQUATION\nwhere ppm i q is the probability for token m i output by M. 2016)), we translate each relation into a sentence, and then mask out its object. For example, the relation \"the Eiffel tower-/r/LocatedAt-Paris\" is translated into \"The Eiffel Tower is located at Paris.\", then \"Paris\" is substituted with the mask token, and the model is trained to fill the mask. \u201a Unstructured knowledge For unstructured knowledge (e.g., downstream unlabeled texts), we use the masked language model (MLM) similar to RoBERTa pretraining. Some tokens are randomly sampled from the input sentence and replaced with the special token <mask>, and the model is trained to predict the masked token. The cross-entropy loss is calculated to optimize the model. For old-domain knowledge and sampling loss, we train the model on general corpora including Wikipedia and BookCorpus (Zhu et al., 2015) . Specifically, for each batch, sentences randomly sampled from the dataset are input into the model. Given L layers that have domain-adapters installed, for each such layer l, we collect token representations from the FFN F l , and representations from the domain-adapter K l . The goal is to keep them as similar as possible. Thus, we calculate the sampling loss L S with L2 loss:\nL S \" 1 L L \u00ff l\"1 ||F l \u00b4Kl || 2 2 .\n(3)\n\nTask-Adapter\nAfter training domain-adapters, the model is aware of the domain knowledge, which is not directly related to downstream tasks though. Therefore, we add task-adapters on top of the domain-adapter to adapt to downstream tasks. For example, a domainadapter trained in biomedical knowledge can sup- \n\nMixture-of-Adapters Gate\nOn downstream tasks, it is possible that the output from the FFN, or a weighted sum of the two, produces better results. Therefore, in Stage 2, we train an additional mixture-of-adapters (MoA) gate simultaneously. The MoA gate receives the outputs from the attention layer q, the domain-adapter K, and the FFN F . q is first sent into a multi-layer perceptron (MLP):\nEQUATION\n)\nThe MLP is composed of a down-projection layer W d and an up-projection layer W u , and h \" W u \u03c3pW d qq, where \u03c3 is the nonlinearity function.\nThen, h is input into a Sigmoid layer to generate the weights of the FFNs and other domain-adapters:\nw \" Sigmoidphq.\n(5)\nThe final output o is a weighted sum of the outputs of the FFNs and the domain-adapter:\nEQUATION\nwhere r; s denotes matrix concatenation.\n\nExperimental Settings\nIn this section, we first introduce the datasets, then the baseline models, the evaluation metrics, and implementation details in the following four subsections, respectively.\n\nDatasets\nWe conduct experiments on three types of datasets: in-domain (ID) tasks that require general-domain knowledge; out-of-domain (OOD) tasks that require domain-specific knowledge; knowledgeintensive (KI) tasks that require commonsense knowledge.\n\u201a ID: GLUE Benchmark (Wang et al., 2018) including MNLI (Williams et al., 2017) , CoLA (Warstadt et al., 2019) , MRPC (Dolan and Brockett, 2005) , SST-2 (Socher et al., 2013) , RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) , STS-B (Cer et al., 2017) , WNLI (Levesque et al., 2012) , QNLI (Rajpurkar et al., 2016) , and QQP (Iyer et al., 2017) . \u201a OOD: ChemProt (Kringelum et al., 2016) , RCT (Dernoncourt and Lee, 2017) , IMDB (Maas et al., 2011) , and Amazon (He and McAuley, 2016) . ChemProt is a manually annotated chemical-protein interaction dataset extracted from 5,031 abstractions. RCT is a dataset based on PubMed for sentence classification. IMDB provides 25,000 movie reviews for sentiment analysis. Amazon is a dataset containing product reviews from Amazon, annotated with user ratings. \u201a KI: FEVER (Thorne et al., 2018) and Common-senseQA (CSQA) (Talmor et al., 2019) . FEVER consists of 185,445 claims that correspond to Wikipedia articles and are classified as supported, refuted, and not enough information. Common-senseQA consists of 12,247 questions with 5 choices and requires commonsense knowledge to predict the correct answers.\nFor Stage 1, we train the domain-adapter with unstructured knowledge related to the dataset following Section 3.1. The unstructured knowledge used is listed in Table 1 . We also experiment with structured knowledge in Section 6.2. For Stage 2, we adopt the true few-shot setting following (Perez et al., 2021) to demonstrate the effectiveness of MixDA. For each dataset class, we randomly sample K \" 16 examples from the original training set as the new training set, and another different K \" 16 examples as the validation set. The original validation set will be used as the test set. The Pfeiffer adapter is used in Stage 2 unless stated otherwise.\n\nBaselines\nIn our experiments, we use the following models as the main baselines. For convenience, we refer to them with the abbreviations in the parentheses later. \u201a HOULSBY (HO): Houlsby adapter (Houlsby et al., 2019) Prefix-Tuning trains a number of prompt embeddings for each task and pre-pends it before tokens. \u201a FINE-TUNING (FT): Fine-tuning all of the parameters of the RoBERTa-large model on downstream tasks.\n\nEvaluation Metrics\nWe adopt the Pearson correlation for STS-B since it is a regression task. The remaining are text classification tasks. Following Wang et al. (2018) ; Gururangan et al. ( 2020); Diao et al. (2021) , we adopt macro-F1 for MRPC and QQP, and micro-F1 for others as evaluation metrics. Macro-F1 computes the F1 independently for each metric, while micro-F1 computes an average metric of all classes. To account for the instability of small datasets, we report the average performance and the standard deviation of 3 runs with different random seeds.\n\nImplementation\nWe implement our RoBERTa-large model based on the Transformers library from HuggingFace 2 . The Houlsby adapter, the Pfeiffer adapter, and Prefix-Tuning are implemented based on the adaptertransformers library (Pfeiffer et al., 2020a) . LoRA is implemented based on OpenDelta (Ding et al., 2022) . During Stage 1, we train the domain-adapter with learning rate 1e-4, batch size 20, and weight decay 0.05. The knowledge loss factor \u03bb is set to 0.5. We train the 7 and 11 layers of RoBERTa-large with domain-adapter in 10 epochs. In Stage 2, we use the Pfeiffer adapter as the default task-adapter and train 20 epochs. All the experiments are conducted on Nvidia 2080Ti GPUs. We find the best hyper-parameters through grid search and the best results are listed in Appendix A. The computation time can be found in Appendix B.\n\nExperimental Results\nWe compare the performance of MixDA with our baselines on 15 datasets. First, we train the domainadapter for each domain individually and then perform each task with its corresponding domainadapter, which shows significant improvement over our baselines. Next, we plug in several domainadapters trained on different domains parallelly to verify the scalability of our model. detect the chemical-protein interaction. For example, MixDA shows more familiarity with words associated with that field, such as \"gefitinib\" and \"tyrosine kinase inhibitor\". In contrast, MixDA falters on STS-B, falling behind Pfeiffer by 0.8%. This is because the knowledge in Stage 1 is not effectively utilized. STS-B consists of sentence pairs like \"The cat sat on the mat\" and \"The cat did not sit on the mat\", with little need for additional knowledge. Across the three task domains, MixDA has an average improvement of 4.8% over RoBERTa + Pfeiffer on out-of-domain tasks, 2.5% on indomain tasks, and 5.0% on knowledge-intensive tasks. It shows that MixDA is not only effective for out-of-domain tasks and knowledge-intensive tasks that require additional knowledge but is helpful for general-domain language tasks as well, demonstrating its ability to excel at both in-domain and out-of-domain tasks (reliability).\n\nParallel Domain Adapters\nIn the previous section, we explored using a single domain-adapter for each downstream task. Next, we show the scalability of MixDA by using parallel domain-adapters and only train the MoA layer and task-adapters in Stage 2. The training process in Stage 2 follows the previous experiments. Table 3 shows the comparison across single domainadapter, parallel domain-adapters, and RoBERTa + Pfeiffer on 7 datasets. On average, parallel domainadapters show an improvement of 0.6% over vanilla RoBERTa + Pfeiffer, even though they fall behind the single domain adapter by 1.9%. This could be attributed to the MoA gate choosing the suboptimal domain-adapter for some test data. Still, considering its improvement over Pfeiffer, the MoA gate chooses the correct domain-adapter in most cases. Therefore, MixDA demonstrates its scalability, allowing end users to train Stage 1 on different datasets and combine them later. Overall, in both single and parallel situations, MixDA significantly improves upon the vanilla RoBERTa + Pfeif- fer model with a small increase in model size. This is due to the ability of MixDA to capture knowledge and the MoA to select useful knowledge for downstream tasks.\n\nAnalysis\nIn this section, we analyze the respective contributions of each part of MixDA through detailed analysis, including the Stage 1 training, task-adapters in Stage 2, and the mixture-of-adapters gate.\n\nAblation Study\nIn this section, we conduct an ablation study to reveal the contributions of each part of the model. There are three variants: (1) We remove the MoA gate and choose the domain-adapter instead of the RoBERTa feed-forward layer (-MoA). ( 2 \n\nStructured and Unstructured Knowledge\nIn Section 5, the MixDA is only trained on unstructured knowledge. As a comparison, we also train the domain adapter on ConceptNet, a structured knowledge dataset, and then attach both the unstructured and structured to our model and train the MoA layer and the task-adapter during Stage 2.\nTable 5 shows the result of combining structured and unstructured knowledge in Stage 1. FEVER and CSQA, two knowledge-intensive tasks, have the greatest improvement: 10.3% for FEVER and 1.2% for CSQA. This is because ConceptNet stores commonsense knowledge that can help both tasks. Meanwhile, MRPC and STS-B also obtain improvement, showing that ConceptNet can benefit general language tasks as well. In conclusion, the experiment demonstrates the ability of MixDA to utilize structured knowledge, the extensibility of our model, and the possible benefits of structured knowledge.\n\nEffectiveness of Task-Adapters\nIn most experiments of this paper, we adopt Pfeiffer as the task-adapter unless otherwise specified. In this section, we test the performance of MixDA combined with other kinds of task-adapters, including Houlsby, Prefix-Tuning, LoRA, and Pfeiffer. parameters compared to Houlsby, making it the optimal choice of task-adapters in our experiment.\n\nConclusion\nIn this paper, we proposed MixDA, a mixture of adapters for domain adaptation. We first decouple the knowledge modules (i.e., FFNs) into the old-domain and domain-specific FFNs. Then we propose a two-stage adapter tuning strategy: first tuning the domain adapter on each domain and then tuning the task adapter on each task. Moreover, our model could be scaled to multiple domains easily with the introduction of the mixture-of-adapters gate. Empirically, MixDA achieved significant improvement over in-domain tasks, out-of-domain tasks, and knowledge-intensive tasks. Further analyses demonstrate the reliability, scalability, and efficiency of our method.\n"}
{"question": "What is our main contribution?", "evidence": "  In this paper, we propose a novel gradient tradeoff multi-task learning approach to mitigate the task conflict problem, which can achieve a specific trade-off among different tasks nearby the main objective of multi-task text classification problem. Moreover, we present a series of theoretical proofs to illustrate the effectiveness and superiority of our GetMTL. Experimental results on two benchmark datasets show that our GetMTL achieves state-ofthe-art performance in Multi-task Text Classification problem.\n ", "options": ["A. We introduce a novel approach known as gradient tradeoff multi-task learning to address the issue of task conflict.", "B.  we prove the effectiveness and superiority of our GetMTL. ", "C.  We deal with some Multi-task Text Classification problems.", "D. We develop current theories."], "answer": "A", "content": "\nIntroduction\nMulti-task Learning (MTL), which aims to learn a single model that can tackle multiple correlated but different tasks simultaneously, makes multiple tasks benefit from each other and obtain superior performance over learning each task independently (Caruana, 1997; Ruder, 2017; Liu et al., 2015; Mao et al., 2020) . By discovering shared information/structure across the tasks, it has gained attention in many areas of research and industrial communities, such as computer vision (Misra et al., 2016; Gao et al., 2019; Yogamani et al., 2019; Sun et al., 2020 ) and text classification (Liu et al., 2017; Xiao et al., 2018; Mao et al., 2021 Mao et al., , 2022)) .\nHowever, it is observed in multi-task text classification (MTC) scenarios that some tasks could conflict with each other, which may be reflected via conflicting gradients or dominating gradients (Yu et al., 2020; Vandenhende et al., 2022) , leading to the degraded performance of MTL due to poor training. How to make a proper trade-off among jointing different tasks in MTC is a difficult problem. Recently, several methods have been proposed to mitigate gradient conflicts issue via both loss balance (linear weighted scalarization) such as homoscedastic uncertainty (Kendall et al., 2018) and task variance regularization (Mao et al., 2021) , and gradient balance like Pareto optimality (Sener and Koltun, 2018; Mao et al., 2020) . Existing methods devote to finding an arbitrary Pareto optimality solution in the Pareto set, which achieve a single arbitrary trade-off among all tasks. However, they can only satisfy the improved performance on part of tasks, not all tasks simultaneously. This means that these methods can not converge to a minimum average loss of all objectives.\nTo illustrate our idea, we give a two-task learning example shown in Figure 1 . As shown in Figure (1a) , it is observed that Pareto optimality-based methods can generate a set of Pareto solutions for a given two-task learning problem. However, some of Pareto solutions can increase the task 1 error while decreasing task 2 error, leading to unsatisfactory overall performance for MTL model. This im-plies that not all Pareto solutions always satisfy the goal of mitigating the tasks conflicts in MTL, and thus failing to achieve a better trade-off between tasks. Therefore, it is necessary to find a specific trade-off between tasks that is beyond what only using Pareto optimality can achieve.\nTo address this issue, inspired by multi-objective optimization (Sener and Koltun, 2018) , we argue that a more efficient way to mitigate task conflicts is to find a gradient trade-off between tasks in the neighborhood of the average loss rather than exhaustively searching for a proper solution from the set of Pareto solutions. As shown in Figure 1b , the Pareto solutions nearby the average loss can achieve a better trade-off between task 1 and task 2, leading to better performance on both tasks at the same time. Based on it, in this paper, we propose a novel gradient trade-off multi-task learning approach, named GetMTL, to mitigate task conflicts in multi-task text classification. Specifically, the gradients of each task are utilized to derive an update vector that can minimize the conflicts among task gradients in the neighborhood of the average gradient, so as to achieve a better trade-off performance among joint training tasks. In summary, the main contributions of our work are as follows:\n\u2022 A novel multi-task learning approach based on gradient trade-off between different tasks (GetMTL) is proposed to deal with task conflict in multi-task text classification problems, so as to improve the performance of all tasks simultaneously. \u2022 We give in-depth theoretical proofs and experimental analyses on establishing converge guarantees of our GetMTL. \u2022 We extensively verify the effectiveness of our GetMTL on two real-world text classification datasets, and the results show that our GetMTL performs competitively with a variety of state-of-the-art methods under a different number of task sets.\n\nRelated Works\nMulti-task Learning methods jointly minimize all task losses based on either loss balance methods (Kendall et al., 2018; Chen et al., 2018; Mao et al., 2021 Mao et al., , 2022) ) or gradient balance methods (Sener and Koltun, 2018; Mao et al., 2020) .\nThe loss balance methods adaptively adjust the tasks weights during training based on various heuristic approaches, such as task uncertainty quan-tification (Kendall et al., 2018) , gradient normalization (Chen et al., 2018) , task difficulty prioritization (Guo et al., 2018) , dynamic weight average (Liu et al., 2019) , random loss weighting (Lin et al., 2021) , task variance regularization (Mao et al., 2021) , and meta learning-based approach (Mao et al., 2022) . These methods are mostly heuristic and can have unstable performance while ignoring the task conflicts among all tasks, leading to the bad generalization performance of MTL models.\nRecently, some gradient balance based methods have been proposed to mitigate task conflicts for improving task performance. For example, D\u00e9sid\u00e9ri (2012) leverages multiple-gradient descent algorithm (MGDA) to optimize multiple objectives. Due to the guarantee of convergence to Pareto stationary point, this is an appealing approach. Sener and Koltun (2018) cast the multi-objective problem as a multi-task problem and devote to finding an arbitrary Pareto optimal solution. Mao et al. (2020) propose a novel MTL method based Tchebycheff procedure for achieving Pareto optimal without any convex assumption. However, these methods only consider achieving an arbitrary Pareto optimal solution while it is not the main objective. Unlike these methods, we propose an MTL approach based on multi-objective optimization and seek to find a set of solutions that are Pareto optimality and nearby the main MTC objective L 0 .\n\nPreliminaries\nConsider a multi-task learning problem with T 1 tasks over an input space X and a collection of task spaces {Y t } t\u2208 [T ] , where each task contains a set of i.i.d. training samples\nD t = {x i , y t i } i\u2208[nt] ,\nT is the number of tasks, and n t is the number of training samples of task t. The goal of MTL is to find parameters {\u03b8 sh , \u03b8 1 , ..., \u03b8 T } of a model F that can achieve high average performance across all training tasks over X , defined as F(X , \u03b8 sh , \u2022 \u2022 \u2022 , \u03b8 t ) : X \u2192 Y, where \u03b8 sh denotes the parameters shared between tasks and \u03b8 t denotes the task-specific parameters of task t. In particular, we further consider a parametric taskspecific map as f t (\u2022, \u03b8 sh , \u03b8 t ) : X \u2192 Y t . We also consider task-specific loss functions t (\u2022, \u2022) : Y t \u00d7 Y t \u2192 R + . We also denote the multi-task loss as L(\u03b8) = T i i (\u03b8), and the gradients of each task as g i = \u2207 i (\u03b8) for the particular \u03b8. In this paper, we choose the average loss as main objective of MTC problem, defined as L 0 (\u03b8) = 1 T T i i (\u03b8).\n\nMTL as Multi-objective Optimization\nMTL can be formulated as a specific case of multiple-objective optimization (MOO), which optimizes a set of potentially conflicting objectives (Sener and Koltun, 2018; Mao et al., 2020) . Given objective functions of T tasks, 1 , . . . , T , we formulate the optimization objective of MTL as the vectors of objective values :\nmin \u03b8 sh ,\u03b8 1 ,...,\u03b8 T (\u03b8 sh , \u03b8 1 ), . . . , (\u03b8 sh , \u03b8 T ) (1)\nSince there is no natural linear ordering on vectors, it is not possible to compare solutions and thus no single solution can optimize all objectives simultaneously. In other words, there is no clear optimal value. Alternatively, we can achieve Pareto optimality to obtain different optimal trade-offs among all objectives to solve MOO problem.\nDefinition 1 (Pareto dominance). Given two points {\u03b8, \u03b8} in \u2126, a point \u03b8 Pareto dominates \u03b8 (\u03b8 \u03b8) for MTL if two conditions are satisfied:\n(i) No one strictly prefers \u03b8 to \u03b8, that is, \u2200i \u2208 {1, . . . , T }, i (\u03b8 sh , \u03b8 i ) \u2264 i (\u03b8 sh , \u03b8 i ).\n(ii) At least one point strictly prefers \u03b8 to \u03b8, that is, \u2203j \u2208 {1, ..., T }, j (\u03b8 sh , \u03b8 j ) < j (\u03b8 sh , \u03b8 j ).\nDefinition 2 (Pareto optimality). \u03b8 * is a Pareto optimal point and (\u03b8 * ) is a Pareto optimal objective vector if it does not exist \u03b8 \u2208 \u2126 such that \u03b8 \u03b8 * . That is, a solution that is not dominated by any other is called Pareto optimal.\nThe set of all Pareto optimal solutions is called the Pareto set, and the image of Pareto set in the loss space is called Pareto front (Lin et al., 2019) . In this paper, we focus on gradient-based multiobjective optimization to achieve an appropriate Pareto trade-off among all tasks, which can approximate the Pareto front that minimizes the average loss.\n\nGradient-based Multi-Objective Optimization\nGradient-based MOO (Sener and Koltun, 2018) aims to find a direction d that we can iteratively find the next solution \u03b8 (t+1) that dominates the previous one \u03b8 (t) ( (\u03b8 (t+1) ) \u2264 (\u03b8 (t) )) by moving against d with step size \u03b7, i.e. \u03b8 (t+1) = \u03b8 (t) \u2212 \u03b7d. D\u00e9sid\u00e9ri (2012) ; Sener and Koltun (2018) propose to use multiple gradient descent algorithm (MGDA) that converges to a local Pareto optimal by iteratively using the descent direction d, which can be obtained as follows:\nd * = arg min d\u2208R m ,\u03b1\u2208R \u03b1 + 1 2 d 2 s.t. \u2207 i (\u03b8 (t) ) T d \u2264 \u03b1, i = 1, ..., T.\n(\nEQUATION\nwhere d * is the direction that can improve all tasks. Essentially, gradient-based MOO methods minimize the loss by combining gradients with adaptive weights, and obtaining an arbitrary Pareto optimality solution, ignoring the true objective (the average loss) (Liu et al., 2021) . In this paper, we generalize this method and propose a novel gradient-based approach to achieve a gradient trade-off among tasks for mitigating task conflicts, as well as constrain the solution that can minimize the average loss (L 0 (\u03b8)).\n\nGradient Trade-offs for Multi-task Text Classification\nFollowing most MTL methods, as shown in Figure 2 , we employ the hard parameter sharing MTL architecture, which includes f sh parameterized by heavy-weight task-shared parameters \u03b8 sh and f t parameterized by light-weight task-specific parameters \u03b8 t . All tasks take the same shared intermediate feature z = f sh (x; \u03b8 sh ) as input, and the t-th taskspecific network outputs the prediction as f t (z; \u03b8 t ).\nSince task-shared parameters \u03b8 sh are shared by all tasks, the different tasks may conflict with each other, leading to the degraded performance of MTL model. In this paper, we hypothesize that one of the main reasons for task conflicts arises from gradients from different tasks competing with each other in a way that is detrimental to making progress.\nWe propose a novel gradient-based MOO optimization to find a gradient trade-off among tasks in the neighborhood of the average loss, so as to mitigate task conflicts. Note that, we omit the subscript sh of task-shared parameters \u03b8 sh for the ease of notation.\n\nGetMTL\nGiven a task i, we define its gradient as g i = \u2207 i (\u03b8) via back-propagation from the raw loss i , and g i represents the optimal update direction for task i. However, due to the inconsistency of the MOO method and our GetMTL on three gradients (g 1 , g 2 and g 3 ) in R 3 , where g i denotes the gradient (black) of i-th task, g 0 is the average gradient, and blue arrows denote the projections of update direction to each task gradient.\noptimal update direction of task-shared parameters for each task, different task gradients may conflict with each other, leading to the training of networks being stuck in the over-training of some tasks and the under-training of other tasks. Intuitively, it is desirable to find a direction that can minimize the task conflicts among different tasks as well as achieve Pareto optimality to improve the performance of MTL model. We first achieve an arbitrary Pareto optimal via finding a descent direction d des by searching for a minimum-norm point in the Convex Hull CH of gradients, defined by,\nEQUATION\ns.t. S T = \u03b2 \u2208 R T + T j=1 \u03b2 j = 1 (4)\nwhere G \u2208 R T \u00d7m = {g 1 , ..., g T } is the matrix of task gradient, S T is the T -dimensional regular simplex. We use the multiple gradient descent algorithm (MGDA) (Sener and Koltun, 2018) to obtain an arbitrary Pareto optimal by iteratively using the descent direction, defined by,\nd des = arg min d\u2208CH d 2 2\n(5)\nIn addition, the d des can be reformulated as a linear combination of all task gradients, defined by,\nd des = T i=1 \u03b2 i g i (6)\nwhere g i = \u2207 i (\u03b8) is the i-th task gradient. It implies that, when converges to an arbitrary Pareto optimal, the optimal gradient value of each task via back-propagation is \u03b2 i g i , defined as\ng \u03b2 i = \u03b2 i g i .\nHowever, moving against d des does not guarantee that the solution meets the requirements of multi-task text classification task (MTC), that is, to alleviate the gradient conflict among tasks in MTC, so as to improve the performance of all tasks. To address this issue, we seek a direction that enables us to move from a solution \u03b8 (t) to \u03b8 (t+1) such that both \u03b8 (t+1) dominates \u03b8 (t) (L(\u03b8 (t+1) ) \u2264 L(\u03b8 (t) )) and alleviate the gradient conflict among all tasks. Based on it, as shown in Figure 2 (b), we propose to search for an update direction d in the Convex Hull CH \u03b2 of back-propagation gradients such that it can improve any worst objective and converge to an optimum of MTC objective L 0 (\u03b8). We first find the worst task gradient with respect to the update direction d, that is, it has a maximum angle with d, which can be formulated via the following optimization problem,\nEQUATION\nwhere g \u03b2 i is the i-task gradient after optimizing by MGDA algorithm.\nTo improve the worst gradient of any task and achieve a trade-off between all task gradients in a neighborhood of the average gradient (defined as g 0 = 1 T T i=1 g i ), we formulate this gradient trade-off optimization problem via the following Maximin Optimization Problem (dual problem).\nProblem 1.\nmax d\u2208R m min i\u2208[T ] g \u03b2 i , d s.t. d \u2212 g 0 \u2264 \u03b5g T 0 d, \u2212 g T 0 d \u2264 0 (8)\nwhere g \u03b2 i = \u03b2 i g i is the back-propagation gradient value of i-th task via solving Eq. ( 5), \u03b5 \u2208 (0, 1] is a hyper-parameter that controls the stability of MTC model.\n\nSolving Maximin Problem\nSince the optimal direction d can also be defined in the convex hull CH \u03b2 of g \u03b2 i , we can get\nEQUATION\nwhere\nG \u03b2 \u2208 R T \u00d7m = {g \u03b2 1 , ..., g \u03b2 T } is task gradi- ent matrix, W T = { w \u2208 R T + T j=1 w j = 1} is the T -dimensional\nprobability simplex, and w = (w 1 , ..., w T ). Therefore, we can get min i g \u03b2 i , d = min w\u2208W T i w i g \u03b2 i , d and Problem 1 can be transformed into the following form.\nAlgorithm 1: GetMTL Algorithm.\nInput: The number of task T , loss functions { i } T i=1 , network parameters \u03b8 (t) at t step, the pre-specified hyper-parameter \u03b5 \u2208 (0, 1] and step size \u00b5 \u2208 R + . 1: Task Gradients:\ng i = \u2207 i (\u03b8 (t) ), i \u2208 [T ] 2: Main Objective: g 0 = T i=1 g i 3:\nObtain {\u03b2 1 , ...\u03b2 T } by solving Eq.( 5). 4: Compute g w = i w i g \u03b2 i , where g \u03b2 i = \u03b2 i g i 5: Obtain {w 1 , ..., w T } by solving Eq.( 14) 6: Find direction d * by using Eq.( 13)\nEQUATION\nwhere g w = T i=1 w i g \u03b2 i is the convex combination in CH \u03b2 . For a given vector \u03bb \u2208 R + with non-negative components, the corresponding Lagrangian associated with the Eq.( 10) is defined as\nEQUATION\n11) Since the objective for d is concave with linear constraints and w \u2208 W T is a compact set 2 , according to the Sion's minimax theorem (Kindler, 2005) , we can switch the max and min without changing the solution of Problem 2. Formally,\nmin \u03bb,w\u2208W T max d\u2208R m g T w d \u2212 \u03bb d \u2212 g 0 2 /2 + \u03bb\u03b5 2 (g T 0 d) 2 /2 (12)\nWe get the optimal solution of primal problem (Problem 1) by solving the dual problem of Eq.( 12) (See the Appendix A for a detailed derivation procedure). Then we have\nd * = g w + \u03bb * g 0 (1 \u2212 \u03b5 2 g 2 0 )\u03bb * , where \u03bb * = g w \u03b5 g 0 2 (13)\nwhere \u03bb * is the optimal Lagrange multiplier, d * is the optimal update direction of MTC model. We can reformulate the problem of Eq.( 12) as following optimization problem w.r.t. w.\nEQUATION\n2 Compact set: a set that is bounded and closed. where g w is defined as\ng w = T i=1 w i g \u03b2 i .\nThe detailed derivation is provided in Appendix A. Algorithm 1 shows all the steps of GetMTL algorithm in each iteration.\n\nTheoretical Analysis\nIn this section, we analyze the equivalence of solutions to dual problem and then give a theoretical analysis about convergence of GetMTL algorithm. We define the Lagrangian of problem in Eq.( 10), Theorem 4.2 (Convergence of GetMTL). Assume loss functions i are convex and differential, and \u2207 i (\u03b8 (t) ) is L-lipschitz continuous with L > 0. The update rule is \u03b8 (t+1) = \u03b8 (t) \u2212 \u00b5 (t) d, where d is defined in Eq.( 13) and\nL(d, \u03bb, w) = g T w d \u2212 \u03bb 2 ( d \u2212 g 0 2 \u2212 \u03b5 2 (g T 0 d) 2 ) Theorem 4.1 (Equivalence of\n\u00b5 (t) = min i\u2208[k] d\u2212g 0 c\u2022L\u2022d 2 . All the loss functions 1 (\u03b8 (t) ) \u2022 \u2022 \u2022 T (\u03b8 (t) ) converges to ( 1 (\u03b8 * ) \u2022 \u2022 \u2022 T (\u03b8 * )).\nProof. The proof is provided in Appendix C.\n\nExperimental Datasets\nWe conduct experiments on two MTC benchmarks to evaluate the proposed GetMTL. 1) Amazon Review dataset (Blitzer et al., 2007) contains product reviews from 14 domains (See Details in Appendix D), including apparel, video, books, electronics, DVDs and so on. Each domain gives rise to a binary classification task and we follow Mao et al. (2021) to treat 14 domains in the dataset as distinct tasks, creating a dataset with 14 tasks, with 22180 training instances and 5600 test instances in total. 2) Topic classification dataset, 20 Newsgroup 3 , consists of approximately 20,000 newsgroup documents, partitioned evenly across 20 different newsgroups. We follow Mao et al. (2021) to select 16 newsgroups from 20 Newsgroup dataset shown in Table 1 and then divide them into four groups. Each group gives rise to a 4-way classification task, creating a dataset with four 4-way classification tasks, which is a more challenging dataset than amazon review dataset.\n\nExperimental Implementation\nWe follow the standard MTC setting and adopt the same network architectures with the most recent baselines for fair comparisons (Mao et al., 2021) . We adopt the hard parameter sharing MTL framework shown in Figure 2 , where task-shared network is a TextCNN with kernel size of 3,5,7 and taskspecific network is a fully connected layer with a softmax function. Adam is utilized as the optimizer to train the model over 3000 epochs with a learning rate of 1e-3 for both sentiment analysis and topic classification. We set the batch size to 256. \n\nComparison Models\nWe compare the proposed GetMTL with a series of MTC baselines, including Single-Task Learning (STL): learning each task independently.\nUniform Scaling: learning tasks simultaneously with uniform task weights.\nUncertainty: using the uncertainty weighting method (Kendall et al., 2018) .\nGradNorm: learning tasks simultaneously with gradient normalization method (Chen et al., 2018) .\nTchebycheffAdv: using adversarial Tchebycheff procedure (Mao et al., 2020) .\nMGDA: using gradient-based multi-objective optimization method (Sener and Koltun, 2018) .\nBanditMTL: learning tasks simultaneously with multi-armed bandit method (Mao et al., 2021) .\nMetaWeighting: using adaptive task weighting method (Mao et al., 2022) .\n\nMain Results\nThe main comparison results of GetMTL on two benchmark datasets are shown in Figure 3 and 4 . It is clear that (See detailed numerical comparison results in Appendix D), our proposed GetMTL model performs consistently better than the all comparison methods on all tasks of both amazon review and topic classification datasets, and its average performance is superior to that of all baselines. This verifies the effectiveness of our GetMTL method in MTC problem. More concretely, in comparison with the gradient-based MOO optimization model (MGDA), our GetMTL achieves significant improvement across all datasets. This indicates that achieving a gradient trade-off nearby average loss to mitigate task conflicts can better improve all task performance and generalization ability of MTC model. \n\nEmpirical Analysis on Convergence\nIn Section 4.3, we theoretically prove the convergence of our proposed GetMTL. Furthermore, we conduct extensive experiments about the convergence to better demonstrate the advantages of GetMTL shown in Figure 5 . It is clear that the learning curve of GetMTL is constantly decreasing as the number of iterations increases and converges to the lowest loss value compared with other baselines. It indicates that GetMTL can guarantee the convergence of the objective value and obtain better performance of all learning tasks.\nIn addition, we also conduct extensive experiments to investigate how GetMTL mitigates task conflict during training. We plot the task variance (variance between the task-specific losses) of all baselines on both amazon review and topic classification datasets shown in Figure 6 . It can be observed that all MTL baselines have lower task variance than STL method, which illustrates that MTL methods can indeed boost the learning of all tasks compared with STL method. Moreover, GetMTL has the lowest task variance and smoother evolution during training than other MTL baselines. This implies that our proposed GetMTL indeed mitigates task conflicts compared with other MTL methods.\n\nThe Evolution of Task Weight w\nIn this section, we visualize the task weights of our GetMTL and two weight adaptive MTL methods (MGDA and BanditMTL) throughout the training process using the topic classification dataset shown in Figure 7 . It can be observed from these four figures that the weight adaption process of our GetMTL is different from that of MGDA and Ban-ditMTL. GetMTL can automatically learn the task weights without pre-defined heuristic constraints. The weights adaption process of GetMTL is more stable and the search space is more compact compared with other MTL baselines.\n\nImpact of the Values of \u03b5\nTo investigate the impact of using different values of \u03b5 on the performance of our GetMTL, we conduct experiments on two datasets, and the results are shown in Figure 8 . Noting that model with \u03b5 = 0.0075 and \u03b5 = 0.025 perform overall better than other values on these two datasets, respectively. The model with larger value of \u03b5 performs unsatisfactorily overall all tasks on two datasets, one possible reason is that larger \u03b5 makes d pull far away from the average loss g 0 (see the conditions in Eq. ( 9)). That is, Pareto optimality found by GetMTL is getting further and further away from MTC objective L 0 , which can be quite detrimental to some tasks' performance, leading to degraded average performance.\n\nConclusion\nIn this paper, we propose a novel gradient tradeoff multi-task learning approach to mitigate the task conflict problem, which can achieve a specific trade-off among different tasks nearby the main objective of multi-task text classification problem. Moreover, we present a series of theoretical proofs to illustrate the effectiveness and superiority of our GetMTL. Experimental results on two benchmark datasets show that our GetMTL achieves state-ofthe-art performance in Multi-task Text Classification problem.\n"}
{"question": "Which is not our study\u2019s importance?", "evidence": "Automatic evaluation is critical for the SG task since it enables efficient, systematic, and scalable comparisons between models in general (Celikyilmaz et al., 2020) . However, existing studies are inadequate for effective SG evaluation. Task-agnostic automatic metrics (Papineni et al., 2002; Zhang et al., 2019; Li et al., 2016) are widely adopted for SG evaluation (Zhang et al., 2021; Lai and Nissim, 2022) , which have several limitations: (1) The simile components should receive more attention than other words during SG evaluation (e.g. "he" and "wolf " in Fig. 1 ), while there are no automatic metrics that consider the key components. (2) The SG task is open-ended, allowing for multiple plausible generations for the same input (Chakrabarty et al., 2020 ) (e.g. the howling man can be compared to "wolf ", "buffalo", or "tiger" in Fig. 1 ). Hence, the metrics based on word overlap with a few references are inadequate to accurately mea-", "options": ["Automatic evaluation is critical for the SG task since it enables efficient, systematic, and scalable comparisons between models in general (Celikyilmaz et al., 2020) . However, existing studies are inadequate for effective SG evaluation. Task-agnostic automatic metrics (Papineni et al., 2002; Zhang et al., 2019; Li et al., 2016) are widely adopted for SG evaluation (Zhang et al., 2021; Lai and Nissim, 2022) , which have several limitations: (1) The simile components should receive more attention than other words during SG evaluation (e.g. \"he\" and \"wolf \" in Fig. 1 ), while there are no automatic metrics that consider the key components. (2) The SG task is open-ended, allowing for multiple plausible generations for the same input (Chakrabarty et al., 2020 ) (e.g. the howling man can be compared to \"wolf \", \"buffalo\", or \"tiger\" in Fig. 1 ). Hence, the metrics based on word overlap with a few references are inadequate to accurately mea-\n"], "answer": "A", "content": "\nIntroduction\nSimiles play a vital role in human expression, making literal sentences imaginative and graspable. For example, Robert Burns famously wrote \"My Luve is like a red, red rose\" to metaphorically depict the beloved as being beautiful. In this simile, \"Luve\" (a.k.a. topic) is compared with \"red rose\" (a.k.a. vehicle) via the implicit property \"beautiful\" and the event \"is\". Here, topic, vehicle, property, and event are four main simile components (Hanks, 2013) . As a figure of speech, similes have been widely used in literature and conversations (Zheng et al., 2019; Chakrabarty et al., 2022) .\nSimile generation (SG) is a crucial task in natural language processing (Chakrabarty et al., 2020; Zhang et al., 2021; Lai and Nissim, 2022) , with the aim of polishing literal sentences into similes. In Fig. 1 , the literal sentence \"He yelps and howls.\" is polished into a simile by inserting the phrase \"like a wolf \", resulting in \"He yelps and The commonly used automatic metric BLEU deems the second candidate as the most high-quality one among all the generated similes, while our proposed metrics HAUSER deem the first candidate as the best one regarding its quality, creativity and informativeness, which better correlates with human ratings and also provides more criteria for SG evaluation.\nhowls like a wolf \". The ability to generate similes can assist various downstream tasks, such as making the generations more imaginative in story or poet generation task (Tartakovsky and Shen, 2018; Chakrabarty et al., 2022) and the generated response more human-like in dialogue generation task (Zheng et al., 2019) .\nAutomatic evaluation is critical for the SG task since it enables efficient, systematic, and scalable comparisons between models in general (Celikyilmaz et al., 2020) . However, existing studies are inadequate for effective SG evaluation. Task-agnostic automatic metrics (Papineni et al., 2002; Zhang et al., 2019; Li et al., 2016) are widely adopted for SG evaluation (Zhang et al., 2021; Lai and Nissim, 2022) , which have several limitations: (1) The simile components should receive more attention than other words during SG evaluation (e.g. \"he\" and \"wolf \" in Fig. 1 ), while there are no automatic metrics that consider the key components. (2) The SG task is open-ended, allowing for multiple plausible generations for the same input (Chakrabarty et al., 2020 ) (e.g. the howling man can be compared to \"wolf \", \"buffalo\", or \"tiger\" in Fig. 1 ). Hence, the metrics based on word overlap with a few references are inadequate to accurately mea-\n\nCriterion Literal Sentence\nExample Simile Candidates Quality Relevance Some raindrops struck the roof, window and ran down its panes. Some raindrops struck the roof, window and ran down its panes (like tears | like arrows).\n\nLogical Consistency\nStefan moved, every movement easy and precisely controlled.\nStefan moved (like lightning | like a dancer), every movement easy and precisely controlled.\n\nSentiment Consistency\nThe idea resounded throughout the land. The idea resounded (like an earthquake | like a thunderous wave) throughout the land.\n\nCreativity\nHe possessed a power of sarcasm which could scorch.\nHe possessed a power of sarcasm which could scorch (like vitriol | like fire).\n\nInformativeness\nThey gleamed. They gleamed (like the eyes of a cat | like the eyes of an angry cat).\nTable 1 : Examples of our criteria for Simile Generation (SG) Evaluation. We design five criteria from three perspectives. The vehicles of the better simile candidates given by each criterion are highlighted in bold.\nsure the overall quality of generated similes. As shown in Fig. 1 , the commonly used metric BLEU deems the second candidate as the highest quality, as it has more overlapped words with the only referenced groundtruth, while human deems the first candidate as the most coherent one.\n(3) The existing metrics are inadequate to provide fine-grained and comprehensive SG evaluation, considering that the creative generation tasks have distinct criteria for desired generations (Celikyilmaz et al., 2020) , such as novelty and complexity for story generation (Chhun et al., 2022) and logical consistency for dialogue generation (Pang et al., 2020) . However, establishing a comprehensive, efficient, and reliable evaluation system for SG is nontrivial, which raises three main concerns: (1) What criteria should be adopted to evaluate the SG task in a comprehensive and non-redundant fashion? (2) How to quantify each criterion into a metric thus enabling efficient and objective SG evaluation, given that the human evaluation of creative generation task is not only time-consuming but also subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014; Celikyilmaz et al., 2020) ? (3) Whether the proposed metrics are effective in providing useful scores to guide actual improvements in the realworld application of the SG model?\nIn this paper, we establish HAUSER, a Holistic and AUtomatic evaluation system for Simile gEneRation task, consisting of five criteria (Tab. 1):\n(1) The relevance between topic and vehicle, as the foundation of a simile is to compare the two via their shared properties (Paul, 1970) . (2) The logical consistency between the literal sentence and generated simile, since the aim of SG task is to polish the original sentence without altering its semantics (Tversky, 1977) . (3) The sentiment consistency between the literal sentence and generated simile, since similes generally transmit certain sentiment polarity (Qadir et al., 2015) . (4,5) The creativity and informativeness of the simile, since novel similes or those with richer content can enhance the literary experience (Jones and Estes, 2006; Roncero and de Almeida, 2015; Addison, 2001) . Overall, these five criteria can be categorized into three perspectives: quality (which considers relevance, logical, and sentiment consistency jointly), creativity, and informativeness. We further quantify each criterion into automatic metrics (Fig. 2 ) and prove their effectiveness through extensive experiments.\nTo the best of our knowledge, we are the first to systematically investigate the automatic evaluation of the SG task. To summarize, our contributions are mainly three-fold: (1) We establish a holistic and automatic evaluation system for the SG task, consisting of five criteria based on linguistic theories, facilitating both human and automatic evaluation of this task. (2) We design automatic metrics for each criterion, facilitating efficient and objective comparisons between SG models. (3) We conduct extensive experiments to verify that our metrics are significantly more correlated with human ratings than prior metrics.\n\nSimile Generation Task\nThere are two primary forms of the simile generation (SG) task: simile triplet completion and literal sentence polishing. For simile triplet completion, a model receives simile components, topic and property, and is required to generate the vehicle (Roncero and de Almeida, 2015; Zheng et al., 2019; Chen et al., 2022; He et al., 2022) . For literal sentence polishing, a model receives a literal sentence and is expected to convert it into similes (Zhang et al., 2021; Stowe et al., 2020; Chakrabarty et al., 2020; Lai and Nissim, 2022) . We focus on the latter. However, prior works mainly adopt task-agnostic automatic metrics to evaluate the SG task, raising concern as to whether the claimed improvements are comprehensive and reliable.\n\nAutomatic Evaluation for NLG Systems\nExisting automatic metrics for Natural Language Generation (NLG) evaluation can be categorized into task-agnostic and task-specific metrics. Taskagnostic metrics can be applied to various NLG tasks, which generally focus on the coherence of generations (Papineni et al., 2002; Zhang et al., 2019) , including n-gram-based metrics (Papineni et al., 2002; Lin, 2004; Denkowski and Lavie, 2014) and embedding-based metrics (Zhang et al., 2019; Zhao et al., 2019) . There are also many metrics for evaluating the diversity of generations (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) . Task-specific metrics are proposed to evaluate NLG systems on specific tasks (Tao et al., 2018; Dhingra et al., 2019; Ren et al., 2020) . Specifically, various works systematically study the evaluation of the creative generation task (Pang et al., 2020; Tevet and Berant, 2021; Chhun et al., 2022) . Different from these works, we revisit SG evaluation, propose holistic criteria based on linguistic theories, and design effective automatic metrics for it.\n\nHAUSER for SG evaluation\nWe establish HAUSER, a holistic and automatic evaluation system for SG evaluation, containing five criteria from three perspectives, and further design automatic metrics for each criterion (Fig. 2 ).\n\nQuality\nWe measure the overall quality of generated similes using three criteria: relevance, logical consistency, sentiment consistency. The key simile components -topic and vehicle -should be relevant, as the foundation of a simile is to compare the two via their shared properties (relevance) (Paul, 1970) . In Tab. 1, comparing \"raindrops\" to \"tears\" is more coherent than to \"arrows\". Additionally, the generated simile should remain logically consistent with the original sentence (logical consistency), as the SG task aims to polish the plain text without changing its semantics (Tversky, 1977) . In Tab. 1, comparing \"Stefan\" to \"dancer\" better depicts his controlled and easy movement than to \"lightning\". Furthermore, as similes generally transmit certain sentiment polarity (Qadir et al., 2015) , the generated simile should enhance the sentiment polarity of the original sentence (sentiment consistency). In Tab. 1, the vehicle \"thunderous wave\" enhances the positive polarity of the original sentence, while the vehicle \"earthquake\" brings a negative sentiment polarity in opposition to the original sentence.\n\nRelevance\nFor the relevance score, if the components of one simile are relevant, they tend to co-occur in simile sentences (Xiao et al., 2016; He et al., 2022) and possess shared properties (Paul, 1970; Tversky, 1977) . Hence, obtaining the relevance score requires large-scale simile sentences as references, as well as knowledge about the properties (adjectives) of each simile component. For a simile s, the relevance score is defined as follows:\nEQUATION\nwhere there are m p topic-vehicle pairs extracted from simile s, each denoted as (t, v) 1 . \u0393(t, v) is the set of similes containing (t, v) as simile components, each denoted as e. P e (t, v) is the probability that the simile components (t, v) share properties in the context of the simile sentence e.\nAn effective way to obtain the frequency information \u0393(t, v) and property knowledge P e (t, v) is to utilize the large-scale probabilistic simile knowledge base MAPS-KB (He et al., 2022) , which contains millions of simile triplets in the form of (topic, property, vehicle), along with frequency and two probabilistic metrics to model each triplet 2 . Specifically, the probabilistic metric Plausibility is calculated based on the confidence score of the simile instance (topic, property, vehicle, simile sentence) supporting the triplet, indicating the probability that the topic and vehicle share the property. The relevance score r can be calculated as follow:\nEQUATION\nwhere G (t,v) is the set of triplets (t, p ,v) containing the (t, v) pair in MAPS-KB, with p referring to the property. n and P are the metrics provided by MAPS-KB, where n and P denote the frequency and the plausibility of the triplet respectively. It is noticed that the metric is not coupled with MAPS-KB, as the frequency information can be obtained by referencing a large set of simile sentences and the property knowledge can be contained via other knowledge bases. More methods are beyond the scope of this paper. However, we additionally provide a method to approximate the relevance score. If we assume the probability that the simile components (t, v) share properties in each sentence is 1, the relevance score can be approximated as:\nEQUATION\nwhere n(t, v) denotes the number of samples that contain the simile components (t, v) in large-scale simile sentences. We discuss the effects of the referenced dataset size in Sec. 4.2.1.\n\nLogical Consistency\nThe literal sentence and the generated simile that are logically inconsistent generally exhibit contra-1 All the simile components in our work are extracted and cleaned using rules from (He et al., 2022) which determines the optimal semantics a component should carry, e.g., \"a kid in a candy store\" instead of just \"a kid\".\n2 More details of MAPS-KB is provided in Appx. D dictory logic. Hence, for a generated simile, we input the <literal text(l), simile(s)> sentence pair into existing pre-trained Multi-Genre Natural Language Inference (MNLI) model 3 , which determines the relation between them is entailment, neutral, or contradiction. The logical consistency score c l of this simile is defined as follows (Pang et al., 2020) :\nEQUATION\nwhere P (h <l,s> = c) represents the probability that the model predicts the relation of the sentence pair < l, s > to be contradiction (denoted as c).\n\nSentiment Consistency\nBetter similes tend to enhance the sentiment polarity of the original sentence (Qadir et al., 2015) . Hence, we first apply the model fine-tuned on the GLUE SST-2 dataset 4 to classify each simile as being either positive or negative. Then, the sentiment consistency score c s is defined as follows:\nEQUATION\nwhere a is the sentiment polarity of the literal sentence (positive or negative) predicted by the model. P (h s = a) and P (h l = a) denote the probabilities that the model predicts the sentiment polarity of the simile s and the literal sentence l to be a, respectively. It is noticed that different <topic, vehicle> pairs within a sentence may have distinct sentiment polarities, such as <She, scared rabbit> and <I, bird> in the simile \"If she escapes like a scared rabbit, I will fly like a bird to catch her.\". Directly inputting text containing multiple topic-vehicle pairs into the sentiment classification model will result in inferior performance. Therefore, for each simile, only the text from the beginning up to the first vehicle is input into the model (i.e. \"If she escapes like a scared rabbit\" in the given example), and for each literal sentence, the text from the beginning up to the first event (i.e. \"If she escapes\" in the given example) is input into the model.\nSince the aim of the SG task is to polish the plain text, the quality of similes generated from different texts can not be compared. Therefore, the normalized score among the simile candidates for each original text is utilized. Suppose there are m simile candidates S = {s 1 , s 2 , ..., s m } for the literal text l, the original relevance scores of R is R = {r 1 , r 2 , ..., r m } respectively. The normalized relevance score r \u2032 i of s i is formulated as follows:\nEQUATION\nwhich ranges from 0 to 1. Then, the normalized logical and sentiment consistency score c \u2032 li , c \u2032 si for each simile s i are obtained in the same manner 5 .\nFinally, the quality for simile s i is defined as the weighted combination of three parts as follows:\nEQUATION\nwhere \u03b1, \u03b2, and \u03b3 are hyperparameters.\n\nCreativity\nCreative similes can provide a better literary experience (Jones and Estes, 2006). In Tab. 1, comparing \"sarcasm\" to \"vitriol\" is less common than to \"fire\", yet it better conveys the intensity of a person's sarcasm. Hence, we design creativity score. Previous studies mainly evaluate the creativity of text generation tasks via human evaluation (Sai et al., 2022) , since measuring the creativity of openended text is a relatively difficult task (Celikyilmaz et al., 2020) . Although there have been many works evaluating the diversity of open-ended text generation (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) , these metrics are not suitable for measuring the creativity of the text. Because the diversity metrics take a set of generated text as input and output one score, while a creativity metric is required to measure each text individually and output a set of corresponding scores.\nDifferent from other open-ended generation tasks, the components of the generated similes enable us to evaluate creativity automatically. According to linguists, the creativity of a simile is determined by vehicles (Pierce and Chiappe, 2008; Roncero and de Almeida, 2015) . Intuitively, the generated simile may be less creative if its extracted 5 If all the relevance scores ri in R are the same, the normalized relevance scores r \u2032 i in R \u2032 are set to 0.5 uniformly.\ntopic-vehicle pair co-occurs frequently, or if many topics are compared to its vehicle in the corpus. Therefore, we adopt large-scale corpora as references when designing our creativity metric. The creativity score of s is calculated as follows:\nEQUATION\nwhere there are m v vehicles extracted from the simile s, each denoted as v. N v denotes the frequency of the vehicles appearing in the similes in the corpora. The log transformation aims to reduce the influence of extreme values.\nAn effective way to obtain the adequate frequency information N v is to utilize the millionscale simile knowledge base MAPS-KB, where the N v can be defined as follows:\nEQUATION\nG v is the set of triplets containing the vehicle v in MAPS-KB, n denotes the frequency of the triplet.\nIt is noticed that the metric is not coupled with MAPS-KB, as N v can also be obtained by counting the samples containing the vehicle v in largescale simile sentences. The method of obtaining the simile sentences is beyond the scope of this paper. Nevertheless, we discuss the effects of the referenced dataset size in Sec. 4.2.2.\n\nInformativeness\nThe vehicle with richer content can create a more impact and vivid impression (Addison, 2001) . In the example from Tab. 1, the addition of the word \"angry\" makes the similes more expressive. Therefore, we design the metric informativeness to measure the content richness of the vehicles.\nIntuitively, the more words a vehicle contains, the richer its content will be. Hence, for a given simile s, we adopt the average length of the extracted vehicles to be the informativeness score 6 (Chakrabarty et al., 2020; Zhang et al., 2021) , defined as I i = 1 mv v\u2208s len(v), where there are m v vehicles extracted from simile s. Here, BLEU2, Rouge2, and BERTScorelarge are presented since they perform the best in their respective category. To avoid overlapping points, random jitters sampled from N (0, 0.05 2 ) were added to human ratings after fitting the regression.\n\nSimile Generation\nThe existing datasets for the SG task are either Chinese (Zhang et al., 2021) , limited to the simile triplet completion (Roncero and de Almeida, 2015; Chen et al., 2022) , or having all vehicles located at end of the sentence (Chakrabarty et al., 2022; Lai and Nissim, 2022) , which are not practical for English simile generation in a real-world application.\nTo bridge the gap, we construct a large-scale English dataset for SG task based on simile sentences from (He et al., 2022) , which contains 524k simile sentences labeled with topic and vehicle. The output decoder target is the simile sentence s and the input encoder source is s rewritten to drop the comparator \"like\" and the vehicle. For example, given s = \"The idea resounded like a thunderclap throughout the land.\", the encoder source would be \"The idea resounded throughout the land.\". In particular, we remove the simile sentences whose event is a linking verb (e.g. be, seem, turn) as they would be meaningless after the vehicle is removed. The final train, validation and test sets contain 139k, 2.5k, and 2.5k sentence pairs, respectively. Based on our constructed dataset, we finetune a pre-trained sequence-to-sequence model, BART (Lewis et al., 2020) , for the SG task, which has been demonstrated to be an effective framework for various figurative language generation (Zhang and Wan, 2021; Chakrabarty et al., 2022; He et al., 2022; Lai and Nissim, 2022) . The experiments are run on RTX3090 GPU and the implementation of BART is based on the HuggingFace Transformers 7 . The experiments are run with a batch size of 16, a max sequence length of 128, and a learning rate of 4e-5 for 10 epochs.\n\nEvaluation Dataset Construction\nFirstly, we randomly sample 50 literal sentences from the test set and adopt the trained SG model to generate five candidates for each one. Then, for each perspective, three raters are asked to rate each 7 https://github.com/huggingface/transformers/ simile from 1 to 5, where 1 denotes the worst and 5 denotes the best 8 . Since evaluating the quality of generated similes is subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014), we remove the simile-literal sentence pairs if (1) raters argue that the pairs lack context and are difficult to rate (e.g. \"Nobody can shoot.\") or (2) some raters rate them as low quality (quality score of 1-2), while others rate them as high quality (scores of 4-5) (Niculae and Danescu-Niculescu-Mizil, 2014) . Moreover, we measure the inter-rater agreement by holding out the ratings of one rater at a time, calculating the correlations with the average of the other rater's ratings, and finally calculating the average or maximum of all the held-out correlations (denoted as \"Mean\" and \"Max\", respectively). The inter-rater agreement before and after applying the filtering strategies is shown in Tab. 2. Overall, the final inter-rater agreement ensures the reliability of our evaluation of automatic metrics and the filtering strategies improve the inter-rater agreement generally. We finally get 150 simile candidates generated from 44 literal sentences.\n\nQuality\nWe compare our quality metric with the following automatic metrics 9 : (1) BLEU (Papineni et \n\n2002) calculates the precision of n-gram matches,\n(2) RougeL (Lin, 2004 ) is a recall-oriented metric, (3) METEOR (Denkowski and Lavie, 2014) proposes a set of linguistic rules to compare the hypothesis with the reference, ( 4) BERTScore (Zhang et al., 2019) calculates the cosine similarity between the BERT embeddings, ( 5) Perplexity (Pang et al., 2020) measures the proximity of a language model, the inverse of which is utilized.\nCorrelations with Human Ratings. Tab. 3 shows the correlation coefficients between automatic metrics and human ratings. Firstly, our metrics are significantly more correlated with human ratings than prior automatic metrics. Moreover, all the sentence-level metrics, which consider the semantics of the entire sentence, perform worse than almost all the n-gram-level metrics, which compare the n-grams between the hypothesis and the reference, which reveals that simile components need to be specifically considered during SG evaluation.\nAccording to the visualized correlation result in Fig. 3 , datapoints from prior automatic metrics tend to scatter at 0 or 1, while the datapoints from our metric are distributed closer to the fitter line, proving that our metric can better measure the quality.\nRecommendation Task. We compare the rankings given by automatic metrics with human rankings 10 . We adopt the following metrics: Hit Ratio at rank K (HR@K(K=1,3)), Norgenerated from different literal sentences can not be compared. Please refer to Appx. C for the implementation of them. 10 We remove the literal sentences with fewer than three valid simile candidates in this task, as they are too simple to rank. We finally get 134 sentences from 35 literal sentences.\n\nMetrics\nHR@1 HR@3 nDCG@1 nDCG@3 MRR malized Discounted Cumulative Gain at rank K (NDCG@K(K=1,3)) 11 , and Mean Reciprocal Rank (MRR). From Tab. 4, our metric achieves significant improvement compared to other metrics, indicating that our metric can yield more accurate rankings for quality. Also, the n-gram-level metrics generally outperform sentence-level metrics, which is consistent with the result in Tab. 3. Ablation Study. To investigate the importance of different sub-metrics in quality metric, we compare the correlation between quality metric and human ratings after removing each sub-metric individually. From Tab. 3, the removal of any sub-metric leads to a decline in performance, which proves the effectiveness of each sub-metric. Among three components, the removal of the relevance results in the largest performance drop, which reveals that relevance is the most important sub-metric.\nThe Effects of Hyperparameters. Since different sub-metrics have varying levels of importance, we study the correlation results when gradually increasing the weight of relevance component and decreasing the weight of sentiment consistency component (as in Tab. 5). From Fig. 4 (left), increasing the weight of the relevance component consistently results in improved performance, peaking at the combination [7](\u03b1, \u03b2, \u03b3 = 3/6, 2/6, 1/6), before eventually causing a decline in performance. This reveals that although relevance is the most important sub-metric, too much weight on it can be detrimental.\nThe Effects of Referenced Dataset Size. We sample different numbers of simile sentences from (He et al., 2022) as references for relevance 5/6, 1/12, 1/12\nTable 5 : The setting of each hyperparameters combination for the quality metric. The result is shown in Fig. 4 (left).\n[\n1] [2] [3] [4] [5] [6] [7] [8] [9]\nHyper-parameters Combination score and study the correlation between the quality metric and human ratings 12 . From Fig. 4 (right) 13 , correlations grow linearly with exponential growth in referenced dataset size, indicating that using datasets larger than 100k will improve the correlation coefficients. Moreover, the performance at the peak surpasses the prior automatic metrics, proving the effectiveness of our approximation method.\n\nCreativity\nWe compare our creativity metric with the following automatic metrics: (1) Perplexity which is often utilized to measure diversity as well (Tevet and Berant, 2021) , (2) Self-BLEU (Zhu et al., 2018) calculates the BLEU score of each generation against all other generations as references, (3) Distinct n-grams(Dist) (Tevet and Berant, 2021) , which is the fraction of distinct n-grams from all possible n-grams across all generations.\nCorrelations with Human Ratings. From Tab. 6, our metric creativity is significantly more correlated with human evaluation scores compared with prior diversity metrics. According to the visualized correlation result in Fig. 5 , the prior diversity metrics have either wide confidence intervals (Perplexity, Dist) or scattered datapoints (self-BLEU), whereas our creativity metrics exhibit stronger linear correlation and narrower confidence intervals (Creativty w/ Log), implying higher reliability.\nRecommendation Task. We compare the rankings given by automatic metrics with human rank- ings. According to Tab. 7, our creativity metric outperforms prior automatic metrics, which proves our metric can better measure the creativity of simile candidates given a literal sentence, which is consistent with the results in Tab. 6. Ablation Study. According to Tab. 6, removing the log transformation leads to significant performance drops. According to the visualized correlation result in Fig. 5 , the datapoints are distributed closer to the fitter line and exhibit narrower confidence intervals after applying the log transformation, which further proves that log transformation is essential for our creativity metric.\nThe Effects of Referenced Dataset Size. According to Fig. 6 (left), the correlation coefficients increase continuously and eventually converge as the number of referenced sentences increases. Moreover, the performance after convergence is comparable to that given by the creativity metric based on the simile KB. The trend reveals that our metric referencing 10k similes can achieve a promising correlation with human ratings.\n\nInformativeness\nThe Pearson and Spearman correlation coefficients between our informativeness metric and human ratings are 0.798 and 0.882, respectively. According to Fig. 6 (right), the strong linear correlation between the metric and human ratings proves that our informativeness metric is simple yet quite effective.\n\nRelation between Metrics\nWe present pair-wise correlations between the three automatic metrics in Tab. 8 and also visualize them in Fig. 7 . Among the three metrics, creativity correlates with informativeness moderately, mainly because shorter vehicles tend to be less creative than longer ones. The correlations of all other pairwise metrics are relatively weak. Thus, it is evident that the three metrics are independent of each other and it is necessary to measure each one of them to obtain a holistic view of SG evaluation. \n\nHAUSER Application\nWe perform a case study to prove that our designed automatic metrics are effective for various methods. Here, we apply our metrics to a retrieval method (Zhang et al., 2021) (denoted as BM25), which utilizes the 20 context words around the insertion position given by groundtruth to retrieve the 5 most similar samples based on the BM25 ranking score from the training set, and adopts the vehicles from these samples to be those of simile candidates. This method ensures the diversity of generated similes. The method introduced in Sec. 4.1 is denoted as Ours. Given the candidates generated by each method, we rerank them using a weighted combination of quality, creativity, and informativeness rankings obtained by HAUSER, with a ratio of 2:2:1. From Tab. 11 in Appendix, the candidates generated by various methods can be more correlated with human rankings after being ranked by our metrics, thus proving the generality of our metrics. It is noticed that the insertion position for BM25 is provided by the groundtruth, while the insertion position for Ours is predicted by the model, thus proving the effectiveness of our generation method.\n\nConclusion\nIn this work, we systematically investigate the evaluation of the Simile Generation (SG) task. We establish a holistic and automatic evaluation system for the SG task, containing five criteria from three perspectives, and propose holistic automatic metrics for each criterion. Extensive experiments verify the effectiveness of our metrics. more correlated with humans than prior referencebased metrics (e.g. BLEU, Rouge, BERTScore), our metrics are still reference-based and rely on the quality and scale of referenced data. We have discussed the effect of referenced dataset size in our paper and will design reference-free metrics to further complement our metrics in future work. Additionally, since our metrics utilize a million-scale simile knowledge base or large-scale simile sentences as references, the efficiency of our method is slightly lower than the automatic metrics based on a few references. Nevertheless, this limitation does not prevent our metrics from performing systematic and scalable comparisons between SG models.\n"}
{"question": "Which of the following main results is true?", "evidence": "  We observe that SG-CQG outperforms other methods on most of the metrics, except Distinct and BERTScore.  The metric Conv-Distinct reasonably penalizes models that generate too short conversations, on which SG-CQG achieves the best results. Conversational coherence is indeed im-proved.  SG-CQG also achieves a significantly higher Context Coverage (CC) score compared to CoHS-CQG.  ", "options": ["A. CoHS-CQC achieves a higher Context Coverage(CC) score .", "B. Conversational coherence is not indeed im-proved.", "C. SG-CQG achieves the best results on the metric Conv-Distinct.", "D. SG-CQG outperforms other methods on all of the metrics."], "answer": "C", "content": "\nIntroduction\nBuilding systems that can comprehend human speech and provide assistance to humans through conversations is one of the main objectives in AI. Asking questions during a conversation is a crucial conversational behavior that helps AI agents communicate with humans more effectively (Allen et al., 2007; Li et al., 2016b) . This line of research is known as Conversational Question Generation (CQG), which targets generating questions given the context and conversational history (Nakanishi et al., 2019; Pan et al., 2019a; Gu et al., 2021; Do et al., 2022) . Compared to traditional single-turn question generation (Pan et al., 2019b) , CQG is more challenging as the generated multi-turn questions in a conversation need not only to be coherent but also follow a naturally conversational flow.\nGenerally, there are two main settings for the CQG task: answer-aware and answer-unaware. In the answer-aware setting, the expected answers of the (to be) generated questions are exposed to the models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . In reality, however, the answers are only \"future\" information that are unknown beforehand. Thus, growing attention has been on the more realistic answer-unaware setting, in which the answers are unknown to the CQG model (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nPrior studies either attempt to ask the questions first, and compute the reward function to evaluate their answerability (Pan et al., 2019a) or informativeness (Qi et al., 2020) ; or they extract the answer spans from the context as the what-to-ask first, and generate the questions based on them (Nakanishi et al., 2019; Do et al., 2022) . However, it has been argued that the former approach tends to generate repetitive questions (Qi et al., 2020; Do et al., 2022) . For the latter approach, Do et al. (2022) recently proposed a selection module to shorten the context and history of the input and achieved stateof-the-art performance. Nonetheless, it simply employs a naive heuristic to select the earliest forward sentence (without traceback) in the context as the rationale to extract the answer span. Although such heuristics ensure the flow of the generated questions is aligned with the context, we argue that the resulting conversations may not be natural enough, because, in reality, the interlocutors often talk about the relevant parts that may not form a sequential context. Furthermore, previous studies (Gao et al., 2019; Do et al., 2022) trained the models to decide the type of the question (boolean/span-based) to be generated implicitly. We argue that modeling question type explicitly is critical since in this setting, the answer, which hints the models to generate a boolean or span-based question, is unavailable.\nTo address the above problems, we propose a two-stage CQG framework based on a semantic graph, SG-CQG, which consists of two main components: what-to-ask and how-to-ask. In particular, given the referential context and dialog history, the what-to-ask module (1) constructs a semantic graph, which integrates the information of coreference, co-occurrence, and named entities from the context to capture the keyword chains for the possible \"jumping\" purpose; (2) traverses the graph to retrieve a relevant sentence as the rationale; and\n(3) extracts the expected answer span from the selected rationale (Section 3.1). Next, the how-to-ask module decides the question type (boolean/spanbased) via two explicit control signals and conducts question generation and filtering (Section 3.2).\nIn order to exhaustively assess the quality of the generated question-answer pairs, we propose a set of metrics to measure the diversity, dialog entailment, relevance, flexibility, and context coverage through both standard and human evaluations. Compared with the existing answer-unaware CQG models, our proposed SG-CQG achieves state-ofthe-art performance on the standard benchmark, namely the CoQA dataset (Reddy et al., 2019) .\nOur contributions can be summarized as follows:\n(1) We propose SG-CQG, a two-stage framework, which consists of two novel modules: whatto-ask encourages the models to generate coherent conversations; and how-to-ask promotes generating naturally diverse questions. Our codes will be released at https://github.com/ dxlong2000/SG-CQG.\n(2) SG-CQG achieves state-of-the-art performance on answer-unaware CQG on CoQA.\n(3) To the best of our knowledge, we are the first to propose a set of criteria to comprehensively evaluate the generated conversations. Moreover, we propose Conv-Distinct to measure the diversity of the generated conversation from a context, which takes the context coverage into account.\n(4) We conduct thorough analysis and evaluation of the questions and answers of our generated conversations, which can bring some inspiration for future work on the answer-unaware CQG.\n\nRelated Work\nOur work is closely related to two lines of prior work. Extended related work is in Appendix A.1.\n\nConversational Question Generation\nQuestion Generation has gained much attention from the research community over the years (Pan et al., 2019b; Lu and Lu, 2021) . Despite such intensive exploration, much less attention has been drawn to Conversational QG or CQG. Generally, CQG has been considered in two main settings: answer-aware and answer-unaware. In the answeraware setting, the expected answers are revealed to models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . However, this is not always the case in reality, as the answers are \"future information\". The answer-unaware setting; therefore, receives growing interests recently (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nTo tackle the what-to-ask problem, prior studies (Pan et al., 2019a; Do et al., 2022) selected the next sentence in the context as the rationale. Do et al. (2022) extract the target answer span from the rationale, while Pan et al. (2019a) generate the question, and compute a reward function to fine-tune the model by reinforcement learning. The howto-ask challenge was simply formulated as that in the answer-aware setting. In contrast, we attempt to model the rationale selection in a more coherent way by constructing and traversing a semantic graph, which simulates the keyword chains. We further propose control signals to promote diversity and fluency in question generation.\n\nKnowledge-grounded Conversation Generation\nLeveraging graphs to enhance dialog response generation has received growing interest (Moghe et al., 2018; Liu et al., 2019b; Xu et al., 2020 Xu et al., , 2021)) .\nIn particular, Xu et al. (2020) proposed to extract event chains (Mostafazadeh et al., 2016) , and utilised them to help determine a sketch of a multi-turn dialog. Nonetheless, the situation differs significantly when it comes to the CQG task. The responses in the dialog response generation task are normally full sentences with enough relevant mentions. However, in CQG, the questions and answers are mostly short and lack clear keywords, which makes the existing keyword-graph not applicable. We thus present a semantic graph, which incorporates the coreference, co-occurrence, and named entities information from the context.\n\nSG-CQG\nWe formulate the answer-unaware conversational question generation (CQG) task as:\ngiven the referential context C = {s 1 , s 2 , ..., s m } with s i being the i-th sentence in context, and the conversational history H n = {(q 1 , a 1 ), (q 2 , a 2 ), ..., (q n\u22121 , a n\u22121 )} with (q i , a i ) being the i-th turn of the question-answer pairs, as input D n = {C, H n }, the model learns to generate the current question q n and answer a n .\nFigure 1 demonstrates an overview of our proposed framework. It consists of two main components: (1) A what-to-ask module aims to select a reasonable sentence in the referential context C as the current rationale r n and thereby a span in r n as the target answer a n , given D n . (2) A how-to-ask module aims to generate the question q n , guided by the rationale r n and target answer a n .\n\nWhat-to-ask Module (WTA)\nExisting answer-unaware CQG models (Pan et al., 2019a; Do et al., 2022) commonly utilize the next sentence of r n\u22121 in the context as the current rationale r n . Although such heuristics can guarantee that the flow of the generated questions is consistent with the narrative in context, the generated conversation may not always be as natural as in reality, since human speakers often jump back and forth across the relevant but not sequential contents in context. To facilitate the models in selecting the current rationale and target answer appropriately and further improve the semantic diversity of dialogue flow, we design a what-to-ask module, which consists of two components: semantic graph construction and graph traversal algorithm.\nSemantic Graph Construction (SGC) Figure 1 shows an example of our semantic graph. Each node is displayed as a textual span and the index of the sentence it belongs to. To construct the semantic graph G = {V, E}, we first obtain the corefer-ence clusters from the context C by AllenNLP (Shi and Lin, 2019) and build the set of initial nodes from phrases in the clusters. We then connect all the nodes in the same cluster as a chain: each node in the cluster (except the one that appears last in the context) is connected to the nearest forward one in the context. We denote this type of relation as Coreference. To enhance the connectedness of G, we extract all named entities by spaCy 1 and add them as additional nodes if they are not in any clusters. We then connect all the nodes in the same sentence in the context in the same chaining style and name those edges as Same Sentence. Finally, we add a type of Extra edges between all connected subgraphs to make G fully-connected. Since those Extra edges do not bring any semantic relation to the graph, our objective is to minimize the number of those edges. Specifically, we gradually select, and connect two sentences such that their nodes are in different connected components and have the smallest indexes with the smallest difference, until the graph is fully-connected. To connect two sentences, we add an Extra edge between the last phrase in the smaller-index sentence and the first phrase in the remaining sentence. The adding-Extra-edges algorithm is in Appendix A.4.\nGraph Traversal Algorithm (GTA) Given the conversational history H n and the semantic graph G, we create a queue q to store nodes for traversing. We first add the nodes that appear in any previous turn' rationale to q in the index order 2 . We then traverse G by popping the nodes in q until it becomes empty. For each node, we retrieve the sentence that contains it as the rationale r n . If the model can generate a valid question from r n and any answer span extracted from r n , we add all unvisited neighbors of the current node to the beginning of q. A question is considered being valid if it passes the QF module (Section 3.2). Prepending the neighbors to queue is to prioritize the nodes that are connected so that the generated conversation can be formed from a chain of relevant sentences, which consolidates the coherence of the conversation. If the model cannot generate any valid q n by the current node, we add its unvisited neighbors to the end of q. The pseudocode of our proposed Graph Traversal Algorithm is described in Appendix A.2. et al. (2022) to design the answer span extractor module. In particular, a T5 model is trained on SQuAD (Rajpurkar et al., 2016) to predict the target answer span (a), given its original sentence in context (r). We use this pretrained model to extract a n from r n . Note that we also deselect the answer spans that are the same as those of previous turns.\n\nHow-to-ask Module (HTA)\nA high ratio of boolean questions in conversational datasets such as CoQA (Reddy et al., 2019) (around 20%) is one of the main challenges for current CQG studies (Gao et al., 2019; Pan et al., 2019a; Gu et al., 2021) . To the best of our knowledge; however, there is no up-to-date work which attempts to tackle this challenge. This problem is even worse in the answer-unaware setting since there is no\nYes/No answer to be provided to guide the generation of the models. Previous studies (Pan et al., 2019a; Do et al., 2022) simply train the CQG models to let them implicitly decide when to generate the boolean and span-based questions without any explicit modeling of the question type. We argue that explicitly modeling the question type is critical, as the models will gain more control on generating diverse questions, thus making the conversation become more natural. To this end, we introduce two control signals as the additional input to the QG model, and develop a simple mechanism to select the signal for the current turn.\nQuestion Type Classifier (QTC) We design two control signals to guide the QG model: <BOOLEAN> is prepended to the textual input if we expect the model to generate a boolean question, and <NORMAL> otherwise. To classify which signal should be sent to the QG model, we train a RoBERTa (Liu et al., 2019a) as our Question Type Classifier. This binary clasifier takes the rationale r n and the answer span a n generated from what-toask module, the context and the shortened conversational history as the input, and generates the label 0/1 corresponding to <NORMAL>/<BOOLEAN>. We conduct additional experiments to discuss why the control_signals work in Section 6.3.\nRewriting and Filtering (RF) Our RF module serves two purposes. Firstly, following Do et al.\n(2022), we train a T5 model on CoQA (Reddy et al., 2019) as our CQA model to answer the generated questions. A question is passed this filtering step if the answer generated by the CQA model has a fuzzy matching score greater or equal to 0.8 with the input answer span. Secondly, when invigilating the generated conversations, we observe multiple other errors that the blackbox model encounters, as shown in Table 1 . We thus propose extra post-processing heuristics to filter out the gen-erated questions and try to avoid the following issues: (1) Wrong answer. Unlike Do et al. ( 2022) that took the extracted spans as the conversational answers, we rewrite the extracted answer spans for the boolean questions by selecting the answers generated from the CQA model;\n(2) Irrelevant. For each generated question, we remove stopwords and question marks only for filtering purpose, and we check if all the remaining tokens exist in the context C;\n(3) Uninformative. To remove the turns like (\"Who woke up?\", \"Justine\"), we check validity if no more than 50% of the tokens of r n exist in any previously generated QA pairs; (4) Redundant. Unlike previous studies (Qi et al., 2020; Do et al., 2022) which only considered the redundant information from the generated answers, for each generated question that has more than 3 tokens, we filter it out if it has a fuzzy matching score >= 0.8 with any of the previously generated questions.\nQuestion Generation (QG) We fine-tune a T5 model (Raffel et al., 2020) to generate conversational questions. We concatenate the input\nD a n = {C, H n , a n , r n , control_signal} in the for- mat: Signal: control_signal Answer: a n , r n Context: C [SEP] H sub , where H sub \u2208 H n .\nThe model then learns to generate the target question q n . In our experiments, H sub is the shortened H n , in which we keep at most three previous turns. It was shown to improve upon training with the whole H n significantly (Do et al., 2022) . The performance of the QG model is in Appendix A.3.\n\nExperimental Settings\nDataset We use CoQA (Reddy et al., 2019) , a large-scale CQA dataset, in our experiments. Each conversation includes a referential context and multiple question-answer pairs, resulting in a total of 127k question-answer pairs. Among them, around 20% of questions are boolean, which makes this dataset become challenging for the CQG task (Pan et al., 2019a; Gu et al., 2021) . Since the test set of CoQA is unavailable, we follow Do et al. (2022) to keep the original validation set as our test set and randomly sample 10% of the original training set as our new validation set.\nAutomatic Evaluation We utilise BERTScore (Zhang et al., 2020) as our dialog entailment metric (BERTScore-entailment), a generalization of Dziri et al. (2019) . It considers the generated re-sponse (question/answer) as the premise, and the utterances in the conversational history as the hypothesis, and measures their similarity score as the topic coherence score. This property is crucial as the questions/answers should focus on the same topic as the previous turn(s). In our experiment, we measure the dialog entailment score with 1, 2, and all previous turn(s). To measure the relevance between the generated conversation and the context, we concatenate the generated QA pairs and compute the BERTScore. It provides how the generated conversation is explicitly relevant to the context.\nWe observe short conversations with very few generated turns tend to yield very high scores on the available diversity measurement metrics such as Distinct (Li et al., 2016a) . Since the conversation is generated from a given context, we argue that how much information from the given context the generated conversation covers should be taken into account. To this end, we introduce Context Coverage (CC) to measure the percentage of the sentences in the context that are the rationales of generated QA pairs. Our proposed Conv-Distinct of a generated conversation is then computed by multiplying the Distinct score of the generated conversation with its CC score, to measure the diversity of the turns generated from a given context:\nConv-Distinct = CC * Distinct (1)\nWe further provide Jumping Score (JS) to measure the flexibility of the generated conversation. JS is defined as the percentage of turns in which the model jumps back to any previous content of their previous turn (i.e. trace-back). It is worth noting that we do not rank the models based on JS score. Details of proposed metrics are in Appendix A.7.\nHuman Evaluation Human evaluation is critical to evaluate the quality of the generated conversations since the CQG model may generate reasonable conversations but unmatched well with the provided ground-truth ones. We randomly select 25 contexts in our test set and take the first five generated turns from the output of each model to compare, resulting in 125 samples in total. We hire three annotators who are English native speakers. Each generated question is rated by annotators on a 1-3 scale (3 is the best). We follow Do et al. (2022) to utilize three criteria: (1) Factuality measures the factual correctness and meaning of generated questions, (2) Conversational Alignment measures how aligned the generated questions are with the history, (3) Answerability measures how answerable the generated questions are by the given context. Given the fact that LMs can generate fluent texts, we omit using Fluency and Grammaticality.\nWe measure the annotators' agreement by Krippendorff's alpha (Krippendorff, 2011) . Our human rating instructions are in Appendix A.9.\nImplementation Details We fine-tune a RoBERTa large (Liu et al., 2019a) as our binary Question Type Classifier with the pretrained checkpoints from fairseq (Ott et al., 2019) on CoQA. We use a learning rate of 1e-5, a window size of 512, a batch size of 4, and AdamW (Loshchilov and Hutter, 2019) as our optimizer.\nOur classifier achieves an accuracy of 95.6%. The model is finetuned on a P40 Colab GPU for 10 epochs. Details of the input format are in Appendix A.5. We initialise SG-CQG with pretrained checkpoints of T5 base model (Raffel et al., 2020) from Huggingface (Wolf et al., 2020) . We also use AdamW (Loshchilov and Hutter, 2019) as our optimizer with a warmup of 0.1 and an initial learning rate of 1e-4. We train the model for 100k iterations with a standard window size of 512, a batch size of 4, and use a Beam search decoding strategy with a beam size of 4. \n\nMain Results\nTo evaluate the performance of SG-CQG on the answer-unaware CQG task, we employ 4 baselines for comparison, as shown in Table 2 .\n(1) T5 base (Raffel et al., 2020) , (2) BART base (Lewis et al., 2020) , (3) GPT-2 (Radford et al., 2019) , which are fine-tuned to generate conversational questionanswer pairs end-to-end, and (4) CoHS-CQG (Do et al., 2022) which adopts a strategy to shorten the context and history of the input, achieves the SoTA performance on CoQA in answer-aware and answer-unaware CQG. Firstly, we observe that SG-CQG outperforms other methods on most of the metrics, except Distinct and BERTScore. The reason is that BART and T5 often generate short QA pairs (the CC scores are 8.62% and 23.33% on average, respectively), and copy more from the context, thus they get higher scores on Distinct and BERTScore. Secondly, the metric Conv-Distinct reasonably penalizes models that generate too short conversations, on which SG-CQG achieves the best results. Thirdly, by allowing the model to jump back and forth across the relevant contents in the context by the semantic graph, SG-CQG outperforms other methods significantly on BERTScore-entailment, which indicates that conversational coherence is indeed im-proved. Furthermore, SG-CQG achieves the highest JS score, which demonstrates that the whatto-ask module allows our model to be most flexible in selecting rationales compared to the baselines. SG-CQG also achieves a significantly higher Context Coverage (CC) score compared to CoHS-CQG. Finally, compared with the results of Oracle, which are from the human-generated conversations, SG-CQG achieves commensurate performance on BERTScore-entailment and BERTScore. It demonstrates that our generated conversations are as closely coherent as human-generated ones.\nQuestion Generation Evaluation We compare the generated conversational questions of our model with 4 baselines: (1) ReDR (Pan et al., 2019a) is an encoder-decoder framework which incorporates a reasoning procedure to better understand what has been asked and what to ask next about the passage; (2) T5 base (Raffel et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . For T5, GPT-2 and CoHS-CQG, we extract the generated questions from the generated conversations for comparison. We measure the diversity of the generated questions by Distinct (Li et al., 2016a) and our proposed Conv-Distinct. Table 3 shows evaluation results of the generated conversational questions. We observe that SG-CQG achieves the best performance on Conv-Distinct, which takes the context coverage into account.\n\nAnswer Span Extraction Evaluation\nWe further evaluate the generated conversational answers of our model with 4 baselines: (1) T5 base (Raffel et al., 2020) ; (2) BART base (Lewis et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . We extract the generated conversational answers from the generated conversations of the models for comparison. We train another T5 base model on CoQA for the CQA task (see Appendix A.6) and utilize it to generate the groundtruth answers for the generated questions of the models. We then evaluate the quality of the generated conversational answers by measuring the Exact Match (EM) and F1 scores with the groundtruth ones. Table 4 shows the evaluation results. We observe that the generated conversational answers extracted by SG-CQG achieve the best EM and F1 scores, which are significantly higher than the other baselines.\n\nHuman Evaluation\nThe results of the human evaluation are present in et al., 2022) . Compared to CoHS-CQG, it achieves higher scores on all metrics except the Context Coverage (CC), which reflects that the quality of the generated conversations is indeed improved. These improvements are expected as the model in this case gains more control over generating boolean questions and has a stricter filtering process. This stricter filtering process also explains why it gets a lower CC score compared to CoHS-CQG.\n\nAblation of Question Type Classifier (QTC)\nWe conduct an ablation study of the Question Type Classifier (QTC) module. We name this experiment SG-CQG + w/o QTC. Table 2 shows the evaluation results of generated question-answer pairs. Compared with SG-CQG, the performance of SG-CQG + w/o QTC drops slightly on nearly all metrics (except Distinct), which consolidates our hypothesis that explicitly modeling the question type improves the overall coherency of the conversation. Furthermore, Table 3 shows that QTC enhances the diversity of the generated questions, while Table 4 illustrates that QTC improves the quality of the 2 ) and questions (Table 3 ) significantly. Notably, without the RF module, the extracted answer spans by SG-CQG + w/o RF can be very different from the true conversational answers, resulting in very low F1 and EM scores (Table 4 ). Although the CC score is perfect, the generated question-answer pairs from this experiment are of bad-quality.\n\nCase Study\nWe present one conversation generated by our proposed SG-CQG in Table 6 . We observe that the rationale of Q2-A2 is the 3-rd sentence in the context, and the rationale of Q3-A3 is the 8-th sentence, which is a forward jump of the model. On the other hand, the rationale of the Q4-A4 is the 7-th sentence, which is a traceback. Such a traceback enhances reasonable coherence between Q3-A3 and Q4-A4. Furthermore, Q5-A5 to Q6-A6 is also a traceback, and especially, Q6 is a boolean question. More case studies are shown in Appendix A.10.\n\nWhy Do Control Signals Work?\nExperimental Settings We design the experiments to verify the helpfulness of our two proposed control_signals: <BOOLEAN> and <NORMAL>.\nIn particular, we train a T5 model (Raffel et al., 2020 ) in the answer-aware setting. Given the input D a n = {C, H n , a n , r n } with C, H n , a n , r n as the context, ground-truth conversational history, ground-truth answer, and round-truth rationale, respectively, we conduct three experiments in Table 9 : original input with Yes/No keyword (With Y/N ), original input without Yes/No keyword (W/o Y/N ), original input without Yes/No and with the ground-truth control_signal (W/o Y/N + control_signal). Note that we train the model with the whole context, and a maximum of three previous history turns, as discussed in Appendix A.3. We measure the performance of the answer-aware CQG model separately on two types of questions: boolean and span-based by ROUGE-L (Lin, 2004) and BERTScore (Zhang et al., 2020) .\nObservations Table 9 shows the experimental results. We derive two main observations. Firstly, without knowing the keyword Yes/No (W/o Y/N ) -this is the case in the answer-unaware setting, the model performs worse. This decrease shows that the Yes/No keyword is indeed helpful in hinting the model towards generating the correct questions. Secondly, by inputting the groundtruth control_signal into the model (W/o Y/N + control_signal), the performance is improved by a large margin compared to (W/o Y/N ). We obtain three implications from the above improvement. Firstly, it consolidates our hypothesis that inputting the ground-truth control_signal is truly helpful. Secondly, by training with the control_signal, the performance of the model is even higher than with Y/N in the span-based cases, which indicates that training the model with control_signal makes it more stable to generate the correct questions. Thirdly, the performance of (W/o Y/N + control_signal) is lower than (With Y/N ) in boolean cases. The reason is <BOOLEAN> only informs the model to generate a boolean question without informing to generate an Yes or No one.\n\nConclusion\nThis paper presents SG-CQG, a two-stage framework for the CQG task in the answer-unaware setting. Firstly, the what-to-ask module aims to select a sentence as the rationale by the proposed semantic graph and extract the answer span from it. The how-to-ask module classifies the type of the question before generating and filtering it. Additionally, we propose a set of automatic evaluation criteria for answer-unaware CQG, especially a novel metric, Conv-Distinct, to evaluate the generated conversation from a context. Extensive automatic evaluation and human evaluation show that our method achieves state-of-the-art performances in the answer-unaware setting on CoQA, with a significant improvement in the conversational alignment property compared to previous frameworks. In the future, we will focus on how to reason over our semantic graph to select the rationale, and further improve the performances of how-to-ask module.\n"}
{"question": "On which dataset did the model achieve better results", "evidence": "  On Jigsaw and Contextual Abuse datasets using the Hate+Abuse List and derived CAD Ruleset, RBE outperforms SOTA by an increased margin of 4.1/2.3%, and 4.3/1.3% respectively. Contrary to HateXplain, these two datasets are more heavily imbalanced toward non-hateful examples and thus more representative of the real-world case of content moderation where most content is considered benign ", "options": ["As seen in Table 1 , on its own the Hate+Abuse ruleset performs poorly on each dataset in both precision and recall."], "answer": "B", "content": "\nIntroduction\nContent moderation is a major challenge confronting the safety of online social platforms such as Facebook, Twitter, YouTube, Twitch, etc. (Vaidya et al., 2021) . Major technology corporations are increasingly allocating valuable resources towards the development of automated systems for the detection and moderation of harmful content in addition to hiring and training expert human moderators to combat the growing menace of negativity and toxicity online (Wagner and Bloomberg, 2021; Liu et al., 2022) .\nDespite the popularity of deep learning approaches, many practical solutions used in products today are comprised of rule-based techniques based on expertly curated signals such as block lists, key phrases, and regular expressions (Gillespie, 2018; Zhang, 2019; Dada et al., 2019) . Such methods are widely used due to their transparency, ease of customization, and interpretability. However, they have the disadvantage of being difficult to maintain and scale, in addition to being inherently fragile and noisy (Zhang, 2019; Davidson et al., 2017; Lee, 2022; Lai et al., 2022) . Figure 1 shows an example where logical rules, while explainable in nature, face the problem of being inflexible to their context of use in natural language. While a given rule may be too specific and fail to capture different variations of usage commonly found in content online, rules can also be too broad and incorrectly block lexically similar content.\nIn contrast to the challenges faced by rule-based methods, data-driven deep learning approaches have shown great promise across a wide range of content moderation tasks and modalities (Malik et al., 2022; Shido et al., 2022; Lai et al., 2022) . Fueled by large amounts of data and deep neural networks, these complex models are capable of learning richer representations that better generalize to unseen data. The impressive performances of these models have resulted in significant industry investment in content moderation as-a-service. Several technology companies such as Google 1 , OpenAI 2 , and Microsoft 3 use these models to offer services to aid in content moderation. However, despite their significant investment, they face adoption challenges due to the inability of customers to understand how these complex models reason about their decisions (Tarasov, 2021; Haimson et al., 2021; Juneja et al., 2020) . Additionally, with the increasing attention around online content moderation and distrust amongst consumers, explainability and transparency are at the forefront of demands (Kemp and Ekins, 2021; Mukherjee et al., 2022) . This presents the challenging open question of how we can leverage the robustness and predictive performance of complex deep-learning models whilst allowing the transparency, customizability, and interpretability that rule-based approaches provide.\nPrior works such as Awasthi et al. (2020) ; Seo et al. (2021) ; Pryzant et al. (2022) have explored learning from rules for tasks such as controlling neural network learning, assisting in human annotation, and improving self-supervised learning in low data scenarios. Awasthi et al. (2020) propose a rule-exemplar training method for noisy supervision using rules. While performant in denoising over-generalized rules in the network via a soft implication loss, similar to other ML approaches, this method lacks the ability to interpret model predictions at inference time. Pryzant et al. (2022) propose a general-purpose framework for the automatic discovery and integration of symbolic rules into pre-trained models. However, these symbolic rules are derived from low-capacity ML models on a reduced feature space. While less complex than large deep neural networks, these low-capacity models are still not easily interpretable by humans. Therefore, the task of combining the explainability of rules and the predictive power of deep learning models remains an open problem.\nIn order to tackle this problem, we introduce Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is comprised of two neural networks, a rule encoder, and a text encoder, which jointly learn rich embedding representations for hateful content and the logical rules that govern them. Through the use of contrastive learning, our framework uses a semantic similarity objective that pairs hateful examples with clusters of rule exemplars that govern it. Through this approach, RBE is able to provide more explainable predictions by allowing for what we define as Rule-grounding. This means that our model is able to ground its predictions by showing the corresponding explainable logical rule and the exemplars that constitute that rule.\nWe evaluate RBE in both supervised and unsupervised settings using a suite of rulesets. Our results show that with as little as one exemplar per rule, RBE is capable of outperforming state-of-theart hateful text classifiers across three benchmark content moderation datasets in both settings. In summary, the contributions of this paper are:\n\u2022 Rule By Example (RBE): a novel exemplarbased contrastive learning approach to learn from logical rules for the task of textual content moderation. 4 \u2022 We demonstrate how RBE can be easily integrated to boost model F1-score by up to 4% on three popular hate speech classification datasets.\n\u2022 A detailed analysis and insights into the customizability and interpretability features of RBE to address the problem of emerging hateful content and model transparency.\n\nRule By Example Framework\nIn this section, we outline the Rule By Example framework, define its operational terms, and describe its end-to-end architecture. We first formally describe the two main operational terms used in our framework: 1) Ruleset -a ruleset is comprised of a series of executable functions that when given text as input \"fire\" if and only if all conditions defined in the rule are met by the input. Figure 1 shows an example of a simple rule that is triggered if a given text contains the keywords \"hate\" or \"loathe\" and contains \"women\". Rules can be any programmable function that acts on text such as regular expressions, blocklists, keywords, etc. In the scope of this work, we only consider simple rules that humans can easily interpret. As such an ML model cannot be considered a rule, given their black-box nature. 2) Exemplar -an exemplar is a given textual example that well-defines the type of content governed by a rule. For example, X 1 and X 2 in Figure 1 can be considered exemplars of rule R 1 since they correctly match the conditions of R 1 .\nConsider a ruleset of rule-exemplar pairs R\"tpr 1 , e 1 q, pr 2 , e 2 q, ..., pr n , e n qu where r i denotes a defined rule and e i denotes an exemplar for which r i correctly fires. For a given corpus X comprising labeled examples X\"tpx 1 , y 1 q, px 2 , y 2 q, ..., px m , y m qu, each rule r i can be used as a black-box function R i : x \u00d1 ty i , Hu to noisily label each instance x such that it assigns a label y or no label at all. An instance may be covered by more than one rule or no rule at all. Additionally, the cover set C denotes the set of instances in X where a rule r i fires. The generalization problem that arises when rules are applied noisily is two-fold. When rules are too broad the cover set C is large and incorrectly labels a large amount of non-hateful content. Likewise, when rules are too strict and fragile, the cover set C is too small, and lexically and semantically similar content that is hateful ends up being ignored. Our goal is to leverage these rules and their exemplars to facilitate explainable model learning. for each instance xi in X b do 5:\nGet exemplars ei \" doRulesetpR, xiq 6:\nConcatenate exemplars ei 7:\nend for 8:\nGet \u0398rpE b q and \u0398tpX b q 9: Compute L\" 1 2 pY b D 2 `p1 \u00b4Yb qmaxpmargin D, 0q 2 q 10: Update parameters of \u0398r and \u0398t 11: end while\n\nDual Encoder Architecture\nThe Dual-Encoder architecture, as illustrated in Figure 2 , is commonly used in dense retrieval systems and multi-modal applications (Clarke et al., 2022; Reimers and Gurevych, 2019; Xu et al., 2022) . Our architecture consists of a Rule Encoder \u0398 r and a Text Encoder \u0398 t . These are two Bert-like bidirectional transformer models (Devlin et al., 2018) each responsible for learning embedding representations of their respective inputs. This Dual Encoder architecture enables pre-indexing of exemplars allowing for faster inference at runtime after training.\nEncoding Pipeline Given an input text x t , we first extract the set of applicable rules and their respective exemplars from the ruleset R. We then concatenate each extracted exemplar to form x e . In the event that no rules are applicable to x t , we randomly sample exemplars from the en-tire ruleset to form x e . Using the form x e \" \u2423 rCLSs , e ( where e n k is the k-th token of the n-th exemplar and rSEP s and rCLSs are special tokens. Similarly, using the text encoder \u0398 t , we encode x t . In order to obtain a dense representation, we apply a mean pooling operation to the hidden states and derive a fixed-sized sentence embedding. After obtaining the representation for both the exemplars x e and the text x t , we use the cosine function to measure the similarity between them: simpx e , x t q \" \u0398 r px e q \u00a8\u0398t px t q }\u0398 r px e q} }\u0398 t px t q} (1)\nWe employ a contrastive loss (Hadsell et al., 2006) to learn the embedding representations for our rule and text encoder. Contrastive learning encourages the model to maximize the representation similarity between same-label examples and to minimize it for different-label examples. This enables the embedding representations of our encoded ruleset to match the representation of the text correctly covered by cover set C. Likewise, for benign examples that rules incorrectly cover, our contrastive learning objective increases the distance between those representations, thus restricting the over-generalization of certain rules in the ruleset. Let Y t be the correct label of the texts X t , D be the cosine distance of px e , x t q and m be the margin, our contrastive learning loss function is defined as follows:\nL \" 1 2 pYtD 2 `p1 \u00b4Ytqmaxpm \u00b4D, 0q 2 q (2)\nThe training loop, with the encoding pipeline and constrastive loss step, are detailed in Algorithm 1.\n\nRule-Grounding\nBy taking an embeddings-based approach to learning representations, RBE enables what we define as rule-grounding. Rule-grounding enables us to trace our model predictions back to the explainable ruleset accompanied by the exemplars that define each rule. For any input x t that has been marked as positive by our dual encoder, we perform a rules search to find which rules fire on that input as well as an embedding similarity search to find the nearest exemplars and the rules those exemplars belong to. Table 2 shows an example of this.\n\nExperimental Setup\nTraining We train all models with AdamW optimizer and weight decay of 0.01 on all data. We employ early stopping with a ceiling of 10 epochs, a learning rate of 2e-5, batch size of 8, and linear learning rate warmup over the first 10% steps with a cosine schedule. Our models are trained with NVIDIA Tesla V100 32GB GPUs using Azure Machine Learning Studio. We pre-process data and train all models with different random seeds over multiple runs. Our implementation of RBE is based on Huggingface Transformers (Wolf et al., 2020) and Sentence Transformers (Reimers and Gurevych, 2019) . RBE utilizes two Bert-based networks consisting of 110 million parameters each.\nApproximately 2,000 GPU hours were required to train all hyperparameter variations of RBE plus the Bert baseline across all 3 test sets.\nBaselines We evaluate our training algorithms in both supervised and unsupervised settings. We compare against the baselines of applying logical rules as is and the current SOTA approach of training transformer-based sequence classifiers (Mathew et al., 2020) .\n\nDatasets\nWe evaluate RBE across three datasets on the task of hate-speech classification. Across each dataset, we frame the problem as a binary classification task of detecting whether a given text is hateful or nonhateful. We augment each dataset with rulesets that we manually curate. More information on each dataset and ruleset is provided below.\nHateXplain (Mathew et al., 2020 ) is a large-scale benchmark dataset for explainable hate speech detection that covers multiple aspects of hate speech detection. It consists of \"20k samples across 3 labels \"hateful\", \"offensive\", and \"normal\". Additionally, each sample is accompanied by a corresponding target group and explainable rationales. In our experiments, we combine the output classes of hateful and offensive into one resulting in \"8k/1k/1k hateful samples and \"6k/781/782 non-hateful samples for train/validation/test respectively. Additionally, we utilize the accompanying rationales for ruleset construction.\nJigsaw 5 is a large-scale dataset of Wikipedia comments labeled by human raters for toxic behavior. The defined types of toxicity are \"toxic\", \"severe toxic\", \"obscene\", \"threat\", \"insult\", and \"identity hate\". Each comment can have any one or more of these labels. In total, it contains \"230k samples. In our experiments, we define examples of the \"identity hate\" class as hateful and the rest as non-hateful resulting in a dataset of 1405/100/712 hateful samples and \"158k/1k/63k non-hateful examples for train/validation/test respectively.\nContextual Abuse Dataset (CAD) (Vidgen et al., 2021) is annotated dataset of \"25k Reddit entries labeled across six conceptually distinct primary categories of \"Identity-directed\", \"Persondirected\", \"Affiliation directed\", \"Counter Speech\", \"Non-hateful Slurs\", and \"Neutral\". In our experiment, we define examples of the \"identity-directed\" class as hateful and treat the remaining examples as non-hateful resulting in a dataset of 1353/513/428 hateful samples and \"12k/4k/4k non-hateful samples for train/validation/test.\n\nRuleset Construction\nHate+Abuse List We utilize a ruleset targeting identity hate which we'll refer to as Hate+Abuse List. It consists of a list of n-grams representing harmful language such as slurs or hate verbs.\nHate+Abuse List is similar to the publically available bad word lists commonly found online. We treat each n-gram entry in Hate+Abuse List as its own rule that proposes a positive label if the ngram is in the input text. In total, Hate+Abuse List consists of 2957 distinct identity hate rules.\nHateXplain Rationale Ruleset Using the labeled annotator rationales included in the HateXplain dataset, we programmatically generate a Ruleset for HateXplain. To do so, we extract 1, 2, and 3-gram substrings from the annotator rationales and cluster them by annotator-identified target demographic groups. We then take the top N n-grams per each demographic group and automatically create rules for each of them. This results in rules similar in nature to our Hate+Abuse List. Using a default cluster size of 100 across the 25 target categories defined in HateXplain, we generated a total of 670 distinct rules for HateXplain.\nContextual Abuse Rationale Ruleset Similar to our derived HateXplain ruleset we programmatically generate a Ruleset for the Contextual Abuse Dataset using annotator-labeled rationales. Following the identical process outlined before, this results in a total of 2712 distinct rules for CAD.\nExemplar Selection For each dataset we complete our Ruleset construction by pairing each rule with accompanying exemplars. To achieve this, we first run our Ruleset on the dataset trainset and extract instances for which a rule correctly fires.\nFor each rule that correctly fires, we then randomly select N instances to act as the exemplars. Additionally, to restrict potentially overgeneralized rules we enforce the condition that no two rules can be mapped to the same exemplar. Unless stated otherwise, we report results using just one exemplar per rule in our experiments.\n\nUnsupervised Setting\nIn addition to evaluating RBE in supervised settings, we investigate the applicability of RBE in unsupervised settings where no labeled data is present.\nIn this setting, we are presented with a large unlabeled corpus T and a given ruleset R. This setting is particularly challenging due to the inherent generalization problem of rules. Loosely applying rules as is in this setting results in the model overfitting to the distribution of the ruleset as seen in Table 3 . To combat this issue, we design three different semantic clustering-based strategies for determining rule quality in an unsupervised setting: Mean, Concat, and Distance clustering. Given an unlabeled corpus T \" tt 1 , t 2 , ..., t n u, ruleset R \" tpr 1 , e 1 q, ..., pr n , e n qu, and a threshold k, we first encode the entire corpus T using a pre-trained sentence embedding model E \u0398 . In our case, we use a fine-tuned version of MPNet (Song et al., 2020) from the Sentence Transformers library. After receiving our encoded corpus E \u0398 pT q, for the Mean and Concat, we construct a rule embedding r i \u0398 for each rule r i in the ruleset. In the Mean strategy, this is obtained by taking the mean of all rule exemplars \u00b5pr i \u0398 q \" p 1 m \u0159 m i e i m q. For Concat, this is calculated by concatenating all rule exemplars \u00b5pr i q \" E \u0398 pe i 1 } ... } e i m q and encoding the concatenated representation. Once r i \u0398 is constructed, we then label each text in the corpus whose cosine similarity is within the threshold k: \nEQUATION\nIn contrast to the Mean and Concat strategies, the Distance strategy takes a rule elimination approach. Given an unlabeled corpus T \" tt 1 , t 2 , ..., t n u, ruleset R \" tpr 1 , e 1 q, ..., pr n , e n qu, and a threshold k, we first noisily label the entire corpus using the ruleset R i : x t \u00d1 t1, Hu such that each rule is paired with a cover set R \" tpr 1 , e 1 , c 1 q, ..., pr n , e n , c n qu where c i is the set of texts in covered by r i . Next, for each rule, we encode text in its cover set E \u0398 pc i q and calculate the average cosine distance between each embedding and its neighboring examples in c i .\navgDistpE \u0398 pc i qq \" 1 n n \u00ff i distpc i j , c i j\u00b41 q (4)\nLastly, once the average distance for each rule is calculated, using the defined threshold k, we flip any weakly labeled examples in the cover set if the average distance for that rule is above the threshold k:\nEQUATION\n4\n\nResults and Discussion\nWe analyze the results of our experiments, detail our insights, and discuss the implications of applying RBE for explainable hate speech detection.\nEvaluation Metrics: The precision, recall, and F1 score for each dataset in a supervised setting are reported in Table 1 . Due to the highly skewed class distribution, we favor macro F1 scores as our main evaluation metric. We also report accuracy scores (the fraction of entries for which the full set of labels matches) as another metric.\n\nSupervised Performance\nTable 1 reports our results on three hate speech classification datasets in the supervised setting. We observe that RBE is able to outperform SOTA transformer-based models BERT and MPNet by 1.3/1.4%, 4.1/2.3%, and 4.3/1.3% in F1-score on HateXplain, Jigsaw, and CAD respectively. This improvement highlights the impact of leveraging rules in the training process of our framework. Additionally, it is important to note that this increase was achieved using only 1 exemplar per rule in the ruleset. These exemplars were also used to train the comparative baseline models, ensuring that all approaches were trained on the same number of samples. This further showcases how lightweight and flexible RBE is to integrate into a content moderation workflow. For HateXplain, our experiments show that the combination of MPNet as the initialized encoder with both the HateXplain Rationale and Hate+Abuse Ruleset delivers the best performance. Upon deeper analysis, we find that this is due to two main factors: 1) Ruleset Size and Alignment -As explained in Section 3.2 the HateXplain Rationale Ruleset was automatically crafted using rationale labels from expert annotators. This results in a powerful ruleset capable of identifying a large amount of hateful content in the HateXplain dataset as shown 2) Embedding Initialization -Out of the box, pre-trained BERT does not produce meaningfully distinct sentence representations. In practice, the BERT [CLS] token as well as averaged BERT outputs can contain useful after downstream fine-tuning. This is shown by the BERT performance in Table 1 . However, when the pretrained model output is pooled across all dimensions and used for calculating semantic similarity, this results in similar representations even for completely different input text. As a result, if applied to the HateXplain dataset without any fine-tuning, BERT embeddings obtain a precision, recall, and F1-score of 59%, 100%, and 75% respectively, where every example is labeled as hateful. This lack of varied sentence representation coupled with a verbose ruleset such as the HateXplain Rationale Ruleset results in an initial biasing towards hateful examples as shown by the high recall scores. As such, utilizing a pre-trained sentence embedder, such as MPNet, with a pre-train task more optimized for semantic embeddings results in better performance. We observe a similar trend when utilizing our derived ruleset for CAD. Note: When trained longer, the bias of the BERT model decreases as more varied sentence representations are learned.\nOn Jigsaw and Contextual Abuse datasets using the Hate+Abuse List and derived CAD Ruleset, RBE outperforms SOTA by an increased margin of 4.1/2.3%, and 4.3/1.3% respectively. Contrary to HateXplain, these two datasets are more heavily imbalanced toward non-hateful examples and thus more representative of the real-world case of content moderation where most content is consid-ered benign. This increased performance highlights the power of incorporating logical rules to assist model learning and also the ability of RBE to better generalize rules. As seen in Table 1 , on its own the Hate+Abuse ruleset performs poorly on each dataset in both precision and recall. Despite RBE's reliance on this ruleset to guide model learning, when combined with labeled training data, RBE is capable of both restricting over-generalized rules and leveraging its understanding of semantic similarity to extend fragile rules regardless of the base model. Additionally, when using the CAD ruleset which is heavily overfitted to the CAD dataset, as shown by the skewed recall score, RBE is still capable of outperforming the baselines.\nOut-of-domain Rulesets Our Hate+Abuse ruleset is a generic ruleset unrelated to any of the datasets evaluated, and thereby an out-of-domain ruleset. This provides an example of out-of-domain performance using rules not derived from the target dataset. We observe that even when applying RBE with the Hate+Abuse ruleset we are able to outperform the baselines on each dataset. When applying RBE to new domain settings, all that is required is the authoring of additional rules for this new domain. This can be done manually, or more scalably by automatically deriving rules from the new domain data.\n\nInterpretability\nIn addition to its improved performance, another advantage of RBE lies in its ability to perform Rule-grounding. As explained in section 2.2, Rulegrounding enables us to trace our model predictions back to their respective rule accompanied by the exemplars that define that rule. Table 2 shows Rule-grounding examples extracted from each of our tested datasets. By nature, Rule-grounding enables two main features in RBE:\n1) Customizability/Ruleset Adaptation: Given the vast reach of online applications, content mod- 2) Prediction Transparency: By facilitating model interpretations via rule-grounding, users of online systems are offered tangible guidance should their content be flagged, potentially increasing user trust in the system. Additionally, this acts as a direct indicator of the type of content the rule authors want to moderate.\n\nUnsupervised Performance\nTable 3 reports our results in the unsupervised setting. We observe that RBE is able to outperform SOTA trained on noisy rules labeled samples for the HateXplain and Jigsaw dataset while also outperforming the ruleset as is on all three datasets. Across each dataset, we find that RBE's Distance based strategy produces the most consistent performance, outperforming SOTA on HateXplain and CAD while performing on par with SOTA on Jigsaw. We observe that this stability in performance is due to this strategy's rule elimination objective. As opposed to the Mean and Concat strategies which focus on deriving rule representations in a self-supervised manner, the Distance strategy instead focuses on eliminating over-generalized rules whose cover set of examples are semantically dissimilar. This is particularly useful in cases where precision scores are low due to a large number of false positives.\nFor Jigsaw, we observe a slight decrease in performance compared to SOTA. Upon further analysis, we posit that this is a result of RBE's overreliance on the ruleset in this setting, particularly for the Mean and Concat strategies. This is because the ruleset directly influences the derived rule embedding due to its labeling of the cover set C. As such when the ruleset is over-generalized, as is the case of Hate+Abuse rules on Jigsaw, RBE is likely to match the distribution of the ruleset. We find that performing self-supervised model pre-training (Gao et al., 2021) on the target corpus circumvents this trend for the Mean and Concat strategy. As such, with a more refined ruleset, a performance increase is expected as seen in HateXplain and CAD.\n\nRelated Work\nThere has been active work on detecting hate speech in language (Poletto et al., 2021; Al-Makhadmeh and Tolba, 2020; Schmidt and Wie-gand, 2017) . Hate Speech detection has proven to be a nuanced and difficult task, leading to the development of approaches and datasets targeted at various aspects of the problem (Vidgen et al., 2021; Mathew et al., 2020; Mody et al., 2023) . However, few attempts have been made to focus on the explainability of these models, which is an increasing area of concern surrounding their use online (Tarasov, 2021; Haimson et al., 2021) , thus leading to the continued utilization of less powerful but more explainable methods such as rules. Prior works have explored incorporating logical rules into model learning. Awasthi et al. (2020) proposed to weakly learn from rules by pairing them with exemplars and training a denoising model. However, this requires defining rules for all output classes, making it inapplicable to the task of hate speech detection. Additionally, this method only focuses on decreasing rule scope to solve the overgeneralization problem. It does not simultaneously tackle the over-specificity problem demonstrated in Figure 1 . Finally, this method does not provide a way for interpreting model predictions during inference. Seo et al. (2021) proposes a way to control neural network training and inference via rules, however, their framework represents rules as differentiable functions requiring complex perturbations to incorporate, making it more suitable to numerical rules such as those defined in healthcare and finance as opposed to the complex nuances of language. Pryzant et al. (2022) proposes a framework for the automatic induction of symbolic rules from a small set of labeled data. However, these rules are derived from low-capacity ML models and are as a result not human-readable or explainable.\n\nConclusion\nWe introduce Rule By Example, an exemplar-based contrastive learning framework that enables learning from logical rules for accurate and explainable hate speech detection. Specifically, we propose a novel dual-encoder model architecture designed to produce meaningful rule and text representations. RBE leverages a novel exemplar-based contrastive learning objective that converges the representations of rules and text inputs of similar classes. We share results on three public datasets for hate speech detection that validate the Rule By Example framework can not only vastly outperform the initial ruleset but also outperform baseline SOTA classification methods in both supervised and unsupervised settings. Moreover, RBE enables rule-grounding which allows for more explainable model prediction benefits not available in SOTA classification methods alongside additional flexibility via Ruleset Adaptation.\n"}
{"question": "Which step in evaluation dataset construction is wrong?", "evidence": "  Evaluation Dataset Construction\nFirstly, we randomly sample 50 literal sentences from the test set and adopt the trained SG model to generate five candidates for each one. Then, for each perspective, three raters are asked to rate each 7 https://github.com/huggingface/transformers/ simile from 1 to 5, where 1 denotes the worst and 5 denotes the best 8 . Since evaluating the quality of generated similes is subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014), we remove the simile-literal sentence pairs if (1) raters argue that the pairs lack context and are difficult to rate (e.g. \"Nobody can shoot.\") or (2) some raters rate them as low quality (quality score of 1-2), while others rate them as high quality (scores of 4-5) (Niculae and Danescu-Niculescu-Mizil, 2014) . Moreover, we measure the inter-rater agreement by holding out the ratings of one rater at a time, calculating the correlations with the average of the other rater's ratings, and finally calculating the average or maximum of all the held-out correlations (denoted as \"Mean\" and \"Max\", respectively). The inter-rater agreement before and after applying the filtering strategies is shown in Tab. 2. Overall, the final inter-rater agreement ensures the reliability of our evaluation of automatic metrics and the filtering strategies improve the inter-rater agreement generally. We finally get 150 simile candidates generated from 44 literal sentences.\n\n ", "options": ["A. We randomly select 50 literal sentences from the test set and utilize the trained SG model to generate five candidate outputs for each of the selected sentences.", "B. For each perspective, three assessors are assigned the task of rating each item on a scale from 1 to 5, with 1 indicating the lowest or worst rating and 5 indicating the highest or best rating.", "C. We remove  the simile-literal sentence pairs if  raters argue that the pairs lack context and are difficult to rate.", "D. we measure the inter-rater agreement by remove the ratings of one rater at each time"], "answer": "D", "content": "\nIntroduction\nSimiles play a vital role in human expression, making literal sentences imaginative and graspable. For example, Robert Burns famously wrote \"My Luve is like a red, red rose\" to metaphorically depict the beloved as being beautiful. In this simile, \"Luve\" (a.k.a. topic) is compared with \"red rose\" (a.k.a. vehicle) via the implicit property \"beautiful\" and the event \"is\". Here, topic, vehicle, property, and event are four main simile components (Hanks, 2013) . As a figure of speech, similes have been widely used in literature and conversations (Zheng et al., 2019; Chakrabarty et al., 2022) .\nSimile generation (SG) is a crucial task in natural language processing (Chakrabarty et al., 2020; Zhang et al., 2021; Lai and Nissim, 2022) , with the aim of polishing literal sentences into similes. In Fig. 1 , the literal sentence \"He yelps and howls.\" is polished into a simile by inserting the phrase \"like a wolf \", resulting in \"He yelps and The commonly used automatic metric BLEU deems the second candidate as the most high-quality one among all the generated similes, while our proposed metrics HAUSER deem the first candidate as the best one regarding its quality, creativity and informativeness, which better correlates with human ratings and also provides more criteria for SG evaluation.\nhowls like a wolf \". The ability to generate similes can assist various downstream tasks, such as making the generations more imaginative in story or poet generation task (Tartakovsky and Shen, 2018; Chakrabarty et al., 2022) and the generated response more human-like in dialogue generation task (Zheng et al., 2019) .\nAutomatic evaluation is critical for the SG task since it enables efficient, systematic, and scalable comparisons between models in general (Celikyilmaz et al., 2020) . However, existing studies are inadequate for effective SG evaluation. Task-agnostic automatic metrics (Papineni et al., 2002; Zhang et al., 2019; Li et al., 2016) are widely adopted for SG evaluation (Zhang et al., 2021; Lai and Nissim, 2022) , which have several limitations: (1) The simile components should receive more attention than other words during SG evaluation (e.g. \"he\" and \"wolf \" in Fig. 1 ), while there are no automatic metrics that consider the key components. (2) The SG task is open-ended, allowing for multiple plausible generations for the same input (Chakrabarty et al., 2020 ) (e.g. the howling man can be compared to \"wolf \", \"buffalo\", or \"tiger\" in Fig. 1 ). Hence, the metrics based on word overlap with a few references are inadequate to accurately mea-\n\nCriterion Literal Sentence\nExample Simile Candidates Quality Relevance Some raindrops struck the roof, window and ran down its panes. Some raindrops struck the roof, window and ran down its panes (like tears | like arrows).\n\nLogical Consistency\nStefan moved, every movement easy and precisely controlled.\nStefan moved (like lightning | like a dancer), every movement easy and precisely controlled.\n\nSentiment Consistency\nThe idea resounded throughout the land. The idea resounded (like an earthquake | like a thunderous wave) throughout the land.\n\nCreativity\nHe possessed a power of sarcasm which could scorch.\nHe possessed a power of sarcasm which could scorch (like vitriol | like fire).\n\nInformativeness\nThey gleamed. They gleamed (like the eyes of a cat | like the eyes of an angry cat).\nTable 1 : Examples of our criteria for Simile Generation (SG) Evaluation. We design five criteria from three perspectives. The vehicles of the better simile candidates given by each criterion are highlighted in bold.\nsure the overall quality of generated similes. As shown in Fig. 1 , the commonly used metric BLEU deems the second candidate as the highest quality, as it has more overlapped words with the only referenced groundtruth, while human deems the first candidate as the most coherent one.\n(3) The existing metrics are inadequate to provide fine-grained and comprehensive SG evaluation, considering that the creative generation tasks have distinct criteria for desired generations (Celikyilmaz et al., 2020) , such as novelty and complexity for story generation (Chhun et al., 2022) and logical consistency for dialogue generation (Pang et al., 2020) . However, establishing a comprehensive, efficient, and reliable evaluation system for SG is nontrivial, which raises three main concerns: (1) What criteria should be adopted to evaluate the SG task in a comprehensive and non-redundant fashion? (2) How to quantify each criterion into a metric thus enabling efficient and objective SG evaluation, given that the human evaluation of creative generation task is not only time-consuming but also subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014; Celikyilmaz et al., 2020) ? (3) Whether the proposed metrics are effective in providing useful scores to guide actual improvements in the realworld application of the SG model?\nIn this paper, we establish HAUSER, a Holistic and AUtomatic evaluation system for Simile gEneRation task, consisting of five criteria (Tab. 1):\n(1) The relevance between topic and vehicle, as the foundation of a simile is to compare the two via their shared properties (Paul, 1970) . (2) The logical consistency between the literal sentence and generated simile, since the aim of SG task is to polish the original sentence without altering its semantics (Tversky, 1977) . (3) The sentiment consistency between the literal sentence and generated simile, since similes generally transmit certain sentiment polarity (Qadir et al., 2015) . (4,5) The creativity and informativeness of the simile, since novel similes or those with richer content can enhance the literary experience (Jones and Estes, 2006; Roncero and de Almeida, 2015; Addison, 2001) . Overall, these five criteria can be categorized into three perspectives: quality (which considers relevance, logical, and sentiment consistency jointly), creativity, and informativeness. We further quantify each criterion into automatic metrics (Fig. 2 ) and prove their effectiveness through extensive experiments.\nTo the best of our knowledge, we are the first to systematically investigate the automatic evaluation of the SG task. To summarize, our contributions are mainly three-fold: (1) We establish a holistic and automatic evaluation system for the SG task, consisting of five criteria based on linguistic theories, facilitating both human and automatic evaluation of this task. (2) We design automatic metrics for each criterion, facilitating efficient and objective comparisons between SG models. (3) We conduct extensive experiments to verify that our metrics are significantly more correlated with human ratings than prior metrics.\n\nSimile Generation Task\nThere are two primary forms of the simile generation (SG) task: simile triplet completion and literal sentence polishing. For simile triplet completion, a model receives simile components, topic and property, and is required to generate the vehicle (Roncero and de Almeida, 2015; Zheng et al., 2019; Chen et al., 2022; He et al., 2022) . For literal sentence polishing, a model receives a literal sentence and is expected to convert it into similes (Zhang et al., 2021; Stowe et al., 2020; Chakrabarty et al., 2020; Lai and Nissim, 2022) . We focus on the latter. However, prior works mainly adopt task-agnostic automatic metrics to evaluate the SG task, raising concern as to whether the claimed improvements are comprehensive and reliable.\n\nAutomatic Evaluation for NLG Systems\nExisting automatic metrics for Natural Language Generation (NLG) evaluation can be categorized into task-agnostic and task-specific metrics. Taskagnostic metrics can be applied to various NLG tasks, which generally focus on the coherence of generations (Papineni et al., 2002; Zhang et al., 2019) , including n-gram-based metrics (Papineni et al., 2002; Lin, 2004; Denkowski and Lavie, 2014) and embedding-based metrics (Zhang et al., 2019; Zhao et al., 2019) . There are also many metrics for evaluating the diversity of generations (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) . Task-specific metrics are proposed to evaluate NLG systems on specific tasks (Tao et al., 2018; Dhingra et al., 2019; Ren et al., 2020) . Specifically, various works systematically study the evaluation of the creative generation task (Pang et al., 2020; Tevet and Berant, 2021; Chhun et al., 2022) . Different from these works, we revisit SG evaluation, propose holistic criteria based on linguistic theories, and design effective automatic metrics for it.\n\nHAUSER for SG evaluation\nWe establish HAUSER, a holistic and automatic evaluation system for SG evaluation, containing five criteria from three perspectives, and further design automatic metrics for each criterion (Fig. 2 ).\n\nQuality\nWe measure the overall quality of generated similes using three criteria: relevance, logical consistency, sentiment consistency. The key simile components -topic and vehicle -should be relevant, as the foundation of a simile is to compare the two via their shared properties (relevance) (Paul, 1970) . In Tab. 1, comparing \"raindrops\" to \"tears\" is more coherent than to \"arrows\". Additionally, the generated simile should remain logically consistent with the original sentence (logical consistency), as the SG task aims to polish the plain text without changing its semantics (Tversky, 1977) . In Tab. 1, comparing \"Stefan\" to \"dancer\" better depicts his controlled and easy movement than to \"lightning\". Furthermore, as similes generally transmit certain sentiment polarity (Qadir et al., 2015) , the generated simile should enhance the sentiment polarity of the original sentence (sentiment consistency). In Tab. 1, the vehicle \"thunderous wave\" enhances the positive polarity of the original sentence, while the vehicle \"earthquake\" brings a negative sentiment polarity in opposition to the original sentence.\n\nRelevance\nFor the relevance score, if the components of one simile are relevant, they tend to co-occur in simile sentences (Xiao et al., 2016; He et al., 2022) and possess shared properties (Paul, 1970; Tversky, 1977) . Hence, obtaining the relevance score requires large-scale simile sentences as references, as well as knowledge about the properties (adjectives) of each simile component. For a simile s, the relevance score is defined as follows:\nEQUATION\nwhere there are m p topic-vehicle pairs extracted from simile s, each denoted as (t, v) 1 . \u0393(t, v) is the set of similes containing (t, v) as simile components, each denoted as e. P e (t, v) is the probability that the simile components (t, v) share properties in the context of the simile sentence e.\nAn effective way to obtain the frequency information \u0393(t, v) and property knowledge P e (t, v) is to utilize the large-scale probabilistic simile knowledge base MAPS-KB (He et al., 2022) , which contains millions of simile triplets in the form of (topic, property, vehicle), along with frequency and two probabilistic metrics to model each triplet 2 . Specifically, the probabilistic metric Plausibility is calculated based on the confidence score of the simile instance (topic, property, vehicle, simile sentence) supporting the triplet, indicating the probability that the topic and vehicle share the property. The relevance score r can be calculated as follow:\nEQUATION\nwhere G (t,v) is the set of triplets (t, p ,v) containing the (t, v) pair in MAPS-KB, with p referring to the property. n and P are the metrics provided by MAPS-KB, where n and P denote the frequency and the plausibility of the triplet respectively. It is noticed that the metric is not coupled with MAPS-KB, as the frequency information can be obtained by referencing a large set of simile sentences and the property knowledge can be contained via other knowledge bases. More methods are beyond the scope of this paper. However, we additionally provide a method to approximate the relevance score. If we assume the probability that the simile components (t, v) share properties in each sentence is 1, the relevance score can be approximated as:\nEQUATION\nwhere n(t, v) denotes the number of samples that contain the simile components (t, v) in large-scale simile sentences. We discuss the effects of the referenced dataset size in Sec. 4.2.1.\n\nLogical Consistency\nThe literal sentence and the generated simile that are logically inconsistent generally exhibit contra-1 All the simile components in our work are extracted and cleaned using rules from (He et al., 2022) which determines the optimal semantics a component should carry, e.g., \"a kid in a candy store\" instead of just \"a kid\".\n2 More details of MAPS-KB is provided in Appx. D dictory logic. Hence, for a generated simile, we input the <literal text(l), simile(s)> sentence pair into existing pre-trained Multi-Genre Natural Language Inference (MNLI) model 3 , which determines the relation between them is entailment, neutral, or contradiction. The logical consistency score c l of this simile is defined as follows (Pang et al., 2020) :\nEQUATION\nwhere P (h <l,s> = c) represents the probability that the model predicts the relation of the sentence pair < l, s > to be contradiction (denoted as c).\n\nSentiment Consistency\nBetter similes tend to enhance the sentiment polarity of the original sentence (Qadir et al., 2015) . Hence, we first apply the model fine-tuned on the GLUE SST-2 dataset 4 to classify each simile as being either positive or negative. Then, the sentiment consistency score c s is defined as follows:\nEQUATION\nwhere a is the sentiment polarity of the literal sentence (positive or negative) predicted by the model. P (h s = a) and P (h l = a) denote the probabilities that the model predicts the sentiment polarity of the simile s and the literal sentence l to be a, respectively. It is noticed that different <topic, vehicle> pairs within a sentence may have distinct sentiment polarities, such as <She, scared rabbit> and <I, bird> in the simile \"If she escapes like a scared rabbit, I will fly like a bird to catch her.\". Directly inputting text containing multiple topic-vehicle pairs into the sentiment classification model will result in inferior performance. Therefore, for each simile, only the text from the beginning up to the first vehicle is input into the model (i.e. \"If she escapes like a scared rabbit\" in the given example), and for each literal sentence, the text from the beginning up to the first event (i.e. \"If she escapes\" in the given example) is input into the model.\nSince the aim of the SG task is to polish the plain text, the quality of similes generated from different texts can not be compared. Therefore, the normalized score among the simile candidates for each original text is utilized. Suppose there are m simile candidates S = {s 1 , s 2 , ..., s m } for the literal text l, the original relevance scores of R is R = {r 1 , r 2 , ..., r m } respectively. The normalized relevance score r \u2032 i of s i is formulated as follows:\nEQUATION\nwhich ranges from 0 to 1. Then, the normalized logical and sentiment consistency score c \u2032 li , c \u2032 si for each simile s i are obtained in the same manner 5 .\nFinally, the quality for simile s i is defined as the weighted combination of three parts as follows:\nEQUATION\nwhere \u03b1, \u03b2, and \u03b3 are hyperparameters.\n\nCreativity\nCreative similes can provide a better literary experience (Jones and Estes, 2006). In Tab. 1, comparing \"sarcasm\" to \"vitriol\" is less common than to \"fire\", yet it better conveys the intensity of a person's sarcasm. Hence, we design creativity score. Previous studies mainly evaluate the creativity of text generation tasks via human evaluation (Sai et al., 2022) , since measuring the creativity of openended text is a relatively difficult task (Celikyilmaz et al., 2020) . Although there have been many works evaluating the diversity of open-ended text generation (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) , these metrics are not suitable for measuring the creativity of the text. Because the diversity metrics take a set of generated text as input and output one score, while a creativity metric is required to measure each text individually and output a set of corresponding scores.\nDifferent from other open-ended generation tasks, the components of the generated similes enable us to evaluate creativity automatically. According to linguists, the creativity of a simile is determined by vehicles (Pierce and Chiappe, 2008; Roncero and de Almeida, 2015) . Intuitively, the generated simile may be less creative if its extracted 5 If all the relevance scores ri in R are the same, the normalized relevance scores r \u2032 i in R \u2032 are set to 0.5 uniformly.\ntopic-vehicle pair co-occurs frequently, or if many topics are compared to its vehicle in the corpus. Therefore, we adopt large-scale corpora as references when designing our creativity metric. The creativity score of s is calculated as follows:\nEQUATION\nwhere there are m v vehicles extracted from the simile s, each denoted as v. N v denotes the frequency of the vehicles appearing in the similes in the corpora. The log transformation aims to reduce the influence of extreme values.\nAn effective way to obtain the adequate frequency information N v is to utilize the millionscale simile knowledge base MAPS-KB, where the N v can be defined as follows:\nEQUATION\nG v is the set of triplets containing the vehicle v in MAPS-KB, n denotes the frequency of the triplet.\nIt is noticed that the metric is not coupled with MAPS-KB, as N v can also be obtained by counting the samples containing the vehicle v in largescale simile sentences. The method of obtaining the simile sentences is beyond the scope of this paper. Nevertheless, we discuss the effects of the referenced dataset size in Sec. 4.2.2.\n\nInformativeness\nThe vehicle with richer content can create a more impact and vivid impression (Addison, 2001) . In the example from Tab. 1, the addition of the word \"angry\" makes the similes more expressive. Therefore, we design the metric informativeness to measure the content richness of the vehicles.\nIntuitively, the more words a vehicle contains, the richer its content will be. Hence, for a given simile s, we adopt the average length of the extracted vehicles to be the informativeness score 6 (Chakrabarty et al., 2020; Zhang et al., 2021) , defined as I i = 1 mv v\u2208s len(v), where there are m v vehicles extracted from simile s. Here, BLEU2, Rouge2, and BERTScorelarge are presented since they perform the best in their respective category. To avoid overlapping points, random jitters sampled from N (0, 0.05 2 ) were added to human ratings after fitting the regression.\n\nSimile Generation\nThe existing datasets for the SG task are either Chinese (Zhang et al., 2021) , limited to the simile triplet completion (Roncero and de Almeida, 2015; Chen et al., 2022) , or having all vehicles located at end of the sentence (Chakrabarty et al., 2022; Lai and Nissim, 2022) , which are not practical for English simile generation in a real-world application.\nTo bridge the gap, we construct a large-scale English dataset for SG task based on simile sentences from (He et al., 2022) , which contains 524k simile sentences labeled with topic and vehicle. The output decoder target is the simile sentence s and the input encoder source is s rewritten to drop the comparator \"like\" and the vehicle. For example, given s = \"The idea resounded like a thunderclap throughout the land.\", the encoder source would be \"The idea resounded throughout the land.\". In particular, we remove the simile sentences whose event is a linking verb (e.g. be, seem, turn) as they would be meaningless after the vehicle is removed. The final train, validation and test sets contain 139k, 2.5k, and 2.5k sentence pairs, respectively. Based on our constructed dataset, we finetune a pre-trained sequence-to-sequence model, BART (Lewis et al., 2020) , for the SG task, which has been demonstrated to be an effective framework for various figurative language generation (Zhang and Wan, 2021; Chakrabarty et al., 2022; He et al., 2022; Lai and Nissim, 2022) . The experiments are run on RTX3090 GPU and the implementation of BART is based on the HuggingFace Transformers 7 . The experiments are run with a batch size of 16, a max sequence length of 128, and a learning rate of 4e-5 for 10 epochs.\n\nEvaluation Dataset Construction\nFirstly, we randomly sample 50 literal sentences from the test set and adopt the trained SG model to generate five candidates for each one. Then, for each perspective, three raters are asked to rate each 7 https://github.com/huggingface/transformers/ simile from 1 to 5, where 1 denotes the worst and 5 denotes the best 8 . Since evaluating the quality of generated similes is subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014), we remove the simile-literal sentence pairs if (1) raters argue that the pairs lack context and are difficult to rate (e.g. \"Nobody can shoot.\") or (2) some raters rate them as low quality (quality score of 1-2), while others rate them as high quality (scores of 4-5) (Niculae and Danescu-Niculescu-Mizil, 2014) . Moreover, we measure the inter-rater agreement by holding out the ratings of one rater at a time, calculating the correlations with the average of the other rater's ratings, and finally calculating the average or maximum of all the held-out correlations (denoted as \"Mean\" and \"Max\", respectively). The inter-rater agreement before and after applying the filtering strategies is shown in Tab. 2. Overall, the final inter-rater agreement ensures the reliability of our evaluation of automatic metrics and the filtering strategies improve the inter-rater agreement generally. We finally get 150 simile candidates generated from 44 literal sentences.\n\nQuality\nWe compare our quality metric with the following automatic metrics 9 : (1) BLEU (Papineni et \n\n2002) calculates the precision of n-gram matches,\n(2) RougeL (Lin, 2004 ) is a recall-oriented metric, (3) METEOR (Denkowski and Lavie, 2014) proposes a set of linguistic rules to compare the hypothesis with the reference, ( 4) BERTScore (Zhang et al., 2019) calculates the cosine similarity between the BERT embeddings, ( 5) Perplexity (Pang et al., 2020) measures the proximity of a language model, the inverse of which is utilized.\nCorrelations with Human Ratings. Tab. 3 shows the correlation coefficients between automatic metrics and human ratings. Firstly, our metrics are significantly more correlated with human ratings than prior automatic metrics. Moreover, all the sentence-level metrics, which consider the semantics of the entire sentence, perform worse than almost all the n-gram-level metrics, which compare the n-grams between the hypothesis and the reference, which reveals that simile components need to be specifically considered during SG evaluation.\nAccording to the visualized correlation result in Fig. 3 , datapoints from prior automatic metrics tend to scatter at 0 or 1, while the datapoints from our metric are distributed closer to the fitter line, proving that our metric can better measure the quality.\nRecommendation Task. We compare the rankings given by automatic metrics with human rankings 10 . We adopt the following metrics: Hit Ratio at rank K (HR@K(K=1,3)), Norgenerated from different literal sentences can not be compared. Please refer to Appx. C for the implementation of them. 10 We remove the literal sentences with fewer than three valid simile candidates in this task, as they are too simple to rank. We finally get 134 sentences from 35 literal sentences.\n\nMetrics\nHR@1 HR@3 nDCG@1 nDCG@3 MRR malized Discounted Cumulative Gain at rank K (NDCG@K(K=1,3)) 11 , and Mean Reciprocal Rank (MRR). From Tab. 4, our metric achieves significant improvement compared to other metrics, indicating that our metric can yield more accurate rankings for quality. Also, the n-gram-level metrics generally outperform sentence-level metrics, which is consistent with the result in Tab. 3. Ablation Study. To investigate the importance of different sub-metrics in quality metric, we compare the correlation between quality metric and human ratings after removing each sub-metric individually. From Tab. 3, the removal of any sub-metric leads to a decline in performance, which proves the effectiveness of each sub-metric. Among three components, the removal of the relevance results in the largest performance drop, which reveals that relevance is the most important sub-metric.\nThe Effects of Hyperparameters. Since different sub-metrics have varying levels of importance, we study the correlation results when gradually increasing the weight of relevance component and decreasing the weight of sentiment consistency component (as in Tab. 5). From Fig. 4 (left), increasing the weight of the relevance component consistently results in improved performance, peaking at the combination [7](\u03b1, \u03b2, \u03b3 = 3/6, 2/6, 1/6), before eventually causing a decline in performance. This reveals that although relevance is the most important sub-metric, too much weight on it can be detrimental.\nThe Effects of Referenced Dataset Size. We sample different numbers of simile sentences from (He et al., 2022) as references for relevance 5/6, 1/12, 1/12\nTable 5 : The setting of each hyperparameters combination for the quality metric. The result is shown in Fig. 4 (left).\n[\n1] [2] [3] [4] [5] [6] [7] [8] [9]\nHyper-parameters Combination score and study the correlation between the quality metric and human ratings 12 . From Fig. 4 (right) 13 , correlations grow linearly with exponential growth in referenced dataset size, indicating that using datasets larger than 100k will improve the correlation coefficients. Moreover, the performance at the peak surpasses the prior automatic metrics, proving the effectiveness of our approximation method.\n\nCreativity\nWe compare our creativity metric with the following automatic metrics: (1) Perplexity which is often utilized to measure diversity as well (Tevet and Berant, 2021) , (2) Self-BLEU (Zhu et al., 2018) calculates the BLEU score of each generation against all other generations as references, (3) Distinct n-grams(Dist) (Tevet and Berant, 2021) , which is the fraction of distinct n-grams from all possible n-grams across all generations.\nCorrelations with Human Ratings. From Tab. 6, our metric creativity is significantly more correlated with human evaluation scores compared with prior diversity metrics. According to the visualized correlation result in Fig. 5 , the prior diversity metrics have either wide confidence intervals (Perplexity, Dist) or scattered datapoints (self-BLEU), whereas our creativity metrics exhibit stronger linear correlation and narrower confidence intervals (Creativty w/ Log), implying higher reliability.\nRecommendation Task. We compare the rankings given by automatic metrics with human rank- ings. According to Tab. 7, our creativity metric outperforms prior automatic metrics, which proves our metric can better measure the creativity of simile candidates given a literal sentence, which is consistent with the results in Tab. 6. Ablation Study. According to Tab. 6, removing the log transformation leads to significant performance drops. According to the visualized correlation result in Fig. 5 , the datapoints are distributed closer to the fitter line and exhibit narrower confidence intervals after applying the log transformation, which further proves that log transformation is essential for our creativity metric.\nThe Effects of Referenced Dataset Size. According to Fig. 6 (left), the correlation coefficients increase continuously and eventually converge as the number of referenced sentences increases. Moreover, the performance after convergence is comparable to that given by the creativity metric based on the simile KB. The trend reveals that our metric referencing 10k similes can achieve a promising correlation with human ratings.\n\nInformativeness\nThe Pearson and Spearman correlation coefficients between our informativeness metric and human ratings are 0.798 and 0.882, respectively. According to Fig. 6 (right), the strong linear correlation between the metric and human ratings proves that our informativeness metric is simple yet quite effective.\n\nRelation between Metrics\nWe present pair-wise correlations between the three automatic metrics in Tab. 8 and also visualize them in Fig. 7 . Among the three metrics, creativity correlates with informativeness moderately, mainly because shorter vehicles tend to be less creative than longer ones. The correlations of all other pairwise metrics are relatively weak. Thus, it is evident that the three metrics are independent of each other and it is necessary to measure each one of them to obtain a holistic view of SG evaluation. \n\nHAUSER Application\nWe perform a case study to prove that our designed automatic metrics are effective for various methods. Here, we apply our metrics to a retrieval method (Zhang et al., 2021) (denoted as BM25), which utilizes the 20 context words around the insertion position given by groundtruth to retrieve the 5 most similar samples based on the BM25 ranking score from the training set, and adopts the vehicles from these samples to be those of simile candidates. This method ensures the diversity of generated similes. The method introduced in Sec. 4.1 is denoted as Ours. Given the candidates generated by each method, we rerank them using a weighted combination of quality, creativity, and informativeness rankings obtained by HAUSER, with a ratio of 2:2:1. From Tab. 11 in Appendix, the candidates generated by various methods can be more correlated with human rankings after being ranked by our metrics, thus proving the generality of our metrics. It is noticed that the insertion position for BM25 is provided by the groundtruth, while the insertion position for Ours is predicted by the model, thus proving the effectiveness of our generation method.\n\nConclusion\nIn this work, we systematically investigate the evaluation of the Simile Generation (SG) task. We establish a holistic and automatic evaluation system for the SG task, containing five criteria from three perspectives, and propose holistic automatic metrics for each criterion. Extensive experiments verify the effectiveness of our metrics. more correlated with humans than prior referencebased metrics (e.g. BLEU, Rouge, BERTScore), our metrics are still reference-based and rely on the quality and scale of referenced data. We have discussed the effect of referenced dataset size in our paper and will design reference-free metrics to further complement our metrics in future work. Additionally, since our metrics utilize a million-scale simile knowledge base or large-scale simile sentences as references, the efficiency of our method is slightly lower than the automatic metrics based on a few references. Nevertheless, this limitation does not prevent our metrics from performing systematic and scalable comparisons between SG models.\n"}
{"question": "What is the primary purpose of the boundary forward diffusion process in the DIFFUSIONNER model?", "evidence": "  In order to align the number of entities in different instances, we first expand the entity set to a fixed number K (> N )   ", "options": ["A. To add noise to entity boundaries for denoising.", "B. To extract entities from the sentence.", "C. To calculate sentence encodings.", "D. To classify entities.", "Boundary forward diffusion is the process of adding noise to the entity boundary in a stepwise manner. "], "answer": "A", "content": "\nIntroduction\nNamed Entity Recognition (NER) is a basic task of information extraction (Tjong Kim Sang and De Meulder, 2003) , which aims to locate entity mentions and label specific entity types such as person, location, and organization. It is fundamental to many structured information extraction tasks, such as relation extraction (Li and Ji, 2014; Miwa and Bansal, 2016) and event extraction (McClosky et al., 2011; Wadden et al., 2019) .\nMost traditional methods (Chiu and Nichols, 2016) formulate the NER task into a sequence labeling task by assigning a single label to each token. To accommodate the nested structure between entities, some methods (Ju et al., 2018; Wang et al., + + \u00b2 \u00bb N (0; 1) + \u00b2 \u00bb N (0; 1)\nFigure 1 : Boundary diffusion in named entity recognition. The fixed forward diffusion process adds Gaussian noise to the entity boundaries at each timestep, and the noisy boundaries recover their original state by denoising with the learnable reverse diffusion process. For inference, the reverse diffusion process generates entity boundaries and performs entity typing based on the noisy spans sampled from the Gaussian distribution. 2020) further devise cascaded or stacked tagging strategies. Another class of methods treat NER as a classification task on text spans (Sohrab and Miwa, 2018; Eberts and Ulges, 2020) , and assign labels to word pairs (Yu et al., 2020; Li et al., 2022a) or potential spans (Lin et al., 2019; Shen et al., 2021a) . In contrast to the above works, some pioneer works (Paolini et al., 2021; Yan et al., 2021b; Lu et al., 2022) propose generative NER methods that formulate NER as a sequence generation task by translating structured entities into a linearized text sequence. However, due to the autoregressive manner, the generation-based methods suffer from inefficient decoding. In addition, the discrepancy between training and evaluation leads to exposure bias that impairs the model performance.\nWe move to another powerful generative model for NER, namely the diffusion model. As a class of deep latent generative models, diffusion models have achieved impressive results on image, audio and text generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021; Li et al., 2022b; Gong et al., 2022) . The core idea of diffusion models is to systematically perturb the data through a forward diffusion process, and then recover the data by learning a reverse diffusion process.\nInspired by this, we present DIFFUSIONNER, a new generative framework for named entity recognition, which formulates NER as a denoising diffusion process (Sohl-Dickstein et al., 2015; Ho et al., 2020) on entity boundaries and generates entities from noisy spans. As shown in Figure 1 , during training, we add Gaussian noise to the entity boundaries step by step in the forward diffusion process, and the noisy spans are progressively denoised by a reverse diffusion process to recover the original entity boundaries. The forward process is fixed and determined by the variance schedule of the Gaussian Markov chains, while the reverse process requires learning a denoising network that progressively refines the entity boundaries. For inference, we first sample noisy spans from a prior Gaussian distribution and then generate entity boundaries using the learned reverse diffusion process.\nEmpowered by the diffusion model, DIFFUSION-NER presents three advantages. First, the iterative denoising process of the diffusion model gives DIFFUSIONNER the ability to progressively refine the entity boundaries, thus improve performance. Second, independent of the predefined number of noisy spans in the training stage, DIF-FUSIONNER can sample a different number of noisy spans to decode entities during evaluation. Such dynamic entity sampling makes more sense in real scenarios where the number of entities is arbitrary. Third, different from the autoregressive manner in generation-based methods, DIFFUSION-NER can generate all entities in parallel within several denoising timesteps. In addition, the shared encoder across timesteps can further speed up inference. We will further analyze these advantages of DIFFUSIONNER in \u00a7 6.2. In summary, our main contributions are as follows:\n\u2022 DIFFUSIONNER is the first to use the diffusion model for NER, an extractive task on discrete text sequences. Our exploration provides a new perspective on diffusion models in natural language understanding tasks.\n\u2022 DIFFUSIONNER formulates named entity recognition as a boundary denoising diffusion process from the noisy spans. DIFFUSION-NER is a novel generative NER method that generates entities by progressive boundary refinement over the noisy spans.\n\u2022 We conduct experiments on both nested and flat NER to show the generality of DIFFU-SIONNER. Experimental results show that our model achieves better or competitive performance against the previous SOTA models.\n2 Related Work\n\nNamed Entity Recognition\nNamed entity recognition is a long-standing study in natural language processing. Traditional methods can be divided into two folders: tagging-based and span-based. For tagging-based methods (Chiu and Nichols, 2016; Ju et al., 2018; Wang et al., 2020) , they usually perform sequence labeling at the token level and then translate into predictions at the span level. Meanwhile, the span-based methods (Sohrab and Miwa, 2018; Eberts and Ulges, 2020; Shen et al., 2021a,b; Li et al., 2022a) directly perform entity classification on potential spans for prediction. Besides, some methods attempt to formulate NER as sequence-to-set (Tan et al., 2021 (Tan et al., , 2022;; Wu et al., 2022) or reading comprehension (Li et al., 2020; Shen et al., 2022) tasks for prediction. In addition, autoregressive generative NER works (Athiwaratkun et al., 2020; De Cao et al., 2021; Yan et al., 2021b; Lu et al., 2022) linearize structured named entities into a sequence, relying on sequence-to-sequence language models (Lewis et al., 2020; Raffel et al., 2020) to decode entities. These works designed various translation schemas, including from word index sequence to entities (Yan et al., 2021b) and from label-enhanced sequence to entities (Paolini et al., 2021) , to unify NER to the text generation task and achieved promising performance and generalizability. Other works (Zhang et al., 2022) focus on the disorder of the entities and mitigate incorrect decoding bias from a causal inference perspective. Different from previous works, our proposed DIFFUSIONNER is the first one to explore the utilization of the generative diffusion model on NER, which enables progressive refinement and dynamic sampling of entities. Furthermore, compared with previous generation-based methods, our DIFFUSIONNER can also decode entities in a nonautoregressive manner, and thus result in a faster inference speed with better performance.\n\nDiffusion Model\nDiffusion model is a deep latent generative model proposed by (Sohl-Dickstein et al., 2015) . With the development of recent work (Ho et al., 2020) , diffusion model has achieved impressive results on image and audio generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021) . Diffusion model consists of the forward diffusion process and the reverse diffusion process. The former progressively disturbs the data distribution by adding noise with a fixed variance schedule (Ho et al., 2020) , and the latter learns to recover the data structure. Despite the success of the diffusion model in continuous state spaces (image or waveform), the application to natural language still remains some open challenges due to the discrete nature of text (Austin et al., 2021; Hoogeboom et al., 2022; Strudel et al., 2022; He et al., 2022) . Diffusion-LM (Li et al., 2022b) models discrete text in continuous space through embedding and rounding operations and proposes an extra classifier as a guidance to impose constraints on controllable text generation. DiffuSeq (Gong et al., 2022) and SeqDiffuSeq (Yuan et al., 2022a) extend diffusionbased text generation to a more generalized setting. They propose classifier-free sequence-to-sequence diffusion frameworks based on encoder-only and encoder-decoder architectures, respectively.\nAlthough diffusion models have shown their generative capability on images and audio, its potential on discriminative tasks has not been explored thoroughly. Several pioneer works (Amit et al., 2021; Baranchuk et al., 2022; Chen et al., 2022) have made some attempts on diffusion models for object detection and semantic segmentation. Our proposed DIFFUSIONNER aims to solve an extractive task on discrete text sequences.\n\nPreliminary\nIn diffusion models, both the forward and reverse processes can be considered a Markov chain with progressive Gaussian transitions. Formally, given a data distribution x 0 \u223c q (x 0 ) and a predefined variance schedule {\u03b2 1 , . . . , \u03b2 T }, the forward process q gradually adds Gaussian noise with variance \u03b2 t \u2208 (0, 1) at timestep t to produce latent variables x 1 , x 2 , . . . , x T as follows:\nq (x 1 , . . . , x T | x 0 ) = T t=1 q (x t | x t\u22121 )\n(1)\nq (x t | x t\u22121 ) = N x t ; 1 \u2212 \u03b2 t x t\u22121 , \u03b2 t I (2)\nAn important property of the forward process is that we can sample the noisy latents at an arbitrary timestep conditioned on the data x 0 . With the notation \u03b1 t := 1 \u2212 \u03b2 t and \u1fb1t := t s=0 \u03b1 s , we have:\nEQUATION\nAs \u1fb1T approximates 0, x T follows the standard Gaussian distribution: p (x T ) \u2248 N (x T ; 0, I). Unlike the fixed forward process, the reverse process p \u03b8 (x 0:T ) is defined as a Markov chain with learnable Gaussian transitions starting at a prior p (x T ) = N (x T ; 0, I):\np \u03b8 (x 0:T ) = p (x T ) T t=1 p \u03b8 (x t\u22121 | x t ) p \u03b8 (x t\u22121 | x t ) = N (x t\u22121 ; \u00b5 \u03b8 (x t , t) , \u03a3 \u03b8 (x t , t))\nwhere \u03b8 denotes the parameters of the model and \u00b5 \u03b8 and \u03a3 \u03b8 are the predicted covariance and mean of q \n(x t\u22121 | x t ). We set \u03a3 \u03b8 (x t , t) = \u03c3 2 t I and build a neural network f \u03b8 to predict the data x 0 , denoted as x0 = f \u03b8 (x t , t). Then we have \u00b5 \u03b8 (x t , t) = \u03bct (x t , x0 ) = \u03bct (x t , f \u03b8 (x t , t)), where \u03bct denotes the mean of posterior q (x t\u22121 | x t , x0 ).\n\nMethod\nIn this section, we first present the formulation of diffusion model for NER (i.e., the boundary denoising diffusion process) in \u00a7 4.1. Then, we detail the architecture of the denoising network for boundary reverse process in \u00a7 4.2. Finally, we describe the inference procedure of DIFFUSIONNER in \u00a7 4.3.\n\nBoundary Denoising Diffusion Model\nGiven a sentence S with length M , the named entity recognition task is to extract the entities E = {(l i , r i , t i )} N i=0 contained in the sentence, where N is the number of entities and l i , r i , t i denote the left and right boundary indices and type of the i-th entity. We formulate NER as a boundary denoising diffusion process, as shown in Figure 2 . We regard entity boundaries as data samples, then the boundary forward diffusion is to add Gaussian noise to the entity boundaries while the reverse diffusion process is to progressively recover the original entity boundaries from the noisy spans. Boundary Forward Diffusion Boundary forward diffusion is the process of adding noise to the entity boundary in a stepwise manner. In order to align the number of entities in different instances, we first expand the entity set to a fixed number K (> N ). There are two ways to expand the entities, repetition strategy and random strategy, which add K \u2212 N entities by duplicating entities or sampling random spans from a Gaussian distribution 2 . For convenience, we use B \u2208 R K\u00d72 to denote the boundaries of the K expanded entities, with all of them normalized by the sentence length M and scaled to (\u2212\u03bb, \u03bb) interval. Formally, given the entity boundaries as data samples x 0 = B, we can obtain the noisy spans at timestep t using the forward diffusion process. According to Equation (3), we have:\nx t = \u221a \u1fb1t x 0 + \u221a 1 \u2212 \u1fb1t \u03f5 (4)\nwhere \u03f5 \u223c N (0, I) is the noise sampled from the standard Gaussian. At each timestep, the noisy spans have the same shape as x 0 , i.e.,\nx 1 , x 2 , . . . , x T \u2208 R K\u00d72 .\nBoundary Reverse Diffusion Starting from the noisy spans x T sampled from the Gaussian distribution, boundary reverse diffusion adopts a non-Markovian denoising practice used in DDIM (Song et al., 2021) to recover entities boundaries. Assuming \u03c4 is an arithmetic subsequence of the complete timestep sequence [1, . . . , T ] of length \u03b3 with \u03c4 \u03b3 = T . Then we refine the noisy spans x \u03c4 i to 2 We will discuss these two practices in \u00a7 6.3.\nx \u03c4 i\u22121 as follows:\nEQUATION\n)\n\u03b5\u03c4 i = x \u03c4 i \u2212 \u221a \u03b1 \u03c4 i x0 \u221a 1 \u2212 \u03b1 \u03c4 i (6) x \u03c4 i\u22121 = \u221a \u03b1 \u03c4 i\u22121 x0 + 1 \u2212 \u03b1 \u03c4 i\u22121 \u03b5\u03c4 i (7)\nwhere x0 and \u03b5\u03c4 i are the predicted entity boundary and noise at timestep \u03c4 i . f \u03b8 (x t , S, t) is a learnable denoising network and we will cover the network architecture in the next section ( \u00a7 4.2). After \u03b3 iterations of DDIM, the noisy spans are progressively refined to the entity boundaries.\n\nNetwork Architecture\nDenoising network f \u03b8 (x t , S, t) accepts the noisy spans x t and the sentence S as inputs and predicts the corresponding entity boundaries x0 . As shown in Figure 2 , we parameterize the denoising network with a sentence encoder and an entity decoder.\nSentence Encoder consists of a BERT (Devlin et al., 2019) plus a stacked bi-directional LSTM.\nThe whole span encoder takes the sentence S as input and outputs the sentence encoding H S \u2208 R M \u00d7h . The sentence encoding H S will be calculated only once and reused across all timesteps to save computations.\nEntity Decoder uses the sentence encoding H S to first compute the representations of K noisy spans x t and then predicts the corresponding entity boundaries. Specifically, we discretize the noisy spans into word indexes by rescaling, multiplying and rounding 3 , then perform mean pooling over the Take gradient descent step by optimize\n\u2212 K i=1 log P c i (\u03c0 c (i)) + \u03b4\u2208l,r log P \u03b4 i (\u03c0 \u03b4 (i)) 10 until converged;\ninner-span tokens. The extracted span representations can be denoted as H X \u2208 R K\u00d7h . To further encode the spans, we design a span encoder that consists of a self-attention and a cross-attention layer. The former enhances the interaction between spans with key, query, and value as H X . And the latter fuses the sentence encoding to the span representation with key, value as H S , and query as H X . We further add the sinusoidal embedding E t (Vaswani et al., 2017) of timestep t to the span representations. Thus the new representations HX of the noisy spans can be computed:\nHX = SpanEncoder(H S , H X ) + E t ,\nThen we use two boundary pointers to predict the entity boundaries. For boundary \u03b4 \u2208 {l, r}, we compute the fusion representation H \u03b4 SX \u2208 R K\u00d7M \u00d7h of the noisy spans and the words, and then the probability of the word as the left or right boundaries P \u03b4 \u2208 R K\u00d7M can be computed as:\nH \u03b4 SX = H S W \u03b4 S + HX W \u03b4 X P \u03b4 = sigmoid(MLP(H \u03b4 SX ))\nwhere W \u03b4 S , W \u03b4 X \u2208 R h\u00d7h are two learnable matrixes and MLP is a two-layer perceptron. Based on the boundary probabilities, we can predict the boundary indices of the K noisy spans. If the current step is not the last denoising step, we compute x0 by normalizing the indices with sentence length M and scaling to (\u2212\u03bb, \u03bb) intervals. Then we conduct the next iteration of the reverse diffusion process according to Equations ( 5) to (7).\nIt is worth noting that we should not only locate entities but also classify them in named entity recognition. Therefore, we use an entity classifier to classify the noisy spans. The classification probability P c \u2208 R K\u00d7C is calculated as follows:\nP c = Classifier( HX ) Algorithm 2: Inference 1 xT \u223c N (0, I) \u2208 R K eval \u00d72\n2 \u03c4 is an arithmetic sequence of length \u03b3 with \u03c4\u03b3 = T 3 for i = \u03b3, . . . , 1 do 4 Compute x0, P l , P r and P c via f \u03b8 (xt, S, t)\n5 x\u03c4 i\u22121 = \u221a \u03b1\u03c4 i\u22121 x0 + 1 \u2212 \u03b1\u03c4 i\u22121 \u2022 x\u03c4 i \u2212 \u221a \u03b1\u03c4 i x0 \u221a 1\u2212\u03b1\u03c4 i 6 end 7 Decode entities (li, ri, ci) K eval i=0\n, where \u03b4i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c} 8 Perform post-processing on (li, ri, ci) K eval i=0 9 return final entities where C is the number of entity types and Classifier is a two-layer perceptron with a softmax layer.\nTraining Objective With K entities predicted from the noisy spans and N ground-truth entities, we first use the Hungarian algorithm (Kuhn, 1955) to solve the optimal matching \u03c0 between the two sets 4 as in Carion et al. (2020) . \u03c0(i) denotes the ground-truth entity corresponding to the i-th noisy span. Then, we train the boundary reverse process by maximizing the likelihood of the prediction:\nL = \u2212 K i=1 \u03b4\u2208{l,r,c} log P \u03b4 i \u03c0\u03b4 (i)\nwhere \u03c0l (i), \u03c0r (i) and \u03c0c (i) denote the left and right boundary indexes and type of the \u03c0(i) entity. Overall, Algorithm 1 displays the whole training procedure of our model for an explanation.\n\nInference\nDuring inference, DIFFUSIONNER first samples K eval noisy spans from a Gaussian distribution, then performs iterative denoising with the learned boundary reverse diffusion process based on the denoising timestep sequence \u03c4 . Then with the predicted probabilities on the boundaries and type, we can decode K eval candidate entities (l i , r i , c i ) K eval i=0 , where \u03b4 i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c}. After that, we employ two simple post-processing operations on these candidates: de-duplication and filtering. For spans with identical boundaries, we keep the one with the maximum type probability. For spans with the sum of prediction probabilities less than the threshold \u03c6, we discard them. The inference procedure is shown in Algorithm 2. 5 Experimental Settings\n\nDatasets\nFor nested NER, we choose three widely used datasets for evaluation: ACE04 (Doddington et al., 2004) , ACE05 (Walker et al., 2006) , and GE-NIA (Ohta et al., 2002) . ACE04 and ACE05 belong to the news domain and GENIA is in the biological domain. For flat NER, we use three common datasets to validate: CoNLL03 (Tjong Kim Sang and De Meulder, 2003) , OntoNotes (Pradhan et al., 2013) , and MSRA (Levow, 2006) . More details about datasets can be found in Appendix B.\n\nBaselines\nWe choose a variety of recent advanced methods as our baseline, which include: 1) Tagging-based methods (Strakov\u00e1 et al., 2019; Ju et al., 2018; Wang et al., 2020) ; 2) Span-based methods (Yu et al., 2020; Li et al., 2020; Wan et al., 2022; Lou et al., 2022; Zhu and Li, 2022; Yuan et al., 2022b) ; 3) Generation-based methods (Tan et al., 2021; Yan et al., 2021b; Lu et al., 2022) . More details about baselines can be found in Appendix D.\n\nImplementation Details\nFor a fair comparison, we use bert-large (Devlin et al., 2019) ary refinement, and thus obtain better performance.\nThe results also validate that our DIFFUSIONNER can recover entity boundaries from noisy spans via boundary denoising diffusion.\n\nAnalysis\nInference Efficiency To further validate whether our DIFFUSIONNER requires more inference computations, we also conduct experiments to compare the inference efficiency between DIFFUSIONNER and other generation-based models (Lu et al., 2022; Yan et al., 2021a) . Just as shown in Table 3 , we find that DIFFUSIONNER could achieve better performance while maintaining a faster inference speed with minimal parameter scale. Even with a denoising timestep of \u03b3 = 10, DIFFUSIONNER is 18\u00d7 and 3\u00d7 faster than them. This is because DIFFU-SIONNER generates all entities in parallel within several denoising timesteps, which avoids generating the linearized entity sequence in an autoregressive manner. In addition, DIFFUSIONNER shares sentence encoder across timesteps, which further accelerates inference speed. speed of DIFFUSIONNER under various numbers of noisy spans. Just as shown in Figure 3 , we find that, with an increase of denoising steps, the model obtains incremental performance improvement while sacrificing inference speed. Considering the trade-off between performance and efficiency, we set \u03b3 = 5 as the default setting. In addition, when the noisy spans are smaller, the improvement brought by increasing the denoising timesteps is more obvious. This study indicates that our DiffusionNER can effectively counterbalance the negative impact of undersampling noise spans on performance by utilizing additional timesteps. \n\nSampling Number\nAs a generative latent model, DIFFUSIONNER can decouple training and eval-uation, and dynamically sample noisy spans during evaluation. To manifest this advantage, we train DIFFUSIONNER on ACE04 with K = 60 noisy spans and evaluate it with different sampling numbers K eval . The results are shown in Figure 4 . Overall, the model performance becomes better as the sampling number of noisy spans increases. Specifically, we find that DIFFUSIONNER performs worse when K eval < 30. We guess this is because fewer noisy spans may not cover all potential entities. When sampling number K eval > 60, we find it could also slightly improve model performance. Overall, the dynamic sampling of noisy spans in DIFFUSIONNER has the following advantages: 1) we can improve model performance by controlling it to sample more noisy spans; 2) dynamic sampling strategy also allows the model to predict an arbitrary number of entities in any realworld application, avoiding the limitations of the sampling number at the training stage.\n\nAblation Study\nNetwork Architecture As shown in Table 4 , we conduct experiments to investigate the network architecture of the boundary reverse diffusion process. We found that DIFFUSIONNER performs better with a stronger pre-trained language model (PLM), as evidenced by an improvement of +0.53% on ACE04 and +0.11% on CoNLL03 when using roberta-large. Additionally, for the span encoder, we find that directly removing self-attention between noisy spans or cross-attention of spans to the sentence can significantly impair performance. When both are ablated, model performance decreases by 1.37% and 1.15% on ACE04 and CoNLL03. These results indicate that the interaction between the spans or noisy spans and the sentence is necessary. the added noise at each timestep during boundary forward diffusion process. Therefore, we analyze the performance of DIFFUSIONNER on different variance schedulers with different noise timesteps T . The results on ACE04 and CoNLL03 are shown in Table 5 . We find that the cosine scheduler generally yields superior results on the ACE04, while the linear scheduler proves to be more effective on CoNLL03. In addition, the performance of DIFFU-SIONNER varies with the choice of noise timestep, with the best performance achieved at T = 1000 for ACE04 and T = 1500 for CoNLL03.\n\nExpansion Stratagy\nThe expansion stratagy of the entity set can make the number of K noisy spans consistent across instances during training.\nWe conduct experiments to analyze the performance of DIFFUSIONNER for different expansion strategies with various numbers of noisy spans. The experimental results are shown in Table 6 . Generally, we find that the random strategy could achieve similar or better performance than the repetitive strategy. In addition, Table 6 shows that DIFFU-SIONNER is insensitive to the number of noisy spans during training. Considering that using more noisy spans brings more computation and memory usage, we set K = 60 as the default setting.\n\nConclusion\nIn this paper, we present DIFFUSIONNER, a novel generative approach for NER that converts the task into a boundary denoising diffusion process. Our evaluations on six nested and flat NER datasets show that DIFFUSIONNER achieves comparable or better performance compared to previous stateof-the-art models. Additionally, our additional analyses reveal the advantages of DIFFUSIONNER in terms of inference speed, progressive boundary refinement, and dynamic entity sampling. Overall, this study is a pioneering effort of diffusion models for extractive tasks on discrete text sequences, and we hope it may serve as a catalyst for more research about the potential of diffusion models in natural language understanding tasks.\n"}
{"question": "What is not our task?", "evidence": "  To address these shortcomings, our setup aims to edit a claim using a given piece of grounded evidence that supports or refutes the original claim (see Figure 2 ). Using gold-standard evidence avoids the issue where a system outputs the correct answer by chance due to hallucinations. In our setting, a system must be faithful to the evidence to correct factual errors, allowing us to evaluate system performance more fairly. Furthermore, we require the model not to edit the original claim if it is already factually consistent with the provided evidence.\n ", "options": ["A. Revise a claim by utilizing a provided piece of grounded evidence that either supports or contradicts the initial claim.", "B. Prevents the occurrence of systems generating correct answers by chance as a result of hallucinations.", "C. Evaluate system performance in a more fairly way.", "D. Stipulate the model to edit the original claim if it is  factually  in line with the given evidence.\n"], "answer": "D", "content": "\nIntroduction\nThe task of correcting factual errors is in high demand and requires a significant amount of human effort. The English Wikipedia serves as a notable case in point. It is continually updated by over 120K editors, with an average of around six factual edits made per minute 2 . Using machines to correct factual errors could allow the articles to be updated with the most current information automatically. This process, due to its high speed, can help retain the integrity of the content and prevent the spread of false or misleading information.\nIn addition, the hallucination issues have been shown to be a prime concern for neural models,\n\nEvidence\nThe novel COVID-19 is highly contagious and is transmitted mostly through respiratory droplets. But, whether its transmission can be forwarded by touching a surface (i.e., a fomite) is uncertain.... COVID-19 has a case fatality rate of below 2%.\n\nFinal Correction\nCOVID-19 is not infectious.\n\nInput Claim\nFigure 1 : An example of a factual but unfaithful correction leading to misleading information. While it is technically true that the majority of people infected with COVID-19 will recover, there is no information in the evidence that supports the final correction. Additionally, when this statement is taken out of context, it could mislead people to believe that COVID-19 is not dangerous and that there is no need for precautions, which is false. A factual and faithful correction is \"COVID-19 is highly contagious.\".\nwhere they are prone to generate content factually inconsistent with the input sources due to the unfaithful training samples (Maynez et al., 2020) and the implicit \"knowledge\" it learned during pre-training (Niven and Kao, 2019) . Factual error correction can be used in both pre-processing and post-processing steps to rectify the factual inconsistencies in training data and generated texts, respectively. This can help build trust and confidence in the reliability of language models.\nPrior work typically formulates factual error correction as a sequence-to-sequence task, either in a fully supervised or in a distantly supervised manner (Shah et al., 2020; Thorne and Vlachos, 2021) . While these approaches have made great strides in generating fluent and grammatically valid corrections, they only focus on the aspect of factuality: whether the outputs are aligned with facts. Little emphasis was placed on faithfulness: the factual consistency of the outputs with the evidence. Faithfulness is critical in this task as it indicates whether a generated correction reflects the information we intend to update. If faithfulness is not ensured, this could lead to the spread of misleading content, causing serious consequences. Figure 1 shows a concrete example. In the context of automatically updating textual knowledge bases, the topic of an unfaithful output would likely deviate much from that of the expected correction. Therefore, such an edit is not desirable, even if it is factual.\nIn this work, we present the first study on the faithfulness aspect of factual error correction. To address faithfulness, we propose a zero-shot factual error correction framework (ZEROFEC), inspired by how humans verify and correct factual errors. When humans find a piece of information suspicious, they tend to first identify potentially false information units, such as noun phrases, then ask questions about each information unit, and finally look for the correct answers in trustworthy evidence (Saeed et al., 2022; Chen et al., 2022) . Following a similar procedure, ZEROFEC breaks the factual error correction task into five sub-tasks:\n(1) claim answer generation: extracting all information units, such as noun phrases and verb phrases, from the input claim; (2) question generation: generating question given each claim answer and the original claim such that each claim answer is the answer to each generated question; (3) question answering: answering each generated question using the evidence as context; (4) QA-to-claim: converting each pair of generated question and answer to a declarative statement; (5) correction scoring: evaluating corrections based on their faithfulness to the evidence, where faithfulness is approximated by the entailment score between the evidence and each candidate correction. The highest-scoring correction is selected as the final output. An overview of our framework is shown in Figure 2 . Our method ensures the corrected information units are derived from the evidence, which helps improve the faithfulness of the generated corrections. In addition, our approach is naturally interpretable since the questions and answers generated directly reflect which information units are being compared with the evidence.\nOur contributions can be summarized as follows:\n\u2022 We propose ZEROFEC, a factual error correction framework that effectively addresses faithfulness by asking questions about the input claim, seeking answers in the evidence, and scoring the outputs by faithfulness. \u2022 Our approach outperforms all prior methods, including fully-supervised approaches trained on 58K instances, in ensuring faithfulness on two factual error correction datasets, FEVER (Thorne et al., 2018) and SCIFACT (Wadden et al., 2020) . \u2022 We analyze the correlation of human judgments with automatic metrics to provide intuition for future research on evaluating the faithfulness, factuality, and intelligibility of factual error corrections.\n\nTask\nIn Thorne and Vlachos (2021)'s setting, retrieved evidence is used, which means the model may be able to correct factual errors, even though there is no supporting information in the evidence. In this case, although the prediction is considered correct, the model is hallucinating, which is not a desired property. Additionally, due to the way data was collected, they require systems to alter the input claim even if the input claim is already faithful to the evidence. We argue that no edit is needed for claims that are faithful to the evidence.\nTo address these shortcomings, our setup aims to edit a claim using a given piece of grounded evidence that supports or refutes the original claim (see Figure 2 ). Using gold-standard evidence avoids the issue where a system outputs the correct answer by chance due to hallucinations. In our setting, a system must be faithful to the evidence to correct factual errors, allowing us to evaluate system performance more fairly. Furthermore, we require the model not to edit the original claim if it is already factually consistent with the provided evidence.\nConcretely, the input to our task is a claim C and a piece of gold-standard evidence E that supports or refutes C. The goal of factual error correction is to produce a corrected claim \u0108 that fixes factual errors in C while being faithful to E. If C is already supported by E, models should output the original claim (i.e. \u0108 = C).\n\nProposed Methods\nOur framework, ZEROFEC, faithfully corrects factual errors using question-answering and entailment.\nSpecifically, we represent the input claim C as question-answer pairs \n{(Q 1 , A C 1 ), ..., (Q n , A C n )} such that each question Q i reflects the corresponding information unit A C i ,\n\nCandidate Corrections\nNight of the Living Dead is a horror film.\n\nFinal Correction\nFigure 2 : An overview of our framework. First, given an input claim, we generate the claim answers by enumerating all information units in the input claim. Second, conditioned on each extracted answer and the input claim, a question is generated. Third, each question is then fed to a question answering model to produce an evidence answer using the given evidence as context. Fourth, using a sequence-to-sequence approach, each evidence answer and the corresponding question are transformed into a statement, which serves as a candidate correction. Finally, the final correction is produced by scoring candidate corrections based on faithfulness.\nanswer A E i in the given evidence E using a learned QA model ( \u00a73.3). Each candidate correction S i is obtained by converting the corresponding pair of Q i and A E i into a declarative statement ( \u00a73.4). This guarantees that the corrected information units we replace factual errors with are derived from the evidence and thus ensures high faithfulness. The final output of ZEROFEC is the S i with the highest faithfulness score computed by an entailment model ( \u00a73.5). An overview of our framework is shown in Figure 2 .\nOne major challenge that makes our task more difficult than prior studies on faithfulness (Wang et al., 2020; Fabbri et al., 2022a ) is that we need to handle more diverse factual errors, such as negation errors and errors that can only be abstractively corrected. For instance, in the second example of in Table 2 , the QA model should output \"Yes\" as the answer, which cannot be produced by extractive QA systems. To address this issue, we adopt abstractive QG and QA models that can handle diverse question types and train our QA-to-claim model on multiple datasets to cover cases that cannot be handled by extractive systems. The following subsections illustrate the details of each component in our framework.\n\nClaim Answer Generation\nThe goal of claim answer generation is to identify information units in the input claim that may be unfaithful to E. We aim to maximize the recall in this step since the missed candidates cannot be recovered in later steps. Therefore, we extract all noun chunks and named entities using Spacy 3 and extract nouns, verbs, adjectives, adverbs, noun phrases, verb phrases using Stanza 4 . Additionally, we also extract negation terms, such as \"not\" and \"never\", from the input claim. We name the extracted information units claim answers, denoted as\nA C = {A C 1 , A C 2 , ..., A C n }.\n\nQuestion Generation\nUpon claim answers are produced, we generate questions that will be later used to look for correct information units in the evidence. Questions are generated conditioned on the claim answers using the input claim as context. We denote the question generator as G. Each claim answer\nA C i is concatenated with the input claim C to generate a question Q i = G(A C i , C).\nWe utilize MixQG (Murakhovs 'ka et al., 2022) as our question generator G to cover the wide diversity of factual errors and candidates extracted. MixQG was trained on nine question generation datasets with various answer types, including boolean, multiple-choice, extractive, and abstractive answers.\n\nQuestion Answering\nThe question answering step identifies the correct information units A E i corresponding to each question Q i in the given evidence E. Our QA module answers questions from the question generation steps with the given evidence as context. Let F denote our QA model. We feed the concatenation of a generated question and the evidence to the QA model to produce an evidence answer (Khashabi et al., 2022) is used as our question answering model. UnifiedQA-v2 is a T5-based (Raffel et al., 2020b) abstractive QA model trained on twenty QA datasets that can handle diverse question types.\nA E i = F(Q i , E). UnifiedQA-v2\n\nQA-to-Claim\nAfter questions and answers are generated, we transform each pair of question and answer into a declarative statement, which serves as a candidate correction that will be scored in the next step. Previous studies on converting QAs to claims focus on extractive answer types only (Pan et al., 2021) . To accommodate diverse types of questions and answers, we train a sequence-to-sequence model that generates a claim given a question-answer pair on three datasets: QA2D (Demszky et al., 2018) for extractive answers, BoolQ (Clark et al., 2019) for boolean answers, and SciTail (Khot et al., 2018) for covering scientific domain QAs. Note that samples in BoolQ do not contain converted declarative statements. Using Stanza's constituency parser, we apply heuristics to transform all QAs to their declarative forms in BoolQ. Our QA-to-claim model is a T5-base fine-tuned on these three datasets. Concretely, let M denote our QA-to-claim model. M takes in a generated question Q i and an evidence answer A E i as inputs and outputs a statement\nS i = M(Q i , A E i ).\n\nCorrection Scoring\nThe final correction is produced by scoring the faithfulness of each candidate correction from the previous steps w.r.t. the evidence. We use entailment score to approximate faithfulness. Here, DocNLI (Yin et al., 2021) is used to compute such document-sentence entailment relations. Doc-NLI is more generalizable than other documentsentence entailment models, such as FactCC (Kryscinski et al., 2020) , since it was trained on five datasets of various tasks and domains. Conventional NLI models trained on sentence-level NLI datasets, such as MNLI (Williams et al., 2018) , are not applicable since previous work has found that these models are ill-suited for measuring entailment beyond the sentence level (Falke et al., 2019) . In addition, to prevent the final correction from deviating too much from the original claim, we also consider ROUGE-1 scores, motivated by Wan and Bansal (2022) . The final metric used for scoring is the sum of ROUGE-1 score 5 and DocNLI entailment score. Formally,\nEQUATION\nEQUATION\nwhere C \u2032 is the final correction produced by our framework. Furthermore, to handle cases where the input claim is already faithful to the evidence, we include the input claim in the candidate correction list to be scored.\n\nDomain Adaptation\nDuring the early stage of our experiments, we found that our proposed framework did not perform well in correcting factual errors in biomedical claims. This results from the fact that our QA and entailment models were not fine-tuned on datasets in the biomedical domain. To address this issue, we adapt UNIFIEDQA-V2 and DOCNLI on two biomedical QA datasets, PUBMEDQA (Jin et al., 2019) and BIOASQ (Tsatsaronis et al., 2015) , by further fine-tuning them for a few thousand steps. We later show that this simple domain adaptation technique successfully improves our overall factual error correction performance on a biomedical dataset without decreasing performance in the Wikipedia domain (see \u00a75.1).\n4 Experimental Setup\n\nDatasets\nWe conduct experiments on two English datasets, FEVER and SCIFACT. FEVER (Thorne and Vla-chos, 2021 ) is repurposed from the corresponding fact-checking dataset (Thorne et al., 2018 ) that consists of evidence collected from Wikipedia and claims written by humans that are supported or refuted by the evidence. Similarly, SCIFACT is another fact-checking dataset in the biomedical domain (Wadden et al., 2020) . We repurpose it for the factual error correction task using the following steps. First, we form faithful claims by taking all claims supported by evidence. Then, unfaithful claims are generated by applying Knowledge Base Informed Negations (Wright et al., 2022) , a semantic altering transformation technique guided by knowledge base, to a subset of the faithful claims. Appendix A shows detailed statistics.\n\nEvaluation Metrics\nOur evaluation focuses on faithfulness. Therefore, we adopt some recently developed metrics that have been shown to correlate well with human judgments in terms of faithfulness. BARTScore (Yuan et al., 2021) computes the semantic overlap between the input claim and the evidence by calculating the logarithmic probability of generating the evidence conditioned on the claim. FactCC (Kryscinski et al., 2020) is an entailment-based metric that predicts the faithfulness probability of a claim w.r.t. the evidence. We report the average of the COR-RECT probability across all samples. In addition, we consider QAFACTEVAL (Fabbri et al., 2022a) , a recently released QA-based metric that achieves the highest performance on the SUMMAC factual consistency evaluation benchmark (Laban et al., 2022) . Furthermore, we also report performance on SARI (Xu et al., 2016) , a lexical-based metric that has been widely used in the factual error correction task (Thorne and Vlachos, 2021; Shah et al., 2020) .\n\nBaselines\nWe compare our framework with the following baseline systems. T5-FULL (Thorne and Vlachos, 2021) is a fully-supervised model based on T5-base (Raffel et al., 2020a) that generates the correction conditioned on the input claim and the given evidence. MASKCORRECT (Shah et al., 2020) and T5-DISTANT (Thorne and Vlachos, 2021) are both distantly-supervised methods that are composed of a masker and a sequence-to-sequence (seq2seq) corrector. The masker learns to mask out information units that are possibly false based on a learned fact verifier or an explanation model (Ribeiro et al., 2016) and the seq2seq corrector learns to fill in the masks with factual information. The biggest difference is in the choice of seq2seq corrector. T5-DISTANT uses T5-base, while MASKCOR-RECT utilizes a two-encoder pointer generator. For zero-shot baselines, we selected two post-hoc editing frameworks that are trained to remove hallucinations from summaries, REVISEREF (Adams et al., 2022) and COMPEDIT (Fabbri et al., 2022b) .\nREVISEREF is trained on synthetic data where hallucinating samples are created by entity swaps.\nCOMPEDIT learns to remove factual errors with sentence compression, where training data are generated with a separate perturber that inserts entities into faithful sentences.\n\nImplementation Details\nNo training is needed for ZEROFEC. As for ZEROFEC-DA, we fine-tune UNIFIEDQA-V2 and DOCNLI on the BIOASQ and PUBMEDQA datasets for a maximum of 5,000 steps using AdamW (Loshchilov and Hutter, 2019) with a learning rate of 3e-6 and a weight decay of 1e-6.\nDuring inference time, all generative components use beam search with a beam width of 4.\n\nMain Results\nTable 1 summarizes the main results on the FEVER and SCIFACT datasets. Both ZEROFEC and ZEROFEC-DA achieve significantly better performance than the distantly-supervised and zeroshot baselines. More impressively, they surpass the performance of the fully-supervised model on most metrics, even though the fully-supervised model is trained on 58K samples in the FEVER experiment.\nThe improvements demonstrate the effectiveness of our approach in producing faithful factual error correction by combining question answering and entailment predictions. In addition, even though our domain adaptation technique is simple, it successfully boosts the performance on the SCIFACT dataset while pertaining great performance on the FEVER dataset. The first example in It is true that ZEROFEC-DA requires additional training, which is different from typical zero-shot methods. However, the key point remains that our framework does not require any task-specific training data. Hence, our approach still offers the benefits of zero-shot learning by not requiring any additional training data beyond what was already available for the question answering task, a field with much richer resources compared to the factchecking field.\n\nQualitative Analysis\nTo provide intuition for our framework's ability to produce faithful factual error corrections, we manually examined 50 correct and 50 incorrect outputs made by ZEROFEC on the FEVER dataset. The interpretability of ZEROFEC allows for insightful examinations of the outputs. Among the correct samples, our framework produces faithful corrections because all intermediate outputs are accurately produced rather than \"being correct by chance\". For the incorrect outputs, we analyze the source of mistakes, as shown in Figure 3 . The vast majority of failed cases result from DocNLI's failure to score candidate corrections faithfully. In addition to the mediocre performance of DocNLI, one primary reason is that erroneous outputs from other compo-nents would not be considered mistakes so long as the correction scoring module determines the resulting candidate corrections unfaithful to the evidence. A possible solution to improve DocNLI is to further fine-tune it on synthetic data generated by perturbing samples in FEVER and SCIFACT. Examples of correct and incorrect outputs are presented in Table 7 and Table 8 \n\nHuman Evaluation\nTo further validate the effectiveness of our proposed method, we recruited three graduate students who are not authors to conduct human evaluations on 100 and 40 claims from FEVER and SCIFACT, respectively. For each claim, human judges are presented with the ground-truth correction, the goldstandard evidence, and output produced by a factual error correction system and tasked to assess the quality of the correction with respect to three dimensions. Intelligibility evaluates the fluency of the correction. An intelligible output is free of grammatical mistakes, and its meaning must be T5-DISTANT's output: Fuller House ( TV series ) isn't airing on HBO.\nTable 2 : Example outputs from different approaches. The outputs from our framework are directly interpretable, as the generated questions and answers reflect which information units in the input claim are erroneous and which information in the evidence supports the final correction. We only show the generated answers and questions directly related to the gold correction. In the first example, ZEROFEC-DA corrects a mistake made by ZEROFEC thanks to domain adaptation. In the second example, ZEROFEC successfully produces a faithful factual error correction, whereas the output of T5-DISTANT, the distantly-supervised baseline, is factual yet unfaithful to the evidence.\nunderstandable by humans without further explanation. Factuality considers whether the input claim is aligned with facts. Systems' output can be factual and semantically different from the gold correction as long as it is consistent with the world's knowledge. Faithfulness examines whether the input is factually consistent with the given evidence. Note that a faithful output must be factual since we assume all evidence is free of factual error. To evaluate the annotation quality, we compute the inter-annotator agreement. Krippendorff's Alpha (Krippendorff, 2011 ) is 68.85%, which indicates a moderate level of agreement. Details of our human evaluation can be found in Appendix B.\nThe human evaluation results are demonstrated in Table 3 . We observe that: (1) ZEROFEC and ZEROFEC-DA achieve the best overall performance in Factuality and Faithfulness on both datasets, even when compared to the fully-supervised method, suggesting that our approach is the best in ensuring faithfulness for factual error correction.\n(2) Our domain adaptation for the biomedical domain surprisingly improves faithfulness and factuality in the Wikipedia domain (i.e. FEVER). This suggests that fine-tuning the components of our framework on more datasets helps improve robustness in terms of faithfulness.\n(3) Factual output produced by ZEROFEC and ZEROFEC-DA are always faithful to the evidence, preventing the potential spread of misleading information caused by factual but unfaithful corrections. The second example in Table 2 demonstrates an instance of factual but unfaithful correction made by baseline models. Here, the output of T5-DISTANT is unfaithful since the evidence does not mention whether Fuller House airs on HBO. In fact, although Fuller House was not on HBO when it premiered, it was later accessible on HBO Max. Therefore, the correction produced by T5-DISTANT is misleading.\n\nCorrelation with Human Judgments\nRecent efforts on faithfulness metrics have been mostly focusing on the summarization task. No prior work has studied the transferability of these metrics to the factual error correction task. We seek to bridge this gap by showing the correlation between the automatic metrics used in measure, the results are summarized in Table 4 .\nWe have the following observations. (1) SARI is the most consistent and reliable metric for evaluating Factuality and Faithfulness across two datasets. Although the other three metrics developed more recently demonstrate high correlations with human judgments of faithfulness in multiple summarization datasets, their transferability to the factual error correction task is limited due to their incompatible design for this particular task. For example, QA-based metrics like QAFACTEVAL are less reliable for evaluating faithfulness in this task due to their inability to extract a sufficient number of answers from a single-sentence input claim. In contrast, summaries in summarization datasets generally consist of multiple sentences, enabling the extraction of a greater number of answers. To validate this, we analyzed the intermediate outputs of QAFACTEVAL. Our analysis confirms that it extracts an average of only 1.95 answers on the FEVER dataset, significantly lower than the more than 10 answers typically extracted for summaries. (2) Across the two datasets, the correlations between all automatic metrics and Intelligibility are low. The extremely high proportion of intelligible outputs may explain the low correlation. (3) The correlation for learning-based metrics, including QAFACTEVAL and FACTCC, drop significantly when applied to SCIFACT. This is likely caused by the lack of fine-tuning or pre-training with biomedical data.\n6 Related Work\n\nFactual Error Correction\nAn increasing number of work began to explore factual error correction in recent years, following the rise of fact-checking (Thorne et al., 2018; Wadden et al., 2020; Gupta and Srikumar, 2021; Huang et al., 2022b) and fake news detection (Shu et al., 2020; Fung et al., 2021; Wu et al., 2022; Huang et al., 2022a) . Shah et al. (2020) propose a distant supervision learning method based on a masker-corrector architecture, which assumes access to a learned fact verifier. Thorne and Vlachos (2021) created the first factual error correction dataset by repurposing the FEVER (Thorne et al., 2018) dataset, which allows for fully-supervised training of factual error correctors. They also extended Shah et al. (2020) 's method with more advanced pre-trained sequence-to-sequence models. Most recently, Schick et al. (2022) proposed PEER, a collaborative language model that demonstrates superior text editing capabilities due to its multiple text-infilling pre-training objectives, such as planning and realizing edits as well as explaining the intention behind each edit 6 .\n\nFaithfulness\nPrevious studies addressing faithfulness are mostly in the summarization field and can be roughly divided into two categories, evaluation and enhancement. Within faithfulness evaluation, one line of work developed entailment-based metrics by training document-sentence entailment models on synthetic data (Kryscinski et al., 2020; Yin et al., 2021) or human-annotated data (Ribeiro et al., 2022; Chan et al., 2023) , or applying conventional NLI models at the sentence level (Laban et al., 2022) . Another line of work evaluates faithfulness by comparing information units extracted from summaries and input sources using QA (Wang et al., 2020; Deutsch et al., 2021) . There is a recent study that integrates QA into entailment by feeding QA outputs as features to an entailment model (Fabbri et al., 2022a) . We combine QA and entailment by using entailment to score the correction candidates produced by QA. Within faithfulness enhancement, some work improves factual consistency by incorporating auxiliary losses into the training process (Nan et al., 2021; Cao and Wang, 2021; Tang et al., 2022; Huang et al., 2023) . Some other work devises factuality-aware pre-training and fine-tuning objectives to reduce hallucinations (Wan and Bansal, 2022) . The most similar to our work are studies that utilize a separate rewriting model to fix hallucinations in summaries. For example, Cao et al. (2020) present a post-hoc corrector trained on synthetic data, where negative samples are created via perturbations. Adams et al. (2022) fix factually inconsistent information in the reference summaries to prevent the summarization from learning hallucinating examples. Fabbri et al. (2022b) propose a compression-based post-editor to correct extrinsic errors in the generated summaries. By contrast, we leverage the power of QA and entailment together to address faithfulness.\n\nConclusions and Future Work\nWe have presented ZEROFEC, a zero-shot framework that asks questions about an input claim and seeks answers from the given evidence to correct factual errors faithfully. The experimental results demonstrate the superiority of our approach over prior methods, including fully-supervised methods, as indicated by both automatic metrics and human evaluations. More importantly, the decomposability of ZEROFEC naturally offers interpretability, as the questions and answers generated directly reflect which information units in the input claim are incorrect and why. Furthermore, we reveal the most suitable metric for assessing faithfulness of factual error correction by analyzing the correlation between the reported automatic metrics and human judgments. For future work, we plan to extend our framework to faithfully correct misinformation in social media posts and news articles to inhibit the dissemination of false information. In addition, it may be meaningful to explore extending zero-shot factual error correction to multimedia task settings, such as identifying inconsistencies between chart and text (Zhou et al., 2023) .\n"}
{"question": "In the study mentioned in the passage, what is the primary focus of the investigation regarding NLP models?", "evidence": "  Our main contributions are: 1) we release the first publicly available metalinguistic QA dataset, 3 focused on the English language; 2) we present a taxonomy of questions in the corpus along with analysis; and 3) we investigate to what extent LLMs are able to articulate appropriate generalizations about language in response to these questions.  ", "options": ["Although most of these models achieve high ratings for well-formedness, the validity of their answers is significantly lower than that of human-authored answers, indicating that this type of metalinguistic QA task is challenging even for large language models."], "answer": "B", "content": "\nIntroduction\nLanguage is so powerful that it can be reflected back on itself. Statements like \"In informal usage, a steep learning curve means something that is difficult (and takes much effort) to learn\" or \"In some cases, an adjective has both -ic and -ical forms, with no difference in meaning\" expressly concern linguistic inventories, structures, and behaviors. In other words, they are metalinguistic-they use language to discuss language (cf. Wilson, 2013) . They may concern a particular instance of language use, or properties of a language or speaker in general; either way, they are metalinguistic in making linguistic phenomena the subject matter of a linguistic utterance. For the rest of this paper, the term metalanguage is used for natural language text in which natural language is also the subject matter.\nWhile NLP models have become powerful at predicting text in many settings, it remains to be seen whether such capability extends to metalanguagewhere linguistic strings are not being deployed to contribute to the discourse with their normal denotations, but rather, are treated as entities with linguistic properties (e.g., grammar, meaning). One way this can be explored is in a question answering framework, which requires suitable datasets, ideally based on questions that are realistic and paired with high-quality answers.\nIn this paper, we present a corpus of metalinguistic questions and answers about English. The corpus is collected and carefully processed from two Stack Exchange forum sites: English Language & Usage (ENG) and English Language Learners (ELL). It covers more than 70k questions on numerous topics about English such as grammar, meaning, fluency, and etymology along with answers. Our corpus, ELQA (English Language Questions and Answers), can serve as a tool to facilitate metalinguistic studies. Moreover, since questions in ELQA cover a variety of topics in English, it can be used in the educational and English language learning domains.\nAs the first case study of ELQA, we investigate the performance of current state-of-the-art NLP technology on free-form question answering in the English language domain. Additionally, we explore the possibility of building NLP models that can directly answer questions from language learners. We process a subset of ELQA and make it appropriate for this task. Then, we report on the results of both automatic and human evaluations using different experimental settings of T5 1 and GPT-3 2 models. Although most of these models achieve high ratings for well-formedness, the validity of their answers is significantly lower than that of human-authored answers, indicating that this type of metalinguistic QA task is challenging even for large language models.\nOur main contributions are: 1) we release the first publicly available metalinguistic QA dataset, 3 focused on the English language; 2) we present a taxonomy of questions in the corpus along with analysis; and 3) we investigate to what extent LLMs are able to articulate appropriate generalizations about language in response to these questions.\n\nRelated Work\nStack Exchange is a network of numerous CQA sites (originally and most famously, Stack Over-3 https://github.com/shabnam-b/ELQA flow) built on a common platform. Stack Exchange forums have been featured in a number of previous datasets (Yao et al., 2013; Hoogeveen et al., 2015; Ahmad et al., 2018; Penha et al., 2019; Campos et al., 2020; Kumar and Black, 2020; Rogers et al., 2023) , including the English site (our ENG) along with others such as Ask Ubuntu, Android, Gaming and WordPress (dos Santos et al., 2015; Nakov et al., 2017) . We focus on ENG and ELL as they concern the English language itself; we show that these datasets cover a wide range of metalinguistic questions.\nOur use of these forums contrasts with previous work on metalanguage in corpora, which annotated and quantified mentions (Anderson et al., 2004; Wilson, 2010 Wilson, , 2011 Wilson, , 2012 Wilson, , 2017)) , but did not consider entire questions and answers about language. Taylor (2015) studied metalanguage in online forums, but with a focus on the usage of metalinguistic expressions of mock politeness. More recently, Bogetic (2021) published the first corpus of contemporary Slovene, Croatian and Serbian media metalanguage texts.\nSo far, metalanguage has not been a focus in the QA domain-ours is the first publicly available English metalinguistic QA dataset. Most QA tasks are set up to have a question and a reference document, where the objective is to find the answer based on the document (Fan et al., 2019; Kwiatkowski et al., 2019) . In this paper, we explored a type of \"closed-book\" question answering task (Roberts et al., 2020; Khashabi et al., 2021) . To the best of our knowledge, this task has not been explored to date within the realm of English language questions that require significant generalization and adaptation rather than looking up facts.\n\nConstructing the Dataset\nWe collect our data from two sites on Stack Exchange: English Language & Usage (ENG) 4 and English Language Learners (ELL). 5 Sample screenshots of the site are shown in Figure 1 . The Stack Exchange data is publicly released under the CC-BY-SA 3.0 license. We preprocessed the data until 2021-12-06 collected from the Internet Archive 6 to be suitable for NLP studies and release it as ELQA. Additionally, some cleanup (e.g., removing posts marked as \"spam\" or \"offensive\") was done. Fields for each entry (question) include the title, body, user bio (if available), score (which is calculated based on up-votes and down-votes by other users), tags (user-assigned, related to the area/topic of the question), favorite count, and a list of answers. Textual content (body and user bio) is provided in two formats: HTML and plain text without HTML tags. We release two versions of ELQA based on different preprocessing steps. In ELQA-large, we keep questions as long as they don't include any images (<img> HTML tag) and have an answer with a score of at least 2 (meaning at least two people other than the user posting the answer found it helpful). For ELQA-small, we applied further filtering to ensure that the data has the least amount of noise: a) questions should have a score of at least 4 https://english.stackexchange.com/ 5 https://ell.stackexchange.com/ 6 https://archive.org/ 2 (ensuring questions are clear and coherent), b) question has an answer with a score higher than 3 and c) there are no hyperlinks in at least one of the high-rated answers. The last step reduces noise and facilitates a fair comparison for the closed-book question-answering task ( \u00a74) with model-generated answers, as models cannot be expected to have access to the web to suggest valid URLs compared to humans who would search the web for appropriate resources to include in their answers.\nFor quality assurance, we also did a human annotation on ELQA-small. Two of the authors annotated 250 question and answer pairs for the following: 1) Is the question answerable? and 2) Does the answer fully address the question? We found 99.2% of the questions answerable and 91.8% of the answers acceptable.\nTable 1 contains overall statistics on both versions. Figure 2 shows the distribution of the 10 most common tags in each of the sites. Since users assign these tags to their questions (0 to multiple), similar or near-duplicate tags are common within the collection. Some form more general and more fine-grained variants, e.g. 'meaning' and 'meaningin-context'. In addition to available user-assigned tags, we manually inspected a large subset of the data to identify salient types of questions. These are defined below and illustrated in Table 2 . We then labeled 100 random questions to get a rough estimate of their frequencies (two annotators annotated these 100 samples and they agreed on 92% of cases in an overlapping subset).\n\u2022 Fluency (\u224838% of questions): Usually asking about a particular sentence, comparison of multiple sentences, and/or probing how an expression should be used in general. The user wants to know if X is correct, or to decide between multiple choices, which one is correct. \"Correct\" could mean grammatical, most natural/idiomatic, stylistically appropriate, conveying the intended meaning, etc. In Qs where options are provided by the user, there are cases in which 1) none of the choices are correct, 2) multiple choices are correct, and 3) only one is correct. \u2022 Form to Meaning (Interpretation) (\u224819% of questions): Questions such as \"What does X mean?\" (of an expression in general, or an encountered passage) or \"What's the difference in meaning between X and Y?\". \u2022 Meaning to Form (Encoding) (\u224820% of questions): In these questions, the user gives some As can be seen from the examples in Table 2 , it is common for questions and answers to contain example usages, often visually distinguished with Markdown formatting (such as blockquotes, bullets, and italics) which we retain in the processed corpus markup. Examples can be incorporated into a post in a variety of ways-e.g., asking for an interpretation of one usage, as in the Form to Meaning example in Table 2 , or contrasting multiple usages such as in the following question:\n\nDid VS Have done\nWhat is difference between the following statements: Did you tell your parents yet? Have you told your parents yet? Haven't you told your parents yet? Are these questions correct? why do we use one over another in some cases? What is the difference in meaning?\nUsage examples provided in a question may be instances that the author encountered \"in the wild\" (such as in a novel or film), or in a grammar book or dictionary, or they may have been constructed by the user. Answers sometimes include examples found through a corpus search.\n\nEnglish Language Question Answering\nLarge language models can produce output that is fluent and (at times) informationally adequate when presented with factual questions about entities in the world (Roberts et al., 2020) . But how do such models perform when asked questions about the language itself? In this section, we investigate the free-form English language question answering task.\nThis task has the potential to benefit educational applications for language learners. Research on NLP for educational purposes has investigated tasks such as automated grammatical error correction (Dale et al., 2012; Ng et al., 2014; Bryant et al., 2019; Wang et al., 2021, inter alia) , question and quiz generation for language learning (Sakaguchi et al., 2013; Chinkina and Meurers, 2017; Marrese-Taylor et al., 2018; Vachev et al., 2021) , and automated essay scoring (Burstein, 2003; Farag et al., 2018, inter alia) . Nevertheless, an application that has not been taken up by the educational NLP community is free-form question answering about language. Second language learners possess a degree of metalinguistic awareness about the language they are learning, and often turn to teachers or more advanced speakers with explicit questions about vocabulary, grammar, and usage. Community Question Answering (CQA) websites such as Stack Exchange have sites for language learners' questions and answers. These sites require consid- erable effort by volunteers, and learners may have to wait for an answer-if an answer is provided at all. In fact, looking at the data from 2021-12-06 for ENG and ELL, 9% of questions have no answers.\n\nData\nWe randomly divided ELQA-small into train/test/dev splits. This resulted in 21,175 Q&A pairs in the train split and 3,107 Q&A pairs in each of the dev and test splits. Answers in these splits have a score of at least 4. If there are multiple high-rated answers to a question, we include all of them for training. Some of these questions can be answered by looking at a dictionary or vocabulary list for descriptions. But many of them are explanations in relation to particular instances of language use and require significant reasoning rather than looking up facts. Thus in this setup, we do not have any external context/reference available at evaluation time, i.e. this is a closed-book QA task.\nThe input for the task is Title:\n[Q title] <sep> Body: [Q body].\nWe use the HTML version of ELQA for this task since metalinguistic mentions are usually distinguished via formatting (e.g., blockquotes, bullets) and the ultimate goal is a system that humans can easily use to get answers to their language-related questions.\n\nSetup\nWe use T5 (Raffel et al., 2020; Roberts et al., 2022) and GPT-3 (Brown et al., 2020) as our models since they have been shown to be strong baselines in other QA domains. We believe the questions in ELQA offer new challenges for the QA task since they require different types of knowledge/understanding to be able to generate answers. Addition-ally, these questions contain noise (grammatical errors) and cases of textual metalanguage which is likely harder to comprehend for a model.\nWe fine-tune T5-l and T5-xxl for this task. 7 We saved multiple checkpoints during fine-tuning and evaluated them with the interpolation of BLEU (Papineni et al., 2002) , BERTScore (Zhang et al., 2020) and ROUGE (Lin, 2004) on the dev set to choose the best-performing one (checkpoint at 75k updates, hyperparameters available in Table 8 in the Appendix).\nWith GPT-3 we used text-davinci-003 and experimented with both fine-tuning (FT) on 100 and 1000 samples and a few-shot (FS) setting in which the model is given a few demonstrations of the questions and answers at inference time as conditioning, but no weights are updated (Radford et al., 2019) . In the FS setting, we show the model four Q&A pairs since we wanted the model to see different question types but there were also limits on the input length. To select these 4 pairs, we randomly created 5 different sets of Q&A pairs, evaluated on a subset of dev, and chose the best-performing set for the experiments (dev results available in Appendix, Table 9 ).\n\nAutomatic Evaluation\nResults are shown in Table 3 . GPT-3 FS outperforms all other methods in all metrics with a large margin except for BLEU Score. We also observed that using GPT-3 in a few-shot setup worked much better than the fine-tuned version. Looking at some of the model-generated answers, we noticed that the fine-tuned model tends to generate longer an- swers containing redundant text. We observed improvements when we used 1000 samples instead of 100 for fine-tuning and hence, fine-tuning on larger data might result in better performance, however, we only experimented with 100 and 1000 samples in this paper due to having limited resources.\nBased on Table 3 , T5-xxl seems to perform similarly to GPT-3 FT-1000. However, a small manual evaluation showed otherwise (GPT-3 FT-1000 answers were slightly better). Furthermore, we observe that the scores for even the best system are very low, but manual evaluations showed that the GPT-3 FS generates fairly good answers in many cases. Due to these observations and also given the well-known limitations of automatic metrics for evaluating generation tasks (Kasai et al., 2022; Celikyilmaz et al., 2020; Bhakthavatsalam et al., 2021) , we believe conducting human evaluation for deeper analysis is necessary for this task.\nIn Table 4 , we show results for each site to see if one is more challenging than the other. Overall, models perform slightly better on ELL based on automatic metrics-but we see in the next section (Table 5 ) that there isn't really a meaningful difference between the sites when humans evaluate the answers.\n\nHuman Evaluation\nHuman evaluators were presented with the question title and body, and then asked to rate 5 answers: a top-rated human-provided answer, a low-rated human-provided answer, and answers generated by 3 of our best models: GPT-3 FS, GPT3 FT-1000, T5-xxl.\nThey were asked to give ratings (via a slider widget, on a 1-5 integer scale-the higher, the better) for two criteria (C1 & C2): 8 1. Does the answer look grammatically/ structurally like a good answer (ignoring whether it answers the question)? 2. Is the information in this answer a valid response to the question (ignoring formatting/ stylistic issues)?\nThe first criterion aims to get a score for fluency and coherence and the second one for correctness and completeness.\nWe collected ratings for a set of 75 questions (375 different answers). Each question with its set of answers was evaluated by at least 2 raters, and then the average score was calculated based on their responses. 9 We also report the average z-score which is calculated over each annotator's raw score distribution for each metric, intended to normalize interannotator variation in how the scale is interpreted for each of the two metrics (details in Appendix B).\nThe results of this study are shown in Table 5 . Overall, answers generated by GPT-3 FS have a small gap with human answers in both C1 and C2. 10 GPT-3 FT-1000 comes next, with less accurate answers containing redundant text and hallucinations. The smallest model, T5-xxl, ranks last.\nRankings based on human evaluations are available in Table 6 . These results are also indicating that model-generated answers are fluent in most cases, but they are not as good as human answers Title Question Explain the verb tense in \"All that glitters is not gold\" I am confused by the maxim All that glitters is not gold. May I know why the word is is used there? In my known according the English grammar glitters is plural, So after plural we add verb+s. As in above example use auxiliary verb so why not arewhy is? Source Answer Top-rated human Good question, but it's worth pointing out that glitters in this context is a verb, not a noun. The subject of the sentence is all. When all is used as the subject of a sentence, it functions as a pronoun, and it can function as a singular or a plural pronoun.\nFrom NOAD <blockquote> all (pronoun) [with clause ] the only thing (used for emphasis) : all I want is to be left alone.\n(used to refer to surroundings or a situation in general) everything : all was well | it was all very strange. </blockquote> So: <blockquote> All that glitters is not gold </blockquote> is roughly equivalent to <blockquote> Everything that glitters is not gold </blockquote> and the verb tense agrees. but: <blockquote> All who eat are not gluttons </blockquote> is roughly equivalent to: <blockquote> All the people who eat are not gluttons </blockquote> so there is no problem there, either.\n\nLow-rated human\nGrammatically is is used with third person singular and also s form of verb is used. \"All\" indicates a third person single group.\n\nGPT-3 FS\nThe phrase all that glitters is not gold is an idiom, which means that things that appear to be valuable may not actually be so. It is an old proverb, and the verb tense used in the phrase is an example of the so-called \"historic present\". The historic present is a literary device used to give a sense of immediacy to a narrative. It is used to describe events that happened in the past, but are being described as if they are happening in the present. In this case, the phrase is used to emphasize the timelessness of the proverb. So, while the verb glitters is indeed plural, the verb is is used in the historic present to give the phrase a sense of immediacy and timelessness. GPT-3 FT-1000\nThe maxim is in the present tense, so the verb is also in the present tense: Glitters. The past tense of glitter is glittered, and the past participle is glittered. So if the maxim was in the past tense, it would be: <blockquote> All that glittered was not gold. </blockquote> The present tense maxim is a generalisation about something that is true in the speaker's experience. The past tense maxim would be a historical statement about what was true in the past.\n\nT5-xxl\nThis is an interesting question, because it is not immediately obvious what the grammar rules are here. It is not a fixed phrase, and so subject-verb agreement must be based on the context. In this case, the that is a prepositional phrase acting as a complement to the preposition all. It could be rephrased as <blockquote> All that glitters is not gold </blockquote> It is true that all means \"everything that glitters\" -but it is equally true that not everything that glitters means \"not everything that glitters is gold\". It is much the same as <blockquote> Everything that glitters is not gold. </blockquote> The subject all is singular, so the verb must be singular as well -is. If it was twice plural, then are would be used, but it is not. It is interesting to note that Shakespeare used the same construction in Hamlet: <blockquote> Everything that glitters is not gold. </blockquote> The subject that is singular, and so the auxiliary verb is agrees with the singular noun. when correctness/completeness is considered (GPT-3 FS is ranked first or as good as a top-rated human answer in only 45% of cases).\nFor each criterion and Top-rated human, Lowrated human and GPT-3 FS, histograms of the average score of the two annotators are plotted in Figure 3 . We can observe that GPT-3 FS and Lowrated human have very similar numbers of highscoring answers (human evaluation scores), but the number of low-scoring human answers drops off gradually as quality decreases, while the distribution is more spread out for GPT-3 FS. I.e., the model has some moderately bad answers as well as some good ones, whereas Low-rated human answers cluster more on the upper end of the scale. C1 (fluency/coherence). All models generated fairly fluent and well-structured answers. We even notice that GPT-3 FS scores higher in wellformedness than human answers. We looked at those samples and we believe there are two main reasons for this: 1) Some human answers were very long, containing multiple different quotes from different sources. On average, our evaluators preferred the structure of answers from GPT-3 FS, which took the form of a short paragraph addressing the question. 2) Some human answers have a more casual/conversational nature. On the other hand, GPT-3 FS generated more authoritativesounding, teacher-like answers with complete sentences, which were preferred by our evaluators in some cases. C2 (correctness/completeness). On average, models are worse on this metric, though sometimes they did produce acceptable answers (perhaps because variants of the question are commonly discussed on the web).\nOne challenge in this domain is that questions, particularly from ELL, may not be fully fluent. In the Fluency example from Table 7 we see that there are some grammatical errors in the question that are unrelated to the topic of the question. In addition, the questioner uses incorrect terminology, mentioning verb tense in the post title even though the question is actually about subject-verb agreement with respect to number (as can be inferred from the examples). While the good human response correctly flags the incorrect terminology and answers the underlying question, GPT-3 models give irrelevant answers about tense.\nAnother correctness failure from GPT-3 FS can be seen in the following pair involving a Meaningto-Form question:\nWhat is the analog equivalent of 'digitally'? In electronics, we can solve a problem digitally or analoguely ? That doesn't sound right, but surely there must be a word I can use? Update: some definitions: digitally: Relating to or being a service that provides information expressed in discrete numerical form. analoguely: Relating to or being a service that provides information expressed in continuous, non-discrete form. (This is my made up definition of a word I made up). analogously: Similar or alike in such a way as to permit the drawing of an analogy. As you can see, the word analogously doesn't mean the opposite of digitally, it means something else. Answer: The analog equivalent of 'digitally' is 'analogously'. Analogously means \"in a similar way\" or \"by comparison\". It is used to describe a situation where two things are similar in some way, but not necessarily opposite. For example, you could say that a digital signal is analogous to an analog signal, because they are both signals, but they are different in terms of how they are represented.\nWe see that the model begins by ignoring the user's explanation that analogously does not have the desired meaning, and suggests it anyway. The rest of the model's answer gives a (valid) general definition of analogously and clumsily attempts to apply it to the two kinds of signals, which is not responsive to the question. It may be the case that models particularly struggle with questions for which there is no straightforward solution (in this case, no adverb derived from analogue has the user's intended meaning).\n\nMetrics\nOur human evaluations found that the high-rated human answers are the best, and GPT-3 FS is roughly on par with the low-rated human answer, while other models are worse.\nAs noted in \u00a74.3.1, we did not find the automatic metrics very informative. We believe this is due to the long and free-form nature of this task: concepts can be explained in different ways and answers can include different examples which make automatic evaluations very challenging. For example, for the question Why is this sentence: \"Additional nine features were added. . . \" incorrect? a human answer is:\nIt has to do with the order of the adjectives. For example, consider this sentence: <blockquote> Happy nine men walk into a bar. </blockquote> Both nine and happy are adjectives, but we are really intending nine to describe the happy men, not happy describing the nine men. I don't .... Model-generated answers mostly mirror the examples in the question. But answers given by humans mention new examples-which are not in the question (and therefore unlikely to come up in other answers). Thus a good answer may mention superficially off-topic terms metalinguistically, which presents evaluation challenges unlike those of summarization or MT, for example (Reiter, 2018; Celikyilmaz et al., 2020) . For QA tasks in metalinguistic domains such as ours, caution is warranted when applying automatic metrics that are unaware of the use/mention distinction. Human evaluation is likely to be more illuminating. We release our full human evaluation data to enable detailed comparisons in future research.\n\nQualitative Analysis\nWe examined behavioral patterns for the best model, GPT-3 FS, organized by question type, with observations as follows. Interpretation and Encoding: These are the categories where the model performs the best, especially if the question has a straightforward answer (for example, there aren't several possible answers based on context). Topics in these categories often center on definitions of vocabulary and idioms. These questions may involve less reasoning to answer, and models may be leveraging content from the online dictionaries and thesauri in their web training data.\nGrammatical Analysis: When asked about general concepts and rules of grammar, the modelgenerated answers are usually acceptable:\n-ed and -ing adjectives What is the difference? <blockquote> I am interesting in mathematics. </blockquote> versus <blockquote> I am interested in mathematics. </blockquote> Murphy intermediate grammar Unit 97 tells me that I cannot use the former. Extended and general explanations will be appreciated. Answer: The difference between -ed and -ing adjectives is that the -ed adjectives describe a feeling or emotion that is felt by the subject, while the -ing adjectives describe a quality of the subject itself. In the example sentence, interested is the correct word to use because it describes the feeling of interest that the subject has in mathematics. Interesting would be incorrect because it describes the quality of the subject itself, not the feeling of the subject. For example, if you were to say \"This book is interesting,\" you would be describing the book itself, not the feeling you have towards the book. On the other hand, if you were to say \"I am interested in this book,\" you would be describing the feeling you have towards the book.\nHowever, when questions become more specific to examples, sometimes the model struggles:\nWhat does \"in\" refer to? <blockquote> A symbiotic relationship is an interaction between two or more species in which one species lives in or on another species. Answer: It refers to the other species. The symbiotic species lives in the other species.\nThe example in the question uses the word in twice. Rather than addressing the first token, which the questioner has bolded, the answer addresses the second one (denoting the physical relationship between one species and another). Fluency: The model correctly answers many of the fluency questions. Others require reasoning and understanding of context, and the model struggles to generate a correct answer. For example:\nReturning for the nth time Suppose I went to the supermarket three times. Is my third trip considered my \"second time returning there\" or my \"third time returning there?\" Thanks. (I know this question sounds silly, but English is my second language...) Answer: Your third trip to the supermarket would be considered your third time returning there. The phrase returning for the nth time is used to refer to the nth time you have returned to a place, not the n-1th time.\nWe also observe that when the correct usage of a case is not mentioned in the question itself, the model sometimes suggests incorrect corrections and wrong explanations:\nWhy is this usage of \"to\" incorrect? I am in communication with a friend who is learn-ing English. I noticed her using \"to\" in the wrong context, but I'm unable to explain which rule she is breaking. <blockquote> I can't access to Gmail. </blockquote> Answer: In this sentence, the word to is incorrect because it is being used as a preposition. The correct word to use in this context is for. <block-quote> I can't access for Gmail. </blockquote> The word to is used as a preposition when it is followed by a verb, such as go, come, run, etc. For example: <blockquote> I am going to the store. </blockquote> However, when to is followed by a noun, it is usually incorrect. In this case, the correct word to use is for. <blockquote> I need a gift for my friend.</blockquote>\n\nOutlook\nOverall, we were surprised by the quality of many of the answers from GPT-3 FS: many would likely have received high user ratings if submitted as answers on the site. At the same time, the model is not to the point where we would want to trust its answers without human review. We believe that answer confidence estimation-so that users can be shown only the best model-generated answers-is an important direction for using learner QA models in practice (Jiang et al., 2021) .\n\nConclusion\nWe presented ELQA, a dataset containing metalinguistic questions and answers about the English language. We provided analysis and a taxonomy of the data, along with experiments on free-form answer generation and investigated the extent to which language models can articulate their generalizations about language. Since many of the questions in ELQA were asked by language learners, it forms a potentially useful and so far untapped resource for educational NLP purposes and metalinguistic question answering. We release the dataset to enable further studies of this task.\n"}
{"question": "Which of the following is not our main contributions?", "evidence": "  Our main contributions are summarized as follows: (1) We integrate inter-sample contrastive learning and intra-sample modality decomposition into a simple unified loss function, based on a customized data sampler that allows us to sample positive/negative data pairs to perform both learning tasks. (2) We propose to decompose each modality into a similarity feature and a dissimilarity feature, and use the similarity feature of the text as an anchor to build the contrastive relation among all decomposed features. This is due to the observation that sentiment analysis is still largely centered around text and spoken language, while other modalities can provide extra information to assist with prediction. (3) Based on multimodal representation learning proposed above, we further introduce a multi-task prediction loss that depends on each decomposed modality representation and enables the model to learn from both multimodal prediction and unimodal prediction.\n ", "options": ["A. We combine inter-sample contrastive learning and intra-sample modality decomposition into a unified and straightforward loss function.", "B. We use the similarity feature of the text to build the contrastive relation among all decomposed features. ", "C. we introduce a multi-task prediction loss which enables the model to learn from both multimodal prediction and unimodal prediction.\nD. We produce a customized data sampler."], "answer": "D", "content": "\nIntroduction\nMultimodal deep learning involves interpreting and analyzing multimodal signals together, where each modality refers to a way in which something is experienced and felt, e.g., the visual, audio, or language modality. With the widespread popularity of online social media, such as Instagram, Tik-Tok, Facebook, etc., videos containing multiple modalities have become a major information carrier, which brings new challenges to content recommendation and classification, e.g., video question answering (Lei et al., 2021; Li et al., 2020) , video captioning (Ging et al., 2020; Li et al., 2020) , and video retrieval (Akbari et al., 2021; Lei et al., 2021) .\nWhile traditional sentiment analysis is mainly based on language, multimodal sentiment analysis (MSA) predicts the human emotion by utilizing extra information available in visual and audio modalities of the content to assist with language-based prediction. Here, the text modality contains the semantic meaning of the spoken language. The visual modality extracts the facial characteristics (e.g., head orientation, facial expressions, and pose) of the speaker. The audio modality reflects the emphasis on the utterance (e.g., through pitch, bandwidth and intensity). MSA has recently gained much attention in research for several reasons. On one hand, because of the abundance of social media content, commercial interests are switching from gauging user opinions/emotions from text only to more thorough multimodal analysis based on videos. On the other hand, short video platforms (e.g., TikTok, Instagram) allow users to easily create multimodal content including visual information, audio, and inserted text, while these modalities are sometimes noisy or even contradicting each other in sentiments. Therefore, the presence of multimodal information in addition to the text or language itself is necessary to make a thorough conclusion about the overall sentiment of a video.\nMultimodal fusion has become essential to gaining a deeper understanding of these video scenes (Baltru\u0161aitis et al., 2018) and has proven to be helpful in many downstream tasks. Various multimodal fusion techniques have been proposed for MSA, among which a basic solution is concatenating the extracted feature of each modality before performing downstream regression or classification. Recent work has recognized the importance of identifying modality-invariant information across modalities and fuse them to strengthen sentiment prediction (Hazarika et al., 2020; Zadeh et al., 2018a; Rahman et al., 2020; Sun et al., 2020) .\nAlthough modality-invariant information helps reinforce the understanding of the content, there are also cases where sentiments of different modalities contradict each other. For example, when one thanks someone with phrases like \"Finally I can rest easy tonight\" or \"I can't thank you enough\", it is very hard to conclude whether the sentiment is positive or negative without looking at the nonverbal cues, such as tones, facial expressions, and gestures. In fact, many sarcastic opinions are expressed by non-linguistic markers. In these cases, the overall sentiment cannot simply be judged by a majority vote among all modalities. Thus, multimodal representation learning that respects both consistency and incongruity between modalities have recently shown great promise (Yu et al., 2020; Hazarika et al., 2020) .\nIn this paper, we propose ConFEDE, a Contrastive FEature DEcomposition framework, which integrates both modality decomposition within each sample and supervised contrastive learning across samples in a single unified contrastive learning framework. Our main contributions are summarized as follows: (1) We integrate inter-sample contrastive learning and intra-sample modality decomposition into a simple unified loss function, based on a customized data sampler that allows us to sample positive/negative data pairs to perform both learning tasks. (2) We propose to decompose each modality into a similarity feature and a dissimilarity feature, and use the similarity feature of the text as an anchor to build the contrastive relation among all decomposed features. This is due to the observation that sentiment analysis is still largely centered around text and spoken language, while other modalities can provide extra information to assist with prediction. (3) Based on multimodal representation learning proposed above, we further introduce a multi-task prediction loss that depends on each decomposed modality representation and enables the model to learn from both multimodal prediction and unimodal prediction.\nWe mainly evaluated ConFEDE on CH-SIMS (Yu et al., 2020) benchmark, which contains both unimodal and overall sentiment labels for each sample. The result shows that the proposed method significantly outperforms a wide range of stateof-the-art multimodal sentiment analysis methods. To test the capability when no unimodal labels are provided, we further conduct experiments on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , which contain only an overall sentiment label for each sample, which shows that our proposed method can also achieve better per-formance than state-of-the-art methods on a number of performance metrics without unimodal labels. We provide extensive ablation studies to show the effectiveness and necessity of each design component in ConFEDE. The code is released at https://github.com/XpastaX/ConFEDE/.\n\nRelated Work\nIn this section, we discuss the related work in MSA and contrastive representation learning.\n\nMultimodal Sentiment Analysis\nPrior works on multimodal sentiment analysis mostly focus on predicting sentiments based on text and vision (Zhu et al., 2022; Ji et al., 2019; Liu et al., 2019) .However, there is growing interest in analyzing sentiment using all three modalities: text, audio, and vision (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020) . Zadeh et al. (2016) were among the first to propose a multimodal dictionary that could learn the dynamic interactions between facial gestures and spoken words to model sentiments. They later introduced a Tensor Fusion Network (TFN) to learn the intra-modality and inter-modality dynamics of three modalities in an end-to-end way (Zadeh et al., 2017) . Furthermore, they presented a Memory Fusion Network (MFN) which is composed of Long Short Term Memories (LSTMs) to learn the view-specific and cross-view interactions of three views (text, video, and audio) to improve sentiment analysis performance. Rahman et al. (2020) proposed a Multimodal Adaptation Gate (MAG) to fine-tune BERT (Devlin et al., 2019) on multimodal data to improve sentiment analysis performance. However, these prior works do not consider modality-specific information.\nTo better study the impacts that modalityspecific information can bring to MSA, Yu et al. (2020) construct a new multimodal sentiment analysis dataset CH-SIMS, which contains a unimodal label for each modality of a sample. Experiments show a great improvement in overall sentiment prediction after simply integrating unimodal predictions as subtasks in the learning objective. Hazarika et al. (2020) further decompose each modality into a modality-invariant and a modalityspecific representation, and employ squared Frobenius norm loss as the regularizer. However, they treat all modalities equally while regularizing the prediction result, which ignores the different effectiveness of modalities. In real cases, the text is usually more effective on MSA tasks compared to vision and audio. In other words, it is less \"noisy\" than the other two modalities. Also, they employ Central Moment Discrepancy loss to push the modality-invariant representations close and a Frobenius norm to push modality-specific representations to be orthogonal, while in our method, we integrate the above mechanism into a single loss function. Moreover, they regularize the decomposed features by reconstructing the original features with the generated features. We, instead, avoid using such a method and regularize the decomposed features with unimodal prediction tasks. To improve the decomposition performance, we further aggregate the supervised contrastive learning between samples into our frameworks by a custom-designed sampling method.\nA concurrent work HyCon (Mai et al., 2021) introduces a contrastive learning method for MSA, taking both inter-sample and intra-sample contrasts into consideration. However, they ignore the regularization for each decomposed feature. In contrast, in ConFEDE, within-sample feature contrasts are constructed based on a specific pattern centered around text similarity features. Also, when performing inter-sample contrastive learning, Hy-Con samples positive and negative pairs randomly based on MSA labels. In contrast, we design a data sampler that considers both the labels and similarities between modalities to retrieve positive/negative pairs. Due to these reasons, our method beats Hy-Con on most metrics on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , and is able to utilize unimodal labels to further boost performance, e.g., on CH-SIMS (Yu et al., 2020) .\n\nContrastive Representation Learning\nContrastive learning has achieved great success in representation learning by contrasting positive pairs against negative pairs (Akbari et al., 2021; Hassani and Khasahmadi, 2020; Chen et al., 2020) . Akbari et al. (2021) train a Video-Audio-Text Transformer (VATT) using multimodal contrastive learning for the alignment of video-text and videoaudio pairs, and thus achieve state-of-the-art on various computer vision tasks (e.g., audio classification and video action recognition). Hassani and Khasahmadi (2020) propose to learn node and graph level representations by contrasting encodings obtained from different structural views of graphs and achieve the state-of-the-art on various graph classification benchmarks. Chen et al. (2020) present a self-supervised framework, SimCLR, to learn visual representations through a contrastive loss between augmented views of the same image sample. Khosla et al. (2020) extend self-supervised contrastive learning to the supervised setting, i.e., contrasting samples from different classes. They also claim that the supervised setting is more stable for hyperparameters. We design a novel contrastive learning framework that utilizes the contrasts of modalities both within a sample and between samples to enhance multimodal representation in a unified contrastive loss guided by a specific pairing pattern. Furthermore, we propose a data sampler to retrieve similar samples as positive pairs, which is in contrast to the above prior work that obtains positive pairs by data augmentation.\n\nMethodology\nIn this section, we introduce the overall architecture of ConFEDE followed by a detailed description of the contrastive feature decomposition process for learning multimodal representations.\n\nModel Architecture\nThe overall architecture of ConFEDE is shown in Figure 1 . Given a sample, we first encode each modality with corresponding feature extractors. Specifically, we use the [CLS] tag of BERT to encode text (i.e., T), and two separate transformer encoders to encode vision and audio modalities (i.e., V and A), respectively. After that, we decompose each encoded modality into a similarity feature (i.e., T s /V s /A s in Figure 1 ) and a dissimilarity feature (i.e., T d / V d /A d in Figure 1 ) with different projectors. Each projector is composed of layer normalization, a linear layer with the Tanh activation, and a dropout layer. Finally, we update the six decomposed features and fuse them to train the ConFEDE model with the following multi-task learning objective function:\nL all = L pred + \u00b4uni L uni + \u00b4cl L cl ,\nwhere L pred is the multimodal prediction loss, L uni represents the unimodal prediction loss and L cl represents the contrastive loss. \u00b4cl and \u00b4uni are hyper-parameters that balance the contribution of each regularization component to the overall loss L all . We describe each loss term as follows. L pred -Multimodal Prediction Loss. We use a multilayer perceptron (MLP) with the ReLU activation function as the classifier to get the final predictive result (i.e., \u0177 in Figure 1 ). We concatenate all 6 decomposed modality features to obtain the input to the classifier,\n[T i s ; T i d ; V i s ; V i d ; A i s ; A i d ]\n, where [\u2022; \u2022] denotes the concatenation of two vectors. Denote the set of samples in a batch as B. For a given sample i \u2208 B, let its prediction from the classifier be \u0177i m , we calculate the multimodal prediction loss by mean squared error:\n\u0177i m = MLP([T i s ; V i s ; A i s ; T i d ; V i d ; A i d ]), L pred = 1 n n i=1 (y i m \u2212 \u0177i m ) 2 ,\nwhere n is the number of samples in a batch and y i m is the multimodal label. L uni -Unimodal Prediction Loss. For each sample i, we also feed the 6 decomposed features\n[T i s , V i s , A i s , T i d , V i d , A i d ]\ninto a weight-shared MLP classifier separately to get the 6 predictions denoted by the vector \u00fbi . Specifically, we compute the unimodal prediction loss by:\n\u00fbi = MLP([T i s , V i s , A i s , T i d , V i d , A i d ]), u i = [y i m , y i m , y i m , y i t , y i v , y i a ], L uni = 1 n \u2225u i \u2212 \u00fbi \u2225 2 2 ,\nwhere the vector\nu i = [y i m , y i m , y i m , y i t , y i v , y i a ]\nrepresents the ground-truth labels for unimodal prediction. In other words, each decomposed feature is regularized to perform prediction individually.\nNote that the similarity features T i s , V i s , A i s are mapped through the MLP to predict the multimodal label y i m , whereas the dissimilarity features T i d , V i d , A i d are mapped through the MLP to predict modality-specific labels y i t , y i v , y i a (if available). When modality-specific labels are not available, the dissimilarity features T i d , V i d , A i d will also be used to predict multimodal label y i m . The rationale behind this design is that we let the similarity features capture the consistent information shared across different modalities via the overall multimodal label for the sample, while the dissimilarity features can retain modality-specific information represented by unimodal labels.\nL cl -Contrastive Loss. We further regularize the learning through Contrastive Feature Decomposition in one simple joint contrastive loss that contrasts (1) similar samples against dissimilar samples; (2) similarity features against dissimilarity features within a sample. The contrastive loss is denoted as:\nL cl = 1 n n i=1 \u2113 i cl ,\nwhere \u2113 i cl is the contrastive loss of sample i, the detailed derivation of which will be given in the following subsection. \nT V A T V A T V A Ts k Vs k As k T V A T V A T V A\n\nContrastive Feature Decomposition\nWe unify intra-sample and inter-sample contrastive learning into one simple NT-Xent contrastive loss framework (Chen et al., 2020) to conduct both modality representation learning and modality decomposition simultaneously. The loss for sample i is given by\n\u2113 i cl = (a,p)\u2208P i \u2212 log exp(sim(a, p)/\u00c4 ) (a,k)\u2208N i \u222aP i exp(sim(a, k)/\u00c4 ) ,\nwhere (a, p) and (a, k) denote a pair of decomposed feature vectors either within a sample, e.g., (T i s , V i s ), (T i s , A i d ), or across different samples, e.g., (T i s , T j s ). The sets P i and N i are given by\nP i = P i intra \u222a P i inter , N i = N i intra \u222a N i inter .\nHere P i is the positive pair set that includes both intra-sample positive pairs P i intra and inter-sample positive pairs P i inter , while N i is the negative pair set that consists of both intra-sample negative pairs N i intra and inter-sample negative pairs N i inter . Note that (a, p) is a positive pair in P i , and (a, k) is a pair in P i or N i . Specifically, we use the six decomposed features (T s , V s , A s , T d , V d , A d ) to form intra-sample positive/negative pairs, as shown in Figure 2 (a) , with P i intra and N i intra given by\nP i intra ={(T i s , V i s ), (T i s , A i s )} \u222a {(T j s , V j s ), (T j s , A j s ) |j \u2208 Neighbor i \u222a Outlier i }, N i intra ={(T i s , T i d ), (T i s , V i d ), (T i s , A i d )} \u222a {(T j s , T j d ), (T j s , V j d ), (T j s , A j d ) |j \u2208 Neighbor i \u222a Outlier i },\nwhere Neighbor i and Outlier i represent the similar samples and dissimilar samples for the sample i, respectively, to enlarge the scope of the contrast, the detail of which is given in Algorithm 1 that will be explained subsequently.\nNote that instead of treating all modalities equally as in other contrastive learning schemes, here we choose the text similarity feature T i s as an anchor, such that the visual and audio similarity features V i s and A i s are pushed closer to T i s , while in the meantime, the dissimilarity features in all modalities are pushed away from T i s . This is due to the observation that multimodal sentiment analysis is still largely centered around text information. Although other modalities can provide additional information to assist with sentiment prediction, they may also introduce more noise than text. Therefore, unlike other work, we avoid using visual/audio similarity features as anchors, which may bring noise into contrastive learning and confuse model training.\nWe now describe the data sampler shown in Algorithm 1 that retrieves similar samples for a given sample based on both multimodal features and multimodal labels to perform supervised contrastive learning across samples. Specifically, the sampling procedure can be divided into two steps.\nFirst, given the dataset D that contains |D| samples, for each sample pair (i, j) in D, we calculate the cosine similarity score between them:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]),\nwhere sim(w, v) = w T v/||w|| \u2022 ||v|| denotes the cosine similarity between two vectors w and v. And T, V, and A (in Figure 1 ) are the output of BERT, vision and audio encoders, respectively.\nSecond, we retrieve candidate similar/dissimilar sample sets for each sample. For each sample i, we sort samples that have the same multimodal label y i m according to the similarity scores in ascending order as a candidate similar sample set S i 0 . In contrast, we sort samples that have labels other than y i m as a candidate dissimilar sample set S i 1 . Two similar samples with high cosine similarity scores from S i 0 are randomly selected to form inter-sample positive pairs with sample i, which is denoted as Neighbor i . Four dissimilar samples from S i 1 are selected to form inter-sample negative pairs. We denote them as Outlier i in which two samples Outlier i 1 have low cosine similarity scores and the other two samples Outlier i 2 have high cosine similarity scores.\nUsually, we tend to select the samples in Neighbor i and Outlier i 1 to form positive and negative pairs with sample i, respectively. However, samples in Outlier i 2 have different labels but similar semantic information to sample i, making them hard to distinguish from sample i. Therefore, we additionally add these samples to Outlier i to specifically handle this issue by contrastive learning.\nBased on the samples retrieved by Algorithm 1 and the pairing strategy shown in Figure 2 (b), the inter-sample positive/negative pairs for sample i are given by:\nP i inter ={(T i s , T j s ), (V i s , V j s ), (A i s , A j s ) |j \u2208 Neighbor i } , N i inter ={(T i s , T k s ), (V i s , V k s ), (A i s , A k s ) |k \u2208 Outlier i }.\nNotably, our data sampler enables contrastive learning across samples through decomposed modality features without data augmentation. This contrasts original contrastive learning in image classification, which obtains positive pairs by augmentation applied to images. Moreover, we only use similarity features to obtain inter-sample pairing since the similarity features of similar samples in the same class should be close while the similarity features of samples in different classes should be far apart.\n\nExperiments\nWe mainly evaluate ConFEDE on CH-SIMS (Yu et al., 2020) , since it has unimodal labels, which can best meet the design of ConFEDE. To justify the effectiveness of ConFEDE when unimodal labels are unavailable, we further test ConFEDE on the MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018b) , which are two English MSA datasets. However, they can not best test the performance of ConFEDE.\nWe compare our methods with the state-of-theart baselines in Table 1 and 2: LF-DNN (Yu et al., 2020) , MFN (Zadeh et al., 2018a) , LMF (Liu et al., 2018) , TFN (Zadeh et al., 2017) , MulT (Tsai et al., 2019) , MISA (Hazarika et al., 2020) , MAG-BERT (Rahman et al., 2020) , HyCon (Mai et al., 2021) and Self-MM (Yu et al., 2021) . For a fair comparison, the methods which only report the results of a single run and have no valid official code released Algorithm 1: Data Sampling Algorithm Input: Dataset D with the corresponding features T , V , A and multimodal labels ym. Output: Neighbor i , Outlier i for every i \u2208 D Define: sim(w, v) = w T v/||w|| \u2022 ||v|| for every (i, j) \u2208 D do\nCompute the cosine similarity score:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]), end Define: argsort(X) = indices sort X ascendingly Let |D| = length of D, z = |D| 4 . for every sample i \u2208 D do\nRetrieve the similar sample set S i 0 :\nS i 0 = argsort({C i,j |j : y j m = y i m });\nRetrieve the dissimilar sample set S i 1 :\nS i 1 = argsort({C i,j |j : y j m \u0338 = y i m }),\nRandomly select two samples from the last z elements of S i 0 as Neighbor i ; Randomly select two samples from the first z elements of S i 1 as Outlier i 1 ; Randomly select two samples from the last z elements of S i 1 as Outlier i 2 ;\nOutlier i = Outlier i 1 \u222a Outlier i 2 .\nend for reproduction are not selected. A detailed introduction can be found in the supplementary material. The detailed experimental settings are introduced in Appendix C.\n\nEvaluation Metrics\nFollowing the previous works (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020; Hazarika et al., 2020) , we report our results in (multi-class) classification and regression with the average of 5 runs of different seeds. For classification, we report the multiclass accuracy and weighted F1 score. We calculate the accuracy of 2-class prediction (Acc-2), 3-class prediction (Acc-3), and 5-class (Acc-5) prediction for CH-SIMS and the accuracy of 2-class prediction and 7-class prediction (Acc-7) for MOSI and MOSEI. Besides, Acc-2 and F1-score of MOSI and MOSEI have two forms: negative/non-negative (non-exclude zero) (Zadeh et al., 2017; Yu et al., 2021) and negative/positive (exclude zero) (Tsai et al., 2019; Yu et al., 2021) \n\nResults\nThe performance comparison of all methods on CH-SIMS, MOSI, and MOSEI is summarized in Table 1 and Table 2 . The scores of the proposed method and its variations are the averages of 5 runs. The performances of all other baselines, except for MAG-BERT, have been sourced from published papers or official repositories 1 .\nOn the CH-SIMS dataset, our proposed method outperforms all baselines on all metrics. We achieve superior performance compared to the best baseline model, Self-MM, with an improvement of 2.19% on acc-2 and 1.64% on F1 scores. Additionally, the proposed model demonstrates exceptional ability in multi-class classification, outperforming the best baseline by 4.68% on acc-3 and 4.77% on acc-5.\nAs seen in the results, our proposed method, ConFEDE, consistently outperforms all other baselines on the CH-SIMS dataset. The superior classification performance demonstrates that our designed learning method is more effective than the compared methods. Our method, ConFEDE, effectively distinguishes similarity and dissimilarity information between modalities, providing clearer modality features to the downstream classifier for improved prediction. Additionally, the significant improvement in MAE and Corr further highlights the ability of our model to better understand the CH-SIMS dataset than the other baselines.\nTo further evaluate the effectiveness of our proposed method, ConFEDE, we trained our models on the MOSI and MOSEI datasets without uni-modal labels. Instead, we used their multimodal labels for compatibility. The results are presented in Table 2 . On the MOSI dataset, our method outperforms all other baselines in both the negative/nonnegative (NN) setting and negative/positive (NP) setting for acc-2 and F1 metrics. Additionally, our acc-7 and MAE metrics surpass most of the baselines. For the MOSEI dataset, our ConFEDE method outperforms all baselines in all metrics except for the NN Acc-2 and F1 score. Furthermore, our MAE is significantly lower than all baselines, reaching 0.522.\nIt is worth noting that our models perform much better in NP acc-2 than NN acc-2 for MOSEI, as shown in Table 2 . This is because the NN acc-2 setting is generally more challenging than the NP acc-2 setting, as it places more pressure on a model to classify data samples with a regression label of 0. Specifically, if there are two samples with a regression label of 0, when predicted by a regression model, the results might be -0.01 and 0.01. As the value range of \"Neutral\" is [-0.5,0.5) in MOSI and (-0.1,0.1] in SIMS, these two samples should be classified as \"Neutral\". However, in NN settings, they will be classified into two different classes, resulting in a worse acc-2. In contrast, with the NP setting, all \"Neutral\" samples are abandoned, resulting in a better acc-2.\nIn contrast, our method shows better performance in both NN and NP settings on MOSI when compared to other models. The Acc-7, MAE and Corr are also better or comparable to most baselines.\n\nAblation Study and Analysis\nTo evaluate the impact of our proposed structures, we conducted an ablation study on our proposed method by removing inter-sample contrastive learning and intra-sample contrastive learning. The results are shown in Table 1 . \"Plain\" represents the model without contrastive learning method, \"Inter\" represents the model with inter-sample contrastive learning only, and \"Intra\" represents the model with intra-sample contrastive learning and unimodal prediction as a sub-task.\nThe experiment shows that all three models perform worse than the original model. Among the three models, the plain setting has the lowest performance. Both intra-sample contrastive learning and inter-sample contrastive learning provide positive impacts on performance. Compared with Plain, by using text feature as the anchor, Intra filters out noise (useless information for sentiment analysis) in the vision and audio modality, leading to better prediction. This is also the reason why it reaches better acc-2 accuracy than both the other models. Since the acc-2 metric in CH-SIMS follows the negative/non-negative setting, a feature with lower noise helps the classifier make a more precise prediction value, making it easier to classify the 0-labeled samples. This also explains why we achieve better NN Acc-2 performance than all baselines on MOSI.\nFor the inter-sample contrastive learning method, by learning the common and different information between samples, \"Inter\" performs better on multiclass classification. The result of Inter on CH-SIMS shows great improvements on both acc-5 and MAE compared with the other two models, which proves that Acc-5 and regression performance benefits more from \"Inter\". This can also explain why we have a lower NN Acc-2 performance on MO-SEI. Since MOSEI is much larger than MOSI and CH-SIMS, it introduces more noise in each modality, the contrastive feature decomposition learning needs more epochs and a smaller learning rate to separate the useful information from noise. Meanwhile, inter-sample contrastive learning is more efficient on MOSEI. With the larger amount of samples, it is much easier for the sampler to find the most similar and dissimilar samples with the given sample, from which the model can understand the difference between samples better. Thus, ConFEDE can reach higher Acc-7 and regression performance than all other baselines on MOSEI.\nTo further evaluate the effectiveness of the contrastive feature decomposition method, we conducted an ablation study on Intra using the CH-SIMS dataset. As presented in Table 3 , we created three variations of Intra: 1) Intra with only multimodal labels for unimodal prediction (M-label); 2) Intra without the unimodal prediction component (-uni); 3) Intra without the similarity-dissimilarity learning method (-cl); and 4) Intra that uses all similarity features as anchors (+full), which utilizes T s , V s , and A s as anchors instead of T s only.\nThe results in the table demonstrate that all variations resulted in a decrease in performance compared to the original Intra in classification matrices. Both the intra-sample contrastive learning and the unimodal prediction task can regularize the learned representation, resulting in clearer information that aids the classifier in understanding the sample better. However, the \"+full\" setting introduces more noise by also using V s and A s as anchors, which confuses the model and diminishes the denoising ability of the contrastive feature decomposition learning.\n\nConclusion\nIn this paper, we propose a novel method for multimodal sentiment analysis (MSA) called ConFEDE. The ConFEDE framework is based on contrastive feature decomposition, which utilizes a unified contrastive training loss to capture the consistency and difference across modalities and samples. This approach allows for the simultaneous learning of modality decomposition within each sample and su-pervised contrastive learning across samples. Our proposed method is mainly evaluated on CH-SIMS. The result shows that the proposed method significantly outperforms many state-of-the-art multimodal sentiment analysis methods. We further conduct an extensive experiment on MOSI and MOSEI to test the capability of ConFEDE when no unimodal label is available, where our method achieves better performance than state-of-the-art methods on a number of performance metrics.\n"}
{"question": "Which of the following people's work is closest to the author's contributions?", "evidence": "  Perhaps closest to our contribution is recent work modeling chart captioning as a data-to-text problem.  Similarly, Obeid and Hoque (2020) and Kantharaj et al. (2022) explore how transformer architectures can translate tabular structures into captions.  ", "options": ["A. Anderson et al.", "B. Lundgard and Satyanarayan", "C. Kantharaj et al. ", "D. Demir et al. "], "answer": "C", "content": "\nIntroduction\nStudies have shown that captions can improve the recall and comprehension of the data that charts depict (Hegarty and Just, 1993; Large et al., 1995) . For instance, when a caption emphasizes visually prominent features of a chart, like a peak or a sharply declining trend, readers treat this information as the key takeaway (Kim et al., 2021) . Moreover, for people with visual disabilities, captions (or equivalent descriptions such as alt text) are often the only means of accessing the presented data. However, as evidenced by numerous guidelines (Jung et al., 2021) , producing high-quality * Both authors contributed equally to this work. chart captions is a non-trivial and laborious manual process. Thus, despite these advantages, charts are only rarely captioned in practice (Lundgard and Satyanarayan, 2022) .\nTo bridge this gap, several research communities have begun to explore methods for automatically generating chart captions, including using templates and heuristics (Demir et al., 2008; Srinivasan et al., 2019) , adapting image captioning techniques (Balaji et al., 2018; Chen et al., 2019a) , or via data-to-text machine translation (Kantharaj et al., 2022; Obeid and Hoque, 2020) . While promising, these approaches have largely produced captions that either describe a chart's construction (e.g., \"The graph is plot between 'Number of people' x-axis over 'Movie Genres' y-axis\" (Balaji et al., 2018) ) or provide statistical summaries (e.g., \"Machinery and equipment was the most valuable commodity for Singapore in 2019\" (Kantharaj et al., 2022) ). However, these captions do not articulate the perceptual and cognitive features that make charts a distinctive and compelling medium for communicating data (e.g., \"Prices of Big Tech corporations seem to fluctuate but nevertheless increase over time\" (Lundgard and Satyanarayan, 2022) ). Indeed, as Lundgard and Satyanarayan (2022) find, both sighted and blind readers strongly prefer captions that express this type of content.\nTo automatically produce such semantically richer captions, we introduce VisText: a benchmark dataset of 12,441 pairs of charts and captions. VisText makes two key extensions over prior approaches. First, VisText offers three representations of charts: a rasterized image and backing data table, as in previous work; and a scene graph, a hierarchical representation akin to a web page's Document Object Model (DOM), that presents an attractive midpoint between the affordances of chart-as-image and chart-as-data-table. Second, for each chart, VisText provides a synthetically generated caption detailing its construction as well as a crowdsourced caption describing its statistical, perceptual, and cognitive features. These crowdsourced captions represent a substantial increase in data over prior comparable datasets (Mahinpei et al., 2022; Kantharaj et al., 2022) .\nTo demonstrate the possible uses of the VisText dataset, we train three classes of models -textbased caption models, image-guided captioning models, and semantic prefix-tuning. Text-based captioning models fine-tune large language models for VisText's chart captioning task, revealing that both data table and scene graph representations can produce compelling and semantically rich captions. Following recent advancements in image-guided translation (Sulubacak et al., 2020) , we leverage the additional visual affordances in chart images to develop image-guided chart captioning models. Finally, since users often have varying preferences about the type of semantic content in their captions (Lundgard and Satyanarayan, 2022) , we apply semantic prefix-tuning to each of our models, enabling them to output customizable captions.\nOur models generate coherent, semantically rich captions across the VisText charts. Evaluating against standard machine translation and text generation metrics reveals that our models consistently output captions that accurately describe the chart's construction, such as its chart type, title, and axis ranges. Through qualitative analysis of our model's captions, we find that our model competently outputs semantically rich captions that describe data trends and complex patterns. Further, we categorize six common captioning errors that can inform the future development of chart captioning models on the VisText dataset.\nThe VisText dataset and source code are available at: https://github.com/mitvis/ vistext.\n\nRelated work\nHeuristic-Based Chart Captioning. Automatically generating natural language descriptions of data tables dates back to Reiter and Dale (1997) . Demir et al. (2008 Demir et al. ( , 2010 Demir et al. ( , 2012) ) survey this early work and describe the process of extracting insights from a chart by evaluating a list of propositions and composing selected propositions together to produce a natural language summary. More recently, data visualization researchers have explored heuristics that calculate summary statistics and templates to assemble natural language \"data facts\" (Srini-vasan et al., 2019) or descriptions (Cui et al., 2019) . While useful, these approaches yield standardized descriptions that lack the variation and linguistic construction that characterize semantically rich captions (Lundgard and Satyanarayan, 2022) .\nChart Captioning as Image Captioning. With rapid advances of neural image captioning (Vinyals et al., 2015; Anderson et al., 2018) , researchers have begun to adapt these methods for captioning charts. For instance, Balaji et al. (2018) develop a deep learning pipeline that ingests a PNG chart image, classifies the chart type, detects and classifyies textual content present in the chart, and uses this information to generate a textual description. Chen et al. (2019a Chen et al. ( ,b, 2020) ) propose a simpler workflow using ResNet to encode the chart image and an LSTM with Attention to decode it into a natural language description. Both approaches share a pair of limitations. The captions they produce convey relatively simplistic information about the chart (e.g., title, axis labels, etc.) or articulate concepts in visual rather than data terms (e.g., \"Dark Magenta has the lowest value\"). While both approaches contribute associated datasets, their charts and captions are synthetically generated and may not represent real-world counterparts. SciCap (Hsu et al., 2021) addresses this limitation by scraping real-world charts from 290,000 arXiv papers; however, the baseline models trained on this dataset struggle to generate semantically rich captions.\nChart Captioning as Text Translation. Perhaps closest to our contribution is recent work modeling chart captioning as a data-to-text problem. For instance, Spreafico and Carenini (2020) train an encoder-decoder LSTM architecture to generate a natural language caption from time series data. Similarly, Obeid and Hoque (2020) and Kantharaj et al. (2022) explore how transformer architectures can translate tabular structures into captions. These data-to-text methods are more successful than chart-as-image captioning, yielding captions that better capture relevant information from the charts and have higher BLEU scores. Nevertheless, we observe two limitations with these data-to-text approaches that motivate our contribution. First, data-to-text methods are heavily reliant on access to a chart's data table. In practice, data tables are only rarely published alongside charts and methods that recover equivalent information via OCR experience a significant drop in performance (Kantharaj et al., 2022) . Second, the associated datasets do not contain sufficient training examples of captions that express semantically rich insights about the depicted data (i.e., the perceptual and cognitive phenoma that distinguish charts as a medium as distinct from data tables (Lundgard and Satyanarayan, 2022) ). As a result, while the generated captions are compelling, they are largely limited to reporting statistics which sighted and blind readers prefer less than captions that convey complex trends and patterns (Lundgard and Satyanarayan, 2022) .\n\nThe VisText Dataset\nWe designed the VisText dataset in response to two limitations existing datasets present for generating semantically rich chart captions. First, existing datasets represent charts as either rasterized images or as data tables. While useful, these representations trade off perceptual fidelity and chart semantics in mutually exclusive ways -images capture the perceptual and cognitive phenomena that are distinctive to charts (e.g., trends or outliers) but pixels cannot express the rich semantic relationships between chart elements (e.g., estimating plotted data values using axis labels). While the vice-versa is true (Lundgard and Satyanarayan, 2022) , tables also present additional caveats. There is not always a one-to-one relationship between the semantics of a data table and chart (i.e., one data table may be the source for several distinctly different charts). Moreover, data tables are rarely published alongside charts; and, automatic data table extraction is error-prone due to the diversity of chart types and visual styles as well as the difficulty of reasoning about visual occlusion (Kantharaj et al., 2022; Luo et al., 2021; Jung et al., 2017) ).\nSecond, if existing datasets provide captions that describe perceptual or cognitive features, these captions comprise only a small portion of the dataset. At best, LineCap (Mahinpei et al., 2022) offers 3,528 such captions for line charts only, while Chart-to-Text (Kantharaj et al., 2022) estimates that roughly 15% of the sentences in its captions across a variety of chart types express such content.\nIn contrast, VisText provides 12,441 crowdsourced English captions that articulate statistical, perceptual, and cognitive characteristics of bar, line, and area charts. In VisText, charts are available as not only data tables and rasterized images but also as scene graphs. Scene graphs are hierarchical representations that better preserve perceptual fidelity and chart semantics, are often the format for publishing web-based charts, and can be recovered from chart images (Poco and Heer, 2017) .\n\nData Table Collection\nThe data tables found in VisText are sourced from the Statista dataset of the Chart-to-Text benchmark (Kantharaj et al., 2022) . The tables were collected by crawling Statista.com in December 2020 and contain real-world data related to technology, trade, retail, and sports. We process these tables to make them amenable for chart generation, including stripping formatting symbols (e.g., $ and %), standardizing data strings, and identifying the measure type of each column (i.e., quantitative, categorical, or temporal). Data tables are discarded if they do not contain at least one quantitative field and one categorical or temporal field, or if other errors occur during the processing steps. We further down select to data tables containing between 2 to 20 columns and 10 to 500 rows. If a data table has over 500 rows, we randomly sample rows. In larger data tables, this step potentially affects how salient a trend is.\n\nChart Generation and Representation\nCharts in the Chart-to-Text Statista dataset all feature the same layout and visual appearance. In contrast, we aim for richer visual diversity by generating charts using the Vega-Lite visualization library (Satyanarayan et al., 2016) via the Python Altair package (VanderPlas et al., 2018) . To facilitate collecting high-quality captions, we focus on univariate charts: charts that depict one quantitative observation against a categorical or temporal variable. This focus is informed by recent work in the data visualization research community which has chosen single-series line charts as the target of study for natural language descriptions (Kim et al., 2021; Stokes et al., 2022) . VisText also includes single-series bar and area charts as they typically exhibit similar perceptual features to line charts.\nFor each data table, we iterate through pairs of univariate fields. If the pair contains a temporal field, we randomly generate an area or line chart; if the pair contains a categorical field, we randomly generate a horizontal or vertical bar chart. For diversity in layout and visual appearance, we randomly rotate axis labels and apply one of fourteen themes provided by the Vega-Lite library. These themes mimic the visual style of common chart platforms or publishers (e.g., ggplot2 or the LA Times). \n\nGenerated L1 Caption\nHere is a area chart is labeled Cumulative number of patients diagnosed with coronavirus (COVID-19) in Japan as of December 4, 2020, by place of infection. On the x-axis, Month is measured with a categorical scale starting with April and ending with October. There is a linear scale with a minimum of 0 and a maximum of 150,000 along the y-axis, labeled Patients within Japan.\n\nCrowdsourced L2/L3 Caption\nBy December 4th 2020, approximately 160,000 people in Japan had been diagnosed with COVID-19. The first person diagnosed with COVID-19 in Japan was diagnosed in March 2020. The greatest increase in cumulative number of patients in Japan diagnosed with COVID-19 occurred between November and December 2020. In VisText, each chart is represented as a rasterized image, stored as an RGBA-encoded PNG file, as well as a scene graph. A scene graph is a textual representation of the rendered chart similar to a web page's Document Object Model (DOM). Scene graphs encode the position, value or content, and semantic role of all visual elements within a chart, including the individual marks (i.e., bars or points along the line), titles, axes gridlines, etc. Thus, scene graphs express the perceptual features of rasterized images in a more computationallytractable form.\n\nCumulative number of patients diagnosed with coronavirus (COVID-19) in\nScene graphs are a standard data structure for representing vector-based graphics -the most common format for publishing visualizationsand, thus, can be trivially recovered (e.g., by traversing the SVG text string). We extract the scene graph directly from the rendered chart using the Vega-Lite API. As most text generation models expect a linear set of input tokens, we flatten the scene graph via a depth-first traversal. To scale to large language models, we need to further reduce the size of the scene graph. Thus, we preserve the following elements which we hypothesize as being most critical for generating semantically rich captions: title, title coordinates, axis labels, axis label coordinates, axis tick coordinates, mark coordinates, and mark sizes. VisText includes both the original (hierarchical) and reduced (linearized) scene graphs.\n\nCaption Generation and Collection\nOur captioning process is guided by the framework developed by Lundgard and Satyanarayan (2022) , which identifies four levels of semantic content: L1 content enumerates aspects of the chart's construction (e.g., axis ranges); L2 content reports summary statistics and relations (e.g., extrema); L3 content synthesizes perceptual and cognitive phenomena (e.g., complex trends); and, L4 content describes domain-specific insights (e.g., sociopolitical context). In subsequent studies, the authors find that while sighted readers typically prefer higher levels of semantic content, blind readers are split about the usefulness of L1 and L4 content. Thus, given these differing preferences, we define a single caption to express multiple levels of content separated across clauses or sentences. We only consider the first three levels of this model, and leave L4 content to future work. Following guidelines prescribed by the National Center for Accessible Media (NCAM), our captions begin with L1 content and then turn to L2 and L3 content (Gould et al., 2008) .\nWe algorithmically generate L1 content and use a crowdsourced protocol to collect L2 and L3 content. This approach follows (Lundgard and Satyanarayan, 2022)'s computational considerations as well as results from Morash et al. (2015) who find that, even with instructions and guidelines, crowd workers do not describe a chart's structural elements sufficiently for blind readers. Thus, synthetically generating L1 content allows us to ensure that captions convey complete descriptions of the chart's structural elements. L1 content comprises 1 sentence conveying the chart type and title, and then 1 -2 sentences describing the axes (including the titles, ranges, and scales). We use template randomization to generate a diverse range of L1 captions to mimic human variability and reduce the capacity of the model to overfit to a single L1 style. Three templates are defined for the first sentence and twenty-six template combinations for the subsequent sentences. During generation, we randomly select a pair of templates and fill in in- formation from the abstract chart specification. For additional diversity, we randomly drop scale information and swap template words with synonyms. Templates and synonym replacements are listed in Appendix E.2.\nTo crowdsource L2 and L3 content, we extend the protocol used by Lundgard and Satyanarayan (2022) . After soliciting consent, we introduce the task: participants are presented with a chart image and corresponding L1 description; they are asked to write a description about the trends and patterns they observe without drawing on background knowledge or repeating L1 information. The introduction provides examples and explanations of valid and invalid responses. After acknowledging these examples, participants are asked to complete 5 random iterations of the task. To maximize the quality of our crowdsourced captions, we manually curated the charts and L1 descriptions used in the study. We discarded any charts that were challenging to read (e.g., colors were too similar, marks were not easily readable, etc.). Participants were recruited on the Prolific.co platform, took approximately 14 minutes to complete the study, and were compensated $3.25 ($14/hour). Additional details on our crowdsourcing process are in Appendix E.3.\nWe manually verified charts where participants failed an attention check and discarded invalid descriptions. Additionally, we manually inspected captions for personally identifiable information or offensive content. Using heuristics, we removed captions where respondents described charts as unclear or illegible and replaced newline characters with spaces. Although we attempted to fix incorrect spelling and casing errors using a similar heuristic-based approach, we observed that this process could improperly affect axis and chart names. As a result, these errors remain in our dataset.\n\nDataset Analysis\nFigure 2 shows the distribution and means of the lengths of chart representations and captions. Synthetically generated L1 captions have roughly 1.5x more characters than crowdsourced L2/L3 captions (\u00b5 = 255 vs. \u00b5 = 177) but the average number of sentences are comparable (2.5 vs. 2). The VisText dataset consists of captions for 3,189 area charts, 6,238 bar charts, and 3,014 line charts -the roughly twice-as-many bar charts as area or line charts corresponds to the randomization of temporal fields during chart generation (Sec. 3.2). As some charts have multiple crowdsourced captions, we randomly split our dataset into training, validation, and test sets using the chart IDs to prevent data leakage across sets. This resulted in an approximate ratio of 80:10:10.\nFinally, to understand the distribution of semantic content, we manually coded 2% (230) of crowdsourced captions. We followed a protocol inspired by Lundgard and Satyanarayan (2022) by breaking sentences down into independent statements and mapping these statements to their semantic content level. We marked statements as not categorizable if they did not map to the framework -for instance, if captions expressed commentary from crowd workers such as \"this chart is hard to read.\" Our analysis revealed 11 L1 statements (2.4%), 180 L2 statements (39.7%), 253 L3 statments (55.7%), and 10 not categorizable statements (2.2%). While a handful express L1 content, the bulk of statements (95%) express L2 or L3 content, with approximately 1.4x L3 statements than L2.\n\nChart Captioning Models\nTo demonstrate the affordances of the VisText dataset, we train three classes of models. First, we fine-tune large language models to translate from textual chart representations to natural lan-guage captions. These models evaluate the feasibility and impact of scene-graph models compared to prior data-table approaches (Kantharaj et al., 2022) . Second, as VisText provides multiple chart representations, we adapt image-guided translation (Sulubacak et al., 2020; Cho et al., 2021) to develop two multimodal chart captioning models: image-scene-graph and image-data-table. Finally, since VisText offers captions at different semantic levels and prior work has shown significant differences in readers' preferences (Lundgard and Satyanarayan, 2022) , we explore prefix-tuned models that selectively output L1, L2/L3, or L1+L2/L3 captions. Training details are in Appendix D.\n\nText-Based Chart Captioning\nInformed by prior work (Kantharaj et al., 2022) , we investigate text translation models for generating chart captions. In particular, Kantharaj et al. found that models that translate data tables to chart captions significantly outperform image captioning models. However, when data tables were not available, the authors found a significant drop in their models' ability to extract relevant information from the chart -an effect that was only slightly ameliorated by using OCR methods to extract text from chart images. In contrast, VisText's scene graphs can be more readily recovered from charts when data tables are not available -for instance, by processing the SVG format of web-based visualizations. Moreover, scene graphs offer a potentially richer source of information than data tables as they encode visual properties of the chart (e.g., coordinates and colors) and are less noisy than tokens recovered via OCR. Thus, to evaluate the feasibility and efficacy of scene graphs, we train a scene-graph text translation model and a baseline data-table model for comparison.\nFor each model, we fine-tune a pretrained ByT5 transformer model (Xue et al., 2022) on the Vis-Text dataset. We choose ByT5 over T5 transformers (Raffel et al., 2020) because it uses a token-free, byte-encoding that eliminates the use of a tokenizer. As a result, it is robust to noisy inputs, minimizes the need for text preprocessing, and eliminates the out-of-dictionary problem. This allows our model to handle common typographical and chart reading errors in the crowdsourced L2 and L3 captions and increases generalizability to previously-unseen words that could be present in chart and axes titles.\n\nImage-Guided Chart Captioning\nFollowing recent advancements in image-guided machine translation (Sulubacak et al., 2020) , we train image-guided captioning models using the VisText dataset. Images have improved text-based machine translation models by providing visual information complementary to natural language inputs. Similarly, chart images can contain visuals complementary to the textual specification. For instance, visual affordances that are important for perceiving a trend (e.g., gestalt relations, relative sizes/areas, etc.) may be obfuscated in the scene graph but better captured in the chart image.\nWe train three image-guided chart captioning models: image, image-scene-graph, and image-data-table. All models leverage the vision-language transformer model VL-T5 (Cho et al., 2021) . VL-T5 is pretrained on image captioning and visual grounding tasks and was successfully applied to machine translation, making it suitable for chart captioning. We extract visual features for each VisText chart image using a Bottom-Up Feature Extractor (Anderson et al., 2018) . To explore the value of images to chart captioning, our image model only takes in the image features, while image-scene-graph and image-data-table concatenate the image features with the chart's textual representations (scene graph or data table).\n\nSemantic Prefix-Tuning\nIn real-world chart captioning settings, users want to vary the level of semantic content in their captions. For instance, while some blind users want verbose captions that describe the chart visuals, sighted users may only want captions that help them expose data trends (Lundgard and Satyanarayan, 2022) . To develop models capable of such customization, we leverage prefix-tuning strategies alongside VisText's semantic caption breakdown. Prefix-tuning specifies a task alongside the input, permitting a single large language model to perform many different tasks. In our setting, we use prefix-tuning to specify the level of semantic content to include in the caption (Li and Liang, 2021) .\nWe train each of our models with and without semantic prefix-tuning. With semantic prefix-tuning, we treat chart captioning as a multi-task fine-tuning problem, where the model is trained to generate the L1 and L2/L3 captions separately. In every epoch, the model sees each VisText chart twice, once with the L1 prefix and caption and once with the L2/L3 prefix and caption.\n\nEvaluation and Results\nTo evaluate the VisText dataset and our chart captioning models, we measure the readability and accuracy of generated captions and their similarity to the VisText target caption. We also qualitatively analyze the descriptiveness of generated L2/L3 captions and categorize common errors.\n\nQuantitative Model Performance\nWe evaluate the results of our text-based and imageguided captioning models with and without prefixtuning. We also compare to a current state-of-theart chart captioning model that uses data table chart representations and a T5 generation model (Kantharaj et al., 2022) . To measure the quality of output captions, we evaluate each model on machine translation and language generation metrics (Table 1 ).\nChart images do not support captioning. The image model performs the worst of all the chart captioning models. Its low perplexity and high error rates indicate it is highly confident in its inaccurate captions. While chart images contain the same information encoded in the chart's textual representations, it is presumably not adequately extracted by the model. Both the image model backbone (Cho et al., 2021) and the visual feature extractor (Anderson et al., 2018) are trained on natural images, making chart images out-of-distribution inputs that are likely to be poorly represented by these vision models. As the chart captioning task grows, model backbones, architectures, and feature extractors could be customized to chart images, which may improve image-based chart captioning.\nAll models produce high quality L1 captions. In our chart captioning setting, relation generation (Wiseman et al., 2017) measures how often the chart title, axis names, and axis scales in the input appear in the caption. Every model (except image) achieves a similarly-high relation generation score, indicating that every model can generate detailed L1 captions.\nScene graphs perform as well as data tables. Models trained on scene graph representations achieve similar performance across the evaluative metrics to models trained on data tables. As scene graphs can be more easily extracted from web-based charts images, they may be the preferred representation for future chart captioning models.\nImage-guiding does not improve captioning. Our image-guided captioning models do not experience the significant increase in performance other image-guided translation tasks report. While in image-guided translation, images contain substantial additional information beyond the text, the image and textual representations in chart captioning often contain highly similar information. The small amount of additional information in images might benefit complex captioning tasks on multivariate charts or infographics; however, the current VisText captions rarely reference visual information not present in the scene graph or data table.\nPrefix-tuning is free. Adding semantic prefixtuning to our models does not significantly change their performance. Models trained with and without prefix-tuning are exposed to the same set of charts, so it is consistent that prefix-tuning would not impact the quality of output captions. Given prefix-tuned models are able to output L1, L2/L3, and L1+L2/L3 captions, prefix-tuning may be preferred if users require semantic customization.\n\nQualitative Caption Evaluation\nTo augment our quantitative evaluation, we qualitatively assess the descriptiveness and accuracy of the generated chart captions. Since L1 caption accuracy can be measured at scale via relation generation, we focus our evaluation on L2/L3 predictions.\nPrior analysis tasked annotators with comparing the accuracy, coherence, and fluency of generated captions compared to a target caption (Kantharaj et al., 2022) . Instead, our approach follows an inductive qualitative data analysis approach: iteratively analyzing captions in a \"bottom-up\" fashion to identify emergent patterns in how generated captions compare to the ground truth (Bingham and Witkowsky, 2021). We randomly sample 176 generated captions from the scene-graph model with prefix-tuning and break them into their independent L2 and L3 statements, resulting in 181 (48.27%) L2 statements and 194 (51.73%) L3 statements.\nApproximately half (241 / 512) of the L2 and L3 statements made in the generated captions are factually accurate. Moreover, many of the full sentences are written in a natural, human-like manner and generated captions frequently include both compound and complex sentences. On average, every generated caption has one L3 statement and zero to et al., 2022) . We evaluate each model using machine translation and text generation metrics, including BLEU (Papineni et al., 2002) , Perplexity, Relation Generation (RG) (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD) (Kusner et al., 2015) , and Translational Error Rate (TER) (Snover et al., 2006) . We report the mean and standard deviation of three independent models. Darker colors indicate better scores.\nInput PT BLEU \u2191 Perplexity \u2193 RG \u2191 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193 Kantharaj et\ntwo L2 statements. Often this takes the form of a L3 general trend statement (e.g., \"The median annual family income in Canada has increased from 2000 to 2018\") accompanied by an L2 minimum and maximum statement (\"The highest was in 2015 at 80k and the lowest was in 2000\"). For the remaining half of analyzed captions, we identified the following recurring types of errors:\nIdentity Errors. We identify 86 identity errors (22.93% of analyzed statements). An identity error occurs when an L2 or L3 statement incorrectly reports the independent variable for a given (often correctly identified) trend. For bar charts, this error means incorrectly reporting the categorical label associated with a bar (e.g., in Appendix Figure 5c : \"The most popular music activity is vinyl albums and vinyl singles\" should be \"The most popular music activity is tickets for festivals\"). For area and line charts, this error means incorrectly identifying the temporal point or range of the trend. With bar charts, in particular, we observed that the identities were often \"off-by-one\" (i.e., identifying a minimum or maximum value, but attributing it to the second-highest or second-lowest category).\nValue Errors. A value error occurs when the quantitative data value of a statement is incorrect.\nOf the captions we analyzed, 3.20% (12) of statements contained a value error. For instance, as shown in Appendix Figure 4c , for the caption \"The total gate revenue from sporting events worldwide by region from 2006 to 2015 has increased from around 15 billion dollars to around 15 billion dollars\", the value should be around 18 billion dollars.\nIf it is ambiguous whether an error is an Identity or Value Error, we classify it as the former.\nDirection Errors. A direction error occurs when the direction (which can be increasing, decreasing, or stable) of a trend in an L3 statement is incorrect. We uncovered 32 direction errors (8.53% of analyzed statements). For instance, in the caption \"The per capita consumption of sweet corn in the US has increased from 2000 to 2019\" (Appendix Figure 3c ), the trend is actually decreased. In most direction errors, the identity (i.e., temporal range) is correct.\nStability Errors. A stability error occurs when the magnitude of a direction or the variance in a trend is incorrect. This can often refer to how much a trend is increasing or decreasing, such as rapidly or slowly, as well as whether it's a steady change or highly-fluctuating change. Looking ahead, while accessibility remains a key domain that would benefit from automated chart captioning, and deploying automated chart captioning models into the field is an exciting prospect, we believe the most promising approach for future work lies in \"mixed-initiative\" (i.e., human + AI) chart authoring systems. In particular, as we describe in our Ethics Statement below, chart captioning models are currently prone to make a number of factual inaccuracies which can have severe harmful consequences. On the other hand, by integrating these models into chart authoring systems (e.g., Tableau, Charticulator, Data Illustrator, or Lyra), chart authors can intervene and make any necessary corrections. Indeed, such integration offers exciting opportunities to develop novel interactive methods for verifying generated captions. For instance, models like ours could generate an initial caption (or set of captions) based on the chart currently being authored; as the system has access to all three representations of the chart (the back-ing data table, chart image, and structured scene graph), it might automatically segment the caption into independent \"data segments\" and interactively link and map them to rows in the table or regions on the chart, akin to Kori (Latif et al., 2021) .\n\nLimitations\nComputational Constraints. Despite using modern GPUs, with large amounts of memory, we were forced to use the smallest-parameter variants of T5 and ByT5 as we encountered out-of-memory errors with the larger alternatives. More problematically, the quadratic relationship between sequence length and time/space complexity of transformer architectures (Vaswani et al., 2017) , especially when using byte-level sequences (Xue et al., 2022), has had a significant impact on our model performance.\nIn particular, to be computationally tractable, we were forced us to truncate our input and output sequences to, at most, 1,024 and 512 characters respectively (1,024 coming from the underlying ByT5 architecture (Xue et al., 2022)).\nThese character thresholds have likely had an outsized effect on scene-graph models. For instance, due to these character limits, we reduced scene graph sequences to only a minimal set of visual characteristics; VisText also includes the raw, unprocessed scene graphs which offer a richer source of information about the visual features that are important to how people decode charts (e.g., bounding boxes, color) but were unavailable to our models. Moreover, as Figure 2 shows, even with this reduced representation, the mean length of scene graph sequences is 948 characters (cf. 426 characters for data tables) with a wide distribution. Thus, despite scene-graph models achieving comparable performance to data-table models, the former saw a much smaller proportion of complete sequences as compared to the latter. This truncation step additionally negatively impacts charts with long titles or axis names -in such cases, we observed that the L2 or L3 caption would be altogether truncated before generation.\n\nChart Types and the Visualization Design Space.\nVisText is scoped to only univariate bar, area, and line charts. We chose to begin with these chart types informed by data visualization research that has focused on studying natural language descriptions of single-series line charts -a basic, but commonly occurring chart type that offers a compelling target of study as it most visibly surfaces any poten-tial trends in the data (Kim et al., 2021; Stokes et al., 2022) . Future work can now begin to consider more complex chart forms in a step-by-step manner. For instance, moving from univariate bar, area, and line charts to multivariate versions of these chart types (i.e., stacked bars and areas, grouped bars, and multi-series line charts). From there, work can also consider chart types that surface perceptual and cognitive phenomena in visually distinct ways (e.g., scatterplots, where trends appear as clusters of points; heatmaps, where color saturation often encodes a trend; or maps, where color or other layered elements such as symbols are used to represent data values). Finally, automated methods for captioning visualizations may eschew chart typologies altogether in favor of visualization grammars -by offering a more composable and combinatorial approach to the design space (Wilkinson, 2012) , learning over visualization grammars may offer a more robust approach to captioning highly customized or unique visual forms.\nFor each future work direction, we anticipate scene graph representations to prove more fruitful than the data table. As the complexity of the visualization increases, its relationship to the data table only grows more ambiguous; the scene graph, on the other hand, directly encodes the visual form and thus remains faithful to it. As a result, to support such future work, VisText provides the raw specifications used to produce our charts (via the Vega-Lite visualization grammar (Satyanarayan et al., 2016) ) as well as the raw, hierarchical scene graphs prior to our linearization and reduction step.\n\nEthics Statement\nThe Consequences of Incorrect Captions. Weidinger et al. ( 2021) comprehensively survey the risks associated with the large language models (LLMs) that underlie our contribution. Of the six categories of risk they identify, harms stemming from models producing factually incorrect statements are not only most pertinent to our work, but are likely heighted as compared to general uses of LLMs given the context we are addressing: automatically captioning charts. In particular, people most often consume charts and visualizations in order to make data-driven decisions (Keim et al., 2008) -for instance, about whether to evacuate ahead of a hurricane (Padilla et al., 2018) , or health & safety during the pandemic (Shneiderman, 2020) . Moreover, recent results have shown that readers not only fixate for longer and are more likely to recall the textual content of and around visualizations (Borkin et al., 2015) but this textual content can strongly influence the takeaway message readers leave with even when it is at odds with the depicted data (Kong et al., 2018 (Kong et al., , 2019)) . Finally, these issues are exacerbated by the persuasive and rhetorical force of data and charts (Kennedy et al., 2016; Hullman and Diakopoulos, 2011) , that often project a sense of authority and certainty (Correll, 2019) . As a result, readers may not think to double check the accuracy of chart captions, and inaccurate statements that models may produce could lead to harmful downstream decisions.\nTo proceed ethically with this line of research, we believe that advances in data and modeling need to be closely followed by attention devoted to mitigating the risks of incorrect statements. At base, automatically generated captions should be identified as such at the forefront to raise readers' awareness about the potential for incorrect statements. And, interactive visual linking strategies (such as those explored by Kong and Agrawala (2012) ; Kim et al. ( 2018)) could be deployed to help readers manually verify the constituent statements of a caption against the chart. These strategies, however, place the burden of harm mitigation on readers. Thus, an alternate approach might never surface automatically generated captions to readers directly but instead use them as part of mixed-initiative systems for jointly authoring visualization and text, such as Kori (Latif et al., 2021) . In such systems, automated chart captioning models would help to accelerate the authoring process -combatting the blank slate problem by providing an initial summary of the chart -and chart authors would make any necessary corrections prior to publication.\nBesides these human-computer interaction (HCI) approaches for mitigating harm, an equally important direction for future work should leverage interpretability techniques to more deeply study what the models are learning. To what degree are chart captioning models stochastic parrots (Bender et al., 2021) , and how much do they understand the information charts depict? Automated Captioning for Accessibility. Although accessibility is a guiding motivation for the bulk of work in automated captioning (be it image captioning or, as in our case, chart captioning), studies find mixed reactions, at best, about these approaches among people with disabilities (PWDs).\nFor instance, accessibility educator and researcher Chancey Fleet described Facebook's automatic image descriptions as \"famously useless in the Blind community\" despite \"garner[ing] a ton of glowing reviews from mainstream outlets\" (Fleet, 2021; Hanley et al., 2021) . This disconnect appears to stem from a more fundamental mismatch between what PWDs describe as their captioning needs, and what the research community -particularly through its automatic, quantitative evaluationsprioritizes (Jandrey et al., 2021) . In particular, surveys with PWDs repeatedly surface the contextual nature of captions. Bennett et al. (2021) find that the context of use shapes the degree to which PWD are comfortable with captions describing people's race, gender, and disabilities -for instance, changing their preferences if they were in a white, cisgender, nondisabled, and professional company versus their own community. Similarly, Jung et al. (2022) find shifting preferences for the content image descriptions should convey across different photo activites -for example, when viewing or taking photos, participants wished for descriptions that conveyed spatial cues whereas when searching or reminiscing about photos, participants hoped for descriptions to connect to personal data or differentiating details.\nIn contrast, quantitative metrics of model performance compare generated captions to a single \"ground truth\" caption. This framing of success not only makes it difficult to develop contextuallyvarying caption generation but can actively penalize such investigations. For instance, with our work, we explored how prefix-tuning can be used to develop models that are responsive to users' preferences about semantic content. However, as described in Sec. 5.1, existing quantitative metrics of model performance (e.g., BLEU, ROUGE, WMD, and TER) show a drop in model performance despite our qualitative analysis indicating that these captions are indeed high quality.\nFinally, our exploration of semantic prefixtuning represents only a very preliminary step towards addressing the contextual captioning needs of PWDs. In particular, the semantic labels Vis-Text assigns to captions were derived from prior work (Lundgard and Satyanarayan, 2022) that only explored natural language descriptions when consuming presentations of visualizations -one task from a broader palette (Brehmer and Munzner, 2013) . Future work might instead extend the Vis-Text dataset -and corresponding models -to consider captions for a broader range of tasks including consuming visualizations for scientific discovery, enjoyment or, producing, searching, or querying visualizations (Brehmer and Munzner, 2013) . \n\nModel Generated L1 Caption\nAverage spending per consumer on selected music activities in the United States as of July 2018 is a bar graph. The x-axis measures Response while the y-axis measures $40 to $99.99.\n\nModel Generated L2/L3 Caption\nThe most popular music activity is vinyl albums and vinyl singles.\nThe least popular music activity is vinyl albums. (b) Model results using the L2/L3 captions.\nTable 2 : We separately evaluate our L1 and L2L3 captions on all the same metrics except for Relation Generation.\nIn general, we observe that L1 captions perform better than the L2/L3 captions. Our models generate verbose L1 captions that are similar to the structure of our L1 templates, while the L2/L3 captions are human-generated and contain more variability. Darker colors indicate better scores.\n\nB Additional Evaluations\nB.1 Independent L1 and L2/L3 Caption Evaluation\nTo better understand how our models generate varying levels of semantic content, we separately evaluate our prefix-tuned models on L1 captioning and L2/L3 captioning tasks. Each prefix-tuned model can output an L1 or an L2/L3 caption for each chart. We evaluate these captions to their respective L1 or L2/L3 ground truth captions and report the results in Table 2 . Since we compute Relation Generation using only the L1 chart fields (e.g., chart title, axis scale, etc.), we do not report the results separately for L1 versus L2/L3 captioning. There is no direct Relation Generation analog for L2/L3 captions, since they are human-generated and do not follow a specific template. The Relation Generation for L1 captions is identical to the Relation Generation for L1/L2/L3 captions reported in Table 1 .\n\nB.2 Evaluation Details\nQuantitative Model Performance Metrics. We evaluate our models using NLP and machine translation metrics, including BLUE (Papineni et al., 2002; Lin and Och, 2004) , Perplexity, Relation Generation (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD), and Translation Edit Rate (TER) (Snover et al., 2006; Post, 2018) . We implement Relation Generation per Wiseman et al. (2017) , use the Gensim implementation of WMD, and use the Hugging Face implementation (Wolf et al., 2019) for the remaining metrics.\n\u2022 BLEU: BLEU requires several gold standard references. In our evaluation setup, we use the test set caption as a single reference.\n\u2022 Perplexity: We use a pretrained GPT-2 Medium model to compute Perplexity.\n\u2022 Relation Generation: The fields we evaluate on are the chart title, axis names, and axis scales (if any).\n\u2022 Translation Edit Rate (TER): Edits consist of deletions, additions, and substitutions, as present in SacreBLEU.\nQualitative Caption Evaluation. To produce our qualitative evaluation results (Sec. 5.2), we iteratively evaluated randomly sampled captions until there was no more marginal information about they types of errors to be gained from evaluating more captions. For each L2/L3 caption, we assess the number of independent, mutually-exclusive L2 and L3 claims/statements that are being made. In comparison to evaluating at a sentence-level, this allows us to take a more nuanced approach that isn't limited by where the model has generated a full-stop. This approach allows us to more-accurately evaluate factual precision without overly-penalizing for a single mistake. An example might take the form of \"The lowest value is X (claim 1), the highest value is Y (claim 2), and the second highest is Z (claim 3). Overall, it is increasing over time (claim 4).\" We observe that the first sentence is a compound sentence that consists of three independent clauses, each with a single factual L2 claim, while second sentence is a single factual L3 claim. Let us assume that claim 1 was factually incorrect. If we evaluate at a sentence-level, then the entire first sentence comprising of claim 1, claim 2, and claim 3 would be incorrect. However, by breaking this caption into independent, mutually-exclusive claims, we can more precisely calculate the factual precision of our text generation. \n\u2193 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193\nTransformer Backbone T5-small L2/L3 0.06 \u00b1 2.67e\u22123 35.81 \u00b1 4.13e+0 0.25 \u00b1 6.43e\u22123 0.09 \u00b1 3.43e\u22123 0.22 \u00b1 5.73e\u22123 0.22 \u00b1 5.60e\u22123 0.99 \u00b1 8.70e\u22123 113.33 \u00b1 2.94e+0 Ours (ByT5-small) L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\nL1 Generation new-seed L2/L3 0.08 \u00b1 5.93e\u22123 20.96 \u00b1 2.71e+0 0.29 \u00b1 5.77e\u22123 0.11 \u00b1 2.33e\u22123 0.25 \u00b1 5.30e\u22123 0.25 \u00b1 5.27e\u22123 0.91 \u00b1 1.83e\u22123 116.36 \u00b1 1.11e+1 original-seed L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\n(c) Ablation study results using the L2/L3 captions. \n\nC Ablation Studies\nTo evaluate our modeling and dataset design choices, we run ablation studies measuring the impact of our transformer model backbones and stochastic data generation pipeline. We report the results in Table 3 .\nTransformer Backbone. To understand the impact of our token-free, byte-to-byte architecture ByT5 model backbone, we explore other large language models. Specifically, we compare our 300M parameter ByT5-small model (Xue et al., 2022) with a 60M parameter T5-small (Raffel et al., 2020) and 140M parameter BART-base model (Lewis et al., 2020) . We also apply prefix-tuning to the ByT5 and T5 models. We cannot apply prefix-uning to BART because BART does not support multi-task learning. Quantitatively, using ByT5 does not appear to significantly improve upon T5. However, we theorize that ByT5's token-free paradigm increases the input sequence length by compressing more input text into fewer input tokens.\nL1 Caption Generation. Since we generate L1 captions stochastically, we evaluate whether our initial randomization impacted the model's results. We compare generate a second set of L1 captions using a different random seed. We see the results are nearly identical across all metrics, indicating our dataset captures a diverse set of L1 captions. We estimate that we trained each model between 5 to 10 times to achieved our final results.\n\nD.3 Ablation Models\nWe train our ablation models using the same parameters as our default models, only varying the parameter of interest. We train them on 16 virtual CPU cores on Xeon E5 hypervisors with 128GB of memory and PCI pass-through access to eight NVidia Titan XP GPUs with 12GB of memory.\n\nD.4 Notable Package Versions\nPackage versions are listed in Table 5 .\n\nE Additional VisText Dataset Details E.1 Licensing\nOur use of the raw Statista data from Kantharaj et al. ( 2022) is consistent with its intended use case. The data was licensed under the GNU General Public License v3.0. We release our data and code under GNU General Public License v3.0.\n\nE.2 L1 Caption Generation Process\nThe Level 1 captions are generated from a random process that chooses from 3 title templates and 6 axis templates. The title templates we use are:\n\u2022 This is a [chart-type] titled [chart-title]\n\u2022 This [chart-type] is titled [chart-title]\n\u2022 [chart-title] is a [chart-type]\nThe axis templates we use for each axis are:\n\u2022 For each axis template, we randomly choose whether to include the axis scale. Furthermore, within each template, we further randomly swap words with synonyms. A list of words and their possible synonym substitutions are:\n\u2022 this: here, a\n\u2022 chart: graph, diagram, plot\n\u2022 titled: called, named, labeled\n\u2022 on: along\n\u2022 plotted: defined, measured, drawn, shown\n\u2022 plots: measures, shows\n\u2022 with: using, on, along, as\n\u2022 found: seen\n\u2022 labeled: marked\n\nE.3 Crowdsourced Study Protocol\nFigures 6-10 screenshot the introduction, eligibility and consent statements, instructions, and a task from our crowdsourced study. We recruited participants on the Prolific.co crowdsourcing platform, following conventions in the data visualization research community 3 and recent research results (Tang et al., 2022) that suggest Prolific yields higher quality results than Amazon Mechanical Turk. We conducted multiple pilot runs to calibrate the amount of time it would take participants to complete the study, and found that most participants were able to successfully do so within 14 minutes. Following Silberman et al. (2018) , who advocate for paying workers at least minimum wage at your location, we choose to pay our participants $3.25 -a roughly $14/hour rate in line with the $14.25/hour minimum wage in Massachusetts at the time the study was conducted.\nOur study was determined to be exempt by MIT's institutional review board (IRB). Participants had to explicitly provide their consent in order to proceed with the study -if participants did not consent, they were redirected back to the Prolific platform. The consent statement (Fig. 8 ) reminded participants of their rights (including that their participation is voluntary and consent could be revoked at any time), and encouraged participants to contact either the study PI or IRB board directly should they have any concerns. We constrained our participant pool (and eligibility requirements) to people living within the United States or United Kingdom who self-reported as being sighted with no vision or color impairments. We did not collect any additional demographic data from participants as we did not determine this to bias or otherwise affect the content we hoped to collect. Each task (an example of which is shown in Fig. 10 ) included an attention check where participants were asked to correctly identify the chart type shown. If participants failed more than two attention checks, their submission was flagged for manual review -in practice, the bulk of participants who failed attention checks nevertheless produced valid captions and, thus, were paid fully. The task asked participants to complete a free response question to describe as completely as they could the trends and patterns observed, emphasizing that their response would be evaluated for correctness and completeness. Despite best practices suggesting a more structured, querying approach (called QID) can yield higher quality captions (Morash et al., 2015) , we opted for our free-response approach as the benefits of QID (namely, in expressing the chart type, title, and axes units) would already be captured by our synthetically generated L1 captions. Moreover, in contrast to the templatized output produced by QID, we hoped that our free-response responses would yield more \"natural\" articulations of perceptual and cognitive trends, following the Lundgard and Satyanarayan (2022) framework.\n\n\nhttps://github.com/mitvis/vistext 2 https://github.com/j-min/VL-T5\n"}
{"question": "Which is the correct order of our research?", "evidence": "a.propose the ConFEDE framework     b.  Mainly evaluated our proposed method on CH-SIMS                c.further conduct an extensive experiment on MOSI and MOSEI   In this paper, we propose a novel method for multimodal sentiment analysis (MSA) called ConFEDE. The ConFEDE framework is based on contrastive feature decomposition, which utilizes a unified contrastive training loss to capture the consistency and difference across modalities and samples. This approach allows for the simultaneous learning of modality decomposition within each sample and su-pervised contrastive learning across samples. Our proposed method is mainly evaluated on CH-SIMS. The result shows that the proposed method significantly outperforms many state-of-the-art multimodal sentiment analysis methods. We further conduct an extensive experiment on MOSI and MOSEI to test the capability of ConFEDE when no unimodal label is available, where our method achieves better performance than state-of-the-art methods on a number of performance metrics.\n ", "options": ["A. a--b--c", "B. b--c--a", "C. c--a--b", "D. a--c--b"], "answer": "A", "content": "\nIntroduction\nMultimodal deep learning involves interpreting and analyzing multimodal signals together, where each modality refers to a way in which something is experienced and felt, e.g., the visual, audio, or language modality. With the widespread popularity of online social media, such as Instagram, Tik-Tok, Facebook, etc., videos containing multiple modalities have become a major information carrier, which brings new challenges to content recommendation and classification, e.g., video question answering (Lei et al., 2021; Li et al., 2020) , video captioning (Ging et al., 2020; Li et al., 2020) , and video retrieval (Akbari et al., 2021; Lei et al., 2021) .\nWhile traditional sentiment analysis is mainly based on language, multimodal sentiment analysis (MSA) predicts the human emotion by utilizing extra information available in visual and audio modalities of the content to assist with language-based prediction. Here, the text modality contains the semantic meaning of the spoken language. The visual modality extracts the facial characteristics (e.g., head orientation, facial expressions, and pose) of the speaker. The audio modality reflects the emphasis on the utterance (e.g., through pitch, bandwidth and intensity). MSA has recently gained much attention in research for several reasons. On one hand, because of the abundance of social media content, commercial interests are switching from gauging user opinions/emotions from text only to more thorough multimodal analysis based on videos. On the other hand, short video platforms (e.g., TikTok, Instagram) allow users to easily create multimodal content including visual information, audio, and inserted text, while these modalities are sometimes noisy or even contradicting each other in sentiments. Therefore, the presence of multimodal information in addition to the text or language itself is necessary to make a thorough conclusion about the overall sentiment of a video.\nMultimodal fusion has become essential to gaining a deeper understanding of these video scenes (Baltru\u0161aitis et al., 2018) and has proven to be helpful in many downstream tasks. Various multimodal fusion techniques have been proposed for MSA, among which a basic solution is concatenating the extracted feature of each modality before performing downstream regression or classification. Recent work has recognized the importance of identifying modality-invariant information across modalities and fuse them to strengthen sentiment prediction (Hazarika et al., 2020; Zadeh et al., 2018a; Rahman et al., 2020; Sun et al., 2020) .\nAlthough modality-invariant information helps reinforce the understanding of the content, there are also cases where sentiments of different modalities contradict each other. For example, when one thanks someone with phrases like \"Finally I can rest easy tonight\" or \"I can't thank you enough\", it is very hard to conclude whether the sentiment is positive or negative without looking at the nonverbal cues, such as tones, facial expressions, and gestures. In fact, many sarcastic opinions are expressed by non-linguistic markers. In these cases, the overall sentiment cannot simply be judged by a majority vote among all modalities. Thus, multimodal representation learning that respects both consistency and incongruity between modalities have recently shown great promise (Yu et al., 2020; Hazarika et al., 2020) .\nIn this paper, we propose ConFEDE, a Contrastive FEature DEcomposition framework, which integrates both modality decomposition within each sample and supervised contrastive learning across samples in a single unified contrastive learning framework. Our main contributions are summarized as follows: (1) We integrate inter-sample contrastive learning and intra-sample modality decomposition into a simple unified loss function, based on a customized data sampler that allows us to sample positive/negative data pairs to perform both learning tasks. (2) We propose to decompose each modality into a similarity feature and a dissimilarity feature, and use the similarity feature of the text as an anchor to build the contrastive relation among all decomposed features. This is due to the observation that sentiment analysis is still largely centered around text and spoken language, while other modalities can provide extra information to assist with prediction. (3) Based on multimodal representation learning proposed above, we further introduce a multi-task prediction loss that depends on each decomposed modality representation and enables the model to learn from both multimodal prediction and unimodal prediction.\nWe mainly evaluated ConFEDE on CH-SIMS (Yu et al., 2020) benchmark, which contains both unimodal and overall sentiment labels for each sample. The result shows that the proposed method significantly outperforms a wide range of stateof-the-art multimodal sentiment analysis methods. To test the capability when no unimodal labels are provided, we further conduct experiments on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , which contain only an overall sentiment label for each sample, which shows that our proposed method can also achieve better per-formance than state-of-the-art methods on a number of performance metrics without unimodal labels. We provide extensive ablation studies to show the effectiveness and necessity of each design component in ConFEDE. The code is released at https://github.com/XpastaX/ConFEDE/.\n\nRelated Work\nIn this section, we discuss the related work in MSA and contrastive representation learning.\n\nMultimodal Sentiment Analysis\nPrior works on multimodal sentiment analysis mostly focus on predicting sentiments based on text and vision (Zhu et al., 2022; Ji et al., 2019; Liu et al., 2019) .However, there is growing interest in analyzing sentiment using all three modalities: text, audio, and vision (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020) . Zadeh et al. (2016) were among the first to propose a multimodal dictionary that could learn the dynamic interactions between facial gestures and spoken words to model sentiments. They later introduced a Tensor Fusion Network (TFN) to learn the intra-modality and inter-modality dynamics of three modalities in an end-to-end way (Zadeh et al., 2017) . Furthermore, they presented a Memory Fusion Network (MFN) which is composed of Long Short Term Memories (LSTMs) to learn the view-specific and cross-view interactions of three views (text, video, and audio) to improve sentiment analysis performance. Rahman et al. (2020) proposed a Multimodal Adaptation Gate (MAG) to fine-tune BERT (Devlin et al., 2019) on multimodal data to improve sentiment analysis performance. However, these prior works do not consider modality-specific information.\nTo better study the impacts that modalityspecific information can bring to MSA, Yu et al. (2020) construct a new multimodal sentiment analysis dataset CH-SIMS, which contains a unimodal label for each modality of a sample. Experiments show a great improvement in overall sentiment prediction after simply integrating unimodal predictions as subtasks in the learning objective. Hazarika et al. (2020) further decompose each modality into a modality-invariant and a modalityspecific representation, and employ squared Frobenius norm loss as the regularizer. However, they treat all modalities equally while regularizing the prediction result, which ignores the different effectiveness of modalities. In real cases, the text is usually more effective on MSA tasks compared to vision and audio. In other words, it is less \"noisy\" than the other two modalities. Also, they employ Central Moment Discrepancy loss to push the modality-invariant representations close and a Frobenius norm to push modality-specific representations to be orthogonal, while in our method, we integrate the above mechanism into a single loss function. Moreover, they regularize the decomposed features by reconstructing the original features with the generated features. We, instead, avoid using such a method and regularize the decomposed features with unimodal prediction tasks. To improve the decomposition performance, we further aggregate the supervised contrastive learning between samples into our frameworks by a custom-designed sampling method.\nA concurrent work HyCon (Mai et al., 2021) introduces a contrastive learning method for MSA, taking both inter-sample and intra-sample contrasts into consideration. However, they ignore the regularization for each decomposed feature. In contrast, in ConFEDE, within-sample feature contrasts are constructed based on a specific pattern centered around text similarity features. Also, when performing inter-sample contrastive learning, Hy-Con samples positive and negative pairs randomly based on MSA labels. In contrast, we design a data sampler that considers both the labels and similarities between modalities to retrieve positive/negative pairs. Due to these reasons, our method beats Hy-Con on most metrics on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , and is able to utilize unimodal labels to further boost performance, e.g., on CH-SIMS (Yu et al., 2020) .\n\nContrastive Representation Learning\nContrastive learning has achieved great success in representation learning by contrasting positive pairs against negative pairs (Akbari et al., 2021; Hassani and Khasahmadi, 2020; Chen et al., 2020) . Akbari et al. (2021) train a Video-Audio-Text Transformer (VATT) using multimodal contrastive learning for the alignment of video-text and videoaudio pairs, and thus achieve state-of-the-art on various computer vision tasks (e.g., audio classification and video action recognition). Hassani and Khasahmadi (2020) propose to learn node and graph level representations by contrasting encodings obtained from different structural views of graphs and achieve the state-of-the-art on various graph classification benchmarks. Chen et al. (2020) present a self-supervised framework, SimCLR, to learn visual representations through a contrastive loss between augmented views of the same image sample. Khosla et al. (2020) extend self-supervised contrastive learning to the supervised setting, i.e., contrasting samples from different classes. They also claim that the supervised setting is more stable for hyperparameters. We design a novel contrastive learning framework that utilizes the contrasts of modalities both within a sample and between samples to enhance multimodal representation in a unified contrastive loss guided by a specific pairing pattern. Furthermore, we propose a data sampler to retrieve similar samples as positive pairs, which is in contrast to the above prior work that obtains positive pairs by data augmentation.\n\nMethodology\nIn this section, we introduce the overall architecture of ConFEDE followed by a detailed description of the contrastive feature decomposition process for learning multimodal representations.\n\nModel Architecture\nThe overall architecture of ConFEDE is shown in Figure 1 . Given a sample, we first encode each modality with corresponding feature extractors. Specifically, we use the [CLS] tag of BERT to encode text (i.e., T), and two separate transformer encoders to encode vision and audio modalities (i.e., V and A), respectively. After that, we decompose each encoded modality into a similarity feature (i.e., T s /V s /A s in Figure 1 ) and a dissimilarity feature (i.e., T d / V d /A d in Figure 1 ) with different projectors. Each projector is composed of layer normalization, a linear layer with the Tanh activation, and a dropout layer. Finally, we update the six decomposed features and fuse them to train the ConFEDE model with the following multi-task learning objective function:\nL all = L pred + \u00b4uni L uni + \u00b4cl L cl ,\nwhere L pred is the multimodal prediction loss, L uni represents the unimodal prediction loss and L cl represents the contrastive loss. \u00b4cl and \u00b4uni are hyper-parameters that balance the contribution of each regularization component to the overall loss L all . We describe each loss term as follows. L pred -Multimodal Prediction Loss. We use a multilayer perceptron (MLP) with the ReLU activation function as the classifier to get the final predictive result (i.e., \u0177 in Figure 1 ). We concatenate all 6 decomposed modality features to obtain the input to the classifier,\n[T i s ; T i d ; V i s ; V i d ; A i s ; A i d ]\n, where [\u2022; \u2022] denotes the concatenation of two vectors. Denote the set of samples in a batch as B. For a given sample i \u2208 B, let its prediction from the classifier be \u0177i m , we calculate the multimodal prediction loss by mean squared error:\n\u0177i m = MLP([T i s ; V i s ; A i s ; T i d ; V i d ; A i d ]), L pred = 1 n n i=1 (y i m \u2212 \u0177i m ) 2 ,\nwhere n is the number of samples in a batch and y i m is the multimodal label. L uni -Unimodal Prediction Loss. For each sample i, we also feed the 6 decomposed features\n[T i s , V i s , A i s , T i d , V i d , A i d ]\ninto a weight-shared MLP classifier separately to get the 6 predictions denoted by the vector \u00fbi . Specifically, we compute the unimodal prediction loss by:\n\u00fbi = MLP([T i s , V i s , A i s , T i d , V i d , A i d ]), u i = [y i m , y i m , y i m , y i t , y i v , y i a ], L uni = 1 n \u2225u i \u2212 \u00fbi \u2225 2 2 ,\nwhere the vector\nu i = [y i m , y i m , y i m , y i t , y i v , y i a ]\nrepresents the ground-truth labels for unimodal prediction. In other words, each decomposed feature is regularized to perform prediction individually.\nNote that the similarity features T i s , V i s , A i s are mapped through the MLP to predict the multimodal label y i m , whereas the dissimilarity features T i d , V i d , A i d are mapped through the MLP to predict modality-specific labels y i t , y i v , y i a (if available). When modality-specific labels are not available, the dissimilarity features T i d , V i d , A i d will also be used to predict multimodal label y i m . The rationale behind this design is that we let the similarity features capture the consistent information shared across different modalities via the overall multimodal label for the sample, while the dissimilarity features can retain modality-specific information represented by unimodal labels.\nL cl -Contrastive Loss. We further regularize the learning through Contrastive Feature Decomposition in one simple joint contrastive loss that contrasts (1) similar samples against dissimilar samples; (2) similarity features against dissimilarity features within a sample. The contrastive loss is denoted as:\nL cl = 1 n n i=1 \u2113 i cl ,\nwhere \u2113 i cl is the contrastive loss of sample i, the detailed derivation of which will be given in the following subsection. \nT V A T V A T V A Ts k Vs k As k T V A T V A T V A\n\nContrastive Feature Decomposition\nWe unify intra-sample and inter-sample contrastive learning into one simple NT-Xent contrastive loss framework (Chen et al., 2020) to conduct both modality representation learning and modality decomposition simultaneously. The loss for sample i is given by\n\u2113 i cl = (a,p)\u2208P i \u2212 log exp(sim(a, p)/\u00c4 ) (a,k)\u2208N i \u222aP i exp(sim(a, k)/\u00c4 ) ,\nwhere (a, p) and (a, k) denote a pair of decomposed feature vectors either within a sample, e.g., (T i s , V i s ), (T i s , A i d ), or across different samples, e.g., (T i s , T j s ). The sets P i and N i are given by\nP i = P i intra \u222a P i inter , N i = N i intra \u222a N i inter .\nHere P i is the positive pair set that includes both intra-sample positive pairs P i intra and inter-sample positive pairs P i inter , while N i is the negative pair set that consists of both intra-sample negative pairs N i intra and inter-sample negative pairs N i inter . Note that (a, p) is a positive pair in P i , and (a, k) is a pair in P i or N i . Specifically, we use the six decomposed features (T s , V s , A s , T d , V d , A d ) to form intra-sample positive/negative pairs, as shown in Figure 2 (a) , with P i intra and N i intra given by\nP i intra ={(T i s , V i s ), (T i s , A i s )} \u222a {(T j s , V j s ), (T j s , A j s ) |j \u2208 Neighbor i \u222a Outlier i }, N i intra ={(T i s , T i d ), (T i s , V i d ), (T i s , A i d )} \u222a {(T j s , T j d ), (T j s , V j d ), (T j s , A j d ) |j \u2208 Neighbor i \u222a Outlier i },\nwhere Neighbor i and Outlier i represent the similar samples and dissimilar samples for the sample i, respectively, to enlarge the scope of the contrast, the detail of which is given in Algorithm 1 that will be explained subsequently.\nNote that instead of treating all modalities equally as in other contrastive learning schemes, here we choose the text similarity feature T i s as an anchor, such that the visual and audio similarity features V i s and A i s are pushed closer to T i s , while in the meantime, the dissimilarity features in all modalities are pushed away from T i s . This is due to the observation that multimodal sentiment analysis is still largely centered around text information. Although other modalities can provide additional information to assist with sentiment prediction, they may also introduce more noise than text. Therefore, unlike other work, we avoid using visual/audio similarity features as anchors, which may bring noise into contrastive learning and confuse model training.\nWe now describe the data sampler shown in Algorithm 1 that retrieves similar samples for a given sample based on both multimodal features and multimodal labels to perform supervised contrastive learning across samples. Specifically, the sampling procedure can be divided into two steps.\nFirst, given the dataset D that contains |D| samples, for each sample pair (i, j) in D, we calculate the cosine similarity score between them:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]),\nwhere sim(w, v) = w T v/||w|| \u2022 ||v|| denotes the cosine similarity between two vectors w and v. And T, V, and A (in Figure 1 ) are the output of BERT, vision and audio encoders, respectively.\nSecond, we retrieve candidate similar/dissimilar sample sets for each sample. For each sample i, we sort samples that have the same multimodal label y i m according to the similarity scores in ascending order as a candidate similar sample set S i 0 . In contrast, we sort samples that have labels other than y i m as a candidate dissimilar sample set S i 1 . Two similar samples with high cosine similarity scores from S i 0 are randomly selected to form inter-sample positive pairs with sample i, which is denoted as Neighbor i . Four dissimilar samples from S i 1 are selected to form inter-sample negative pairs. We denote them as Outlier i in which two samples Outlier i 1 have low cosine similarity scores and the other two samples Outlier i 2 have high cosine similarity scores.\nUsually, we tend to select the samples in Neighbor i and Outlier i 1 to form positive and negative pairs with sample i, respectively. However, samples in Outlier i 2 have different labels but similar semantic information to sample i, making them hard to distinguish from sample i. Therefore, we additionally add these samples to Outlier i to specifically handle this issue by contrastive learning.\nBased on the samples retrieved by Algorithm 1 and the pairing strategy shown in Figure 2 (b), the inter-sample positive/negative pairs for sample i are given by:\nP i inter ={(T i s , T j s ), (V i s , V j s ), (A i s , A j s ) |j \u2208 Neighbor i } , N i inter ={(T i s , T k s ), (V i s , V k s ), (A i s , A k s ) |k \u2208 Outlier i }.\nNotably, our data sampler enables contrastive learning across samples through decomposed modality features without data augmentation. This contrasts original contrastive learning in image classification, which obtains positive pairs by augmentation applied to images. Moreover, we only use similarity features to obtain inter-sample pairing since the similarity features of similar samples in the same class should be close while the similarity features of samples in different classes should be far apart.\n\nExperiments\nWe mainly evaluate ConFEDE on CH-SIMS (Yu et al., 2020) , since it has unimodal labels, which can best meet the design of ConFEDE. To justify the effectiveness of ConFEDE when unimodal labels are unavailable, we further test ConFEDE on the MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018b) , which are two English MSA datasets. However, they can not best test the performance of ConFEDE.\nWe compare our methods with the state-of-theart baselines in Table 1 and 2: LF-DNN (Yu et al., 2020) , MFN (Zadeh et al., 2018a) , LMF (Liu et al., 2018) , TFN (Zadeh et al., 2017) , MulT (Tsai et al., 2019) , MISA (Hazarika et al., 2020) , MAG-BERT (Rahman et al., 2020) , HyCon (Mai et al., 2021) and Self-MM (Yu et al., 2021) . For a fair comparison, the methods which only report the results of a single run and have no valid official code released Algorithm 1: Data Sampling Algorithm Input: Dataset D with the corresponding features T , V , A and multimodal labels ym. Output: Neighbor i , Outlier i for every i \u2208 D Define: sim(w, v) = w T v/||w|| \u2022 ||v|| for every (i, j) \u2208 D do\nCompute the cosine similarity score:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]), end Define: argsort(X) = indices sort X ascendingly Let |D| = length of D, z = |D| 4 . for every sample i \u2208 D do\nRetrieve the similar sample set S i 0 :\nS i 0 = argsort({C i,j |j : y j m = y i m });\nRetrieve the dissimilar sample set S i 1 :\nS i 1 = argsort({C i,j |j : y j m \u0338 = y i m }),\nRandomly select two samples from the last z elements of S i 0 as Neighbor i ; Randomly select two samples from the first z elements of S i 1 as Outlier i 1 ; Randomly select two samples from the last z elements of S i 1 as Outlier i 2 ;\nOutlier i = Outlier i 1 \u222a Outlier i 2 .\nend for reproduction are not selected. A detailed introduction can be found in the supplementary material. The detailed experimental settings are introduced in Appendix C.\n\nEvaluation Metrics\nFollowing the previous works (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020; Hazarika et al., 2020) , we report our results in (multi-class) classification and regression with the average of 5 runs of different seeds. For classification, we report the multiclass accuracy and weighted F1 score. We calculate the accuracy of 2-class prediction (Acc-2), 3-class prediction (Acc-3), and 5-class (Acc-5) prediction for CH-SIMS and the accuracy of 2-class prediction and 7-class prediction (Acc-7) for MOSI and MOSEI. Besides, Acc-2 and F1-score of MOSI and MOSEI have two forms: negative/non-negative (non-exclude zero) (Zadeh et al., 2017; Yu et al., 2021) and negative/positive (exclude zero) (Tsai et al., 2019; Yu et al., 2021) \n\nResults\nThe performance comparison of all methods on CH-SIMS, MOSI, and MOSEI is summarized in Table 1 and Table 2 . The scores of the proposed method and its variations are the averages of 5 runs. The performances of all other baselines, except for MAG-BERT, have been sourced from published papers or official repositories 1 .\nOn the CH-SIMS dataset, our proposed method outperforms all baselines on all metrics. We achieve superior performance compared to the best baseline model, Self-MM, with an improvement of 2.19% on acc-2 and 1.64% on F1 scores. Additionally, the proposed model demonstrates exceptional ability in multi-class classification, outperforming the best baseline by 4.68% on acc-3 and 4.77% on acc-5.\nAs seen in the results, our proposed method, ConFEDE, consistently outperforms all other baselines on the CH-SIMS dataset. The superior classification performance demonstrates that our designed learning method is more effective than the compared methods. Our method, ConFEDE, effectively distinguishes similarity and dissimilarity information between modalities, providing clearer modality features to the downstream classifier for improved prediction. Additionally, the significant improvement in MAE and Corr further highlights the ability of our model to better understand the CH-SIMS dataset than the other baselines.\nTo further evaluate the effectiveness of our proposed method, ConFEDE, we trained our models on the MOSI and MOSEI datasets without uni-modal labels. Instead, we used their multimodal labels for compatibility. The results are presented in Table 2 . On the MOSI dataset, our method outperforms all other baselines in both the negative/nonnegative (NN) setting and negative/positive (NP) setting for acc-2 and F1 metrics. Additionally, our acc-7 and MAE metrics surpass most of the baselines. For the MOSEI dataset, our ConFEDE method outperforms all baselines in all metrics except for the NN Acc-2 and F1 score. Furthermore, our MAE is significantly lower than all baselines, reaching 0.522.\nIt is worth noting that our models perform much better in NP acc-2 than NN acc-2 for MOSEI, as shown in Table 2 . This is because the NN acc-2 setting is generally more challenging than the NP acc-2 setting, as it places more pressure on a model to classify data samples with a regression label of 0. Specifically, if there are two samples with a regression label of 0, when predicted by a regression model, the results might be -0.01 and 0.01. As the value range of \"Neutral\" is [-0.5,0.5) in MOSI and (-0.1,0.1] in SIMS, these two samples should be classified as \"Neutral\". However, in NN settings, they will be classified into two different classes, resulting in a worse acc-2. In contrast, with the NP setting, all \"Neutral\" samples are abandoned, resulting in a better acc-2.\nIn contrast, our method shows better performance in both NN and NP settings on MOSI when compared to other models. The Acc-7, MAE and Corr are also better or comparable to most baselines.\n\nAblation Study and Analysis\nTo evaluate the impact of our proposed structures, we conducted an ablation study on our proposed method by removing inter-sample contrastive learning and intra-sample contrastive learning. The results are shown in Table 1 . \"Plain\" represents the model without contrastive learning method, \"Inter\" represents the model with inter-sample contrastive learning only, and \"Intra\" represents the model with intra-sample contrastive learning and unimodal prediction as a sub-task.\nThe experiment shows that all three models perform worse than the original model. Among the three models, the plain setting has the lowest performance. Both intra-sample contrastive learning and inter-sample contrastive learning provide positive impacts on performance. Compared with Plain, by using text feature as the anchor, Intra filters out noise (useless information for sentiment analysis) in the vision and audio modality, leading to better prediction. This is also the reason why it reaches better acc-2 accuracy than both the other models. Since the acc-2 metric in CH-SIMS follows the negative/non-negative setting, a feature with lower noise helps the classifier make a more precise prediction value, making it easier to classify the 0-labeled samples. This also explains why we achieve better NN Acc-2 performance than all baselines on MOSI.\nFor the inter-sample contrastive learning method, by learning the common and different information between samples, \"Inter\" performs better on multiclass classification. The result of Inter on CH-SIMS shows great improvements on both acc-5 and MAE compared with the other two models, which proves that Acc-5 and regression performance benefits more from \"Inter\". This can also explain why we have a lower NN Acc-2 performance on MO-SEI. Since MOSEI is much larger than MOSI and CH-SIMS, it introduces more noise in each modality, the contrastive feature decomposition learning needs more epochs and a smaller learning rate to separate the useful information from noise. Meanwhile, inter-sample contrastive learning is more efficient on MOSEI. With the larger amount of samples, it is much easier for the sampler to find the most similar and dissimilar samples with the given sample, from which the model can understand the difference between samples better. Thus, ConFEDE can reach higher Acc-7 and regression performance than all other baselines on MOSEI.\nTo further evaluate the effectiveness of the contrastive feature decomposition method, we conducted an ablation study on Intra using the CH-SIMS dataset. As presented in Table 3 , we created three variations of Intra: 1) Intra with only multimodal labels for unimodal prediction (M-label); 2) Intra without the unimodal prediction component (-uni); 3) Intra without the similarity-dissimilarity learning method (-cl); and 4) Intra that uses all similarity features as anchors (+full), which utilizes T s , V s , and A s as anchors instead of T s only.\nThe results in the table demonstrate that all variations resulted in a decrease in performance compared to the original Intra in classification matrices. Both the intra-sample contrastive learning and the unimodal prediction task can regularize the learned representation, resulting in clearer information that aids the classifier in understanding the sample better. However, the \"+full\" setting introduces more noise by also using V s and A s as anchors, which confuses the model and diminishes the denoising ability of the contrastive feature decomposition learning.\n\nConclusion\nIn this paper, we propose a novel method for multimodal sentiment analysis (MSA) called ConFEDE. The ConFEDE framework is based on contrastive feature decomposition, which utilizes a unified contrastive training loss to capture the consistency and difference across modalities and samples. This approach allows for the simultaneous learning of modality decomposition within each sample and su-pervised contrastive learning across samples. Our proposed method is mainly evaluated on CH-SIMS. The result shows that the proposed method significantly outperforms many state-of-the-art multimodal sentiment analysis methods. We further conduct an extensive experiment on MOSI and MOSEI to test the capability of ConFEDE when no unimodal label is available, where our method achieves better performance than state-of-the-art methods on a number of performance metrics.\n"}
{"question": "Among the MTC baselines we used to compare with GetMTL,which model was put forward the earliest.", "evidence": "  We compare the proposed GetMTL with a series of MTC baselines, including Single-Task Learning (STL): learning each task independently.\nUniform Scaling: learning tasks simultaneously with uniform task weights.\nUncertainty: using the uncertainty weighting method (Kendall et al., 2018) .\nGradNorm: learning tasks simultaneously with gradient normalization method (Chen et al., 2018) .\nTchebycheffAdv: using adversarial Tchebycheff procedure (Mao et al., 2020) .\nMGDA: using gradient-based multi-objective optimization method (Sener and Koltun, 2018) .\nBanditMTL: learning tasks simultaneously with multi-armed bandit method (Mao et al., 2021) .\nMetaWeighting: using adaptive task weighting method (Mao et al., 2022) .\n ", "options": ["A. Learning (STL)", "B. MetaWeighting", "C. GradNorm", "D. BanditMTL"], "answer": "C", "content": "\nIntroduction\nMulti-task Learning (MTL), which aims to learn a single model that can tackle multiple correlated but different tasks simultaneously, makes multiple tasks benefit from each other and obtain superior performance over learning each task independently (Caruana, 1997; Ruder, 2017; Liu et al., 2015; Mao et al., 2020) . By discovering shared information/structure across the tasks, it has gained attention in many areas of research and industrial communities, such as computer vision (Misra et al., 2016; Gao et al., 2019; Yogamani et al., 2019; Sun et al., 2020 ) and text classification (Liu et al., 2017; Xiao et al., 2018; Mao et al., 2021 Mao et al., , 2022)) .\nHowever, it is observed in multi-task text classification (MTC) scenarios that some tasks could conflict with each other, which may be reflected via conflicting gradients or dominating gradients (Yu et al., 2020; Vandenhende et al., 2022) , leading to the degraded performance of MTL due to poor training. How to make a proper trade-off among jointing different tasks in MTC is a difficult problem. Recently, several methods have been proposed to mitigate gradient conflicts issue via both loss balance (linear weighted scalarization) such as homoscedastic uncertainty (Kendall et al., 2018) and task variance regularization (Mao et al., 2021) , and gradient balance like Pareto optimality (Sener and Koltun, 2018; Mao et al., 2020) . Existing methods devote to finding an arbitrary Pareto optimality solution in the Pareto set, which achieve a single arbitrary trade-off among all tasks. However, they can only satisfy the improved performance on part of tasks, not all tasks simultaneously. This means that these methods can not converge to a minimum average loss of all objectives.\nTo illustrate our idea, we give a two-task learning example shown in Figure 1 . As shown in Figure (1a) , it is observed that Pareto optimality-based methods can generate a set of Pareto solutions for a given two-task learning problem. However, some of Pareto solutions can increase the task 1 error while decreasing task 2 error, leading to unsatisfactory overall performance for MTL model. This im-plies that not all Pareto solutions always satisfy the goal of mitigating the tasks conflicts in MTL, and thus failing to achieve a better trade-off between tasks. Therefore, it is necessary to find a specific trade-off between tasks that is beyond what only using Pareto optimality can achieve.\nTo address this issue, inspired by multi-objective optimization (Sener and Koltun, 2018) , we argue that a more efficient way to mitigate task conflicts is to find a gradient trade-off between tasks in the neighborhood of the average loss rather than exhaustively searching for a proper solution from the set of Pareto solutions. As shown in Figure 1b , the Pareto solutions nearby the average loss can achieve a better trade-off between task 1 and task 2, leading to better performance on both tasks at the same time. Based on it, in this paper, we propose a novel gradient trade-off multi-task learning approach, named GetMTL, to mitigate task conflicts in multi-task text classification. Specifically, the gradients of each task are utilized to derive an update vector that can minimize the conflicts among task gradients in the neighborhood of the average gradient, so as to achieve a better trade-off performance among joint training tasks. In summary, the main contributions of our work are as follows:\n\u2022 A novel multi-task learning approach based on gradient trade-off between different tasks (GetMTL) is proposed to deal with task conflict in multi-task text classification problems, so as to improve the performance of all tasks simultaneously. \u2022 We give in-depth theoretical proofs and experimental analyses on establishing converge guarantees of our GetMTL. \u2022 We extensively verify the effectiveness of our GetMTL on two real-world text classification datasets, and the results show that our GetMTL performs competitively with a variety of state-of-the-art methods under a different number of task sets.\n\nRelated Works\nMulti-task Learning methods jointly minimize all task losses based on either loss balance methods (Kendall et al., 2018; Chen et al., 2018; Mao et al., 2021 Mao et al., , 2022) ) or gradient balance methods (Sener and Koltun, 2018; Mao et al., 2020) .\nThe loss balance methods adaptively adjust the tasks weights during training based on various heuristic approaches, such as task uncertainty quan-tification (Kendall et al., 2018) , gradient normalization (Chen et al., 2018) , task difficulty prioritization (Guo et al., 2018) , dynamic weight average (Liu et al., 2019) , random loss weighting (Lin et al., 2021) , task variance regularization (Mao et al., 2021) , and meta learning-based approach (Mao et al., 2022) . These methods are mostly heuristic and can have unstable performance while ignoring the task conflicts among all tasks, leading to the bad generalization performance of MTL models.\nRecently, some gradient balance based methods have been proposed to mitigate task conflicts for improving task performance. For example, D\u00e9sid\u00e9ri (2012) leverages multiple-gradient descent algorithm (MGDA) to optimize multiple objectives. Due to the guarantee of convergence to Pareto stationary point, this is an appealing approach. Sener and Koltun (2018) cast the multi-objective problem as a multi-task problem and devote to finding an arbitrary Pareto optimal solution. Mao et al. (2020) propose a novel MTL method based Tchebycheff procedure for achieving Pareto optimal without any convex assumption. However, these methods only consider achieving an arbitrary Pareto optimal solution while it is not the main objective. Unlike these methods, we propose an MTL approach based on multi-objective optimization and seek to find a set of solutions that are Pareto optimality and nearby the main MTC objective L 0 .\n\nPreliminaries\nConsider a multi-task learning problem with T 1 tasks over an input space X and a collection of task spaces {Y t } t\u2208 [T ] , where each task contains a set of i.i.d. training samples\nD t = {x i , y t i } i\u2208[nt] ,\nT is the number of tasks, and n t is the number of training samples of task t. The goal of MTL is to find parameters {\u03b8 sh , \u03b8 1 , ..., \u03b8 T } of a model F that can achieve high average performance across all training tasks over X , defined as F(X , \u03b8 sh , \u2022 \u2022 \u2022 , \u03b8 t ) : X \u2192 Y, where \u03b8 sh denotes the parameters shared between tasks and \u03b8 t denotes the task-specific parameters of task t. In particular, we further consider a parametric taskspecific map as f t (\u2022, \u03b8 sh , \u03b8 t ) : X \u2192 Y t . We also consider task-specific loss functions t (\u2022, \u2022) : Y t \u00d7 Y t \u2192 R + . We also denote the multi-task loss as L(\u03b8) = T i i (\u03b8), and the gradients of each task as g i = \u2207 i (\u03b8) for the particular \u03b8. In this paper, we choose the average loss as main objective of MTC problem, defined as L 0 (\u03b8) = 1 T T i i (\u03b8).\n\nMTL as Multi-objective Optimization\nMTL can be formulated as a specific case of multiple-objective optimization (MOO), which optimizes a set of potentially conflicting objectives (Sener and Koltun, 2018; Mao et al., 2020) . Given objective functions of T tasks, 1 , . . . , T , we formulate the optimization objective of MTL as the vectors of objective values :\nmin \u03b8 sh ,\u03b8 1 ,...,\u03b8 T (\u03b8 sh , \u03b8 1 ), . . . , (\u03b8 sh , \u03b8 T ) (1)\nSince there is no natural linear ordering on vectors, it is not possible to compare solutions and thus no single solution can optimize all objectives simultaneously. In other words, there is no clear optimal value. Alternatively, we can achieve Pareto optimality to obtain different optimal trade-offs among all objectives to solve MOO problem.\nDefinition 1 (Pareto dominance). Given two points {\u03b8, \u03b8} in \u2126, a point \u03b8 Pareto dominates \u03b8 (\u03b8 \u03b8) for MTL if two conditions are satisfied:\n(i) No one strictly prefers \u03b8 to \u03b8, that is, \u2200i \u2208 {1, . . . , T }, i (\u03b8 sh , \u03b8 i ) \u2264 i (\u03b8 sh , \u03b8 i ).\n(ii) At least one point strictly prefers \u03b8 to \u03b8, that is, \u2203j \u2208 {1, ..., T }, j (\u03b8 sh , \u03b8 j ) < j (\u03b8 sh , \u03b8 j ).\nDefinition 2 (Pareto optimality). \u03b8 * is a Pareto optimal point and (\u03b8 * ) is a Pareto optimal objective vector if it does not exist \u03b8 \u2208 \u2126 such that \u03b8 \u03b8 * . That is, a solution that is not dominated by any other is called Pareto optimal.\nThe set of all Pareto optimal solutions is called the Pareto set, and the image of Pareto set in the loss space is called Pareto front (Lin et al., 2019) . In this paper, we focus on gradient-based multiobjective optimization to achieve an appropriate Pareto trade-off among all tasks, which can approximate the Pareto front that minimizes the average loss.\n\nGradient-based Multi-Objective Optimization\nGradient-based MOO (Sener and Koltun, 2018) aims to find a direction d that we can iteratively find the next solution \u03b8 (t+1) that dominates the previous one \u03b8 (t) ( (\u03b8 (t+1) ) \u2264 (\u03b8 (t) )) by moving against d with step size \u03b7, i.e. \u03b8 (t+1) = \u03b8 (t) \u2212 \u03b7d. D\u00e9sid\u00e9ri (2012) ; Sener and Koltun (2018) propose to use multiple gradient descent algorithm (MGDA) that converges to a local Pareto optimal by iteratively using the descent direction d, which can be obtained as follows:\nd * = arg min d\u2208R m ,\u03b1\u2208R \u03b1 + 1 2 d 2 s.t. \u2207 i (\u03b8 (t) ) T d \u2264 \u03b1, i = 1, ..., T.\n(\nEQUATION\nwhere d * is the direction that can improve all tasks. Essentially, gradient-based MOO methods minimize the loss by combining gradients with adaptive weights, and obtaining an arbitrary Pareto optimality solution, ignoring the true objective (the average loss) (Liu et al., 2021) . In this paper, we generalize this method and propose a novel gradient-based approach to achieve a gradient trade-off among tasks for mitigating task conflicts, as well as constrain the solution that can minimize the average loss (L 0 (\u03b8)).\n\nGradient Trade-offs for Multi-task Text Classification\nFollowing most MTL methods, as shown in Figure 2 , we employ the hard parameter sharing MTL architecture, which includes f sh parameterized by heavy-weight task-shared parameters \u03b8 sh and f t parameterized by light-weight task-specific parameters \u03b8 t . All tasks take the same shared intermediate feature z = f sh (x; \u03b8 sh ) as input, and the t-th taskspecific network outputs the prediction as f t (z; \u03b8 t ).\nSince task-shared parameters \u03b8 sh are shared by all tasks, the different tasks may conflict with each other, leading to the degraded performance of MTL model. In this paper, we hypothesize that one of the main reasons for task conflicts arises from gradients from different tasks competing with each other in a way that is detrimental to making progress.\nWe propose a novel gradient-based MOO optimization to find a gradient trade-off among tasks in the neighborhood of the average loss, so as to mitigate task conflicts. Note that, we omit the subscript sh of task-shared parameters \u03b8 sh for the ease of notation.\n\nGetMTL\nGiven a task i, we define its gradient as g i = \u2207 i (\u03b8) via back-propagation from the raw loss i , and g i represents the optimal update direction for task i. However, due to the inconsistency of the MOO method and our GetMTL on three gradients (g 1 , g 2 and g 3 ) in R 3 , where g i denotes the gradient (black) of i-th task, g 0 is the average gradient, and blue arrows denote the projections of update direction to each task gradient.\noptimal update direction of task-shared parameters for each task, different task gradients may conflict with each other, leading to the training of networks being stuck in the over-training of some tasks and the under-training of other tasks. Intuitively, it is desirable to find a direction that can minimize the task conflicts among different tasks as well as achieve Pareto optimality to improve the performance of MTL model. We first achieve an arbitrary Pareto optimal via finding a descent direction d des by searching for a minimum-norm point in the Convex Hull CH of gradients, defined by,\nEQUATION\ns.t. S T = \u03b2 \u2208 R T + T j=1 \u03b2 j = 1 (4)\nwhere G \u2208 R T \u00d7m = {g 1 , ..., g T } is the matrix of task gradient, S T is the T -dimensional regular simplex. We use the multiple gradient descent algorithm (MGDA) (Sener and Koltun, 2018) to obtain an arbitrary Pareto optimal by iteratively using the descent direction, defined by,\nd des = arg min d\u2208CH d 2 2\n(5)\nIn addition, the d des can be reformulated as a linear combination of all task gradients, defined by,\nd des = T i=1 \u03b2 i g i (6)\nwhere g i = \u2207 i (\u03b8) is the i-th task gradient. It implies that, when converges to an arbitrary Pareto optimal, the optimal gradient value of each task via back-propagation is \u03b2 i g i , defined as\ng \u03b2 i = \u03b2 i g i .\nHowever, moving against d des does not guarantee that the solution meets the requirements of multi-task text classification task (MTC), that is, to alleviate the gradient conflict among tasks in MTC, so as to improve the performance of all tasks. To address this issue, we seek a direction that enables us to move from a solution \u03b8 (t) to \u03b8 (t+1) such that both \u03b8 (t+1) dominates \u03b8 (t) (L(\u03b8 (t+1) ) \u2264 L(\u03b8 (t) )) and alleviate the gradient conflict among all tasks. Based on it, as shown in Figure 2 (b), we propose to search for an update direction d in the Convex Hull CH \u03b2 of back-propagation gradients such that it can improve any worst objective and converge to an optimum of MTC objective L 0 (\u03b8). We first find the worst task gradient with respect to the update direction d, that is, it has a maximum angle with d, which can be formulated via the following optimization problem,\nEQUATION\nwhere g \u03b2 i is the i-task gradient after optimizing by MGDA algorithm.\nTo improve the worst gradient of any task and achieve a trade-off between all task gradients in a neighborhood of the average gradient (defined as g 0 = 1 T T i=1 g i ), we formulate this gradient trade-off optimization problem via the following Maximin Optimization Problem (dual problem).\nProblem 1.\nmax d\u2208R m min i\u2208[T ] g \u03b2 i , d s.t. d \u2212 g 0 \u2264 \u03b5g T 0 d, \u2212 g T 0 d \u2264 0 (8)\nwhere g \u03b2 i = \u03b2 i g i is the back-propagation gradient value of i-th task via solving Eq. ( 5), \u03b5 \u2208 (0, 1] is a hyper-parameter that controls the stability of MTC model.\n\nSolving Maximin Problem\nSince the optimal direction d can also be defined in the convex hull CH \u03b2 of g \u03b2 i , we can get\nEQUATION\nwhere\nG \u03b2 \u2208 R T \u00d7m = {g \u03b2 1 , ..., g \u03b2 T } is task gradi- ent matrix, W T = { w \u2208 R T + T j=1 w j = 1} is the T -dimensional\nprobability simplex, and w = (w 1 , ..., w T ). Therefore, we can get min i g \u03b2 i , d = min w\u2208W T i w i g \u03b2 i , d and Problem 1 can be transformed into the following form.\nAlgorithm 1: GetMTL Algorithm.\nInput: The number of task T , loss functions { i } T i=1 , network parameters \u03b8 (t) at t step, the pre-specified hyper-parameter \u03b5 \u2208 (0, 1] and step size \u00b5 \u2208 R + . 1: Task Gradients:\ng i = \u2207 i (\u03b8 (t) ), i \u2208 [T ] 2: Main Objective: g 0 = T i=1 g i 3:\nObtain {\u03b2 1 , ...\u03b2 T } by solving Eq.( 5). 4: Compute g w = i w i g \u03b2 i , where g \u03b2 i = \u03b2 i g i 5: Obtain {w 1 , ..., w T } by solving Eq.( 14) 6: Find direction d * by using Eq.( 13)\nEQUATION\nwhere g w = T i=1 w i g \u03b2 i is the convex combination in CH \u03b2 . For a given vector \u03bb \u2208 R + with non-negative components, the corresponding Lagrangian associated with the Eq.( 10) is defined as\nEQUATION\n11) Since the objective for d is concave with linear constraints and w \u2208 W T is a compact set 2 , according to the Sion's minimax theorem (Kindler, 2005) , we can switch the max and min without changing the solution of Problem 2. Formally,\nmin \u03bb,w\u2208W T max d\u2208R m g T w d \u2212 \u03bb d \u2212 g 0 2 /2 + \u03bb\u03b5 2 (g T 0 d) 2 /2 (12)\nWe get the optimal solution of primal problem (Problem 1) by solving the dual problem of Eq.( 12) (See the Appendix A for a detailed derivation procedure). Then we have\nd * = g w + \u03bb * g 0 (1 \u2212 \u03b5 2 g 2 0 )\u03bb * , where \u03bb * = g w \u03b5 g 0 2 (13)\nwhere \u03bb * is the optimal Lagrange multiplier, d * is the optimal update direction of MTC model. We can reformulate the problem of Eq.( 12) as following optimization problem w.r.t. w.\nEQUATION\n2 Compact set: a set that is bounded and closed. where g w is defined as\ng w = T i=1 w i g \u03b2 i .\nThe detailed derivation is provided in Appendix A. Algorithm 1 shows all the steps of GetMTL algorithm in each iteration.\n\nTheoretical Analysis\nIn this section, we analyze the equivalence of solutions to dual problem and then give a theoretical analysis about convergence of GetMTL algorithm. We define the Lagrangian of problem in Eq.( 10), Theorem 4.2 (Convergence of GetMTL). Assume loss functions i are convex and differential, and \u2207 i (\u03b8 (t) ) is L-lipschitz continuous with L > 0. The update rule is \u03b8 (t+1) = \u03b8 (t) \u2212 \u00b5 (t) d, where d is defined in Eq.( 13) and\nL(d, \u03bb, w) = g T w d \u2212 \u03bb 2 ( d \u2212 g 0 2 \u2212 \u03b5 2 (g T 0 d) 2 ) Theorem 4.1 (Equivalence of\n\u00b5 (t) = min i\u2208[k] d\u2212g 0 c\u2022L\u2022d 2 . All the loss functions 1 (\u03b8 (t) ) \u2022 \u2022 \u2022 T (\u03b8 (t) ) converges to ( 1 (\u03b8 * ) \u2022 \u2022 \u2022 T (\u03b8 * )).\nProof. The proof is provided in Appendix C.\n\nExperimental Datasets\nWe conduct experiments on two MTC benchmarks to evaluate the proposed GetMTL. 1) Amazon Review dataset (Blitzer et al., 2007) contains product reviews from 14 domains (See Details in Appendix D), including apparel, video, books, electronics, DVDs and so on. Each domain gives rise to a binary classification task and we follow Mao et al. (2021) to treat 14 domains in the dataset as distinct tasks, creating a dataset with 14 tasks, with 22180 training instances and 5600 test instances in total. 2) Topic classification dataset, 20 Newsgroup 3 , consists of approximately 20,000 newsgroup documents, partitioned evenly across 20 different newsgroups. We follow Mao et al. (2021) to select 16 newsgroups from 20 Newsgroup dataset shown in Table 1 and then divide them into four groups. Each group gives rise to a 4-way classification task, creating a dataset with four 4-way classification tasks, which is a more challenging dataset than amazon review dataset.\n\nExperimental Implementation\nWe follow the standard MTC setting and adopt the same network architectures with the most recent baselines for fair comparisons (Mao et al., 2021) . We adopt the hard parameter sharing MTL framework shown in Figure 2 , where task-shared network is a TextCNN with kernel size of 3,5,7 and taskspecific network is a fully connected layer with a softmax function. Adam is utilized as the optimizer to train the model over 3000 epochs with a learning rate of 1e-3 for both sentiment analysis and topic classification. We set the batch size to 256. \n\nComparison Models\nWe compare the proposed GetMTL with a series of MTC baselines, including Single-Task Learning (STL): learning each task independently.\nUniform Scaling: learning tasks simultaneously with uniform task weights.\nUncertainty: using the uncertainty weighting method (Kendall et al., 2018) .\nGradNorm: learning tasks simultaneously with gradient normalization method (Chen et al., 2018) .\nTchebycheffAdv: using adversarial Tchebycheff procedure (Mao et al., 2020) .\nMGDA: using gradient-based multi-objective optimization method (Sener and Koltun, 2018) .\nBanditMTL: learning tasks simultaneously with multi-armed bandit method (Mao et al., 2021) .\nMetaWeighting: using adaptive task weighting method (Mao et al., 2022) .\n\nMain Results\nThe main comparison results of GetMTL on two benchmark datasets are shown in Figure 3 and 4 . It is clear that (See detailed numerical comparison results in Appendix D), our proposed GetMTL model performs consistently better than the all comparison methods on all tasks of both amazon review and topic classification datasets, and its average performance is superior to that of all baselines. This verifies the effectiveness of our GetMTL method in MTC problem. More concretely, in comparison with the gradient-based MOO optimization model (MGDA), our GetMTL achieves significant improvement across all datasets. This indicates that achieving a gradient trade-off nearby average loss to mitigate task conflicts can better improve all task performance and generalization ability of MTC model. \n\nEmpirical Analysis on Convergence\nIn Section 4.3, we theoretically prove the convergence of our proposed GetMTL. Furthermore, we conduct extensive experiments about the convergence to better demonstrate the advantages of GetMTL shown in Figure 5 . It is clear that the learning curve of GetMTL is constantly decreasing as the number of iterations increases and converges to the lowest loss value compared with other baselines. It indicates that GetMTL can guarantee the convergence of the objective value and obtain better performance of all learning tasks.\nIn addition, we also conduct extensive experiments to investigate how GetMTL mitigates task conflict during training. We plot the task variance (variance between the task-specific losses) of all baselines on both amazon review and topic classification datasets shown in Figure 6 . It can be observed that all MTL baselines have lower task variance than STL method, which illustrates that MTL methods can indeed boost the learning of all tasks compared with STL method. Moreover, GetMTL has the lowest task variance and smoother evolution during training than other MTL baselines. This implies that our proposed GetMTL indeed mitigates task conflicts compared with other MTL methods.\n\nThe Evolution of Task Weight w\nIn this section, we visualize the task weights of our GetMTL and two weight adaptive MTL methods (MGDA and BanditMTL) throughout the training process using the topic classification dataset shown in Figure 7 . It can be observed from these four figures that the weight adaption process of our GetMTL is different from that of MGDA and Ban-ditMTL. GetMTL can automatically learn the task weights without pre-defined heuristic constraints. The weights adaption process of GetMTL is more stable and the search space is more compact compared with other MTL baselines.\n\nImpact of the Values of \u03b5\nTo investigate the impact of using different values of \u03b5 on the performance of our GetMTL, we conduct experiments on two datasets, and the results are shown in Figure 8 . Noting that model with \u03b5 = 0.0075 and \u03b5 = 0.025 perform overall better than other values on these two datasets, respectively. The model with larger value of \u03b5 performs unsatisfactorily overall all tasks on two datasets, one possible reason is that larger \u03b5 makes d pull far away from the average loss g 0 (see the conditions in Eq. ( 9)). That is, Pareto optimality found by GetMTL is getting further and further away from MTC objective L 0 , which can be quite detrimental to some tasks' performance, leading to degraded average performance.\n\nConclusion\nIn this paper, we propose a novel gradient tradeoff multi-task learning approach to mitigate the task conflict problem, which can achieve a specific trade-off among different tasks nearby the main objective of multi-task text classification problem. Moreover, we present a series of theoretical proofs to illustrate the effectiveness and superiority of our GetMTL. Experimental results on two benchmark datasets show that our GetMTL achieves state-ofthe-art performance in Multi-task Text Classification problem.\n"}
{"question": "What is one of the advantages of DIFFUSION-NER over generation-based methods in named entity recognition?", "evidence": "  Third, different from the autoregressive manner in generation-based methods, DIFFUSION-NER can generate all entities in parallel within several denoising timesteps.  In addition, the shared encoder across timesteps can further speed up inference. We will further analyze these advantages of DIFFUSIONNER in \u00a7 6.2.  ", "options": ["A. It has a shared encoder across timesteps.", "B. It uses a predefined number of noisy spans in training.", "C. It follows an autoregressive manner for entity generation.", "D. It suffers from exposure bias during evaluation."], "answer": "A", "content": "\nIntroduction\nNamed Entity Recognition (NER) is a basic task of information extraction (Tjong Kim Sang and De Meulder, 2003) , which aims to locate entity mentions and label specific entity types such as person, location, and organization. It is fundamental to many structured information extraction tasks, such as relation extraction (Li and Ji, 2014; Miwa and Bansal, 2016) and event extraction (McClosky et al., 2011; Wadden et al., 2019) .\nMost traditional methods (Chiu and Nichols, 2016) formulate the NER task into a sequence labeling task by assigning a single label to each token. To accommodate the nested structure between entities, some methods (Ju et al., 2018; Wang et al., + + \u00b2 \u00bb N (0; 1) + \u00b2 \u00bb N (0; 1)\nFigure 1 : Boundary diffusion in named entity recognition. The fixed forward diffusion process adds Gaussian noise to the entity boundaries at each timestep, and the noisy boundaries recover their original state by denoising with the learnable reverse diffusion process. For inference, the reverse diffusion process generates entity boundaries and performs entity typing based on the noisy spans sampled from the Gaussian distribution. 2020) further devise cascaded or stacked tagging strategies. Another class of methods treat NER as a classification task on text spans (Sohrab and Miwa, 2018; Eberts and Ulges, 2020) , and assign labels to word pairs (Yu et al., 2020; Li et al., 2022a) or potential spans (Lin et al., 2019; Shen et al., 2021a) . In contrast to the above works, some pioneer works (Paolini et al., 2021; Yan et al., 2021b; Lu et al., 2022) propose generative NER methods that formulate NER as a sequence generation task by translating structured entities into a linearized text sequence. However, due to the autoregressive manner, the generation-based methods suffer from inefficient decoding. In addition, the discrepancy between training and evaluation leads to exposure bias that impairs the model performance.\nWe move to another powerful generative model for NER, namely the diffusion model. As a class of deep latent generative models, diffusion models have achieved impressive results on image, audio and text generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021; Li et al., 2022b; Gong et al., 2022) . The core idea of diffusion models is to systematically perturb the data through a forward diffusion process, and then recover the data by learning a reverse diffusion process.\nInspired by this, we present DIFFUSIONNER, a new generative framework for named entity recognition, which formulates NER as a denoising diffusion process (Sohl-Dickstein et al., 2015; Ho et al., 2020) on entity boundaries and generates entities from noisy spans. As shown in Figure 1 , during training, we add Gaussian noise to the entity boundaries step by step in the forward diffusion process, and the noisy spans are progressively denoised by a reverse diffusion process to recover the original entity boundaries. The forward process is fixed and determined by the variance schedule of the Gaussian Markov chains, while the reverse process requires learning a denoising network that progressively refines the entity boundaries. For inference, we first sample noisy spans from a prior Gaussian distribution and then generate entity boundaries using the learned reverse diffusion process.\nEmpowered by the diffusion model, DIFFUSION-NER presents three advantages. First, the iterative denoising process of the diffusion model gives DIFFUSIONNER the ability to progressively refine the entity boundaries, thus improve performance. Second, independent of the predefined number of noisy spans in the training stage, DIF-FUSIONNER can sample a different number of noisy spans to decode entities during evaluation. Such dynamic entity sampling makes more sense in real scenarios where the number of entities is arbitrary. Third, different from the autoregressive manner in generation-based methods, DIFFUSION-NER can generate all entities in parallel within several denoising timesteps. In addition, the shared encoder across timesteps can further speed up inference. We will further analyze these advantages of DIFFUSIONNER in \u00a7 6.2. In summary, our main contributions are as follows:\n\u2022 DIFFUSIONNER is the first to use the diffusion model for NER, an extractive task on discrete text sequences. Our exploration provides a new perspective on diffusion models in natural language understanding tasks.\n\u2022 DIFFUSIONNER formulates named entity recognition as a boundary denoising diffusion process from the noisy spans. DIFFUSION-NER is a novel generative NER method that generates entities by progressive boundary refinement over the noisy spans.\n\u2022 We conduct experiments on both nested and flat NER to show the generality of DIFFU-SIONNER. Experimental results show that our model achieves better or competitive performance against the previous SOTA models.\n2 Related Work\n\nNamed Entity Recognition\nNamed entity recognition is a long-standing study in natural language processing. Traditional methods can be divided into two folders: tagging-based and span-based. For tagging-based methods (Chiu and Nichols, 2016; Ju et al., 2018; Wang et al., 2020) , they usually perform sequence labeling at the token level and then translate into predictions at the span level. Meanwhile, the span-based methods (Sohrab and Miwa, 2018; Eberts and Ulges, 2020; Shen et al., 2021a,b; Li et al., 2022a) directly perform entity classification on potential spans for prediction. Besides, some methods attempt to formulate NER as sequence-to-set (Tan et al., 2021 (Tan et al., , 2022;; Wu et al., 2022) or reading comprehension (Li et al., 2020; Shen et al., 2022) tasks for prediction. In addition, autoregressive generative NER works (Athiwaratkun et al., 2020; De Cao et al., 2021; Yan et al., 2021b; Lu et al., 2022) linearize structured named entities into a sequence, relying on sequence-to-sequence language models (Lewis et al., 2020; Raffel et al., 2020) to decode entities. These works designed various translation schemas, including from word index sequence to entities (Yan et al., 2021b) and from label-enhanced sequence to entities (Paolini et al., 2021) , to unify NER to the text generation task and achieved promising performance and generalizability. Other works (Zhang et al., 2022) focus on the disorder of the entities and mitigate incorrect decoding bias from a causal inference perspective. Different from previous works, our proposed DIFFUSIONNER is the first one to explore the utilization of the generative diffusion model on NER, which enables progressive refinement and dynamic sampling of entities. Furthermore, compared with previous generation-based methods, our DIFFUSIONNER can also decode entities in a nonautoregressive manner, and thus result in a faster inference speed with better performance.\n\nDiffusion Model\nDiffusion model is a deep latent generative model proposed by (Sohl-Dickstein et al., 2015) . With the development of recent work (Ho et al., 2020) , diffusion model has achieved impressive results on image and audio generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021) . Diffusion model consists of the forward diffusion process and the reverse diffusion process. The former progressively disturbs the data distribution by adding noise with a fixed variance schedule (Ho et al., 2020) , and the latter learns to recover the data structure. Despite the success of the diffusion model in continuous state spaces (image or waveform), the application to natural language still remains some open challenges due to the discrete nature of text (Austin et al., 2021; Hoogeboom et al., 2022; Strudel et al., 2022; He et al., 2022) . Diffusion-LM (Li et al., 2022b) models discrete text in continuous space through embedding and rounding operations and proposes an extra classifier as a guidance to impose constraints on controllable text generation. DiffuSeq (Gong et al., 2022) and SeqDiffuSeq (Yuan et al., 2022a) extend diffusionbased text generation to a more generalized setting. They propose classifier-free sequence-to-sequence diffusion frameworks based on encoder-only and encoder-decoder architectures, respectively.\nAlthough diffusion models have shown their generative capability on images and audio, its potential on discriminative tasks has not been explored thoroughly. Several pioneer works (Amit et al., 2021; Baranchuk et al., 2022; Chen et al., 2022) have made some attempts on diffusion models for object detection and semantic segmentation. Our proposed DIFFUSIONNER aims to solve an extractive task on discrete text sequences.\n\nPreliminary\nIn diffusion models, both the forward and reverse processes can be considered a Markov chain with progressive Gaussian transitions. Formally, given a data distribution x 0 \u223c q (x 0 ) and a predefined variance schedule {\u03b2 1 , . . . , \u03b2 T }, the forward process q gradually adds Gaussian noise with variance \u03b2 t \u2208 (0, 1) at timestep t to produce latent variables x 1 , x 2 , . . . , x T as follows:\nq (x 1 , . . . , x T | x 0 ) = T t=1 q (x t | x t\u22121 )\n(1)\nq (x t | x t\u22121 ) = N x t ; 1 \u2212 \u03b2 t x t\u22121 , \u03b2 t I (2)\nAn important property of the forward process is that we can sample the noisy latents at an arbitrary timestep conditioned on the data x 0 . With the notation \u03b1 t := 1 \u2212 \u03b2 t and \u1fb1t := t s=0 \u03b1 s , we have:\nEQUATION\nAs \u1fb1T approximates 0, x T follows the standard Gaussian distribution: p (x T ) \u2248 N (x T ; 0, I). Unlike the fixed forward process, the reverse process p \u03b8 (x 0:T ) is defined as a Markov chain with learnable Gaussian transitions starting at a prior p (x T ) = N (x T ; 0, I):\np \u03b8 (x 0:T ) = p (x T ) T t=1 p \u03b8 (x t\u22121 | x t ) p \u03b8 (x t\u22121 | x t ) = N (x t\u22121 ; \u00b5 \u03b8 (x t , t) , \u03a3 \u03b8 (x t , t))\nwhere \u03b8 denotes the parameters of the model and \u00b5 \u03b8 and \u03a3 \u03b8 are the predicted covariance and mean of q \n(x t\u22121 | x t ). We set \u03a3 \u03b8 (x t , t) = \u03c3 2 t I and build a neural network f \u03b8 to predict the data x 0 , denoted as x0 = f \u03b8 (x t , t). Then we have \u00b5 \u03b8 (x t , t) = \u03bct (x t , x0 ) = \u03bct (x t , f \u03b8 (x t , t)), where \u03bct denotes the mean of posterior q (x t\u22121 | x t , x0 ).\n\nMethod\nIn this section, we first present the formulation of diffusion model for NER (i.e., the boundary denoising diffusion process) in \u00a7 4.1. Then, we detail the architecture of the denoising network for boundary reverse process in \u00a7 4.2. Finally, we describe the inference procedure of DIFFUSIONNER in \u00a7 4.3.\n\nBoundary Denoising Diffusion Model\nGiven a sentence S with length M , the named entity recognition task is to extract the entities E = {(l i , r i , t i )} N i=0 contained in the sentence, where N is the number of entities and l i , r i , t i denote the left and right boundary indices and type of the i-th entity. We formulate NER as a boundary denoising diffusion process, as shown in Figure 2 . We regard entity boundaries as data samples, then the boundary forward diffusion is to add Gaussian noise to the entity boundaries while the reverse diffusion process is to progressively recover the original entity boundaries from the noisy spans. Boundary Forward Diffusion Boundary forward diffusion is the process of adding noise to the entity boundary in a stepwise manner. In order to align the number of entities in different instances, we first expand the entity set to a fixed number K (> N ). There are two ways to expand the entities, repetition strategy and random strategy, which add K \u2212 N entities by duplicating entities or sampling random spans from a Gaussian distribution 2 . For convenience, we use B \u2208 R K\u00d72 to denote the boundaries of the K expanded entities, with all of them normalized by the sentence length M and scaled to (\u2212\u03bb, \u03bb) interval. Formally, given the entity boundaries as data samples x 0 = B, we can obtain the noisy spans at timestep t using the forward diffusion process. According to Equation (3), we have:\nx t = \u221a \u1fb1t x 0 + \u221a 1 \u2212 \u1fb1t \u03f5 (4)\nwhere \u03f5 \u223c N (0, I) is the noise sampled from the standard Gaussian. At each timestep, the noisy spans have the same shape as x 0 , i.e.,\nx 1 , x 2 , . . . , x T \u2208 R K\u00d72 .\nBoundary Reverse Diffusion Starting from the noisy spans x T sampled from the Gaussian distribution, boundary reverse diffusion adopts a non-Markovian denoising practice used in DDIM (Song et al., 2021) to recover entities boundaries. Assuming \u03c4 is an arithmetic subsequence of the complete timestep sequence [1, . . . , T ] of length \u03b3 with \u03c4 \u03b3 = T . Then we refine the noisy spans x \u03c4 i to 2 We will discuss these two practices in \u00a7 6.3.\nx \u03c4 i\u22121 as follows:\nEQUATION\n)\n\u03b5\u03c4 i = x \u03c4 i \u2212 \u221a \u03b1 \u03c4 i x0 \u221a 1 \u2212 \u03b1 \u03c4 i (6) x \u03c4 i\u22121 = \u221a \u03b1 \u03c4 i\u22121 x0 + 1 \u2212 \u03b1 \u03c4 i\u22121 \u03b5\u03c4 i (7)\nwhere x0 and \u03b5\u03c4 i are the predicted entity boundary and noise at timestep \u03c4 i . f \u03b8 (x t , S, t) is a learnable denoising network and we will cover the network architecture in the next section ( \u00a7 4.2). After \u03b3 iterations of DDIM, the noisy spans are progressively refined to the entity boundaries.\n\nNetwork Architecture\nDenoising network f \u03b8 (x t , S, t) accepts the noisy spans x t and the sentence S as inputs and predicts the corresponding entity boundaries x0 . As shown in Figure 2 , we parameterize the denoising network with a sentence encoder and an entity decoder.\nSentence Encoder consists of a BERT (Devlin et al., 2019) plus a stacked bi-directional LSTM.\nThe whole span encoder takes the sentence S as input and outputs the sentence encoding H S \u2208 R M \u00d7h . The sentence encoding H S will be calculated only once and reused across all timesteps to save computations.\nEntity Decoder uses the sentence encoding H S to first compute the representations of K noisy spans x t and then predicts the corresponding entity boundaries. Specifically, we discretize the noisy spans into word indexes by rescaling, multiplying and rounding 3 , then perform mean pooling over the Take gradient descent step by optimize\n\u2212 K i=1 log P c i (\u03c0 c (i)) + \u03b4\u2208l,r log P \u03b4 i (\u03c0 \u03b4 (i)) 10 until converged;\ninner-span tokens. The extracted span representations can be denoted as H X \u2208 R K\u00d7h . To further encode the spans, we design a span encoder that consists of a self-attention and a cross-attention layer. The former enhances the interaction between spans with key, query, and value as H X . And the latter fuses the sentence encoding to the span representation with key, value as H S , and query as H X . We further add the sinusoidal embedding E t (Vaswani et al., 2017) of timestep t to the span representations. Thus the new representations HX of the noisy spans can be computed:\nHX = SpanEncoder(H S , H X ) + E t ,\nThen we use two boundary pointers to predict the entity boundaries. For boundary \u03b4 \u2208 {l, r}, we compute the fusion representation H \u03b4 SX \u2208 R K\u00d7M \u00d7h of the noisy spans and the words, and then the probability of the word as the left or right boundaries P \u03b4 \u2208 R K\u00d7M can be computed as:\nH \u03b4 SX = H S W \u03b4 S + HX W \u03b4 X P \u03b4 = sigmoid(MLP(H \u03b4 SX ))\nwhere W \u03b4 S , W \u03b4 X \u2208 R h\u00d7h are two learnable matrixes and MLP is a two-layer perceptron. Based on the boundary probabilities, we can predict the boundary indices of the K noisy spans. If the current step is not the last denoising step, we compute x0 by normalizing the indices with sentence length M and scaling to (\u2212\u03bb, \u03bb) intervals. Then we conduct the next iteration of the reverse diffusion process according to Equations ( 5) to (7).\nIt is worth noting that we should not only locate entities but also classify them in named entity recognition. Therefore, we use an entity classifier to classify the noisy spans. The classification probability P c \u2208 R K\u00d7C is calculated as follows:\nP c = Classifier( HX ) Algorithm 2: Inference 1 xT \u223c N (0, I) \u2208 R K eval \u00d72\n2 \u03c4 is an arithmetic sequence of length \u03b3 with \u03c4\u03b3 = T 3 for i = \u03b3, . . . , 1 do 4 Compute x0, P l , P r and P c via f \u03b8 (xt, S, t)\n5 x\u03c4 i\u22121 = \u221a \u03b1\u03c4 i\u22121 x0 + 1 \u2212 \u03b1\u03c4 i\u22121 \u2022 x\u03c4 i \u2212 \u221a \u03b1\u03c4 i x0 \u221a 1\u2212\u03b1\u03c4 i 6 end 7 Decode entities (li, ri, ci) K eval i=0\n, where \u03b4i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c} 8 Perform post-processing on (li, ri, ci) K eval i=0 9 return final entities where C is the number of entity types and Classifier is a two-layer perceptron with a softmax layer.\nTraining Objective With K entities predicted from the noisy spans and N ground-truth entities, we first use the Hungarian algorithm (Kuhn, 1955) to solve the optimal matching \u03c0 between the two sets 4 as in Carion et al. (2020) . \u03c0(i) denotes the ground-truth entity corresponding to the i-th noisy span. Then, we train the boundary reverse process by maximizing the likelihood of the prediction:\nL = \u2212 K i=1 \u03b4\u2208{l,r,c} log P \u03b4 i \u03c0\u03b4 (i)\nwhere \u03c0l (i), \u03c0r (i) and \u03c0c (i) denote the left and right boundary indexes and type of the \u03c0(i) entity. Overall, Algorithm 1 displays the whole training procedure of our model for an explanation.\n\nInference\nDuring inference, DIFFUSIONNER first samples K eval noisy spans from a Gaussian distribution, then performs iterative denoising with the learned boundary reverse diffusion process based on the denoising timestep sequence \u03c4 . Then with the predicted probabilities on the boundaries and type, we can decode K eval candidate entities (l i , r i , c i ) K eval i=0 , where \u03b4 i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c}. After that, we employ two simple post-processing operations on these candidates: de-duplication and filtering. For spans with identical boundaries, we keep the one with the maximum type probability. For spans with the sum of prediction probabilities less than the threshold \u03c6, we discard them. The inference procedure is shown in Algorithm 2. 5 Experimental Settings\n\nDatasets\nFor nested NER, we choose three widely used datasets for evaluation: ACE04 (Doddington et al., 2004) , ACE05 (Walker et al., 2006) , and GE-NIA (Ohta et al., 2002) . ACE04 and ACE05 belong to the news domain and GENIA is in the biological domain. For flat NER, we use three common datasets to validate: CoNLL03 (Tjong Kim Sang and De Meulder, 2003) , OntoNotes (Pradhan et al., 2013) , and MSRA (Levow, 2006) . More details about datasets can be found in Appendix B.\n\nBaselines\nWe choose a variety of recent advanced methods as our baseline, which include: 1) Tagging-based methods (Strakov\u00e1 et al., 2019; Ju et al., 2018; Wang et al., 2020) ; 2) Span-based methods (Yu et al., 2020; Li et al., 2020; Wan et al., 2022; Lou et al., 2022; Zhu and Li, 2022; Yuan et al., 2022b) ; 3) Generation-based methods (Tan et al., 2021; Yan et al., 2021b; Lu et al., 2022) . More details about baselines can be found in Appendix D.\n\nImplementation Details\nFor a fair comparison, we use bert-large (Devlin et al., 2019) ary refinement, and thus obtain better performance.\nThe results also validate that our DIFFUSIONNER can recover entity boundaries from noisy spans via boundary denoising diffusion.\n\nAnalysis\nInference Efficiency To further validate whether our DIFFUSIONNER requires more inference computations, we also conduct experiments to compare the inference efficiency between DIFFUSIONNER and other generation-based models (Lu et al., 2022; Yan et al., 2021a) . Just as shown in Table 3 , we find that DIFFUSIONNER could achieve better performance while maintaining a faster inference speed with minimal parameter scale. Even with a denoising timestep of \u03b3 = 10, DIFFUSIONNER is 18\u00d7 and 3\u00d7 faster than them. This is because DIFFU-SIONNER generates all entities in parallel within several denoising timesteps, which avoids generating the linearized entity sequence in an autoregressive manner. In addition, DIFFUSIONNER shares sentence encoder across timesteps, which further accelerates inference speed. speed of DIFFUSIONNER under various numbers of noisy spans. Just as shown in Figure 3 , we find that, with an increase of denoising steps, the model obtains incremental performance improvement while sacrificing inference speed. Considering the trade-off between performance and efficiency, we set \u03b3 = 5 as the default setting. In addition, when the noisy spans are smaller, the improvement brought by increasing the denoising timesteps is more obvious. This study indicates that our DiffusionNER can effectively counterbalance the negative impact of undersampling noise spans on performance by utilizing additional timesteps. \n\nSampling Number\nAs a generative latent model, DIFFUSIONNER can decouple training and eval-uation, and dynamically sample noisy spans during evaluation. To manifest this advantage, we train DIFFUSIONNER on ACE04 with K = 60 noisy spans and evaluate it with different sampling numbers K eval . The results are shown in Figure 4 . Overall, the model performance becomes better as the sampling number of noisy spans increases. Specifically, we find that DIFFUSIONNER performs worse when K eval < 30. We guess this is because fewer noisy spans may not cover all potential entities. When sampling number K eval > 60, we find it could also slightly improve model performance. Overall, the dynamic sampling of noisy spans in DIFFUSIONNER has the following advantages: 1) we can improve model performance by controlling it to sample more noisy spans; 2) dynamic sampling strategy also allows the model to predict an arbitrary number of entities in any realworld application, avoiding the limitations of the sampling number at the training stage.\n\nAblation Study\nNetwork Architecture As shown in Table 4 , we conduct experiments to investigate the network architecture of the boundary reverse diffusion process. We found that DIFFUSIONNER performs better with a stronger pre-trained language model (PLM), as evidenced by an improvement of +0.53% on ACE04 and +0.11% on CoNLL03 when using roberta-large. Additionally, for the span encoder, we find that directly removing self-attention between noisy spans or cross-attention of spans to the sentence can significantly impair performance. When both are ablated, model performance decreases by 1.37% and 1.15% on ACE04 and CoNLL03. These results indicate that the interaction between the spans or noisy spans and the sentence is necessary. the added noise at each timestep during boundary forward diffusion process. Therefore, we analyze the performance of DIFFUSIONNER on different variance schedulers with different noise timesteps T . The results on ACE04 and CoNLL03 are shown in Table 5 . We find that the cosine scheduler generally yields superior results on the ACE04, while the linear scheduler proves to be more effective on CoNLL03. In addition, the performance of DIFFU-SIONNER varies with the choice of noise timestep, with the best performance achieved at T = 1000 for ACE04 and T = 1500 for CoNLL03.\n\nExpansion Stratagy\nThe expansion stratagy of the entity set can make the number of K noisy spans consistent across instances during training.\nWe conduct experiments to analyze the performance of DIFFUSIONNER for different expansion strategies with various numbers of noisy spans. The experimental results are shown in Table 6 . Generally, we find that the random strategy could achieve similar or better performance than the repetitive strategy. In addition, Table 6 shows that DIFFU-SIONNER is insensitive to the number of noisy spans during training. Considering that using more noisy spans brings more computation and memory usage, we set K = 60 as the default setting.\n\nConclusion\nIn this paper, we present DIFFUSIONNER, a novel generative approach for NER that converts the task into a boundary denoising diffusion process. Our evaluations on six nested and flat NER datasets show that DIFFUSIONNER achieves comparable or better performance compared to previous stateof-the-art models. Additionally, our additional analyses reveal the advantages of DIFFUSIONNER in terms of inference speed, progressive boundary refinement, and dynamic entity sampling. Overall, this study is a pioneering effort of diffusion models for extractive tasks on discrete text sequences, and we hope it may serve as a catalyst for more research about the potential of diffusion models in natural language understanding tasks.\n"}
{"question": "What is one of the advantages of DIFFUSIONNER mentioned in the paper?", "evidence": "  Our evaluations on six nested and flat NER datasets show that DIFFUSIONNER achieves comparable or better performance compared to previous stateof-the-art models.   ", "options": ["Additionally, our additional analyses reveal the advantages of DIFFUSIONNER in terms of inference speed, progressive boundary refinement, and dynamic entity sampling"], "answer": "C", "content": "\nIntroduction\nNamed Entity Recognition (NER) is a basic task of information extraction (Tjong Kim Sang and De Meulder, 2003) , which aims to locate entity mentions and label specific entity types such as person, location, and organization. It is fundamental to many structured information extraction tasks, such as relation extraction (Li and Ji, 2014; Miwa and Bansal, 2016) and event extraction (McClosky et al., 2011; Wadden et al., 2019) .\nMost traditional methods (Chiu and Nichols, 2016) formulate the NER task into a sequence labeling task by assigning a single label to each token. To accommodate the nested structure between entities, some methods (Ju et al., 2018; Wang et al., + + \u00b2 \u00bb N (0; 1) + \u00b2 \u00bb N (0; 1)\nFigure 1 : Boundary diffusion in named entity recognition. The fixed forward diffusion process adds Gaussian noise to the entity boundaries at each timestep, and the noisy boundaries recover their original state by denoising with the learnable reverse diffusion process. For inference, the reverse diffusion process generates entity boundaries and performs entity typing based on the noisy spans sampled from the Gaussian distribution. 2020) further devise cascaded or stacked tagging strategies. Another class of methods treat NER as a classification task on text spans (Sohrab and Miwa, 2018; Eberts and Ulges, 2020) , and assign labels to word pairs (Yu et al., 2020; Li et al., 2022a) or potential spans (Lin et al., 2019; Shen et al., 2021a) . In contrast to the above works, some pioneer works (Paolini et al., 2021; Yan et al., 2021b; Lu et al., 2022) propose generative NER methods that formulate NER as a sequence generation task by translating structured entities into a linearized text sequence. However, due to the autoregressive manner, the generation-based methods suffer from inefficient decoding. In addition, the discrepancy between training and evaluation leads to exposure bias that impairs the model performance.\nWe move to another powerful generative model for NER, namely the diffusion model. As a class of deep latent generative models, diffusion models have achieved impressive results on image, audio and text generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021; Li et al., 2022b; Gong et al., 2022) . The core idea of diffusion models is to systematically perturb the data through a forward diffusion process, and then recover the data by learning a reverse diffusion process.\nInspired by this, we present DIFFUSIONNER, a new generative framework for named entity recognition, which formulates NER as a denoising diffusion process (Sohl-Dickstein et al., 2015; Ho et al., 2020) on entity boundaries and generates entities from noisy spans. As shown in Figure 1 , during training, we add Gaussian noise to the entity boundaries step by step in the forward diffusion process, and the noisy spans are progressively denoised by a reverse diffusion process to recover the original entity boundaries. The forward process is fixed and determined by the variance schedule of the Gaussian Markov chains, while the reverse process requires learning a denoising network that progressively refines the entity boundaries. For inference, we first sample noisy spans from a prior Gaussian distribution and then generate entity boundaries using the learned reverse diffusion process.\nEmpowered by the diffusion model, DIFFUSION-NER presents three advantages. First, the iterative denoising process of the diffusion model gives DIFFUSIONNER the ability to progressively refine the entity boundaries, thus improve performance. Second, independent of the predefined number of noisy spans in the training stage, DIF-FUSIONNER can sample a different number of noisy spans to decode entities during evaluation. Such dynamic entity sampling makes more sense in real scenarios where the number of entities is arbitrary. Third, different from the autoregressive manner in generation-based methods, DIFFUSION-NER can generate all entities in parallel within several denoising timesteps. In addition, the shared encoder across timesteps can further speed up inference. We will further analyze these advantages of DIFFUSIONNER in \u00a7 6.2. In summary, our main contributions are as follows:\n\u2022 DIFFUSIONNER is the first to use the diffusion model for NER, an extractive task on discrete text sequences. Our exploration provides a new perspective on diffusion models in natural language understanding tasks.\n\u2022 DIFFUSIONNER formulates named entity recognition as a boundary denoising diffusion process from the noisy spans. DIFFUSION-NER is a novel generative NER method that generates entities by progressive boundary refinement over the noisy spans.\n\u2022 We conduct experiments on both nested and flat NER to show the generality of DIFFU-SIONNER. Experimental results show that our model achieves better or competitive performance against the previous SOTA models.\n2 Related Work\n\nNamed Entity Recognition\nNamed entity recognition is a long-standing study in natural language processing. Traditional methods can be divided into two folders: tagging-based and span-based. For tagging-based methods (Chiu and Nichols, 2016; Ju et al., 2018; Wang et al., 2020) , they usually perform sequence labeling at the token level and then translate into predictions at the span level. Meanwhile, the span-based methods (Sohrab and Miwa, 2018; Eberts and Ulges, 2020; Shen et al., 2021a,b; Li et al., 2022a) directly perform entity classification on potential spans for prediction. Besides, some methods attempt to formulate NER as sequence-to-set (Tan et al., 2021 (Tan et al., , 2022;; Wu et al., 2022) or reading comprehension (Li et al., 2020; Shen et al., 2022) tasks for prediction. In addition, autoregressive generative NER works (Athiwaratkun et al., 2020; De Cao et al., 2021; Yan et al., 2021b; Lu et al., 2022) linearize structured named entities into a sequence, relying on sequence-to-sequence language models (Lewis et al., 2020; Raffel et al., 2020) to decode entities. These works designed various translation schemas, including from word index sequence to entities (Yan et al., 2021b) and from label-enhanced sequence to entities (Paolini et al., 2021) , to unify NER to the text generation task and achieved promising performance and generalizability. Other works (Zhang et al., 2022) focus on the disorder of the entities and mitigate incorrect decoding bias from a causal inference perspective. Different from previous works, our proposed DIFFUSIONNER is the first one to explore the utilization of the generative diffusion model on NER, which enables progressive refinement and dynamic sampling of entities. Furthermore, compared with previous generation-based methods, our DIFFUSIONNER can also decode entities in a nonautoregressive manner, and thus result in a faster inference speed with better performance.\n\nDiffusion Model\nDiffusion model is a deep latent generative model proposed by (Sohl-Dickstein et al., 2015) . With the development of recent work (Ho et al., 2020) , diffusion model has achieved impressive results on image and audio generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021) . Diffusion model consists of the forward diffusion process and the reverse diffusion process. The former progressively disturbs the data distribution by adding noise with a fixed variance schedule (Ho et al., 2020) , and the latter learns to recover the data structure. Despite the success of the diffusion model in continuous state spaces (image or waveform), the application to natural language still remains some open challenges due to the discrete nature of text (Austin et al., 2021; Hoogeboom et al., 2022; Strudel et al., 2022; He et al., 2022) . Diffusion-LM (Li et al., 2022b) models discrete text in continuous space through embedding and rounding operations and proposes an extra classifier as a guidance to impose constraints on controllable text generation. DiffuSeq (Gong et al., 2022) and SeqDiffuSeq (Yuan et al., 2022a) extend diffusionbased text generation to a more generalized setting. They propose classifier-free sequence-to-sequence diffusion frameworks based on encoder-only and encoder-decoder architectures, respectively.\nAlthough diffusion models have shown their generative capability on images and audio, its potential on discriminative tasks has not been explored thoroughly. Several pioneer works (Amit et al., 2021; Baranchuk et al., 2022; Chen et al., 2022) have made some attempts on diffusion models for object detection and semantic segmentation. Our proposed DIFFUSIONNER aims to solve an extractive task on discrete text sequences.\n\nPreliminary\nIn diffusion models, both the forward and reverse processes can be considered a Markov chain with progressive Gaussian transitions. Formally, given a data distribution x 0 \u223c q (x 0 ) and a predefined variance schedule {\u03b2 1 , . . . , \u03b2 T }, the forward process q gradually adds Gaussian noise with variance \u03b2 t \u2208 (0, 1) at timestep t to produce latent variables x 1 , x 2 , . . . , x T as follows:\nq (x 1 , . . . , x T | x 0 ) = T t=1 q (x t | x t\u22121 )\n(1)\nq (x t | x t\u22121 ) = N x t ; 1 \u2212 \u03b2 t x t\u22121 , \u03b2 t I (2)\nAn important property of the forward process is that we can sample the noisy latents at an arbitrary timestep conditioned on the data x 0 . With the notation \u03b1 t := 1 \u2212 \u03b2 t and \u1fb1t := t s=0 \u03b1 s , we have:\nEQUATION\nAs \u1fb1T approximates 0, x T follows the standard Gaussian distribution: p (x T ) \u2248 N (x T ; 0, I). Unlike the fixed forward process, the reverse process p \u03b8 (x 0:T ) is defined as a Markov chain with learnable Gaussian transitions starting at a prior p (x T ) = N (x T ; 0, I):\np \u03b8 (x 0:T ) = p (x T ) T t=1 p \u03b8 (x t\u22121 | x t ) p \u03b8 (x t\u22121 | x t ) = N (x t\u22121 ; \u00b5 \u03b8 (x t , t) , \u03a3 \u03b8 (x t , t))\nwhere \u03b8 denotes the parameters of the model and \u00b5 \u03b8 and \u03a3 \u03b8 are the predicted covariance and mean of q \n(x t\u22121 | x t ). We set \u03a3 \u03b8 (x t , t) = \u03c3 2 t I and build a neural network f \u03b8 to predict the data x 0 , denoted as x0 = f \u03b8 (x t , t). Then we have \u00b5 \u03b8 (x t , t) = \u03bct (x t , x0 ) = \u03bct (x t , f \u03b8 (x t , t)), where \u03bct denotes the mean of posterior q (x t\u22121 | x t , x0 ).\n\nMethod\nIn this section, we first present the formulation of diffusion model for NER (i.e., the boundary denoising diffusion process) in \u00a7 4.1. Then, we detail the architecture of the denoising network for boundary reverse process in \u00a7 4.2. Finally, we describe the inference procedure of DIFFUSIONNER in \u00a7 4.3.\n\nBoundary Denoising Diffusion Model\nGiven a sentence S with length M , the named entity recognition task is to extract the entities E = {(l i , r i , t i )} N i=0 contained in the sentence, where N is the number of entities and l i , r i , t i denote the left and right boundary indices and type of the i-th entity. We formulate NER as a boundary denoising diffusion process, as shown in Figure 2 . We regard entity boundaries as data samples, then the boundary forward diffusion is to add Gaussian noise to the entity boundaries while the reverse diffusion process is to progressively recover the original entity boundaries from the noisy spans. Boundary Forward Diffusion Boundary forward diffusion is the process of adding noise to the entity boundary in a stepwise manner. In order to align the number of entities in different instances, we first expand the entity set to a fixed number K (> N ). There are two ways to expand the entities, repetition strategy and random strategy, which add K \u2212 N entities by duplicating entities or sampling random spans from a Gaussian distribution 2 . For convenience, we use B \u2208 R K\u00d72 to denote the boundaries of the K expanded entities, with all of them normalized by the sentence length M and scaled to (\u2212\u03bb, \u03bb) interval. Formally, given the entity boundaries as data samples x 0 = B, we can obtain the noisy spans at timestep t using the forward diffusion process. According to Equation (3), we have:\nx t = \u221a \u1fb1t x 0 + \u221a 1 \u2212 \u1fb1t \u03f5 (4)\nwhere \u03f5 \u223c N (0, I) is the noise sampled from the standard Gaussian. At each timestep, the noisy spans have the same shape as x 0 , i.e.,\nx 1 , x 2 , . . . , x T \u2208 R K\u00d72 .\nBoundary Reverse Diffusion Starting from the noisy spans x T sampled from the Gaussian distribution, boundary reverse diffusion adopts a non-Markovian denoising practice used in DDIM (Song et al., 2021) to recover entities boundaries. Assuming \u03c4 is an arithmetic subsequence of the complete timestep sequence [1, . . . , T ] of length \u03b3 with \u03c4 \u03b3 = T . Then we refine the noisy spans x \u03c4 i to 2 We will discuss these two practices in \u00a7 6.3.\nx \u03c4 i\u22121 as follows:\nEQUATION\n)\n\u03b5\u03c4 i = x \u03c4 i \u2212 \u221a \u03b1 \u03c4 i x0 \u221a 1 \u2212 \u03b1 \u03c4 i (6) x \u03c4 i\u22121 = \u221a \u03b1 \u03c4 i\u22121 x0 + 1 \u2212 \u03b1 \u03c4 i\u22121 \u03b5\u03c4 i (7)\nwhere x0 and \u03b5\u03c4 i are the predicted entity boundary and noise at timestep \u03c4 i . f \u03b8 (x t , S, t) is a learnable denoising network and we will cover the network architecture in the next section ( \u00a7 4.2). After \u03b3 iterations of DDIM, the noisy spans are progressively refined to the entity boundaries.\n\nNetwork Architecture\nDenoising network f \u03b8 (x t , S, t) accepts the noisy spans x t and the sentence S as inputs and predicts the corresponding entity boundaries x0 . As shown in Figure 2 , we parameterize the denoising network with a sentence encoder and an entity decoder.\nSentence Encoder consists of a BERT (Devlin et al., 2019) plus a stacked bi-directional LSTM.\nThe whole span encoder takes the sentence S as input and outputs the sentence encoding H S \u2208 R M \u00d7h . The sentence encoding H S will be calculated only once and reused across all timesteps to save computations.\nEntity Decoder uses the sentence encoding H S to first compute the representations of K noisy spans x t and then predicts the corresponding entity boundaries. Specifically, we discretize the noisy spans into word indexes by rescaling, multiplying and rounding 3 , then perform mean pooling over the Take gradient descent step by optimize\n\u2212 K i=1 log P c i (\u03c0 c (i)) + \u03b4\u2208l,r log P \u03b4 i (\u03c0 \u03b4 (i)) 10 until converged;\ninner-span tokens. The extracted span representations can be denoted as H X \u2208 R K\u00d7h . To further encode the spans, we design a span encoder that consists of a self-attention and a cross-attention layer. The former enhances the interaction between spans with key, query, and value as H X . And the latter fuses the sentence encoding to the span representation with key, value as H S , and query as H X . We further add the sinusoidal embedding E t (Vaswani et al., 2017) of timestep t to the span representations. Thus the new representations HX of the noisy spans can be computed:\nHX = SpanEncoder(H S , H X ) + E t ,\nThen we use two boundary pointers to predict the entity boundaries. For boundary \u03b4 \u2208 {l, r}, we compute the fusion representation H \u03b4 SX \u2208 R K\u00d7M \u00d7h of the noisy spans and the words, and then the probability of the word as the left or right boundaries P \u03b4 \u2208 R K\u00d7M can be computed as:\nH \u03b4 SX = H S W \u03b4 S + HX W \u03b4 X P \u03b4 = sigmoid(MLP(H \u03b4 SX ))\nwhere W \u03b4 S , W \u03b4 X \u2208 R h\u00d7h are two learnable matrixes and MLP is a two-layer perceptron. Based on the boundary probabilities, we can predict the boundary indices of the K noisy spans. If the current step is not the last denoising step, we compute x0 by normalizing the indices with sentence length M and scaling to (\u2212\u03bb, \u03bb) intervals. Then we conduct the next iteration of the reverse diffusion process according to Equations ( 5) to (7).\nIt is worth noting that we should not only locate entities but also classify them in named entity recognition. Therefore, we use an entity classifier to classify the noisy spans. The classification probability P c \u2208 R K\u00d7C is calculated as follows:\nP c = Classifier( HX ) Algorithm 2: Inference 1 xT \u223c N (0, I) \u2208 R K eval \u00d72\n2 \u03c4 is an arithmetic sequence of length \u03b3 with \u03c4\u03b3 = T 3 for i = \u03b3, . . . , 1 do 4 Compute x0, P l , P r and P c via f \u03b8 (xt, S, t)\n5 x\u03c4 i\u22121 = \u221a \u03b1\u03c4 i\u22121 x0 + 1 \u2212 \u03b1\u03c4 i\u22121 \u2022 x\u03c4 i \u2212 \u221a \u03b1\u03c4 i x0 \u221a 1\u2212\u03b1\u03c4 i 6 end 7 Decode entities (li, ri, ci) K eval i=0\n, where \u03b4i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c} 8 Perform post-processing on (li, ri, ci) K eval i=0 9 return final entities where C is the number of entity types and Classifier is a two-layer perceptron with a softmax layer.\nTraining Objective With K entities predicted from the noisy spans and N ground-truth entities, we first use the Hungarian algorithm (Kuhn, 1955) to solve the optimal matching \u03c0 between the two sets 4 as in Carion et al. (2020) . \u03c0(i) denotes the ground-truth entity corresponding to the i-th noisy span. Then, we train the boundary reverse process by maximizing the likelihood of the prediction:\nL = \u2212 K i=1 \u03b4\u2208{l,r,c} log P \u03b4 i \u03c0\u03b4 (i)\nwhere \u03c0l (i), \u03c0r (i) and \u03c0c (i) denote the left and right boundary indexes and type of the \u03c0(i) entity. Overall, Algorithm 1 displays the whole training procedure of our model for an explanation.\n\nInference\nDuring inference, DIFFUSIONNER first samples K eval noisy spans from a Gaussian distribution, then performs iterative denoising with the learned boundary reverse diffusion process based on the denoising timestep sequence \u03c4 . Then with the predicted probabilities on the boundaries and type, we can decode K eval candidate entities (l i , r i , c i ) K eval i=0 , where \u03b4 i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c}. After that, we employ two simple post-processing operations on these candidates: de-duplication and filtering. For spans with identical boundaries, we keep the one with the maximum type probability. For spans with the sum of prediction probabilities less than the threshold \u03c6, we discard them. The inference procedure is shown in Algorithm 2. 5 Experimental Settings\n\nDatasets\nFor nested NER, we choose three widely used datasets for evaluation: ACE04 (Doddington et al., 2004) , ACE05 (Walker et al., 2006) , and GE-NIA (Ohta et al., 2002) . ACE04 and ACE05 belong to the news domain and GENIA is in the biological domain. For flat NER, we use three common datasets to validate: CoNLL03 (Tjong Kim Sang and De Meulder, 2003) , OntoNotes (Pradhan et al., 2013) , and MSRA (Levow, 2006) . More details about datasets can be found in Appendix B.\n\nBaselines\nWe choose a variety of recent advanced methods as our baseline, which include: 1) Tagging-based methods (Strakov\u00e1 et al., 2019; Ju et al., 2018; Wang et al., 2020) ; 2) Span-based methods (Yu et al., 2020; Li et al., 2020; Wan et al., 2022; Lou et al., 2022; Zhu and Li, 2022; Yuan et al., 2022b) ; 3) Generation-based methods (Tan et al., 2021; Yan et al., 2021b; Lu et al., 2022) . More details about baselines can be found in Appendix D.\n\nImplementation Details\nFor a fair comparison, we use bert-large (Devlin et al., 2019) ary refinement, and thus obtain better performance.\nThe results also validate that our DIFFUSIONNER can recover entity boundaries from noisy spans via boundary denoising diffusion.\n\nAnalysis\nInference Efficiency To further validate whether our DIFFUSIONNER requires more inference computations, we also conduct experiments to compare the inference efficiency between DIFFUSIONNER and other generation-based models (Lu et al., 2022; Yan et al., 2021a) . Just as shown in Table 3 , we find that DIFFUSIONNER could achieve better performance while maintaining a faster inference speed with minimal parameter scale. Even with a denoising timestep of \u03b3 = 10, DIFFUSIONNER is 18\u00d7 and 3\u00d7 faster than them. This is because DIFFU-SIONNER generates all entities in parallel within several denoising timesteps, which avoids generating the linearized entity sequence in an autoregressive manner. In addition, DIFFUSIONNER shares sentence encoder across timesteps, which further accelerates inference speed. speed of DIFFUSIONNER under various numbers of noisy spans. Just as shown in Figure 3 , we find that, with an increase of denoising steps, the model obtains incremental performance improvement while sacrificing inference speed. Considering the trade-off between performance and efficiency, we set \u03b3 = 5 as the default setting. In addition, when the noisy spans are smaller, the improvement brought by increasing the denoising timesteps is more obvious. This study indicates that our DiffusionNER can effectively counterbalance the negative impact of undersampling noise spans on performance by utilizing additional timesteps. \n\nSampling Number\nAs a generative latent model, DIFFUSIONNER can decouple training and eval-uation, and dynamically sample noisy spans during evaluation. To manifest this advantage, we train DIFFUSIONNER on ACE04 with K = 60 noisy spans and evaluate it with different sampling numbers K eval . The results are shown in Figure 4 . Overall, the model performance becomes better as the sampling number of noisy spans increases. Specifically, we find that DIFFUSIONNER performs worse when K eval < 30. We guess this is because fewer noisy spans may not cover all potential entities. When sampling number K eval > 60, we find it could also slightly improve model performance. Overall, the dynamic sampling of noisy spans in DIFFUSIONNER has the following advantages: 1) we can improve model performance by controlling it to sample more noisy spans; 2) dynamic sampling strategy also allows the model to predict an arbitrary number of entities in any realworld application, avoiding the limitations of the sampling number at the training stage.\n\nAblation Study\nNetwork Architecture As shown in Table 4 , we conduct experiments to investigate the network architecture of the boundary reverse diffusion process. We found that DIFFUSIONNER performs better with a stronger pre-trained language model (PLM), as evidenced by an improvement of +0.53% on ACE04 and +0.11% on CoNLL03 when using roberta-large. Additionally, for the span encoder, we find that directly removing self-attention between noisy spans or cross-attention of spans to the sentence can significantly impair performance. When both are ablated, model performance decreases by 1.37% and 1.15% on ACE04 and CoNLL03. These results indicate that the interaction between the spans or noisy spans and the sentence is necessary. the added noise at each timestep during boundary forward diffusion process. Therefore, we analyze the performance of DIFFUSIONNER on different variance schedulers with different noise timesteps T . The results on ACE04 and CoNLL03 are shown in Table 5 . We find that the cosine scheduler generally yields superior results on the ACE04, while the linear scheduler proves to be more effective on CoNLL03. In addition, the performance of DIFFU-SIONNER varies with the choice of noise timestep, with the best performance achieved at T = 1000 for ACE04 and T = 1500 for CoNLL03.\n\nExpansion Stratagy\nThe expansion stratagy of the entity set can make the number of K noisy spans consistent across instances during training.\nWe conduct experiments to analyze the performance of DIFFUSIONNER for different expansion strategies with various numbers of noisy spans. The experimental results are shown in Table 6 . Generally, we find that the random strategy could achieve similar or better performance than the repetitive strategy. In addition, Table 6 shows that DIFFU-SIONNER is insensitive to the number of noisy spans during training. Considering that using more noisy spans brings more computation and memory usage, we set K = 60 as the default setting.\n\nConclusion\nIn this paper, we present DIFFUSIONNER, a novel generative approach for NER that converts the task into a boundary denoising diffusion process. Our evaluations on six nested and flat NER datasets show that DIFFUSIONNER achieves comparable or better performance compared to previous stateof-the-art models. Additionally, our additional analyses reveal the advantages of DIFFUSIONNER in terms of inference speed, progressive boundary refinement, and dynamic entity sampling. Overall, this study is a pioneering effort of diffusion models for extractive tasks on discrete text sequences, and we hope it may serve as a catalyst for more research about the potential of diffusion models in natural language understanding tasks.\n"}
{"question": "In the evaluation of the generated conversations, which criteria are used to rate the quality of generated questions by human annotators?", "evidence": "  We follow Do et al. (2022) to utilize three criteria:  (1)Factuality measures the factual correctness and meaning of generated questions (2)Conversational Alignment measures how aligned the generated questions are with the history (3)Answerability measures how answerable the generated questions are by the given context.\"  ", "options": ["A. Factuality, Conversational Alignment, and Answerability", "B. Fluency, Grammaticality, and Factuality", "C. Context Coverage, Distinct score, and Jumping Score", "D. BERTScore-entailment, Conv-Distinct, and JS score"], "answer": "A", "content": "\nIntroduction\nBuilding systems that can comprehend human speech and provide assistance to humans through conversations is one of the main objectives in AI. Asking questions during a conversation is a crucial conversational behavior that helps AI agents communicate with humans more effectively (Allen et al., 2007; Li et al., 2016b) . This line of research is known as Conversational Question Generation (CQG), which targets generating questions given the context and conversational history (Nakanishi et al., 2019; Pan et al., 2019a; Gu et al., 2021; Do et al., 2022) . Compared to traditional single-turn question generation (Pan et al., 2019b) , CQG is more challenging as the generated multi-turn questions in a conversation need not only to be coherent but also follow a naturally conversational flow.\nGenerally, there are two main settings for the CQG task: answer-aware and answer-unaware. In the answer-aware setting, the expected answers of the (to be) generated questions are exposed to the models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . In reality, however, the answers are only \"future\" information that are unknown beforehand. Thus, growing attention has been on the more realistic answer-unaware setting, in which the answers are unknown to the CQG model (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nPrior studies either attempt to ask the questions first, and compute the reward function to evaluate their answerability (Pan et al., 2019a) or informativeness (Qi et al., 2020) ; or they extract the answer spans from the context as the what-to-ask first, and generate the questions based on them (Nakanishi et al., 2019; Do et al., 2022) . However, it has been argued that the former approach tends to generate repetitive questions (Qi et al., 2020; Do et al., 2022) . For the latter approach, Do et al. (2022) recently proposed a selection module to shorten the context and history of the input and achieved stateof-the-art performance. Nonetheless, it simply employs a naive heuristic to select the earliest forward sentence (without traceback) in the context as the rationale to extract the answer span. Although such heuristics ensure the flow of the generated questions is aligned with the context, we argue that the resulting conversations may not be natural enough, because, in reality, the interlocutors often talk about the relevant parts that may not form a sequential context. Furthermore, previous studies (Gao et al., 2019; Do et al., 2022) trained the models to decide the type of the question (boolean/span-based) to be generated implicitly. We argue that modeling question type explicitly is critical since in this setting, the answer, which hints the models to generate a boolean or span-based question, is unavailable.\nTo address the above problems, we propose a two-stage CQG framework based on a semantic graph, SG-CQG, which consists of two main components: what-to-ask and how-to-ask. In particular, given the referential context and dialog history, the what-to-ask module (1) constructs a semantic graph, which integrates the information of coreference, co-occurrence, and named entities from the context to capture the keyword chains for the possible \"jumping\" purpose; (2) traverses the graph to retrieve a relevant sentence as the rationale; and\n(3) extracts the expected answer span from the selected rationale (Section 3.1). Next, the how-to-ask module decides the question type (boolean/spanbased) via two explicit control signals and conducts question generation and filtering (Section 3.2).\nIn order to exhaustively assess the quality of the generated question-answer pairs, we propose a set of metrics to measure the diversity, dialog entailment, relevance, flexibility, and context coverage through both standard and human evaluations. Compared with the existing answer-unaware CQG models, our proposed SG-CQG achieves state-ofthe-art performance on the standard benchmark, namely the CoQA dataset (Reddy et al., 2019) .\nOur contributions can be summarized as follows:\n(1) We propose SG-CQG, a two-stage framework, which consists of two novel modules: whatto-ask encourages the models to generate coherent conversations; and how-to-ask promotes generating naturally diverse questions. Our codes will be released at https://github.com/ dxlong2000/SG-CQG.\n(2) SG-CQG achieves state-of-the-art performance on answer-unaware CQG on CoQA.\n(3) To the best of our knowledge, we are the first to propose a set of criteria to comprehensively evaluate the generated conversations. Moreover, we propose Conv-Distinct to measure the diversity of the generated conversation from a context, which takes the context coverage into account.\n(4) We conduct thorough analysis and evaluation of the questions and answers of our generated conversations, which can bring some inspiration for future work on the answer-unaware CQG.\n\nRelated Work\nOur work is closely related to two lines of prior work. Extended related work is in Appendix A.1.\n\nConversational Question Generation\nQuestion Generation has gained much attention from the research community over the years (Pan et al., 2019b; Lu and Lu, 2021) . Despite such intensive exploration, much less attention has been drawn to Conversational QG or CQG. Generally, CQG has been considered in two main settings: answer-aware and answer-unaware. In the answeraware setting, the expected answers are revealed to models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . However, this is not always the case in reality, as the answers are \"future information\". The answer-unaware setting; therefore, receives growing interests recently (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nTo tackle the what-to-ask problem, prior studies (Pan et al., 2019a; Do et al., 2022) selected the next sentence in the context as the rationale. Do et al. (2022) extract the target answer span from the rationale, while Pan et al. (2019a) generate the question, and compute a reward function to fine-tune the model by reinforcement learning. The howto-ask challenge was simply formulated as that in the answer-aware setting. In contrast, we attempt to model the rationale selection in a more coherent way by constructing and traversing a semantic graph, which simulates the keyword chains. We further propose control signals to promote diversity and fluency in question generation.\n\nKnowledge-grounded Conversation Generation\nLeveraging graphs to enhance dialog response generation has received growing interest (Moghe et al., 2018; Liu et al., 2019b; Xu et al., 2020 Xu et al., , 2021)) .\nIn particular, Xu et al. (2020) proposed to extract event chains (Mostafazadeh et al., 2016) , and utilised them to help determine a sketch of a multi-turn dialog. Nonetheless, the situation differs significantly when it comes to the CQG task. The responses in the dialog response generation task are normally full sentences with enough relevant mentions. However, in CQG, the questions and answers are mostly short and lack clear keywords, which makes the existing keyword-graph not applicable. We thus present a semantic graph, which incorporates the coreference, co-occurrence, and named entities information from the context.\n\nSG-CQG\nWe formulate the answer-unaware conversational question generation (CQG) task as:\ngiven the referential context C = {s 1 , s 2 , ..., s m } with s i being the i-th sentence in context, and the conversational history H n = {(q 1 , a 1 ), (q 2 , a 2 ), ..., (q n\u22121 , a n\u22121 )} with (q i , a i ) being the i-th turn of the question-answer pairs, as input D n = {C, H n }, the model learns to generate the current question q n and answer a n .\nFigure 1 demonstrates an overview of our proposed framework. It consists of two main components: (1) A what-to-ask module aims to select a reasonable sentence in the referential context C as the current rationale r n and thereby a span in r n as the target answer a n , given D n . (2) A how-to-ask module aims to generate the question q n , guided by the rationale r n and target answer a n .\n\nWhat-to-ask Module (WTA)\nExisting answer-unaware CQG models (Pan et al., 2019a; Do et al., 2022) commonly utilize the next sentence of r n\u22121 in the context as the current rationale r n . Although such heuristics can guarantee that the flow of the generated questions is consistent with the narrative in context, the generated conversation may not always be as natural as in reality, since human speakers often jump back and forth across the relevant but not sequential contents in context. To facilitate the models in selecting the current rationale and target answer appropriately and further improve the semantic diversity of dialogue flow, we design a what-to-ask module, which consists of two components: semantic graph construction and graph traversal algorithm.\nSemantic Graph Construction (SGC) Figure 1 shows an example of our semantic graph. Each node is displayed as a textual span and the index of the sentence it belongs to. To construct the semantic graph G = {V, E}, we first obtain the corefer-ence clusters from the context C by AllenNLP (Shi and Lin, 2019) and build the set of initial nodes from phrases in the clusters. We then connect all the nodes in the same cluster as a chain: each node in the cluster (except the one that appears last in the context) is connected to the nearest forward one in the context. We denote this type of relation as Coreference. To enhance the connectedness of G, we extract all named entities by spaCy 1 and add them as additional nodes if they are not in any clusters. We then connect all the nodes in the same sentence in the context in the same chaining style and name those edges as Same Sentence. Finally, we add a type of Extra edges between all connected subgraphs to make G fully-connected. Since those Extra edges do not bring any semantic relation to the graph, our objective is to minimize the number of those edges. Specifically, we gradually select, and connect two sentences such that their nodes are in different connected components and have the smallest indexes with the smallest difference, until the graph is fully-connected. To connect two sentences, we add an Extra edge between the last phrase in the smaller-index sentence and the first phrase in the remaining sentence. The adding-Extra-edges algorithm is in Appendix A.4.\nGraph Traversal Algorithm (GTA) Given the conversational history H n and the semantic graph G, we create a queue q to store nodes for traversing. We first add the nodes that appear in any previous turn' rationale to q in the index order 2 . We then traverse G by popping the nodes in q until it becomes empty. For each node, we retrieve the sentence that contains it as the rationale r n . If the model can generate a valid question from r n and any answer span extracted from r n , we add all unvisited neighbors of the current node to the beginning of q. A question is considered being valid if it passes the QF module (Section 3.2). Prepending the neighbors to queue is to prioritize the nodes that are connected so that the generated conversation can be formed from a chain of relevant sentences, which consolidates the coherence of the conversation. If the model cannot generate any valid q n by the current node, we add its unvisited neighbors to the end of q. The pseudocode of our proposed Graph Traversal Algorithm is described in Appendix A.2. et al. (2022) to design the answer span extractor module. In particular, a T5 model is trained on SQuAD (Rajpurkar et al., 2016) to predict the target answer span (a), given its original sentence in context (r). We use this pretrained model to extract a n from r n . Note that we also deselect the answer spans that are the same as those of previous turns.\n\nHow-to-ask Module (HTA)\nA high ratio of boolean questions in conversational datasets such as CoQA (Reddy et al., 2019) (around 20%) is one of the main challenges for current CQG studies (Gao et al., 2019; Pan et al., 2019a; Gu et al., 2021) . To the best of our knowledge; however, there is no up-to-date work which attempts to tackle this challenge. This problem is even worse in the answer-unaware setting since there is no\nYes/No answer to be provided to guide the generation of the models. Previous studies (Pan et al., 2019a; Do et al., 2022) simply train the CQG models to let them implicitly decide when to generate the boolean and span-based questions without any explicit modeling of the question type. We argue that explicitly modeling the question type is critical, as the models will gain more control on generating diverse questions, thus making the conversation become more natural. To this end, we introduce two control signals as the additional input to the QG model, and develop a simple mechanism to select the signal for the current turn.\nQuestion Type Classifier (QTC) We design two control signals to guide the QG model: <BOOLEAN> is prepended to the textual input if we expect the model to generate a boolean question, and <NORMAL> otherwise. To classify which signal should be sent to the QG model, we train a RoBERTa (Liu et al., 2019a) as our Question Type Classifier. This binary clasifier takes the rationale r n and the answer span a n generated from what-toask module, the context and the shortened conversational history as the input, and generates the label 0/1 corresponding to <NORMAL>/<BOOLEAN>. We conduct additional experiments to discuss why the control_signals work in Section 6.3.\nRewriting and Filtering (RF) Our RF module serves two purposes. Firstly, following Do et al.\n(2022), we train a T5 model on CoQA (Reddy et al., 2019) as our CQA model to answer the generated questions. A question is passed this filtering step if the answer generated by the CQA model has a fuzzy matching score greater or equal to 0.8 with the input answer span. Secondly, when invigilating the generated conversations, we observe multiple other errors that the blackbox model encounters, as shown in Table 1 . We thus propose extra post-processing heuristics to filter out the gen-erated questions and try to avoid the following issues: (1) Wrong answer. Unlike Do et al. ( 2022) that took the extracted spans as the conversational answers, we rewrite the extracted answer spans for the boolean questions by selecting the answers generated from the CQA model;\n(2) Irrelevant. For each generated question, we remove stopwords and question marks only for filtering purpose, and we check if all the remaining tokens exist in the context C;\n(3) Uninformative. To remove the turns like (\"Who woke up?\", \"Justine\"), we check validity if no more than 50% of the tokens of r n exist in any previously generated QA pairs; (4) Redundant. Unlike previous studies (Qi et al., 2020; Do et al., 2022) which only considered the redundant information from the generated answers, for each generated question that has more than 3 tokens, we filter it out if it has a fuzzy matching score >= 0.8 with any of the previously generated questions.\nQuestion Generation (QG) We fine-tune a T5 model (Raffel et al., 2020) to generate conversational questions. We concatenate the input\nD a n = {C, H n , a n , r n , control_signal} in the for- mat: Signal: control_signal Answer: a n , r n Context: C [SEP] H sub , where H sub \u2208 H n .\nThe model then learns to generate the target question q n . In our experiments, H sub is the shortened H n , in which we keep at most three previous turns. It was shown to improve upon training with the whole H n significantly (Do et al., 2022) . The performance of the QG model is in Appendix A.3.\n\nExperimental Settings\nDataset We use CoQA (Reddy et al., 2019) , a large-scale CQA dataset, in our experiments. Each conversation includes a referential context and multiple question-answer pairs, resulting in a total of 127k question-answer pairs. Among them, around 20% of questions are boolean, which makes this dataset become challenging for the CQG task (Pan et al., 2019a; Gu et al., 2021) . Since the test set of CoQA is unavailable, we follow Do et al. (2022) to keep the original validation set as our test set and randomly sample 10% of the original training set as our new validation set.\nAutomatic Evaluation We utilise BERTScore (Zhang et al., 2020) as our dialog entailment metric (BERTScore-entailment), a generalization of Dziri et al. (2019) . It considers the generated re-sponse (question/answer) as the premise, and the utterances in the conversational history as the hypothesis, and measures their similarity score as the topic coherence score. This property is crucial as the questions/answers should focus on the same topic as the previous turn(s). In our experiment, we measure the dialog entailment score with 1, 2, and all previous turn(s). To measure the relevance between the generated conversation and the context, we concatenate the generated QA pairs and compute the BERTScore. It provides how the generated conversation is explicitly relevant to the context.\nWe observe short conversations with very few generated turns tend to yield very high scores on the available diversity measurement metrics such as Distinct (Li et al., 2016a) . Since the conversation is generated from a given context, we argue that how much information from the given context the generated conversation covers should be taken into account. To this end, we introduce Context Coverage (CC) to measure the percentage of the sentences in the context that are the rationales of generated QA pairs. Our proposed Conv-Distinct of a generated conversation is then computed by multiplying the Distinct score of the generated conversation with its CC score, to measure the diversity of the turns generated from a given context:\nConv-Distinct = CC * Distinct (1)\nWe further provide Jumping Score (JS) to measure the flexibility of the generated conversation. JS is defined as the percentage of turns in which the model jumps back to any previous content of their previous turn (i.e. trace-back). It is worth noting that we do not rank the models based on JS score. Details of proposed metrics are in Appendix A.7.\nHuman Evaluation Human evaluation is critical to evaluate the quality of the generated conversations since the CQG model may generate reasonable conversations but unmatched well with the provided ground-truth ones. We randomly select 25 contexts in our test set and take the first five generated turns from the output of each model to compare, resulting in 125 samples in total. We hire three annotators who are English native speakers. Each generated question is rated by annotators on a 1-3 scale (3 is the best). We follow Do et al. (2022) to utilize three criteria: (1) Factuality measures the factual correctness and meaning of generated questions, (2) Conversational Alignment measures how aligned the generated questions are with the history, (3) Answerability measures how answerable the generated questions are by the given context. Given the fact that LMs can generate fluent texts, we omit using Fluency and Grammaticality.\nWe measure the annotators' agreement by Krippendorff's alpha (Krippendorff, 2011) . Our human rating instructions are in Appendix A.9.\nImplementation Details We fine-tune a RoBERTa large (Liu et al., 2019a) as our binary Question Type Classifier with the pretrained checkpoints from fairseq (Ott et al., 2019) on CoQA. We use a learning rate of 1e-5, a window size of 512, a batch size of 4, and AdamW (Loshchilov and Hutter, 2019) as our optimizer.\nOur classifier achieves an accuracy of 95.6%. The model is finetuned on a P40 Colab GPU for 10 epochs. Details of the input format are in Appendix A.5. We initialise SG-CQG with pretrained checkpoints of T5 base model (Raffel et al., 2020) from Huggingface (Wolf et al., 2020) . We also use AdamW (Loshchilov and Hutter, 2019) as our optimizer with a warmup of 0.1 and an initial learning rate of 1e-4. We train the model for 100k iterations with a standard window size of 512, a batch size of 4, and use a Beam search decoding strategy with a beam size of 4. \n\nMain Results\nTo evaluate the performance of SG-CQG on the answer-unaware CQG task, we employ 4 baselines for comparison, as shown in Table 2 .\n(1) T5 base (Raffel et al., 2020) , (2) BART base (Lewis et al., 2020) , (3) GPT-2 (Radford et al., 2019) , which are fine-tuned to generate conversational questionanswer pairs end-to-end, and (4) CoHS-CQG (Do et al., 2022) which adopts a strategy to shorten the context and history of the input, achieves the SoTA performance on CoQA in answer-aware and answer-unaware CQG. Firstly, we observe that SG-CQG outperforms other methods on most of the metrics, except Distinct and BERTScore. The reason is that BART and T5 often generate short QA pairs (the CC scores are 8.62% and 23.33% on average, respectively), and copy more from the context, thus they get higher scores on Distinct and BERTScore. Secondly, the metric Conv-Distinct reasonably penalizes models that generate too short conversations, on which SG-CQG achieves the best results. Thirdly, by allowing the model to jump back and forth across the relevant contents in the context by the semantic graph, SG-CQG outperforms other methods significantly on BERTScore-entailment, which indicates that conversational coherence is indeed im-proved. Furthermore, SG-CQG achieves the highest JS score, which demonstrates that the whatto-ask module allows our model to be most flexible in selecting rationales compared to the baselines. SG-CQG also achieves a significantly higher Context Coverage (CC) score compared to CoHS-CQG. Finally, compared with the results of Oracle, which are from the human-generated conversations, SG-CQG achieves commensurate performance on BERTScore-entailment and BERTScore. It demonstrates that our generated conversations are as closely coherent as human-generated ones.\nQuestion Generation Evaluation We compare the generated conversational questions of our model with 4 baselines: (1) ReDR (Pan et al., 2019a) is an encoder-decoder framework which incorporates a reasoning procedure to better understand what has been asked and what to ask next about the passage; (2) T5 base (Raffel et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . For T5, GPT-2 and CoHS-CQG, we extract the generated questions from the generated conversations for comparison. We measure the diversity of the generated questions by Distinct (Li et al., 2016a) and our proposed Conv-Distinct. Table 3 shows evaluation results of the generated conversational questions. We observe that SG-CQG achieves the best performance on Conv-Distinct, which takes the context coverage into account.\n\nAnswer Span Extraction Evaluation\nWe further evaluate the generated conversational answers of our model with 4 baselines: (1) T5 base (Raffel et al., 2020) ; (2) BART base (Lewis et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . We extract the generated conversational answers from the generated conversations of the models for comparison. We train another T5 base model on CoQA for the CQA task (see Appendix A.6) and utilize it to generate the groundtruth answers for the generated questions of the models. We then evaluate the quality of the generated conversational answers by measuring the Exact Match (EM) and F1 scores with the groundtruth ones. Table 4 shows the evaluation results. We observe that the generated conversational answers extracted by SG-CQG achieve the best EM and F1 scores, which are significantly higher than the other baselines.\n\nHuman Evaluation\nThe results of the human evaluation are present in et al., 2022) . Compared to CoHS-CQG, it achieves higher scores on all metrics except the Context Coverage (CC), which reflects that the quality of the generated conversations is indeed improved. These improvements are expected as the model in this case gains more control over generating boolean questions and has a stricter filtering process. This stricter filtering process also explains why it gets a lower CC score compared to CoHS-CQG.\n\nAblation of Question Type Classifier (QTC)\nWe conduct an ablation study of the Question Type Classifier (QTC) module. We name this experiment SG-CQG + w/o QTC. Table 2 shows the evaluation results of generated question-answer pairs. Compared with SG-CQG, the performance of SG-CQG + w/o QTC drops slightly on nearly all metrics (except Distinct), which consolidates our hypothesis that explicitly modeling the question type improves the overall coherency of the conversation. Furthermore, Table 3 shows that QTC enhances the diversity of the generated questions, while Table 4 illustrates that QTC improves the quality of the 2 ) and questions (Table 3 ) significantly. Notably, without the RF module, the extracted answer spans by SG-CQG + w/o RF can be very different from the true conversational answers, resulting in very low F1 and EM scores (Table 4 ). Although the CC score is perfect, the generated question-answer pairs from this experiment are of bad-quality.\n\nCase Study\nWe present one conversation generated by our proposed SG-CQG in Table 6 . We observe that the rationale of Q2-A2 is the 3-rd sentence in the context, and the rationale of Q3-A3 is the 8-th sentence, which is a forward jump of the model. On the other hand, the rationale of the Q4-A4 is the 7-th sentence, which is a traceback. Such a traceback enhances reasonable coherence between Q3-A3 and Q4-A4. Furthermore, Q5-A5 to Q6-A6 is also a traceback, and especially, Q6 is a boolean question. More case studies are shown in Appendix A.10.\n\nWhy Do Control Signals Work?\nExperimental Settings We design the experiments to verify the helpfulness of our two proposed control_signals: <BOOLEAN> and <NORMAL>.\nIn particular, we train a T5 model (Raffel et al., 2020 ) in the answer-aware setting. Given the input D a n = {C, H n , a n , r n } with C, H n , a n , r n as the context, ground-truth conversational history, ground-truth answer, and round-truth rationale, respectively, we conduct three experiments in Table 9 : original input with Yes/No keyword (With Y/N ), original input without Yes/No keyword (W/o Y/N ), original input without Yes/No and with the ground-truth control_signal (W/o Y/N + control_signal). Note that we train the model with the whole context, and a maximum of three previous history turns, as discussed in Appendix A.3. We measure the performance of the answer-aware CQG model separately on two types of questions: boolean and span-based by ROUGE-L (Lin, 2004) and BERTScore (Zhang et al., 2020) .\nObservations Table 9 shows the experimental results. We derive two main observations. Firstly, without knowing the keyword Yes/No (W/o Y/N ) -this is the case in the answer-unaware setting, the model performs worse. This decrease shows that the Yes/No keyword is indeed helpful in hinting the model towards generating the correct questions. Secondly, by inputting the groundtruth control_signal into the model (W/o Y/N + control_signal), the performance is improved by a large margin compared to (W/o Y/N ). We obtain three implications from the above improvement. Firstly, it consolidates our hypothesis that inputting the ground-truth control_signal is truly helpful. Secondly, by training with the control_signal, the performance of the model is even higher than with Y/N in the span-based cases, which indicates that training the model with control_signal makes it more stable to generate the correct questions. Thirdly, the performance of (W/o Y/N + control_signal) is lower than (With Y/N ) in boolean cases. The reason is <BOOLEAN> only informs the model to generate a boolean question without informing to generate an Yes or No one.\n\nConclusion\nThis paper presents SG-CQG, a two-stage framework for the CQG task in the answer-unaware setting. Firstly, the what-to-ask module aims to select a sentence as the rationale by the proposed semantic graph and extract the answer span from it. The how-to-ask module classifies the type of the question before generating and filtering it. Additionally, we propose a set of automatic evaluation criteria for answer-unaware CQG, especially a novel metric, Conv-Distinct, to evaluate the generated conversation from a context. Extensive automatic evaluation and human evaluation show that our method achieves state-of-the-art performances in the answer-unaware setting on CoQA, with a significant improvement in the conversational alignment property compared to previous frameworks. In the future, we will focus on how to reason over our semantic graph to select the rationale, and further improve the performances of how-to-ask module.\n"}
{"question": "When does the model perform the best in qualitative analysis?", "evidence": "  Grammatical Analysis: When asked about general concepts and rules of grammar, the model-generated answers are usually acceptable. However, when questions become more specific to examples, sometimes the model struggles: The model correctly answers many of the fluency questions. Others require reasoning and understanding of context, and the model struggles to generate a correct answer.  ", "options": ["A. When asked about general concepts and rules of grammar.", "B. When the questions involve specific examples.", "C. When answering fluency-related questions.", "D. When the questions require reasoning and understanding of context."], "answer": "A", "content": "\nIntroduction\nLanguage is so powerful that it can be reflected back on itself. Statements like \"In informal usage, a steep learning curve means something that is difficult (and takes much effort) to learn\" or \"In some cases, an adjective has both -ic and -ical forms, with no difference in meaning\" expressly concern linguistic inventories, structures, and behaviors. In other words, they are metalinguistic-they use language to discuss language (cf. Wilson, 2013) . They may concern a particular instance of language use, or properties of a language or speaker in general; either way, they are metalinguistic in making linguistic phenomena the subject matter of a linguistic utterance. For the rest of this paper, the term metalanguage is used for natural language text in which natural language is also the subject matter.\nWhile NLP models have become powerful at predicting text in many settings, it remains to be seen whether such capability extends to metalanguagewhere linguistic strings are not being deployed to contribute to the discourse with their normal denotations, but rather, are treated as entities with linguistic properties (e.g., grammar, meaning). One way this can be explored is in a question answering framework, which requires suitable datasets, ideally based on questions that are realistic and paired with high-quality answers.\nIn this paper, we present a corpus of metalinguistic questions and answers about English. The corpus is collected and carefully processed from two Stack Exchange forum sites: English Language & Usage (ENG) and English Language Learners (ELL). It covers more than 70k questions on numerous topics about English such as grammar, meaning, fluency, and etymology along with answers. Our corpus, ELQA (English Language Questions and Answers), can serve as a tool to facilitate metalinguistic studies. Moreover, since questions in ELQA cover a variety of topics in English, it can be used in the educational and English language learning domains.\nAs the first case study of ELQA, we investigate the performance of current state-of-the-art NLP technology on free-form question answering in the English language domain. Additionally, we explore the possibility of building NLP models that can directly answer questions from language learners. We process a subset of ELQA and make it appropriate for this task. Then, we report on the results of both automatic and human evaluations using different experimental settings of T5 1 and GPT-3 2 models. Although most of these models achieve high ratings for well-formedness, the validity of their answers is significantly lower than that of human-authored answers, indicating that this type of metalinguistic QA task is challenging even for large language models.\nOur main contributions are: 1) we release the first publicly available metalinguistic QA dataset, 3 focused on the English language; 2) we present a taxonomy of questions in the corpus along with analysis; and 3) we investigate to what extent LLMs are able to articulate appropriate generalizations about language in response to these questions.\n\nRelated Work\nStack Exchange is a network of numerous CQA sites (originally and most famously, Stack Over-3 https://github.com/shabnam-b/ELQA flow) built on a common platform. Stack Exchange forums have been featured in a number of previous datasets (Yao et al., 2013; Hoogeveen et al., 2015; Ahmad et al., 2018; Penha et al., 2019; Campos et al., 2020; Kumar and Black, 2020; Rogers et al., 2023) , including the English site (our ENG) along with others such as Ask Ubuntu, Android, Gaming and WordPress (dos Santos et al., 2015; Nakov et al., 2017) . We focus on ENG and ELL as they concern the English language itself; we show that these datasets cover a wide range of metalinguistic questions.\nOur use of these forums contrasts with previous work on metalanguage in corpora, which annotated and quantified mentions (Anderson et al., 2004; Wilson, 2010 Wilson, , 2011 Wilson, , 2012 Wilson, , 2017)) , but did not consider entire questions and answers about language. Taylor (2015) studied metalanguage in online forums, but with a focus on the usage of metalinguistic expressions of mock politeness. More recently, Bogetic (2021) published the first corpus of contemporary Slovene, Croatian and Serbian media metalanguage texts.\nSo far, metalanguage has not been a focus in the QA domain-ours is the first publicly available English metalinguistic QA dataset. Most QA tasks are set up to have a question and a reference document, where the objective is to find the answer based on the document (Fan et al., 2019; Kwiatkowski et al., 2019) . In this paper, we explored a type of \"closed-book\" question answering task (Roberts et al., 2020; Khashabi et al., 2021) . To the best of our knowledge, this task has not been explored to date within the realm of English language questions that require significant generalization and adaptation rather than looking up facts.\n\nConstructing the Dataset\nWe collect our data from two sites on Stack Exchange: English Language & Usage (ENG) 4 and English Language Learners (ELL). 5 Sample screenshots of the site are shown in Figure 1 . The Stack Exchange data is publicly released under the CC-BY-SA 3.0 license. We preprocessed the data until 2021-12-06 collected from the Internet Archive 6 to be suitable for NLP studies and release it as ELQA. Additionally, some cleanup (e.g., removing posts marked as \"spam\" or \"offensive\") was done. Fields for each entry (question) include the title, body, user bio (if available), score (which is calculated based on up-votes and down-votes by other users), tags (user-assigned, related to the area/topic of the question), favorite count, and a list of answers. Textual content (body and user bio) is provided in two formats: HTML and plain text without HTML tags. We release two versions of ELQA based on different preprocessing steps. In ELQA-large, we keep questions as long as they don't include any images (<img> HTML tag) and have an answer with a score of at least 2 (meaning at least two people other than the user posting the answer found it helpful). For ELQA-small, we applied further filtering to ensure that the data has the least amount of noise: a) questions should have a score of at least 4 https://english.stackexchange.com/ 5 https://ell.stackexchange.com/ 6 https://archive.org/ 2 (ensuring questions are clear and coherent), b) question has an answer with a score higher than 3 and c) there are no hyperlinks in at least one of the high-rated answers. The last step reduces noise and facilitates a fair comparison for the closed-book question-answering task ( \u00a74) with model-generated answers, as models cannot be expected to have access to the web to suggest valid URLs compared to humans who would search the web for appropriate resources to include in their answers.\nFor quality assurance, we also did a human annotation on ELQA-small. Two of the authors annotated 250 question and answer pairs for the following: 1) Is the question answerable? and 2) Does the answer fully address the question? We found 99.2% of the questions answerable and 91.8% of the answers acceptable.\nTable 1 contains overall statistics on both versions. Figure 2 shows the distribution of the 10 most common tags in each of the sites. Since users assign these tags to their questions (0 to multiple), similar or near-duplicate tags are common within the collection. Some form more general and more fine-grained variants, e.g. 'meaning' and 'meaningin-context'. In addition to available user-assigned tags, we manually inspected a large subset of the data to identify salient types of questions. These are defined below and illustrated in Table 2 . We then labeled 100 random questions to get a rough estimate of their frequencies (two annotators annotated these 100 samples and they agreed on 92% of cases in an overlapping subset).\n\u2022 Fluency (\u224838% of questions): Usually asking about a particular sentence, comparison of multiple sentences, and/or probing how an expression should be used in general. The user wants to know if X is correct, or to decide between multiple choices, which one is correct. \"Correct\" could mean grammatical, most natural/idiomatic, stylistically appropriate, conveying the intended meaning, etc. In Qs where options are provided by the user, there are cases in which 1) none of the choices are correct, 2) multiple choices are correct, and 3) only one is correct. \u2022 Form to Meaning (Interpretation) (\u224819% of questions): Questions such as \"What does X mean?\" (of an expression in general, or an encountered passage) or \"What's the difference in meaning between X and Y?\". \u2022 Meaning to Form (Encoding) (\u224820% of questions): In these questions, the user gives some As can be seen from the examples in Table 2 , it is common for questions and answers to contain example usages, often visually distinguished with Markdown formatting (such as blockquotes, bullets, and italics) which we retain in the processed corpus markup. Examples can be incorporated into a post in a variety of ways-e.g., asking for an interpretation of one usage, as in the Form to Meaning example in Table 2 , or contrasting multiple usages such as in the following question:\n\nDid VS Have done\nWhat is difference between the following statements: Did you tell your parents yet? Have you told your parents yet? Haven't you told your parents yet? Are these questions correct? why do we use one over another in some cases? What is the difference in meaning?\nUsage examples provided in a question may be instances that the author encountered \"in the wild\" (such as in a novel or film), or in a grammar book or dictionary, or they may have been constructed by the user. Answers sometimes include examples found through a corpus search.\n\nEnglish Language Question Answering\nLarge language models can produce output that is fluent and (at times) informationally adequate when presented with factual questions about entities in the world (Roberts et al., 2020) . But how do such models perform when asked questions about the language itself? In this section, we investigate the free-form English language question answering task.\nThis task has the potential to benefit educational applications for language learners. Research on NLP for educational purposes has investigated tasks such as automated grammatical error correction (Dale et al., 2012; Ng et al., 2014; Bryant et al., 2019; Wang et al., 2021, inter alia) , question and quiz generation for language learning (Sakaguchi et al., 2013; Chinkina and Meurers, 2017; Marrese-Taylor et al., 2018; Vachev et al., 2021) , and automated essay scoring (Burstein, 2003; Farag et al., 2018, inter alia) . Nevertheless, an application that has not been taken up by the educational NLP community is free-form question answering about language. Second language learners possess a degree of metalinguistic awareness about the language they are learning, and often turn to teachers or more advanced speakers with explicit questions about vocabulary, grammar, and usage. Community Question Answering (CQA) websites such as Stack Exchange have sites for language learners' questions and answers. These sites require consid- erable effort by volunteers, and learners may have to wait for an answer-if an answer is provided at all. In fact, looking at the data from 2021-12-06 for ENG and ELL, 9% of questions have no answers.\n\nData\nWe randomly divided ELQA-small into train/test/dev splits. This resulted in 21,175 Q&A pairs in the train split and 3,107 Q&A pairs in each of the dev and test splits. Answers in these splits have a score of at least 4. If there are multiple high-rated answers to a question, we include all of them for training. Some of these questions can be answered by looking at a dictionary or vocabulary list for descriptions. But many of them are explanations in relation to particular instances of language use and require significant reasoning rather than looking up facts. Thus in this setup, we do not have any external context/reference available at evaluation time, i.e. this is a closed-book QA task.\nThe input for the task is Title:\n[Q title] <sep> Body: [Q body].\nWe use the HTML version of ELQA for this task since metalinguistic mentions are usually distinguished via formatting (e.g., blockquotes, bullets) and the ultimate goal is a system that humans can easily use to get answers to their language-related questions.\n\nSetup\nWe use T5 (Raffel et al., 2020; Roberts et al., 2022) and GPT-3 (Brown et al., 2020) as our models since they have been shown to be strong baselines in other QA domains. We believe the questions in ELQA offer new challenges for the QA task since they require different types of knowledge/understanding to be able to generate answers. Addition-ally, these questions contain noise (grammatical errors) and cases of textual metalanguage which is likely harder to comprehend for a model.\nWe fine-tune T5-l and T5-xxl for this task. 7 We saved multiple checkpoints during fine-tuning and evaluated them with the interpolation of BLEU (Papineni et al., 2002) , BERTScore (Zhang et al., 2020) and ROUGE (Lin, 2004) on the dev set to choose the best-performing one (checkpoint at 75k updates, hyperparameters available in Table 8 in the Appendix).\nWith GPT-3 we used text-davinci-003 and experimented with both fine-tuning (FT) on 100 and 1000 samples and a few-shot (FS) setting in which the model is given a few demonstrations of the questions and answers at inference time as conditioning, but no weights are updated (Radford et al., 2019) . In the FS setting, we show the model four Q&A pairs since we wanted the model to see different question types but there were also limits on the input length. To select these 4 pairs, we randomly created 5 different sets of Q&A pairs, evaluated on a subset of dev, and chose the best-performing set for the experiments (dev results available in Appendix, Table 9 ).\n\nAutomatic Evaluation\nResults are shown in Table 3 . GPT-3 FS outperforms all other methods in all metrics with a large margin except for BLEU Score. We also observed that using GPT-3 in a few-shot setup worked much better than the fine-tuned version. Looking at some of the model-generated answers, we noticed that the fine-tuned model tends to generate longer an- swers containing redundant text. We observed improvements when we used 1000 samples instead of 100 for fine-tuning and hence, fine-tuning on larger data might result in better performance, however, we only experimented with 100 and 1000 samples in this paper due to having limited resources.\nBased on Table 3 , T5-xxl seems to perform similarly to GPT-3 FT-1000. However, a small manual evaluation showed otherwise (GPT-3 FT-1000 answers were slightly better). Furthermore, we observe that the scores for even the best system are very low, but manual evaluations showed that the GPT-3 FS generates fairly good answers in many cases. Due to these observations and also given the well-known limitations of automatic metrics for evaluating generation tasks (Kasai et al., 2022; Celikyilmaz et al., 2020; Bhakthavatsalam et al., 2021) , we believe conducting human evaluation for deeper analysis is necessary for this task.\nIn Table 4 , we show results for each site to see if one is more challenging than the other. Overall, models perform slightly better on ELL based on automatic metrics-but we see in the next section (Table 5 ) that there isn't really a meaningful difference between the sites when humans evaluate the answers.\n\nHuman Evaluation\nHuman evaluators were presented with the question title and body, and then asked to rate 5 answers: a top-rated human-provided answer, a low-rated human-provided answer, and answers generated by 3 of our best models: GPT-3 FS, GPT3 FT-1000, T5-xxl.\nThey were asked to give ratings (via a slider widget, on a 1-5 integer scale-the higher, the better) for two criteria (C1 & C2): 8 1. Does the answer look grammatically/ structurally like a good answer (ignoring whether it answers the question)? 2. Is the information in this answer a valid response to the question (ignoring formatting/ stylistic issues)?\nThe first criterion aims to get a score for fluency and coherence and the second one for correctness and completeness.\nWe collected ratings for a set of 75 questions (375 different answers). Each question with its set of answers was evaluated by at least 2 raters, and then the average score was calculated based on their responses. 9 We also report the average z-score which is calculated over each annotator's raw score distribution for each metric, intended to normalize interannotator variation in how the scale is interpreted for each of the two metrics (details in Appendix B).\nThe results of this study are shown in Table 5 . Overall, answers generated by GPT-3 FS have a small gap with human answers in both C1 and C2. 10 GPT-3 FT-1000 comes next, with less accurate answers containing redundant text and hallucinations. The smallest model, T5-xxl, ranks last.\nRankings based on human evaluations are available in Table 6 . These results are also indicating that model-generated answers are fluent in most cases, but they are not as good as human answers Title Question Explain the verb tense in \"All that glitters is not gold\" I am confused by the maxim All that glitters is not gold. May I know why the word is is used there? In my known according the English grammar glitters is plural, So after plural we add verb+s. As in above example use auxiliary verb so why not arewhy is? Source Answer Top-rated human Good question, but it's worth pointing out that glitters in this context is a verb, not a noun. The subject of the sentence is all. When all is used as the subject of a sentence, it functions as a pronoun, and it can function as a singular or a plural pronoun.\nFrom NOAD <blockquote> all (pronoun) [with clause ] the only thing (used for emphasis) : all I want is to be left alone.\n(used to refer to surroundings or a situation in general) everything : all was well | it was all very strange. </blockquote> So: <blockquote> All that glitters is not gold </blockquote> is roughly equivalent to <blockquote> Everything that glitters is not gold </blockquote> and the verb tense agrees. but: <blockquote> All who eat are not gluttons </blockquote> is roughly equivalent to: <blockquote> All the people who eat are not gluttons </blockquote> so there is no problem there, either.\n\nLow-rated human\nGrammatically is is used with third person singular and also s form of verb is used. \"All\" indicates a third person single group.\n\nGPT-3 FS\nThe phrase all that glitters is not gold is an idiom, which means that things that appear to be valuable may not actually be so. It is an old proverb, and the verb tense used in the phrase is an example of the so-called \"historic present\". The historic present is a literary device used to give a sense of immediacy to a narrative. It is used to describe events that happened in the past, but are being described as if they are happening in the present. In this case, the phrase is used to emphasize the timelessness of the proverb. So, while the verb glitters is indeed plural, the verb is is used in the historic present to give the phrase a sense of immediacy and timelessness. GPT-3 FT-1000\nThe maxim is in the present tense, so the verb is also in the present tense: Glitters. The past tense of glitter is glittered, and the past participle is glittered. So if the maxim was in the past tense, it would be: <blockquote> All that glittered was not gold. </blockquote> The present tense maxim is a generalisation about something that is true in the speaker's experience. The past tense maxim would be a historical statement about what was true in the past.\n\nT5-xxl\nThis is an interesting question, because it is not immediately obvious what the grammar rules are here. It is not a fixed phrase, and so subject-verb agreement must be based on the context. In this case, the that is a prepositional phrase acting as a complement to the preposition all. It could be rephrased as <blockquote> All that glitters is not gold </blockquote> It is true that all means \"everything that glitters\" -but it is equally true that not everything that glitters means \"not everything that glitters is gold\". It is much the same as <blockquote> Everything that glitters is not gold. </blockquote> The subject all is singular, so the verb must be singular as well -is. If it was twice plural, then are would be used, but it is not. It is interesting to note that Shakespeare used the same construction in Hamlet: <blockquote> Everything that glitters is not gold. </blockquote> The subject that is singular, and so the auxiliary verb is agrees with the singular noun. when correctness/completeness is considered (GPT-3 FS is ranked first or as good as a top-rated human answer in only 45% of cases).\nFor each criterion and Top-rated human, Lowrated human and GPT-3 FS, histograms of the average score of the two annotators are plotted in Figure 3 . We can observe that GPT-3 FS and Lowrated human have very similar numbers of highscoring answers (human evaluation scores), but the number of low-scoring human answers drops off gradually as quality decreases, while the distribution is more spread out for GPT-3 FS. I.e., the model has some moderately bad answers as well as some good ones, whereas Low-rated human answers cluster more on the upper end of the scale. C1 (fluency/coherence). All models generated fairly fluent and well-structured answers. We even notice that GPT-3 FS scores higher in wellformedness than human answers. We looked at those samples and we believe there are two main reasons for this: 1) Some human answers were very long, containing multiple different quotes from different sources. On average, our evaluators preferred the structure of answers from GPT-3 FS, which took the form of a short paragraph addressing the question. 2) Some human answers have a more casual/conversational nature. On the other hand, GPT-3 FS generated more authoritativesounding, teacher-like answers with complete sentences, which were preferred by our evaluators in some cases. C2 (correctness/completeness). On average, models are worse on this metric, though sometimes they did produce acceptable answers (perhaps because variants of the question are commonly discussed on the web).\nOne challenge in this domain is that questions, particularly from ELL, may not be fully fluent. In the Fluency example from Table 7 we see that there are some grammatical errors in the question that are unrelated to the topic of the question. In addition, the questioner uses incorrect terminology, mentioning verb tense in the post title even though the question is actually about subject-verb agreement with respect to number (as can be inferred from the examples). While the good human response correctly flags the incorrect terminology and answers the underlying question, GPT-3 models give irrelevant answers about tense.\nAnother correctness failure from GPT-3 FS can be seen in the following pair involving a Meaningto-Form question:\nWhat is the analog equivalent of 'digitally'? In electronics, we can solve a problem digitally or analoguely ? That doesn't sound right, but surely there must be a word I can use? Update: some definitions: digitally: Relating to or being a service that provides information expressed in discrete numerical form. analoguely: Relating to or being a service that provides information expressed in continuous, non-discrete form. (This is my made up definition of a word I made up). analogously: Similar or alike in such a way as to permit the drawing of an analogy. As you can see, the word analogously doesn't mean the opposite of digitally, it means something else. Answer: The analog equivalent of 'digitally' is 'analogously'. Analogously means \"in a similar way\" or \"by comparison\". It is used to describe a situation where two things are similar in some way, but not necessarily opposite. For example, you could say that a digital signal is analogous to an analog signal, because they are both signals, but they are different in terms of how they are represented.\nWe see that the model begins by ignoring the user's explanation that analogously does not have the desired meaning, and suggests it anyway. The rest of the model's answer gives a (valid) general definition of analogously and clumsily attempts to apply it to the two kinds of signals, which is not responsive to the question. It may be the case that models particularly struggle with questions for which there is no straightforward solution (in this case, no adverb derived from analogue has the user's intended meaning).\n\nMetrics\nOur human evaluations found that the high-rated human answers are the best, and GPT-3 FS is roughly on par with the low-rated human answer, while other models are worse.\nAs noted in \u00a74.3.1, we did not find the automatic metrics very informative. We believe this is due to the long and free-form nature of this task: concepts can be explained in different ways and answers can include different examples which make automatic evaluations very challenging. For example, for the question Why is this sentence: \"Additional nine features were added. . . \" incorrect? a human answer is:\nIt has to do with the order of the adjectives. For example, consider this sentence: <blockquote> Happy nine men walk into a bar. </blockquote> Both nine and happy are adjectives, but we are really intending nine to describe the happy men, not happy describing the nine men. I don't .... Model-generated answers mostly mirror the examples in the question. But answers given by humans mention new examples-which are not in the question (and therefore unlikely to come up in other answers). Thus a good answer may mention superficially off-topic terms metalinguistically, which presents evaluation challenges unlike those of summarization or MT, for example (Reiter, 2018; Celikyilmaz et al., 2020) . For QA tasks in metalinguistic domains such as ours, caution is warranted when applying automatic metrics that are unaware of the use/mention distinction. Human evaluation is likely to be more illuminating. We release our full human evaluation data to enable detailed comparisons in future research.\n\nQualitative Analysis\nWe examined behavioral patterns for the best model, GPT-3 FS, organized by question type, with observations as follows. Interpretation and Encoding: These are the categories where the model performs the best, especially if the question has a straightforward answer (for example, there aren't several possible answers based on context). Topics in these categories often center on definitions of vocabulary and idioms. These questions may involve less reasoning to answer, and models may be leveraging content from the online dictionaries and thesauri in their web training data.\nGrammatical Analysis: When asked about general concepts and rules of grammar, the modelgenerated answers are usually acceptable:\n-ed and -ing adjectives What is the difference? <blockquote> I am interesting in mathematics. </blockquote> versus <blockquote> I am interested in mathematics. </blockquote> Murphy intermediate grammar Unit 97 tells me that I cannot use the former. Extended and general explanations will be appreciated. Answer: The difference between -ed and -ing adjectives is that the -ed adjectives describe a feeling or emotion that is felt by the subject, while the -ing adjectives describe a quality of the subject itself. In the example sentence, interested is the correct word to use because it describes the feeling of interest that the subject has in mathematics. Interesting would be incorrect because it describes the quality of the subject itself, not the feeling of the subject. For example, if you were to say \"This book is interesting,\" you would be describing the book itself, not the feeling you have towards the book. On the other hand, if you were to say \"I am interested in this book,\" you would be describing the feeling you have towards the book.\nHowever, when questions become more specific to examples, sometimes the model struggles:\nWhat does \"in\" refer to? <blockquote> A symbiotic relationship is an interaction between two or more species in which one species lives in or on another species. Answer: It refers to the other species. The symbiotic species lives in the other species.\nThe example in the question uses the word in twice. Rather than addressing the first token, which the questioner has bolded, the answer addresses the second one (denoting the physical relationship between one species and another). Fluency: The model correctly answers many of the fluency questions. Others require reasoning and understanding of context, and the model struggles to generate a correct answer. For example:\nReturning for the nth time Suppose I went to the supermarket three times. Is my third trip considered my \"second time returning there\" or my \"third time returning there?\" Thanks. (I know this question sounds silly, but English is my second language...) Answer: Your third trip to the supermarket would be considered your third time returning there. The phrase returning for the nth time is used to refer to the nth time you have returned to a place, not the n-1th time.\nWe also observe that when the correct usage of a case is not mentioned in the question itself, the model sometimes suggests incorrect corrections and wrong explanations:\nWhy is this usage of \"to\" incorrect? I am in communication with a friend who is learn-ing English. I noticed her using \"to\" in the wrong context, but I'm unable to explain which rule she is breaking. <blockquote> I can't access to Gmail. </blockquote> Answer: In this sentence, the word to is incorrect because it is being used as a preposition. The correct word to use in this context is for. <block-quote> I can't access for Gmail. </blockquote> The word to is used as a preposition when it is followed by a verb, such as go, come, run, etc. For example: <blockquote> I am going to the store. </blockquote> However, when to is followed by a noun, it is usually incorrect. In this case, the correct word to use is for. <blockquote> I need a gift for my friend.</blockquote>\n\nOutlook\nOverall, we were surprised by the quality of many of the answers from GPT-3 FS: many would likely have received high user ratings if submitted as answers on the site. At the same time, the model is not to the point where we would want to trust its answers without human review. We believe that answer confidence estimation-so that users can be shown only the best model-generated answers-is an important direction for using learner QA models in practice (Jiang et al., 2021) .\n\nConclusion\nWe presented ELQA, a dataset containing metalinguistic questions and answers about the English language. We provided analysis and a taxonomy of the data, along with experiments on free-form answer generation and investigated the extent to which language models can articulate their generalizations about language. Since many of the questions in ELQA were asked by language learners, it forms a potentially useful and so far untapped resource for educational NLP purposes and metalinguistic question answering. We release the dataset to enable further studies of this task.\n"}
{"question": "How do papyrologists date papyri when direct or indirect dating is not possible?", "evidence": "  When neither direct or indirect dating is possible, papyrologists resort to palaeography, the study of the script. In palaeography, particular writing styles are associated with certain chronological periods. ", "options": ["A. They use carbon dating techniques.", "B. They use the presence of known historical figures in the text.", "C. They rely on palaeography, the study of the script.", "D. They use machine learning algorithms."], "answer": "C", "content": "\nIntroduction\nAncient textual artefacts are arguably the richest source of information on the ancient world. In the Graeco-Roman world and particularly in its Greekspeaking part, the most extensive coeval texts come from inscriptions and papyri. The latter is a collective term used for all ancient manuscripts, regardless of their writing material which, apart from papyrus, may be parchment, pottery, wood, and others. To correctly evaluate and make good use of these texts, we need to determine their date, provenance and historical context of their production and use. As far as dating is concerned, the value of the relevant evidence provided by the artefacts themselves varies considerably, ranging from a direct date in the text (following, of course, the calendar and dating system of the respective historical period) to no evidence at all. In between, there are texts containing references to known historical figures and events of a certain period, papyri which have been found next to other objects that can be dated, or other indirect evidence. The presence or absence of a date depends on the type of text preserved on the papyrus and its use through time, as well as on its state of conservation. Just like in modern times, it is much more likely to include a date in an official letter than in a page torn from a novel book. At the same time, it is more probable to find a date in a fully surviving letter than in a damaged one missing, for instance, the upper part of the first page.\nGreek papyri, which mostly survive in fragments, are divided into two broad categories: books (literary and sub-literary papyri) and documents of all kinds (documentary papyri). The former ones never carry a date, whereas the latter often do, albeit not always unambiguously convertible by modern scholars. Most importantly for our study, literary papyri contain copies of works authored many years (often centuries) before the production of the actual manuscripts. On the other hand, documentary texts were usually written down as they were composed or shortly after that, making the content of their texts contemporary to their writing style or script. Therefore, any temporal indication in the text is also dating evidence regarding the production of the document. Even when there is no direct date in the text (e.g. Figure 1 ), documentary papyri can be dated securely sometimes within a short time-frame, because they may refer to known historical events or concern people known through other sources to have lived at a particular time.\nWhen neither direct or indirect dating is possible, papyrologists resort to palaeography, the study of the script. In palaeography, particular writing styles are associated with certain chronological periods. Therefore, similar writing styles point to similar dates (Mazza, 2019) . Securely dated specimens are used as a guide to chronologically place the undated ones. Growing criticism on the subjectivity of palaeographical dating (Mazza, 2019; Choat, 2019; Nongbri, 2019 Nongbri, , 2014) ) highlights the need for more reliable methods. Recent efforts for computational dating of historical manuscripts are based on the script rather than the text and, although they consider various languages, they disregard Greek (Omayio et al., 2022) .\nIn this study we focus on computational dating of Greek documentary papyri based on their transcriptions, contributing in the following three ways:\n1. We present and publicly release a machineactionable dataset of 389 documentary Greek papyri, containing texts of various aspects of daily life (e.g. contracts, receipts, letters).\n2. We draw the baseline in text regression for the tasks of dating experimenting with Monte Carlo and leave one out cross validation.\n3. We apply a committee of regressors to three papyri, which present different types of dating challenges, and on 159 manuscripts for which only the upper date limit is known.\nThis approach does not apply to literary papyri and our research involves solely documents. Apart from their texts being contemporary with the actual manuscripts (by dating the text, we date the papyrus), nonliterary papyri also include vastly more numerous objectively dated specimens than literary ones. Specific dates on our training set also allow for more accurate (narrower date-spans) predictions by our models.\n\nRelated Work\nDating historical documents with computational means has been studied for many languages (Baledent et al., 2020; Dhali et al., 2020; Li et al., 2015; Hamid et al., 2019; Adam et al., 2018) . However, very limited work has been done for Greek and no published work at all has focused on Greek papyri. The only work to our knowledge is Ithaca, a Transformer trained on ancient Greek inscriptions performing text restoration, geographical attribution, and dating (Assael et al., 2022) . Ithaca has achieved an error of 0.29 centuries in dating epigraphs. This result is by far better than an onomastic baseline using the known distribution of Greek personal names to infer the date, which scored 1.44. Inscriptions differ from papyri in many aspects (such as the genre, the length, and their geographical distribution), but in principle, this system is applicable to our data and was therefore used as a baseline. Below, given the absence of dating studies for Greek, we summarise work for other languages.\nThe studied languages are Latin (Baledent et al., 2020; Wahlberg et al., 2016 Wahlberg et al., , 2015)) , Hebrew (Dhali et al., 2020) , Dutch (Hamid et al., 2019 (Hamid et al., , 2018;; He et al., 2014 He et al., , 2016b,a),a) , Arabic (Adam et al., 2018) , Swedish (Wahlberg et al., 2016 (Wahlberg et al., , 2015)) , French (Baledent et al., 2020) and English (Li et al., 2015; Rastas et al., 2022) . A collection of 595 Dead Sea Scrolls, in Aramaic script, was the dataset with the oldest manuscripts, dated from 250 to 135 BCE, and the only one so far concerning texts written on papyri (Dhali et al., 2020) . The rest of the datasets comprised more data, ranging from less than five (Adam et al., 2018) to more than ten thousand manuscripts (Wahlberg et al., 2015) or more (Rastas et al., 2022) , while the one with the most recent manuscripts comprises historical English-language documents (Li et al., 2015) , printed between the 15th and 19th CE.\nThe employed methods usually were standard machine learning methods, such as KNN (Adam et al., 2018) , decision trees (Baledent et al., 2020) , random forests (Baledent et al., 2020) and support vector machines (Hamid et al., 2019; Dhali et al., 2020; He et al., 2014 He et al., , 2016b,a),a) . Textural features, such as Gabor filters, Uniform Local Binary Patterns and Histogram of Local Binary Patterns are extracted and then fed to the classifiers (Hamid et al., 2018) . The writing style evolution, however, has also been used as an intermediate step (Dhali et al., 2020; Adam et al., 2018) . In this case, the periods are first aligned with specific writing styles. Then, any new manuscript is dated based on the detected style.\nPre-trained convolutional neural networks have been used to extract features, which are passed to a classifier or regressor (Hamid et al., 2019; Wahlberg et al., 2016) , or used in combination with text features extracted with optical character recognition methods (Li et al., 2015) . Transfer learning has been reported to lead to human performance (Wahlberg et al., 2016) . This was deemed to be the most promising direction for the present study on Greek manuscripts, and was, hence, employed.\n\nData\nOur dataset, which we release publicly, 1 comprises the transcriptions of 389 manuscripts, dated from the 3rd century BCE to the 7th century CE, originating from Greco-Roman Egypt (with a few exceptions from the Near-East).\n\nThe source\nThe dataset was compiled mainly from PA-PYRI. INFO. 2 The documents in its collections set a reliable point of reference for scholars who aspire to study the evolution of ancient manuscripts in time. These collections incorporate full transcriptions and references to scholarly editions of the papyri, as well as a set of metadata that can also assist in dating (e.g. provenance).\n\nThe scripts and the language\nNonliterary papyri in Greek from the 3rd c. BCE to the 7th c. CE are written in a great variety of cursive hands (Harrauer, 2010) , posing an extra challenge for image classification methods and calling for other approaches. The language of the papyri, Greek of the Ptolemaic, Roman and early Byzantine periods, reflects the diversity and the diachronic changes of the Greek-speaking communities in Egypt, which is the provenance of most of our specimens.\n\nThe ground truth\nThe date of a manuscript may be found in different forms. It can be an exact date, a range of years, a starting date (not before that date), or an ending date (not after that date), or two-three alternative dates. Our dataset has been curated so that dating applies at the level of the quarter of the century, by considering manuscripts dated exactly or with a period ranging within that quarter. We did not consider manuscripts that were dated only before or after a specific date.\n\nData collection\nOur first dataset comprised 400 manuscripts, 40 samples per century. Our initial pool consisted of 77,040 items and we opted for ones that satisfy the following conditions:\n\u2022 The transcriptions must be available in machine actionable form.\n\u2022 The papyri must contain documents (not works of literature) to ensure that text and papyrus are contemporary. 3\n\u2022 The papyri must be securely and accurately dated. Many papyri do not carry a date and are, therefore, dated with subjective criteria or with a large date span (e.g. 1st-2ndCE).\n\u2022 The image is available, to allow image-based dating and potentially jointly from different modalities: text and image.\nGiven these limitations, it was the 7thCE that dictated the size per century of a balanced dataset, since there are not more than 40 securely dated papyri from 7thCE. For each of these records, the text was retrieved afterwards from PAPYRI.INFO by parsing the respective XML files. We discarded records whose extracted text was less than ten characters, which resulted in our final 389 records. From these records, we extracted the entire text from one side of the papyrus (the side that had more text than the other). In the few cases of papyri with more than one fragment, we only included the first one. This decision was based on weighing the benefit of avoiding a considerable amount of noise during automatic parsing against eliminating a portion of text, in a dataset whose nature is by definition fragmentary.\n\nNormalisation\nThe transcribed text comprises a variety of characters and symbols. We preprocessed the data by lowercasing and normalising the text (see Table 1 ). We (o) '\u1f45', '\u1f43', '\u03cc', '\u03cc', '\u1f44', '\u1f41', '\u1f40', '\u1f78' (\u03b1) '\u1f02', '\u1fb4', '\u1f03', '\u1f85', '\u03ac', '\u1f01', '\u1f04', '\u1fb6', '\u1f00', '\u1fb7', '\u1f70', '\u03ac', '\u1f05' (\u03b7) '\u1f24', '\u1f23', '\u1fc3', '\u1f22', '\u1f20', '\u03ae', '\u1f26', '\u1f25', '\u1fc6', '\u1f27', '\u1f97', '\u1f94', '\u1fc7', '\u1f21', '\u1fc4', '\u1f91', '\u1f74', '\u03ae' (\u03b9) '\u03af', '\u1fd6', '\u1f37', '\u1f31', '\u1f36', '\u1fd2', '\u03af', '\u1f30', '\u1f76', '\u0390', '\u1f34', '\u03b9', '\u03ca', '\u1f33', '\u1f35' (\u03b5) '\u03ad', '\u1f72', '\u03ad', '\u1f14', '\u1f10', '\u1f15', '\u1f11', '\u03b5' (\u03c5) '\u1f57', '\u03cd', '\u1fe6', '\u03cd', '\u1f55', '\u1f53', '\u1f7a', '\u1f51', '\u1f54', '\u1f50', '\u1f56' (\u03c1) '\u1fe5', '\u1fe4' (\u03c9) '\u03ce', '\u1ff6', '\u1f66', '\u1fa4', '\u1f60', '\u1f67', '\u1ff3', '\u1ff7', '\u1f62', '\u1f65', '\u03ce', '\u1fa7', '\u03ce', '\u1fa6', '\u1f64', '\u1fa0', '\u1f7c', '\u1f61', '\u1ff4' (\u03c3) '\u03c3', '\u03c2' Table 1 : Normalisation rules of characters in the dataset, all characters on the right have been replaced by the character on the left. also discarded any character besides the 24 Greek letters, also removing white space and all punctuation marks. We did not eliminate the editors' corrections and supplements nor edit otherwise the data, which often led to duplicate words with alternative orthography (original and normalisation).\nThe transcriptions available are not diplomatic (reflecting exactly what is written) but normalised according to modern conventions, for example as far as punctuation and word separation (or sometimes spelling) are concerned. Therefore, we chose to disregard these conventions, because they do not represent data present in our sources, but normalisation on the papyrologists' part for the purpose of scholarly editions.\nTo provide some more concrete examples, there is no capitalization of proper names or initial words in sentences in papyri. Punctuation is very scarce and sometimes completely absent. Diacritics are not meaningless, but they are extremely rare in documentary papyri (i.e., except diaresis which is used in a different way than modern conventions, to mark iota and upsilon as the first letter of a word). Breathings and accents are marked inconsistently (if at all) by different scribes. Hence, removing diacritics leads to inclusion and can help avoid multiple variations of what is in fact the same word. Regarding spelling, we kept both the original and the corrected form (if provided by the editors), because spelling mistakes reflect language evolution.\n\nExploratory analysis\nThe overall text length per quarter of century varies over time, as can be seen in Figure 2 . Although we have selected an equal number of manuscripts per century ( \u00a73.4), the number of lines within each manuscript varies, and so does the line length. Furthermore, within a century, manuscripts of a spe- cific quarter of a century may be more frequent due to random discoveries, as is the case of 7thCE, where the first quarter holds most of the support, a discrepancy deriving from the reduced number of dated papyri in this century overall.\nThe most frequent character in our dataset is '\u03b1' (35,101 occurrences), followed by '\u03bf' (33,176), '\u03b9' (30,151), and '\u03b5' (25,116) . On the other hand, the least common are '\u03b2' (2520), '\u03be' (1210), '\u03b6' (379), and '\u03c8' (334). These figures are coherent with general frequencies of letters in Ancient and Modern Greek (Mikros et al., 2005) .\nIn order to assess the quality of the ground truth, we employed the Callimachus' Conservation number (CCN), 4 which provides an educated estimation of the preservation and legibility of a papyrus. The lowest score is 0 and the highest score (i.e., 1) indicates readability and 'perfect' conservation of the text. The status of the conservation of a papyrus affects the quality of the transcription, indicating the amount of text that has not been recorded in the transcriptions (or recorded with some level of uncertainty) because of the material state of preservation of the manuscripts. Damage in papyri could affect as little as one or two letters (or even none), to as much as several lines and whole parts of the \n\nMethodology\nTo estimate the date of production of manuscripts, we opted for text regression, taking advantage of the continuous target objective. Statistical validity was established with 5-fold Monte Carlo crossvalidation. The best regression method was used to form a committee of models, which were applied on unseen data in order to analyse the predictions.\n\nBenchmarking\nWe performed Monte Carlo cross-validation, by sampling 90% for training, 10% for validation, and then re-sampling with replacement five times. We report the mean absolute error (MAE), the mean squared error (MSE), and the explained variance (R 2 ). Besides the average results across folds, we also report the best score achieved per metric.\n\nRegression methods\nFern\u00e1ndez-Delgado et al. ( 2019) surveyed 77 regression methods and undertook an experimental analysis on 83 datasets. Regression with extremely randomised trees achieved the best R 2 in many datasets but gradient boosting and random forests were also found to have a promising performance. Following these findings, we opted for extremely randomised trees, random forests, gradient boosting, and linear regression for our experiments. 5 Extremely randomised trees (XTrees) is a treebased ensemble, created with the Extra-Trees algorithm (Geurts et al., 2006) . Although simple in nature, it is both accurate and efficient Fern\u00e1ndez-Delgado et al. (2019) . Compared to other ensembles that use decision trees, XTrees splits the nodes of the tree by choosing randomly cut-off points and the trees grow by using the whole sample to learn instead of bootstrapping.\n\nThe Committee\nUsing the best-performing regression method out of the ones examined, we performed leave one out cross-validation, which allowed an evaluation using the whole dataset. Furthermore, it yielded as many regressors as the data points, which in our case is 389. We used these models to form a committee and date unseen papyri (further discussed in \u00a76).\n\nEmpirical analysis\nThis section presents our experimental results using regression on textual features to date Greek manuscripts. First, we present preliminary experiments and then we analyse the experimental findings from our regression analysis.\n\nPreliminary experiments\nPreliminary experiments comprised image classification (Hamid et al., 2018 ), text classification with Transformers trained on another domain (Assael et al., 2022) , and transferring learning from large language models (Koutsikakis et al., 2020) . Image classification was used prior to using transcribed text as our input, experimenting with using the documents' images (Hamid et al., 2018; Wahlberg et al., 2016; Paparigopoulou et al., 2022) . Vanilla convolutional neural networks were outperformed by a pre-trained one (Tan and Le, 2019), fine-tuned for our dating task. Our estimated MAE, however, was consistently more than a hundred years (Table 2 ), hence we opted for textual input. Ithaca was presented by Assael et al. (2022) , consisting of a Transformer that is trained not only in dating but also in text restoration and geographical attribution. Ithaca has achieved an error of 0.29 centuries in dating inscriptions, which is by far better than an onomastics baseline (error of 144 years).\nBy using the open-access web interface, 6 we scored all our preprocessed texts, 7 registering a MAE of approx. one century by using the maximum decade predicted or the average of the distribution (Table 2). The difference from the published result possibly stems from the fact that this is a model trained and focused on inscriptions, not papyri. Transfer learning was used with GreekBERT, a Transformer that is pre-trained in masked language modelling, among other tasks, in modern Greek (Koutsikakis et al., 2020) . GreekBERT has been further pre-trained in ancient Greek (Singh et al., 2021) . We experimented with fine-tuning both variants in predicting the date, 8 but MAE was approx. one century (Table 2 ).\n\nRegression analysis\nExperiments were undertaken with Google Colaboratory, using a 12GB NVIDIA Tesla K80 GPU. We extracted term-frequency-inverse-documentfrequency features using lower-cased text and character n-grams (from 1-to 5-grams). 9 All other parameters were set to default values. 10\n\nMonte Carlo cross validation\nLinear regression achieved a MAE of 86 years on average (Table 2 ) and a MSE of 1.33. R 2 was similar across folds, around 83. A random forest had an even better MAE of 73 years on average but a worse MSE (1.58). Its average R 2 was lower than that of linear regression, but the maximum one achieved across folds was much better. Random forest also outperformed both gradient boosting methods in MAE but GBoost achieved a better MSE and R 2 on average. XTrees achieved the best results in all metrics, with a MAE of 54 years and the best R 2 climbing up to 95.43.\n\nLeave one out cross validation\nUsing the best performing XTrees, we performed leave one out cross validation, by hiding one instance, training the algorithm on the remaining instances, and then using the model to predict the hidden record. 11 The MAE was found to be 55 years, MSE was 1.11, and R 2 was 85.89, close to the Monte Carlo evaluation scores. In order to better understand the errors, we rounded the predictions and the ground truth, evaluating as if we would in a classification setting. Predictions most often fall on or close to the diagonal (Figure 4 ), which explains the low error. The best result is 8 We used white space, to allow subword computation. 9 Preliminary experiments with centroid or trainable word embeddings before recurrent or convolutional neural networks deteriorated performance.\n10 Manual hyper-parameter tuning per regressor yielded insignificant improvements.\n11 The experiment lasted 15 hours. 3 .\nachieved for the 1st and 2nd CE, followed by the 7th CE (see Table 3 ). The overall accuracy is 60%.\n\nError analysis\nIn very few cases, our leave-one-out regression fell considerably out of its predictions (Figure 4 ). Our analysis showed that these texts happen to contain specific words typical of another period, which confused the prediction. For instance among the highest prediction error were two late texts (6-7thCE) that exceptionally contain \u03a3\u03b5\u03c1\u03b1\u03c0\u03af\u03bf\u03c5 and \u0392\u03b1\u03c3\u03b9\u03bb\u03b5\u03af\u03bf\u03c5, usually found in Ptolemaic time (3rd-1stBCE). In another case, we provided experimentally the longer version of the text, initially parsed only partially ( \u00a73.4). Using the full text led to an accurate prediction, influenced by the word 'indiction' in the additional text ( \u00a77.1).\n\nUse cases\nWe applied our 389 regressors, produced upon leave-one-out cross-validation, to three use cases, which present different types of dating challenges.\n\nPSI 8 934\nThis document 12 preserves the ca. 15 last lines of a land lease. texts from the 6th and early 7thCE c., the Dioscorus archive (Fournet, 2008) , because, among other concordant elements, it contains microtoponyms from the respective village countryside. The notary who signed the contract, Abraam, is known from other documents, which is crucial evidence for the dating of the papyrus. This notary's period of activity has been proven to span at least between 524 and 545 (Fournet, 2003) . This papyrus, therefore, is securely dated by indirect evidence, but no date is explicitly mentioned in the text (Fournet, 2008) . Our average prediction is 310 CE, dated between 260 CE (min) and 352 CE (maximum prediction).\n\nP. Basel 2 15\nThis papyrus, also shown in Figure 1 , is a private letter dated indirectly from the 1st CE. The letter is almost complete, except for a damaged word at the end of line 5. Private letters usually do not bear a date. The dating, therefore, by the editor is done on palaeographical grounds as well as on the basis of scribal habits: \"the hand [...] is more at home in the first century CE than the second, a dating that is supported by the writer's use of iota adscript...\" (Huebner et al., 2020) . Iota adscript is an expected feature in the 3rd BCE, starting to be irregularly written between the 2nd BCE and the first CE to almost completely disappear from the 2nd CE onwards (Clarysse, 1976) . Onomastics strengthen the editor's dating hypothesis: of the three personal names mentioned in the letter (Pasis, Orsenouphis, and Tithoes), the first two are attested from ca. 250 BCE to 250 CE while the last one starts appearing in the papyri only in the 1st c. CE. 13 Our models date this to 140 BCE, from 165 BCE to 112 BCE.\n\nP. Petra 1 5\nThe last manuscript 14 contains a request for transfer of taxation from 538 CE. It is a geographical outsider since it does not come from Egypt but from Petra (Jordan). We tested this manuscript since many of the words found in the text are infrequent in Egyptian manuscripts, on which our models are trained. The date mentioned in the papyrus is \"second indiction\". This refers to the second year of a repeated fifteen-year cycle (indiction) and the year 538 is relative, since it could be the second year of the previous or the next indiction (523 or 553). 538 is logically deduced by the editors in view of the whole dossier of papyri from Petra. Our models date this manuscript to 555 CE (521-575 CE), overcoming the geographical variation.\n\nDiscussion\nThe computational, quantitative method suggested in this work is intended to complement human expertise. Its main contribution lies in providing an additional dating criterion for ancient Greek documents, in addition to the ones usually employed by papyrologists (palaeography, onomastics, prosopography, toponymy, archaeological evidence, etc.). It can predict a date for those papyri that do not include one, narrow down the possible time-span of doubtful dating, or contribute to deciding on one particular date when several alternatives seem possible. Despite the fact that limitations exist (discussed in \u00a77.3), compared to traditional approaches the models trained in this study are expected to reduce biases. Their value is not limited to predicting dates for individual manuscripts, but they can be applied to any attribute of a group of papyri, e.g. the place of provenance or the text's type. At the same time, easily accessible open-source metadata exist for most published papyri ( \u00a73.1).\n\nRationale generation\nThe use of supervised learning, such as the work of Assael et al. (2022) or ours, can yield accurate estimations, which can at least help the human expert. The assistance is greater, however, when explanations are provided for the models' decisions. In our case, we used a committee of hundreds of regressors in order to estimate the date of three use cases. Therefore, we sampled models per case and generated rationales regarding their predictions, by using their Shapley values (Lundberg and Lee, 2017).\nIn the case of PSI 8 934 ( \u00a76.1), our investigation showed that the mention of the name 'Aurelios Victor' ('\u0391\u1f50\u03c1\u03ae\u03bb\u03b9\u03bf\u03c2 \u0392\u03af\u03ba\u03c4\u03c9\u03c1') influenced the decision, resulting to a more recent date than what would have been predicted otherwise. Similarly, in the case of P. Petra 1 5 ( \u00a76.3), the decision was influenced by a reference to 'indiction' ('\u1f30\u03bd\u03b4\u03b9\u03ba\u03c4\u03af\u03c9\u03bd\u03bf\u03c2'), a word that refers to a periodic reassessment of taxation in the Late Roman Empire.\n\nIn the wild\nComputational dating can facilitate a macroscopic analysis of vaguely dated or undated manuscripts. By generating estimated dates for hundreds of such manuscripts, the expert can view from distance the collection, potentially drawing useful conclu-sions or making significant remarks. To test this hypothesis, we collected 220 manuscripts dated with an upper CE date limit (i.e., not after that date). We formed a committee of regressors, 15 and we estimated the minimum, the maximum, and the average chronology of each manuscript. In 28% of them, the maximum prediction exceeded the upper threshold and was discarded to avoid doubting the expert. This process led to the date estimation for 159 manuscripts, which we release publicly in our repository to assist other researchers. As can be seen in Figure 5 , some of our estimations fall far away from the upper limit (in red) while others fall close. The estimated date from our regressors' committee should be read along with other information, which is kept in the shared corpus, such as the place settlement (Figure 6 shows frequent places). We observe, for example, that in some places the estimated dates fall closer to the upper limit (e.g. in Oxyrhynchos and Tebtynis the distance is 132 years) compared to others (e.g. in Antinoopolis and Hermopolis the distance is 283 and 384 years).\n\nChallenges and limitations\nOur experimental analysis proved that text regression is a considerably reliable and accurate tool in dating nonliterary papyri. Limitations and challenges stem mainly from the composition of our dataset, which is balanced as far as the dates of the papyri included are concerned, both at the level of the century (approx. 40 records per century) and at the level of the quarter of the century (albeit less strictly and with the exception of the 7th CE). Furthermore, although we retained a substantial text sample of each papyrus, in approximately 1/4 of the records some text was eliminated.\n\nBiases\nDespite our effort to balance the dataset in terms of dates, biases are present. Since our main concern in collecting the data was for the date distribution, no deliberate selection was made on the basis of the document types. Some types are thus over or underrepresented (e.g. private letters that do not usually bear a date; \u00a76.2). Each type of document has however distinctive linguistic characteristics, such as the level of formality or unusual constructions (e.g. accounts). This uneven typological representation probably affects the performance of the models. Other possible biases in the dataset concern the provenance of papyri, the length of their text, and the state of conservation (sizeable portions of missing text or entirely missing parts of the documents).\n\nChronological analysis of words\nChronological analysis of word occurrence is possible if we detect and collect terms only attested in the papyrological material during a limited period. The word 'denarius' only appears after the 2nd CE and before the 5th CE, its presence in a text thus means that the text must have been written during this timespan. Likewise a text containing the word 'indiction' cannot have been written before the 4th CE. The investigation should also regard the possibility that the models make a prediction for a papyrus based on typical dating formulas present in the text like the name of the ruling emperor. Although our investigation of explanations did not yield any major concerns, a bigger sample of test cases should be created and more explainability methods should be employed (Ribeiro et al., 2016) to make conclusive remarks on this front.\n\nTranscription of papyri is not optional\nTranscription of the papyri is required (at least partial, but substantial) to reach this high degree of accuracy with our method. Thus, while there are transcriptions available for most already published papyri, it is less practical for dating unpublished papyri that have not been yet transcribed to a relatively high standard. In that case, image classification on the scripts can provide a less accurate prediction of the date as starting point.\n\nConclusion\nWe presented a machine-actionable dataset of 389 Greek documentary papyri of (mostly) Egyptian provenance, dated and balanced in terms of chronological quarter-century distribution. We trained extremely randomised trees on top of character n-gram-based features, reaching a mean absolute error of 54 years and 60% in century-level classification accuracy. We then formed a committee of regressors, which we applied to three use cases: a land lease, a private letter, and a geographical outsider (not from Egypt). To assist future research, our committee dated 159 manuscripts, for which only the upper limit is known. Future endeavours for this research extend far beyond the dating of individual manuscripts. It can produce valuable data for the study of the Greek language and its evolution through a millennium, help identify and trace linguistic habits and trends, as well as the history of document production, circulation, and use (e.g. which period produces what kind of texts, which administration relied on what type of documents, etc.). It can also produce further data and resources towards the typology of ancient Greek documents, completing with computational methods the work already underway and well-advanced of the grammateus project. Last, it can in the future fruitfully be combined with computational paleography to analyse the script and content of a given text.\n"}
{"question": "Which of the following limitations were not addressed by the design of the VisText dataset?", "evidence": "  D. There is a one-to-many relationship between the semantics of data tables and charts  There is not always a one-to-one relationship between the semantics of a data table and chart automatic data table extraction is error-prone due to the diversity of chart types and visual styles as well as the difficulty of reasoning about visual occlusion  if existing datasets provide captions that describe perceptual or cognitive features, these captions comprise only a small portion of the dataset.  ", "options": ["A. Headings describing perceptual or cognitive features make up only a small portion of the dataset", "B. Models may encounter out-of-memory errors when using larger alternatives", "C. Because of the inference difficulty of visual occlusion,automatic extraction of data tables is error-prone", "Computational Constraints. Despite using modern GPUs, with large amounts of memory, we were forced to use the smallest-parameter variants of T5 and ByT5 as we encountered out-of-memory errors with the larger alternatives."], "answer": "B", "content": "\nIntroduction\nStudies have shown that captions can improve the recall and comprehension of the data that charts depict (Hegarty and Just, 1993; Large et al., 1995) . For instance, when a caption emphasizes visually prominent features of a chart, like a peak or a sharply declining trend, readers treat this information as the key takeaway (Kim et al., 2021) . Moreover, for people with visual disabilities, captions (or equivalent descriptions such as alt text) are often the only means of accessing the presented data. However, as evidenced by numerous guidelines (Jung et al., 2021) , producing high-quality * Both authors contributed equally to this work. chart captions is a non-trivial and laborious manual process. Thus, despite these advantages, charts are only rarely captioned in practice (Lundgard and Satyanarayan, 2022) .\nTo bridge this gap, several research communities have begun to explore methods for automatically generating chart captions, including using templates and heuristics (Demir et al., 2008; Srinivasan et al., 2019) , adapting image captioning techniques (Balaji et al., 2018; Chen et al., 2019a) , or via data-to-text machine translation (Kantharaj et al., 2022; Obeid and Hoque, 2020) . While promising, these approaches have largely produced captions that either describe a chart's construction (e.g., \"The graph is plot between 'Number of people' x-axis over 'Movie Genres' y-axis\" (Balaji et al., 2018) ) or provide statistical summaries (e.g., \"Machinery and equipment was the most valuable commodity for Singapore in 2019\" (Kantharaj et al., 2022) ). However, these captions do not articulate the perceptual and cognitive features that make charts a distinctive and compelling medium for communicating data (e.g., \"Prices of Big Tech corporations seem to fluctuate but nevertheless increase over time\" (Lundgard and Satyanarayan, 2022) ). Indeed, as Lundgard and Satyanarayan (2022) find, both sighted and blind readers strongly prefer captions that express this type of content.\nTo automatically produce such semantically richer captions, we introduce VisText: a benchmark dataset of 12,441 pairs of charts and captions. VisText makes two key extensions over prior approaches. First, VisText offers three representations of charts: a rasterized image and backing data table, as in previous work; and a scene graph, a hierarchical representation akin to a web page's Document Object Model (DOM), that presents an attractive midpoint between the affordances of chart-as-image and chart-as-data-table. Second, for each chart, VisText provides a synthetically generated caption detailing its construction as well as a crowdsourced caption describing its statistical, perceptual, and cognitive features. These crowdsourced captions represent a substantial increase in data over prior comparable datasets (Mahinpei et al., 2022; Kantharaj et al., 2022) .\nTo demonstrate the possible uses of the VisText dataset, we train three classes of models -textbased caption models, image-guided captioning models, and semantic prefix-tuning. Text-based captioning models fine-tune large language models for VisText's chart captioning task, revealing that both data table and scene graph representations can produce compelling and semantically rich captions. Following recent advancements in image-guided translation (Sulubacak et al., 2020) , we leverage the additional visual affordances in chart images to develop image-guided chart captioning models. Finally, since users often have varying preferences about the type of semantic content in their captions (Lundgard and Satyanarayan, 2022) , we apply semantic prefix-tuning to each of our models, enabling them to output customizable captions.\nOur models generate coherent, semantically rich captions across the VisText charts. Evaluating against standard machine translation and text generation metrics reveals that our models consistently output captions that accurately describe the chart's construction, such as its chart type, title, and axis ranges. Through qualitative analysis of our model's captions, we find that our model competently outputs semantically rich captions that describe data trends and complex patterns. Further, we categorize six common captioning errors that can inform the future development of chart captioning models on the VisText dataset.\nThe VisText dataset and source code are available at: https://github.com/mitvis/ vistext.\n\nRelated work\nHeuristic-Based Chart Captioning. Automatically generating natural language descriptions of data tables dates back to Reiter and Dale (1997) . Demir et al. (2008 Demir et al. ( , 2010 Demir et al. ( , 2012) ) survey this early work and describe the process of extracting insights from a chart by evaluating a list of propositions and composing selected propositions together to produce a natural language summary. More recently, data visualization researchers have explored heuristics that calculate summary statistics and templates to assemble natural language \"data facts\" (Srini-vasan et al., 2019) or descriptions (Cui et al., 2019) . While useful, these approaches yield standardized descriptions that lack the variation and linguistic construction that characterize semantically rich captions (Lundgard and Satyanarayan, 2022) .\nChart Captioning as Image Captioning. With rapid advances of neural image captioning (Vinyals et al., 2015; Anderson et al., 2018) , researchers have begun to adapt these methods for captioning charts. For instance, Balaji et al. (2018) develop a deep learning pipeline that ingests a PNG chart image, classifies the chart type, detects and classifyies textual content present in the chart, and uses this information to generate a textual description. Chen et al. (2019a Chen et al. ( ,b, 2020) ) propose a simpler workflow using ResNet to encode the chart image and an LSTM with Attention to decode it into a natural language description. Both approaches share a pair of limitations. The captions they produce convey relatively simplistic information about the chart (e.g., title, axis labels, etc.) or articulate concepts in visual rather than data terms (e.g., \"Dark Magenta has the lowest value\"). While both approaches contribute associated datasets, their charts and captions are synthetically generated and may not represent real-world counterparts. SciCap (Hsu et al., 2021) addresses this limitation by scraping real-world charts from 290,000 arXiv papers; however, the baseline models trained on this dataset struggle to generate semantically rich captions.\nChart Captioning as Text Translation. Perhaps closest to our contribution is recent work modeling chart captioning as a data-to-text problem. For instance, Spreafico and Carenini (2020) train an encoder-decoder LSTM architecture to generate a natural language caption from time series data. Similarly, Obeid and Hoque (2020) and Kantharaj et al. (2022) explore how transformer architectures can translate tabular structures into captions. These data-to-text methods are more successful than chart-as-image captioning, yielding captions that better capture relevant information from the charts and have higher BLEU scores. Nevertheless, we observe two limitations with these data-to-text approaches that motivate our contribution. First, data-to-text methods are heavily reliant on access to a chart's data table. In practice, data tables are only rarely published alongside charts and methods that recover equivalent information via OCR experience a significant drop in performance (Kantharaj et al., 2022) . Second, the associated datasets do not contain sufficient training examples of captions that express semantically rich insights about the depicted data (i.e., the perceptual and cognitive phenoma that distinguish charts as a medium as distinct from data tables (Lundgard and Satyanarayan, 2022) ). As a result, while the generated captions are compelling, they are largely limited to reporting statistics which sighted and blind readers prefer less than captions that convey complex trends and patterns (Lundgard and Satyanarayan, 2022) .\n\nThe VisText Dataset\nWe designed the VisText dataset in response to two limitations existing datasets present for generating semantically rich chart captions. First, existing datasets represent charts as either rasterized images or as data tables. While useful, these representations trade off perceptual fidelity and chart semantics in mutually exclusive ways -images capture the perceptual and cognitive phenomena that are distinctive to charts (e.g., trends or outliers) but pixels cannot express the rich semantic relationships between chart elements (e.g., estimating plotted data values using axis labels). While the vice-versa is true (Lundgard and Satyanarayan, 2022) , tables also present additional caveats. There is not always a one-to-one relationship between the semantics of a data table and chart (i.e., one data table may be the source for several distinctly different charts). Moreover, data tables are rarely published alongside charts; and, automatic data table extraction is error-prone due to the diversity of chart types and visual styles as well as the difficulty of reasoning about visual occlusion (Kantharaj et al., 2022; Luo et al., 2021; Jung et al., 2017) ).\nSecond, if existing datasets provide captions that describe perceptual or cognitive features, these captions comprise only a small portion of the dataset. At best, LineCap (Mahinpei et al., 2022) offers 3,528 such captions for line charts only, while Chart-to-Text (Kantharaj et al., 2022) estimates that roughly 15% of the sentences in its captions across a variety of chart types express such content.\nIn contrast, VisText provides 12,441 crowdsourced English captions that articulate statistical, perceptual, and cognitive characteristics of bar, line, and area charts. In VisText, charts are available as not only data tables and rasterized images but also as scene graphs. Scene graphs are hierarchical representations that better preserve perceptual fidelity and chart semantics, are often the format for publishing web-based charts, and can be recovered from chart images (Poco and Heer, 2017) .\n\nData Table Collection\nThe data tables found in VisText are sourced from the Statista dataset of the Chart-to-Text benchmark (Kantharaj et al., 2022) . The tables were collected by crawling Statista.com in December 2020 and contain real-world data related to technology, trade, retail, and sports. We process these tables to make them amenable for chart generation, including stripping formatting symbols (e.g., $ and %), standardizing data strings, and identifying the measure type of each column (i.e., quantitative, categorical, or temporal). Data tables are discarded if they do not contain at least one quantitative field and one categorical or temporal field, or if other errors occur during the processing steps. We further down select to data tables containing between 2 to 20 columns and 10 to 500 rows. If a data table has over 500 rows, we randomly sample rows. In larger data tables, this step potentially affects how salient a trend is.\n\nChart Generation and Representation\nCharts in the Chart-to-Text Statista dataset all feature the same layout and visual appearance. In contrast, we aim for richer visual diversity by generating charts using the Vega-Lite visualization library (Satyanarayan et al., 2016) via the Python Altair package (VanderPlas et al., 2018) . To facilitate collecting high-quality captions, we focus on univariate charts: charts that depict one quantitative observation against a categorical or temporal variable. This focus is informed by recent work in the data visualization research community which has chosen single-series line charts as the target of study for natural language descriptions (Kim et al., 2021; Stokes et al., 2022) . VisText also includes single-series bar and area charts as they typically exhibit similar perceptual features to line charts.\nFor each data table, we iterate through pairs of univariate fields. If the pair contains a temporal field, we randomly generate an area or line chart; if the pair contains a categorical field, we randomly generate a horizontal or vertical bar chart. For diversity in layout and visual appearance, we randomly rotate axis labels and apply one of fourteen themes provided by the Vega-Lite library. These themes mimic the visual style of common chart platforms or publishers (e.g., ggplot2 or the LA Times). \n\nGenerated L1 Caption\nHere is a area chart is labeled Cumulative number of patients diagnosed with coronavirus (COVID-19) in Japan as of December 4, 2020, by place of infection. On the x-axis, Month is measured with a categorical scale starting with April and ending with October. There is a linear scale with a minimum of 0 and a maximum of 150,000 along the y-axis, labeled Patients within Japan.\n\nCrowdsourced L2/L3 Caption\nBy December 4th 2020, approximately 160,000 people in Japan had been diagnosed with COVID-19. The first person diagnosed with COVID-19 in Japan was diagnosed in March 2020. The greatest increase in cumulative number of patients in Japan diagnosed with COVID-19 occurred between November and December 2020. In VisText, each chart is represented as a rasterized image, stored as an RGBA-encoded PNG file, as well as a scene graph. A scene graph is a textual representation of the rendered chart similar to a web page's Document Object Model (DOM). Scene graphs encode the position, value or content, and semantic role of all visual elements within a chart, including the individual marks (i.e., bars or points along the line), titles, axes gridlines, etc. Thus, scene graphs express the perceptual features of rasterized images in a more computationallytractable form.\n\nCumulative number of patients diagnosed with coronavirus (COVID-19) in\nScene graphs are a standard data structure for representing vector-based graphics -the most common format for publishing visualizationsand, thus, can be trivially recovered (e.g., by traversing the SVG text string). We extract the scene graph directly from the rendered chart using the Vega-Lite API. As most text generation models expect a linear set of input tokens, we flatten the scene graph via a depth-first traversal. To scale to large language models, we need to further reduce the size of the scene graph. Thus, we preserve the following elements which we hypothesize as being most critical for generating semantically rich captions: title, title coordinates, axis labels, axis label coordinates, axis tick coordinates, mark coordinates, and mark sizes. VisText includes both the original (hierarchical) and reduced (linearized) scene graphs.\n\nCaption Generation and Collection\nOur captioning process is guided by the framework developed by Lundgard and Satyanarayan (2022) , which identifies four levels of semantic content: L1 content enumerates aspects of the chart's construction (e.g., axis ranges); L2 content reports summary statistics and relations (e.g., extrema); L3 content synthesizes perceptual and cognitive phenomena (e.g., complex trends); and, L4 content describes domain-specific insights (e.g., sociopolitical context). In subsequent studies, the authors find that while sighted readers typically prefer higher levels of semantic content, blind readers are split about the usefulness of L1 and L4 content. Thus, given these differing preferences, we define a single caption to express multiple levels of content separated across clauses or sentences. We only consider the first three levels of this model, and leave L4 content to future work. Following guidelines prescribed by the National Center for Accessible Media (NCAM), our captions begin with L1 content and then turn to L2 and L3 content (Gould et al., 2008) .\nWe algorithmically generate L1 content and use a crowdsourced protocol to collect L2 and L3 content. This approach follows (Lundgard and Satyanarayan, 2022)'s computational considerations as well as results from Morash et al. (2015) who find that, even with instructions and guidelines, crowd workers do not describe a chart's structural elements sufficiently for blind readers. Thus, synthetically generating L1 content allows us to ensure that captions convey complete descriptions of the chart's structural elements. L1 content comprises 1 sentence conveying the chart type and title, and then 1 -2 sentences describing the axes (including the titles, ranges, and scales). We use template randomization to generate a diverse range of L1 captions to mimic human variability and reduce the capacity of the model to overfit to a single L1 style. Three templates are defined for the first sentence and twenty-six template combinations for the subsequent sentences. During generation, we randomly select a pair of templates and fill in in- formation from the abstract chart specification. For additional diversity, we randomly drop scale information and swap template words with synonyms. Templates and synonym replacements are listed in Appendix E.2.\nTo crowdsource L2 and L3 content, we extend the protocol used by Lundgard and Satyanarayan (2022) . After soliciting consent, we introduce the task: participants are presented with a chart image and corresponding L1 description; they are asked to write a description about the trends and patterns they observe without drawing on background knowledge or repeating L1 information. The introduction provides examples and explanations of valid and invalid responses. After acknowledging these examples, participants are asked to complete 5 random iterations of the task. To maximize the quality of our crowdsourced captions, we manually curated the charts and L1 descriptions used in the study. We discarded any charts that were challenging to read (e.g., colors were too similar, marks were not easily readable, etc.). Participants were recruited on the Prolific.co platform, took approximately 14 minutes to complete the study, and were compensated $3.25 ($14/hour). Additional details on our crowdsourcing process are in Appendix E.3.\nWe manually verified charts where participants failed an attention check and discarded invalid descriptions. Additionally, we manually inspected captions for personally identifiable information or offensive content. Using heuristics, we removed captions where respondents described charts as unclear or illegible and replaced newline characters with spaces. Although we attempted to fix incorrect spelling and casing errors using a similar heuristic-based approach, we observed that this process could improperly affect axis and chart names. As a result, these errors remain in our dataset.\n\nDataset Analysis\nFigure 2 shows the distribution and means of the lengths of chart representations and captions. Synthetically generated L1 captions have roughly 1.5x more characters than crowdsourced L2/L3 captions (\u00b5 = 255 vs. \u00b5 = 177) but the average number of sentences are comparable (2.5 vs. 2). The VisText dataset consists of captions for 3,189 area charts, 6,238 bar charts, and 3,014 line charts -the roughly twice-as-many bar charts as area or line charts corresponds to the randomization of temporal fields during chart generation (Sec. 3.2). As some charts have multiple crowdsourced captions, we randomly split our dataset into training, validation, and test sets using the chart IDs to prevent data leakage across sets. This resulted in an approximate ratio of 80:10:10.\nFinally, to understand the distribution of semantic content, we manually coded 2% (230) of crowdsourced captions. We followed a protocol inspired by Lundgard and Satyanarayan (2022) by breaking sentences down into independent statements and mapping these statements to their semantic content level. We marked statements as not categorizable if they did not map to the framework -for instance, if captions expressed commentary from crowd workers such as \"this chart is hard to read.\" Our analysis revealed 11 L1 statements (2.4%), 180 L2 statements (39.7%), 253 L3 statments (55.7%), and 10 not categorizable statements (2.2%). While a handful express L1 content, the bulk of statements (95%) express L2 or L3 content, with approximately 1.4x L3 statements than L2.\n\nChart Captioning Models\nTo demonstrate the affordances of the VisText dataset, we train three classes of models. First, we fine-tune large language models to translate from textual chart representations to natural lan-guage captions. These models evaluate the feasibility and impact of scene-graph models compared to prior data-table approaches (Kantharaj et al., 2022) . Second, as VisText provides multiple chart representations, we adapt image-guided translation (Sulubacak et al., 2020; Cho et al., 2021) to develop two multimodal chart captioning models: image-scene-graph and image-data-table. Finally, since VisText offers captions at different semantic levels and prior work has shown significant differences in readers' preferences (Lundgard and Satyanarayan, 2022) , we explore prefix-tuned models that selectively output L1, L2/L3, or L1+L2/L3 captions. Training details are in Appendix D.\n\nText-Based Chart Captioning\nInformed by prior work (Kantharaj et al., 2022) , we investigate text translation models for generating chart captions. In particular, Kantharaj et al. found that models that translate data tables to chart captions significantly outperform image captioning models. However, when data tables were not available, the authors found a significant drop in their models' ability to extract relevant information from the chart -an effect that was only slightly ameliorated by using OCR methods to extract text from chart images. In contrast, VisText's scene graphs can be more readily recovered from charts when data tables are not available -for instance, by processing the SVG format of web-based visualizations. Moreover, scene graphs offer a potentially richer source of information than data tables as they encode visual properties of the chart (e.g., coordinates and colors) and are less noisy than tokens recovered via OCR. Thus, to evaluate the feasibility and efficacy of scene graphs, we train a scene-graph text translation model and a baseline data-table model for comparison.\nFor each model, we fine-tune a pretrained ByT5 transformer model (Xue et al., 2022) on the Vis-Text dataset. We choose ByT5 over T5 transformers (Raffel et al., 2020) because it uses a token-free, byte-encoding that eliminates the use of a tokenizer. As a result, it is robust to noisy inputs, minimizes the need for text preprocessing, and eliminates the out-of-dictionary problem. This allows our model to handle common typographical and chart reading errors in the crowdsourced L2 and L3 captions and increases generalizability to previously-unseen words that could be present in chart and axes titles.\n\nImage-Guided Chart Captioning\nFollowing recent advancements in image-guided machine translation (Sulubacak et al., 2020) , we train image-guided captioning models using the VisText dataset. Images have improved text-based machine translation models by providing visual information complementary to natural language inputs. Similarly, chart images can contain visuals complementary to the textual specification. For instance, visual affordances that are important for perceiving a trend (e.g., gestalt relations, relative sizes/areas, etc.) may be obfuscated in the scene graph but better captured in the chart image.\nWe train three image-guided chart captioning models: image, image-scene-graph, and image-data-table. All models leverage the vision-language transformer model VL-T5 (Cho et al., 2021) . VL-T5 is pretrained on image captioning and visual grounding tasks and was successfully applied to machine translation, making it suitable for chart captioning. We extract visual features for each VisText chart image using a Bottom-Up Feature Extractor (Anderson et al., 2018) . To explore the value of images to chart captioning, our image model only takes in the image features, while image-scene-graph and image-data-table concatenate the image features with the chart's textual representations (scene graph or data table).\n\nSemantic Prefix-Tuning\nIn real-world chart captioning settings, users want to vary the level of semantic content in their captions. For instance, while some blind users want verbose captions that describe the chart visuals, sighted users may only want captions that help them expose data trends (Lundgard and Satyanarayan, 2022) . To develop models capable of such customization, we leverage prefix-tuning strategies alongside VisText's semantic caption breakdown. Prefix-tuning specifies a task alongside the input, permitting a single large language model to perform many different tasks. In our setting, we use prefix-tuning to specify the level of semantic content to include in the caption (Li and Liang, 2021) .\nWe train each of our models with and without semantic prefix-tuning. With semantic prefix-tuning, we treat chart captioning as a multi-task fine-tuning problem, where the model is trained to generate the L1 and L2/L3 captions separately. In every epoch, the model sees each VisText chart twice, once with the L1 prefix and caption and once with the L2/L3 prefix and caption.\n\nEvaluation and Results\nTo evaluate the VisText dataset and our chart captioning models, we measure the readability and accuracy of generated captions and their similarity to the VisText target caption. We also qualitatively analyze the descriptiveness of generated L2/L3 captions and categorize common errors.\n\nQuantitative Model Performance\nWe evaluate the results of our text-based and imageguided captioning models with and without prefixtuning. We also compare to a current state-of-theart chart captioning model that uses data table chart representations and a T5 generation model (Kantharaj et al., 2022) . To measure the quality of output captions, we evaluate each model on machine translation and language generation metrics (Table 1 ).\nChart images do not support captioning. The image model performs the worst of all the chart captioning models. Its low perplexity and high error rates indicate it is highly confident in its inaccurate captions. While chart images contain the same information encoded in the chart's textual representations, it is presumably not adequately extracted by the model. Both the image model backbone (Cho et al., 2021) and the visual feature extractor (Anderson et al., 2018) are trained on natural images, making chart images out-of-distribution inputs that are likely to be poorly represented by these vision models. As the chart captioning task grows, model backbones, architectures, and feature extractors could be customized to chart images, which may improve image-based chart captioning.\nAll models produce high quality L1 captions. In our chart captioning setting, relation generation (Wiseman et al., 2017) measures how often the chart title, axis names, and axis scales in the input appear in the caption. Every model (except image) achieves a similarly-high relation generation score, indicating that every model can generate detailed L1 captions.\nScene graphs perform as well as data tables. Models trained on scene graph representations achieve similar performance across the evaluative metrics to models trained on data tables. As scene graphs can be more easily extracted from web-based charts images, they may be the preferred representation for future chart captioning models.\nImage-guiding does not improve captioning. Our image-guided captioning models do not experience the significant increase in performance other image-guided translation tasks report. While in image-guided translation, images contain substantial additional information beyond the text, the image and textual representations in chart captioning often contain highly similar information. The small amount of additional information in images might benefit complex captioning tasks on multivariate charts or infographics; however, the current VisText captions rarely reference visual information not present in the scene graph or data table.\nPrefix-tuning is free. Adding semantic prefixtuning to our models does not significantly change their performance. Models trained with and without prefix-tuning are exposed to the same set of charts, so it is consistent that prefix-tuning would not impact the quality of output captions. Given prefix-tuned models are able to output L1, L2/L3, and L1+L2/L3 captions, prefix-tuning may be preferred if users require semantic customization.\n\nQualitative Caption Evaluation\nTo augment our quantitative evaluation, we qualitatively assess the descriptiveness and accuracy of the generated chart captions. Since L1 caption accuracy can be measured at scale via relation generation, we focus our evaluation on L2/L3 predictions.\nPrior analysis tasked annotators with comparing the accuracy, coherence, and fluency of generated captions compared to a target caption (Kantharaj et al., 2022) . Instead, our approach follows an inductive qualitative data analysis approach: iteratively analyzing captions in a \"bottom-up\" fashion to identify emergent patterns in how generated captions compare to the ground truth (Bingham and Witkowsky, 2021). We randomly sample 176 generated captions from the scene-graph model with prefix-tuning and break them into their independent L2 and L3 statements, resulting in 181 (48.27%) L2 statements and 194 (51.73%) L3 statements.\nApproximately half (241 / 512) of the L2 and L3 statements made in the generated captions are factually accurate. Moreover, many of the full sentences are written in a natural, human-like manner and generated captions frequently include both compound and complex sentences. On average, every generated caption has one L3 statement and zero to et al., 2022) . We evaluate each model using machine translation and text generation metrics, including BLEU (Papineni et al., 2002) , Perplexity, Relation Generation (RG) (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD) (Kusner et al., 2015) , and Translational Error Rate (TER) (Snover et al., 2006) . We report the mean and standard deviation of three independent models. Darker colors indicate better scores.\nInput PT BLEU \u2191 Perplexity \u2193 RG \u2191 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193 Kantharaj et\ntwo L2 statements. Often this takes the form of a L3 general trend statement (e.g., \"The median annual family income in Canada has increased from 2000 to 2018\") accompanied by an L2 minimum and maximum statement (\"The highest was in 2015 at 80k and the lowest was in 2000\"). For the remaining half of analyzed captions, we identified the following recurring types of errors:\nIdentity Errors. We identify 86 identity errors (22.93% of analyzed statements). An identity error occurs when an L2 or L3 statement incorrectly reports the independent variable for a given (often correctly identified) trend. For bar charts, this error means incorrectly reporting the categorical label associated with a bar (e.g., in Appendix Figure 5c : \"The most popular music activity is vinyl albums and vinyl singles\" should be \"The most popular music activity is tickets for festivals\"). For area and line charts, this error means incorrectly identifying the temporal point or range of the trend. With bar charts, in particular, we observed that the identities were often \"off-by-one\" (i.e., identifying a minimum or maximum value, but attributing it to the second-highest or second-lowest category).\nValue Errors. A value error occurs when the quantitative data value of a statement is incorrect.\nOf the captions we analyzed, 3.20% (12) of statements contained a value error. For instance, as shown in Appendix Figure 4c , for the caption \"The total gate revenue from sporting events worldwide by region from 2006 to 2015 has increased from around 15 billion dollars to around 15 billion dollars\", the value should be around 18 billion dollars.\nIf it is ambiguous whether an error is an Identity or Value Error, we classify it as the former.\nDirection Errors. A direction error occurs when the direction (which can be increasing, decreasing, or stable) of a trend in an L3 statement is incorrect. We uncovered 32 direction errors (8.53% of analyzed statements). For instance, in the caption \"The per capita consumption of sweet corn in the US has increased from 2000 to 2019\" (Appendix Figure 3c ), the trend is actually decreased. In most direction errors, the identity (i.e., temporal range) is correct.\nStability Errors. A stability error occurs when the magnitude of a direction or the variance in a trend is incorrect. This can often refer to how much a trend is increasing or decreasing, such as rapidly or slowly, as well as whether it's a steady change or highly-fluctuating change. Looking ahead, while accessibility remains a key domain that would benefit from automated chart captioning, and deploying automated chart captioning models into the field is an exciting prospect, we believe the most promising approach for future work lies in \"mixed-initiative\" (i.e., human + AI) chart authoring systems. In particular, as we describe in our Ethics Statement below, chart captioning models are currently prone to make a number of factual inaccuracies which can have severe harmful consequences. On the other hand, by integrating these models into chart authoring systems (e.g., Tableau, Charticulator, Data Illustrator, or Lyra), chart authors can intervene and make any necessary corrections. Indeed, such integration offers exciting opportunities to develop novel interactive methods for verifying generated captions. For instance, models like ours could generate an initial caption (or set of captions) based on the chart currently being authored; as the system has access to all three representations of the chart (the back-ing data table, chart image, and structured scene graph), it might automatically segment the caption into independent \"data segments\" and interactively link and map them to rows in the table or regions on the chart, akin to Kori (Latif et al., 2021) .\n\nLimitations\nComputational Constraints. Despite using modern GPUs, with large amounts of memory, we were forced to use the smallest-parameter variants of T5 and ByT5 as we encountered out-of-memory errors with the larger alternatives. More problematically, the quadratic relationship between sequence length and time/space complexity of transformer architectures (Vaswani et al., 2017) , especially when using byte-level sequences (Xue et al., 2022), has had a significant impact on our model performance.\nIn particular, to be computationally tractable, we were forced us to truncate our input and output sequences to, at most, 1,024 and 512 characters respectively (1,024 coming from the underlying ByT5 architecture (Xue et al., 2022)).\nThese character thresholds have likely had an outsized effect on scene-graph models. For instance, due to these character limits, we reduced scene graph sequences to only a minimal set of visual characteristics; VisText also includes the raw, unprocessed scene graphs which offer a richer source of information about the visual features that are important to how people decode charts (e.g., bounding boxes, color) but were unavailable to our models. Moreover, as Figure 2 shows, even with this reduced representation, the mean length of scene graph sequences is 948 characters (cf. 426 characters for data tables) with a wide distribution. Thus, despite scene-graph models achieving comparable performance to data-table models, the former saw a much smaller proportion of complete sequences as compared to the latter. This truncation step additionally negatively impacts charts with long titles or axis names -in such cases, we observed that the L2 or L3 caption would be altogether truncated before generation.\n\nChart Types and the Visualization Design Space.\nVisText is scoped to only univariate bar, area, and line charts. We chose to begin with these chart types informed by data visualization research that has focused on studying natural language descriptions of single-series line charts -a basic, but commonly occurring chart type that offers a compelling target of study as it most visibly surfaces any poten-tial trends in the data (Kim et al., 2021; Stokes et al., 2022) . Future work can now begin to consider more complex chart forms in a step-by-step manner. For instance, moving from univariate bar, area, and line charts to multivariate versions of these chart types (i.e., stacked bars and areas, grouped bars, and multi-series line charts). From there, work can also consider chart types that surface perceptual and cognitive phenomena in visually distinct ways (e.g., scatterplots, where trends appear as clusters of points; heatmaps, where color saturation often encodes a trend; or maps, where color or other layered elements such as symbols are used to represent data values). Finally, automated methods for captioning visualizations may eschew chart typologies altogether in favor of visualization grammars -by offering a more composable and combinatorial approach to the design space (Wilkinson, 2012) , learning over visualization grammars may offer a more robust approach to captioning highly customized or unique visual forms.\nFor each future work direction, we anticipate scene graph representations to prove more fruitful than the data table. As the complexity of the visualization increases, its relationship to the data table only grows more ambiguous; the scene graph, on the other hand, directly encodes the visual form and thus remains faithful to it. As a result, to support such future work, VisText provides the raw specifications used to produce our charts (via the Vega-Lite visualization grammar (Satyanarayan et al., 2016) ) as well as the raw, hierarchical scene graphs prior to our linearization and reduction step.\n\nEthics Statement\nThe Consequences of Incorrect Captions. Weidinger et al. ( 2021) comprehensively survey the risks associated with the large language models (LLMs) that underlie our contribution. Of the six categories of risk they identify, harms stemming from models producing factually incorrect statements are not only most pertinent to our work, but are likely heighted as compared to general uses of LLMs given the context we are addressing: automatically captioning charts. In particular, people most often consume charts and visualizations in order to make data-driven decisions (Keim et al., 2008) -for instance, about whether to evacuate ahead of a hurricane (Padilla et al., 2018) , or health & safety during the pandemic (Shneiderman, 2020) . Moreover, recent results have shown that readers not only fixate for longer and are more likely to recall the textual content of and around visualizations (Borkin et al., 2015) but this textual content can strongly influence the takeaway message readers leave with even when it is at odds with the depicted data (Kong et al., 2018 (Kong et al., , 2019)) . Finally, these issues are exacerbated by the persuasive and rhetorical force of data and charts (Kennedy et al., 2016; Hullman and Diakopoulos, 2011) , that often project a sense of authority and certainty (Correll, 2019) . As a result, readers may not think to double check the accuracy of chart captions, and inaccurate statements that models may produce could lead to harmful downstream decisions.\nTo proceed ethically with this line of research, we believe that advances in data and modeling need to be closely followed by attention devoted to mitigating the risks of incorrect statements. At base, automatically generated captions should be identified as such at the forefront to raise readers' awareness about the potential for incorrect statements. And, interactive visual linking strategies (such as those explored by Kong and Agrawala (2012) ; Kim et al. ( 2018)) could be deployed to help readers manually verify the constituent statements of a caption against the chart. These strategies, however, place the burden of harm mitigation on readers. Thus, an alternate approach might never surface automatically generated captions to readers directly but instead use them as part of mixed-initiative systems for jointly authoring visualization and text, such as Kori (Latif et al., 2021) . In such systems, automated chart captioning models would help to accelerate the authoring process -combatting the blank slate problem by providing an initial summary of the chart -and chart authors would make any necessary corrections prior to publication.\nBesides these human-computer interaction (HCI) approaches for mitigating harm, an equally important direction for future work should leverage interpretability techniques to more deeply study what the models are learning. To what degree are chart captioning models stochastic parrots (Bender et al., 2021) , and how much do they understand the information charts depict? Automated Captioning for Accessibility. Although accessibility is a guiding motivation for the bulk of work in automated captioning (be it image captioning or, as in our case, chart captioning), studies find mixed reactions, at best, about these approaches among people with disabilities (PWDs).\nFor instance, accessibility educator and researcher Chancey Fleet described Facebook's automatic image descriptions as \"famously useless in the Blind community\" despite \"garner[ing] a ton of glowing reviews from mainstream outlets\" (Fleet, 2021; Hanley et al., 2021) . This disconnect appears to stem from a more fundamental mismatch between what PWDs describe as their captioning needs, and what the research community -particularly through its automatic, quantitative evaluationsprioritizes (Jandrey et al., 2021) . In particular, surveys with PWDs repeatedly surface the contextual nature of captions. Bennett et al. (2021) find that the context of use shapes the degree to which PWD are comfortable with captions describing people's race, gender, and disabilities -for instance, changing their preferences if they were in a white, cisgender, nondisabled, and professional company versus their own community. Similarly, Jung et al. (2022) find shifting preferences for the content image descriptions should convey across different photo activites -for example, when viewing or taking photos, participants wished for descriptions that conveyed spatial cues whereas when searching or reminiscing about photos, participants hoped for descriptions to connect to personal data or differentiating details.\nIn contrast, quantitative metrics of model performance compare generated captions to a single \"ground truth\" caption. This framing of success not only makes it difficult to develop contextuallyvarying caption generation but can actively penalize such investigations. For instance, with our work, we explored how prefix-tuning can be used to develop models that are responsive to users' preferences about semantic content. However, as described in Sec. 5.1, existing quantitative metrics of model performance (e.g., BLEU, ROUGE, WMD, and TER) show a drop in model performance despite our qualitative analysis indicating that these captions are indeed high quality.\nFinally, our exploration of semantic prefixtuning represents only a very preliminary step towards addressing the contextual captioning needs of PWDs. In particular, the semantic labels Vis-Text assigns to captions were derived from prior work (Lundgard and Satyanarayan, 2022) that only explored natural language descriptions when consuming presentations of visualizations -one task from a broader palette (Brehmer and Munzner, 2013) . Future work might instead extend the Vis-Text dataset -and corresponding models -to consider captions for a broader range of tasks including consuming visualizations for scientific discovery, enjoyment or, producing, searching, or querying visualizations (Brehmer and Munzner, 2013) . \n\nModel Generated L1 Caption\nAverage spending per consumer on selected music activities in the United States as of July 2018 is a bar graph. The x-axis measures Response while the y-axis measures $40 to $99.99.\n\nModel Generated L2/L3 Caption\nThe most popular music activity is vinyl albums and vinyl singles.\nThe least popular music activity is vinyl albums. (b) Model results using the L2/L3 captions.\nTable 2 : We separately evaluate our L1 and L2L3 captions on all the same metrics except for Relation Generation.\nIn general, we observe that L1 captions perform better than the L2/L3 captions. Our models generate verbose L1 captions that are similar to the structure of our L1 templates, while the L2/L3 captions are human-generated and contain more variability. Darker colors indicate better scores.\n\nB Additional Evaluations\nB.1 Independent L1 and L2/L3 Caption Evaluation\nTo better understand how our models generate varying levels of semantic content, we separately evaluate our prefix-tuned models on L1 captioning and L2/L3 captioning tasks. Each prefix-tuned model can output an L1 or an L2/L3 caption for each chart. We evaluate these captions to their respective L1 or L2/L3 ground truth captions and report the results in Table 2 . Since we compute Relation Generation using only the L1 chart fields (e.g., chart title, axis scale, etc.), we do not report the results separately for L1 versus L2/L3 captioning. There is no direct Relation Generation analog for L2/L3 captions, since they are human-generated and do not follow a specific template. The Relation Generation for L1 captions is identical to the Relation Generation for L1/L2/L3 captions reported in Table 1 .\n\nB.2 Evaluation Details\nQuantitative Model Performance Metrics. We evaluate our models using NLP and machine translation metrics, including BLUE (Papineni et al., 2002; Lin and Och, 2004) , Perplexity, Relation Generation (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD), and Translation Edit Rate (TER) (Snover et al., 2006; Post, 2018) . We implement Relation Generation per Wiseman et al. (2017) , use the Gensim implementation of WMD, and use the Hugging Face implementation (Wolf et al., 2019) for the remaining metrics.\n\u2022 BLEU: BLEU requires several gold standard references. In our evaluation setup, we use the test set caption as a single reference.\n\u2022 Perplexity: We use a pretrained GPT-2 Medium model to compute Perplexity.\n\u2022 Relation Generation: The fields we evaluate on are the chart title, axis names, and axis scales (if any).\n\u2022 Translation Edit Rate (TER): Edits consist of deletions, additions, and substitutions, as present in SacreBLEU.\nQualitative Caption Evaluation. To produce our qualitative evaluation results (Sec. 5.2), we iteratively evaluated randomly sampled captions until there was no more marginal information about they types of errors to be gained from evaluating more captions. For each L2/L3 caption, we assess the number of independent, mutually-exclusive L2 and L3 claims/statements that are being made. In comparison to evaluating at a sentence-level, this allows us to take a more nuanced approach that isn't limited by where the model has generated a full-stop. This approach allows us to more-accurately evaluate factual precision without overly-penalizing for a single mistake. An example might take the form of \"The lowest value is X (claim 1), the highest value is Y (claim 2), and the second highest is Z (claim 3). Overall, it is increasing over time (claim 4).\" We observe that the first sentence is a compound sentence that consists of three independent clauses, each with a single factual L2 claim, while second sentence is a single factual L3 claim. Let us assume that claim 1 was factually incorrect. If we evaluate at a sentence-level, then the entire first sentence comprising of claim 1, claim 2, and claim 3 would be incorrect. However, by breaking this caption into independent, mutually-exclusive claims, we can more precisely calculate the factual precision of our text generation. \n\u2193 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193\nTransformer Backbone T5-small L2/L3 0.06 \u00b1 2.67e\u22123 35.81 \u00b1 4.13e+0 0.25 \u00b1 6.43e\u22123 0.09 \u00b1 3.43e\u22123 0.22 \u00b1 5.73e\u22123 0.22 \u00b1 5.60e\u22123 0.99 \u00b1 8.70e\u22123 113.33 \u00b1 2.94e+0 Ours (ByT5-small) L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\nL1 Generation new-seed L2/L3 0.08 \u00b1 5.93e\u22123 20.96 \u00b1 2.71e+0 0.29 \u00b1 5.77e\u22123 0.11 \u00b1 2.33e\u22123 0.25 \u00b1 5.30e\u22123 0.25 \u00b1 5.27e\u22123 0.91 \u00b1 1.83e\u22123 116.36 \u00b1 1.11e+1 original-seed L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\n(c) Ablation study results using the L2/L3 captions. \n\nC Ablation Studies\nTo evaluate our modeling and dataset design choices, we run ablation studies measuring the impact of our transformer model backbones and stochastic data generation pipeline. We report the results in Table 3 .\nTransformer Backbone. To understand the impact of our token-free, byte-to-byte architecture ByT5 model backbone, we explore other large language models. Specifically, we compare our 300M parameter ByT5-small model (Xue et al., 2022) with a 60M parameter T5-small (Raffel et al., 2020) and 140M parameter BART-base model (Lewis et al., 2020) . We also apply prefix-tuning to the ByT5 and T5 models. We cannot apply prefix-uning to BART because BART does not support multi-task learning. Quantitatively, using ByT5 does not appear to significantly improve upon T5. However, we theorize that ByT5's token-free paradigm increases the input sequence length by compressing more input text into fewer input tokens.\nL1 Caption Generation. Since we generate L1 captions stochastically, we evaluate whether our initial randomization impacted the model's results. We compare generate a second set of L1 captions using a different random seed. We see the results are nearly identical across all metrics, indicating our dataset captures a diverse set of L1 captions. We estimate that we trained each model between 5 to 10 times to achieved our final results.\n\nD.3 Ablation Models\nWe train our ablation models using the same parameters as our default models, only varying the parameter of interest. We train them on 16 virtual CPU cores on Xeon E5 hypervisors with 128GB of memory and PCI pass-through access to eight NVidia Titan XP GPUs with 12GB of memory.\n\nD.4 Notable Package Versions\nPackage versions are listed in Table 5 .\n\nE Additional VisText Dataset Details E.1 Licensing\nOur use of the raw Statista data from Kantharaj et al. ( 2022) is consistent with its intended use case. The data was licensed under the GNU General Public License v3.0. We release our data and code under GNU General Public License v3.0.\n\nE.2 L1 Caption Generation Process\nThe Level 1 captions are generated from a random process that chooses from 3 title templates and 6 axis templates. The title templates we use are:\n\u2022 This is a [chart-type] titled [chart-title]\n\u2022 This [chart-type] is titled [chart-title]\n\u2022 [chart-title] is a [chart-type]\nThe axis templates we use for each axis are:\n\u2022 For each axis template, we randomly choose whether to include the axis scale. Furthermore, within each template, we further randomly swap words with synonyms. A list of words and their possible synonym substitutions are:\n\u2022 this: here, a\n\u2022 chart: graph, diagram, plot\n\u2022 titled: called, named, labeled\n\u2022 on: along\n\u2022 plotted: defined, measured, drawn, shown\n\u2022 plots: measures, shows\n\u2022 with: using, on, along, as\n\u2022 found: seen\n\u2022 labeled: marked\n\nE.3 Crowdsourced Study Protocol\nFigures 6-10 screenshot the introduction, eligibility and consent statements, instructions, and a task from our crowdsourced study. We recruited participants on the Prolific.co crowdsourcing platform, following conventions in the data visualization research community 3 and recent research results (Tang et al., 2022) that suggest Prolific yields higher quality results than Amazon Mechanical Turk. We conducted multiple pilot runs to calibrate the amount of time it would take participants to complete the study, and found that most participants were able to successfully do so within 14 minutes. Following Silberman et al. (2018) , who advocate for paying workers at least minimum wage at your location, we choose to pay our participants $3.25 -a roughly $14/hour rate in line with the $14.25/hour minimum wage in Massachusetts at the time the study was conducted.\nOur study was determined to be exempt by MIT's institutional review board (IRB). Participants had to explicitly provide their consent in order to proceed with the study -if participants did not consent, they were redirected back to the Prolific platform. The consent statement (Fig. 8 ) reminded participants of their rights (including that their participation is voluntary and consent could be revoked at any time), and encouraged participants to contact either the study PI or IRB board directly should they have any concerns. We constrained our participant pool (and eligibility requirements) to people living within the United States or United Kingdom who self-reported as being sighted with no vision or color impairments. We did not collect any additional demographic data from participants as we did not determine this to bias or otherwise affect the content we hoped to collect. Each task (an example of which is shown in Fig. 10 ) included an attention check where participants were asked to correctly identify the chart type shown. If participants failed more than two attention checks, their submission was flagged for manual review -in practice, the bulk of participants who failed attention checks nevertheless produced valid captions and, thus, were paid fully. The task asked participants to complete a free response question to describe as completely as they could the trends and patterns observed, emphasizing that their response would be evaluated for correctness and completeness. Despite best practices suggesting a more structured, querying approach (called QID) can yield higher quality captions (Morash et al., 2015) , we opted for our free-response approach as the benefits of QID (namely, in expressing the chart type, title, and axes units) would already be captured by our synthetically generated L1 captions. Moreover, in contrast to the templatized output produced by QID, we hoped that our free-response responses would yield more \"natural\" articulations of perceptual and cognitive trends, following the Lundgard and Satyanarayan (2022) framework.\n\n\nhttps://github.com/mitvis/vistext 2 https://github.com/j-min/VL-T5\n"}
{"question": "What is the primary advantage of using the Mixture-of-Adapters gate (MoA) in the MixDA approach during Stage 2?", "evidence": "  In the second stage, we train a mixture-of-adapters gate to dynamically select the desired knowledge adapter and a task-specific adapter for task adaptation. ", "options": ["A. Selecting the pre-trained language model most suitable for the task.", "B. Reducing the computational complexity of the model.", "C. Dynamically choosing knowledge and task-specific adapters.", "D. Training additional task-specific adapters. "], "answer": "C", "content": "\nIntroduction\nPre-trained language models (PLMs) have achieved a multitude of successful applications in natural language understanding (Devlin et al., 2018; Liu et al., 2019; He et al., 2021b) and generation (Lewis et al., 2019; Zhang et al., 2020; Yang et al., 2020; Brown et al., 2020) . The predominant methodology for domain adaptation is fine-tuning on labeled domain-specific data or continued pre-training (Gururangan et al., 2020) on unlabeled domain-specific data. Although effective, both fine-tuning and continued pre-training methods require tuning all the parameters of a PLM, raising high costs beyond many institutions' reach. To mitigate this, multiple parameter-efficient fine-tuning (PEFT) methods are proposed, including prompt-based tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . However, they are more concerned about task adaptation and it is still unclear how to regularly, and inexpensively inject domain knowledge into PLMs for different domain-specific tasks. Moreover, directly tuning PLMs on a domain-specific corpus with PEFT methods will lead to the catastrophic forgetting problem (Yogatama et al., 2019; Gururangan et al., 2020) . These limitations highlight an important research question: how to adapt PLMs with the new domain knowledge while keeping the old-domain knowledge unmodified?\nInspired by the recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022 ) that found knowledge is stored in feed-forward networks (FFNs), we decouple the FFNs into two parts: the original pre-trained FFNs to maintain the olddomain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Specifically, we propose Mixture-of-Domain-Adapters (MixDA), a mixture of several domain adapters to inject domain-specific knowledge without affecting the old-domain knowledge. Our model has two stages: piq domain-specific tuning multiple knowledge adapters on unlabeled data and then piiq task-specific tuning adapters on labeled data. In the first stage, we train several domain adapters on both domain-specific corpus and pre-training corpus simultaneously while keeping the original feed-forward networks unchanged. In the second stage, we train a mixture-of-adapters gate to dynamically select the desired knowledge adapter and a task-specific adapter for task adaptation.\nWe conduct experiments on a broad range of tasks, including 4 out-of-domain datasets, 9 in-domain datasets, and 2 knowledge-intensive datasets. Our experimental results demonstrate the effectiveness of MixDA on 15 datasets, spanning biomedical, computer science publications, news, and reviews. Further analysis displays three key properties of our proposed approach: piq Reliability: it shows superior performance on both in-domain and out-of-domain tasks. piiq Scalability: it scales well to the increasing number of domains. piiiq Efficiency: it adds only a small number of parameters per domain. We claim that these properties are helpful for language models as a service, where a frozen PLM is served, and multiple adapters are inserted to support different customized services.\n\nRelated Work\nIn this section, we will review four research lines related to injecting domain knowledge into pretrained language models: knowledge injection, domain adaptation, parameter-efficient fine-tuning, and mixture-of-adapters.\n\nKnowledge Injection\nKnowledge can be injected into PLMs by pretraining or fine-tuning, each corresponding to a separate research direction. During pre-training, the knowledge carried by knowledge graphs (Zhang et al., 2019; He et al., 2020) , entities (Sun et al., 2019; Xiong et al., 2020) , n-grams (Diao et al., 2020) , knowledge embedding (Wang et al., 2021b) , synonym and hyponym-hypernym relations in WordNet (Lauscher et al., 2019) , word-supersense knowledge (Levine et al., 2020) , and knowledge bases (Peters et al., 2019) can be injected into PLMs by feeding knowledge inputs and designing new objectives. However, pre-training-based methods are costly, making the application to huge PLMs (e.g., models with 175 Billion parameters) impossible. Fine-tuning-based methods only require an additional fine-tuning process. Some studies inject extra information into the input sentences, like knowledge triples from knowledge graphs (Liu et al., 2020) and knowledge context (Faldu et al., 2021) , while other studies explored specific model and training designs, like knowledge adapter networks (Wang et al., 2021a) , graph convolutional networks and LSTMs (Lin et al., 2019) , and metalearning (Sinitsin et al., 2020) . Zhu et al. (2020) formulated knowledge injection as a constrained optimization problem by adding a constraint on the loss on the unmodified facts. Recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) reveal that knowledge is stored in the feed-forward networks in PLMs. Inspired by these studies, we propose a new efficient tuning method to inject domain knowledge into feed-forward networks with minimal costs.\n\nDomain Adaptation\nPrevious studies have observed that language models suffer from a significant performance drop during the domain shift (Beltagy et al., 2019; Alsentzer et al., 2019; Huang et al., 2019; Lee et al., 2020; Ke et al., 2022b) . Effective strategies that can bridge the domain gap are introduced. Pre-training language models from scratch is effective but costly, like SciBERT (Beltagy et al., 2019) , BioBERT (Lee et al., 2020) , and ClinicalBERT (Alsentzer et al., 2019) . Recent studies explored continued pretraining (Gururangan et al., 2020) and adapter networks (Diao et al., 2021) to save time by training on unlabeled downstream task data. In this paper, we introduce plug-in domain adaptors for domain adaptation, which are effective and mitigate catastrophic forgetting issues because of the explicit learning strategy and efficient model architecture.\n\nParameter-Efficient Fine-tuning\nAnother relevant research direction is parameterefficient fine-tuning (PEFT), which only fine-tunes a small number of parameters. Existing works solve this problem from two perspectives: promptbased tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . Several works in adapter-based tuning are closely related to ours. AdapterFusion (Pfeiffer et al., 2021) aims to combine multiple task adapters but does not offer specific architecture or training strategies to learn external knowledge. DEMix (Gururangan et al., 2022) and MixDA both train adapters that specialize in domains and use mechanisms to route different adapters, but differ in routing methods, base models, and training strategies. K-Adapter (Wang et al., 2021a ) is re-stricted by its training on T-REx triples and lacks the flexibility to train on unstructured knowledge. Similar to MixDA, CPT (Ke et al., 2022a) integrates domain knowledge into LMs, but it employs a different approach. While MixDA uses domain adapters to substitute FFN layers and task adapters to perform end tasks, CPT adds CL-Plugins that learn domain knowledge. Recent work by He et al. (2021a) presents a unified framework that establishes connections across different PEFT methods. Our work can leverage any PEFT method and complement them.\n\nMixture-of-Experts\nMixture-of-Experts (MoE) (Shazeer et al., 2017) is introduced with several expert networks, gating networks, and load-balancing techniques. The following studies improve MoE on initialization and training schemes (Fedus et al., 2022) , routing mechanisms (Zuo et al., 2021; Yang et al., 2021) , and load-balancing issues (Lewis et al., 2021; Roller et al., 2021) . AdaMix (Wang et al., 2022) proposed a mixture of adapters to improve the downstream task performance. Instead of mixing different designs of adapters, our domain adapter is a feedforward network specifically designed for domain knowledge.\n\nApproach\nGiven a pre-trained language model M, the input is a sentence X \" t 1 t 2 \u00a8\u00a8\u00a8t i \u00a8\u00a8\u00a8t T (t i indicates the i-th token) and the output is the representation of each token. The overall architecture of our model is shown in Figure 1 . The training process is divided into two-stage. In Stage 1 (Figure 1 (a)), we inject new feed-forward networks (FFNs) (namely domain-adapter) paralleled to the original pre-trained FFNs in some Transformer layers, acting as a key-value memory. The newly injected domain-adapter is trained on both domain-specific unlabeled data and original pre-training unlabeled data to store new factual associations while keeping old-domain ones. All modules are frozen except domain-adapter in this stage. In Stage 2 (Figure 1 (b)), we train a mixture-of-adapters (MoA) gate and a task-adapter on downstream tasks with labeled data, and only these two new modules are updated. The MoA gate receives outputs from the old-domain FFNs and domain-adapter, then outputs a weighted sum of them. An additional taskadapter is inserted in each Transformer block to facilitate downstream tasks. Figure 1 (c) shows the structures of the domain-adapter and the MoA gate.\nIn this section, we first introduce domainadapter, which learns and stores domain-specific knowledge, and then describe task-adapters that perform the downstream task. Finally, we discuss how the MoA gate integrates the outputs from the FFN and the domain-adapter.\n\nDomain-Adapter\nPrevious studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) suggest that factual associations are stored in the FFNs of some Transformer layers. To help models learn domain-specific knowledge, we propose a lightweight domain-adapter that works parallel to the FFNs, and a training method to learn domain-specific knowledge alongside keeping old-domain ones. Domain-adapter has a simple bottleneck architecture consisting of a down projection layer, a nonlinearity (such as ReLU (Agarap, 2018)), and an up projection layer. This helps keep the parameter size low (Houlsby et al., 2019) with competitive performance.\nIn Stage 1, the domain-adapter is trained with the domain-specific and old-domain datasets in one batch. Note that all other parameters are frozen except the domain-adapter at this stage. Let L K denote the knowledge loss related to domain-specific knowledge, and L S denote the sampling loss related to old-domain knowledge. The knowledge loss is a cross-entropy loss on predicting masked tokens, and the sampling loss is designed to align the latent spaces of the old-domain knowledge and new domain-specific knowledge. The total loss L is given by a weighted sum of the two, that is:\nEQUATION\nwhere \u03bb is a weight for the knowledge loss.\nThe knowledge loss is implemented by using cross-entropy loss. Given a sentence with M mask tokens whose answers are m 1 , m 2 , \u00a8\u00a8\u00a8, m M , respectively, the knowledge loss L K is given by\nEQUATION\nwhere ppm i q is the probability for token m i output by M. 2016)), we translate each relation into a sentence, and then mask out its object. For example, the relation \"the Eiffel tower-/r/LocatedAt-Paris\" is translated into \"The Eiffel Tower is located at Paris.\", then \"Paris\" is substituted with the mask token, and the model is trained to fill the mask. \u201a Unstructured knowledge For unstructured knowledge (e.g., downstream unlabeled texts), we use the masked language model (MLM) similar to RoBERTa pretraining. Some tokens are randomly sampled from the input sentence and replaced with the special token <mask>, and the model is trained to predict the masked token. The cross-entropy loss is calculated to optimize the model. For old-domain knowledge and sampling loss, we train the model on general corpora including Wikipedia and BookCorpus (Zhu et al., 2015) . Specifically, for each batch, sentences randomly sampled from the dataset are input into the model. Given L layers that have domain-adapters installed, for each such layer l, we collect token representations from the FFN F l , and representations from the domain-adapter K l . The goal is to keep them as similar as possible. Thus, we calculate the sampling loss L S with L2 loss:\nL S \" 1 L L \u00ff l\"1 ||F l \u00b4Kl || 2 2 .\n(3)\n\nTask-Adapter\nAfter training domain-adapters, the model is aware of the domain knowledge, which is not directly related to downstream tasks though. Therefore, we add task-adapters on top of the domain-adapter to adapt to downstream tasks. For example, a domainadapter trained in biomedical knowledge can sup- \n\nMixture-of-Adapters Gate\nOn downstream tasks, it is possible that the output from the FFN, or a weighted sum of the two, produces better results. Therefore, in Stage 2, we train an additional mixture-of-adapters (MoA) gate simultaneously. The MoA gate receives the outputs from the attention layer q, the domain-adapter K, and the FFN F . q is first sent into a multi-layer perceptron (MLP):\nEQUATION\n)\nThe MLP is composed of a down-projection layer W d and an up-projection layer W u , and h \" W u \u03c3pW d qq, where \u03c3 is the nonlinearity function.\nThen, h is input into a Sigmoid layer to generate the weights of the FFNs and other domain-adapters:\nw \" Sigmoidphq.\n(5)\nThe final output o is a weighted sum of the outputs of the FFNs and the domain-adapter:\nEQUATION\nwhere r; s denotes matrix concatenation.\n\nExperimental Settings\nIn this section, we first introduce the datasets, then the baseline models, the evaluation metrics, and implementation details in the following four subsections, respectively.\n\nDatasets\nWe conduct experiments on three types of datasets: in-domain (ID) tasks that require general-domain knowledge; out-of-domain (OOD) tasks that require domain-specific knowledge; knowledgeintensive (KI) tasks that require commonsense knowledge.\n\u201a ID: GLUE Benchmark (Wang et al., 2018) including MNLI (Williams et al., 2017) , CoLA (Warstadt et al., 2019) , MRPC (Dolan and Brockett, 2005) , SST-2 (Socher et al., 2013) , RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) , STS-B (Cer et al., 2017) , WNLI (Levesque et al., 2012) , QNLI (Rajpurkar et al., 2016) , and QQP (Iyer et al., 2017) . \u201a OOD: ChemProt (Kringelum et al., 2016) , RCT (Dernoncourt and Lee, 2017) , IMDB (Maas et al., 2011) , and Amazon (He and McAuley, 2016) . ChemProt is a manually annotated chemical-protein interaction dataset extracted from 5,031 abstractions. RCT is a dataset based on PubMed for sentence classification. IMDB provides 25,000 movie reviews for sentiment analysis. Amazon is a dataset containing product reviews from Amazon, annotated with user ratings. \u201a KI: FEVER (Thorne et al., 2018) and Common-senseQA (CSQA) (Talmor et al., 2019) . FEVER consists of 185,445 claims that correspond to Wikipedia articles and are classified as supported, refuted, and not enough information. Common-senseQA consists of 12,247 questions with 5 choices and requires commonsense knowledge to predict the correct answers.\nFor Stage 1, we train the domain-adapter with unstructured knowledge related to the dataset following Section 3.1. The unstructured knowledge used is listed in Table 1 . We also experiment with structured knowledge in Section 6.2. For Stage 2, we adopt the true few-shot setting following (Perez et al., 2021) to demonstrate the effectiveness of MixDA. For each dataset class, we randomly sample K \" 16 examples from the original training set as the new training set, and another different K \" 16 examples as the validation set. The original validation set will be used as the test set. The Pfeiffer adapter is used in Stage 2 unless stated otherwise.\n\nBaselines\nIn our experiments, we use the following models as the main baselines. For convenience, we refer to them with the abbreviations in the parentheses later. \u201a HOULSBY (HO): Houlsby adapter (Houlsby et al., 2019) Prefix-Tuning trains a number of prompt embeddings for each task and pre-pends it before tokens. \u201a FINE-TUNING (FT): Fine-tuning all of the parameters of the RoBERTa-large model on downstream tasks.\n\nEvaluation Metrics\nWe adopt the Pearson correlation for STS-B since it is a regression task. The remaining are text classification tasks. Following Wang et al. (2018) ; Gururangan et al. ( 2020); Diao et al. (2021) , we adopt macro-F1 for MRPC and QQP, and micro-F1 for others as evaluation metrics. Macro-F1 computes the F1 independently for each metric, while micro-F1 computes an average metric of all classes. To account for the instability of small datasets, we report the average performance and the standard deviation of 3 runs with different random seeds.\n\nImplementation\nWe implement our RoBERTa-large model based on the Transformers library from HuggingFace 2 . The Houlsby adapter, the Pfeiffer adapter, and Prefix-Tuning are implemented based on the adaptertransformers library (Pfeiffer et al., 2020a) . LoRA is implemented based on OpenDelta (Ding et al., 2022) . During Stage 1, we train the domain-adapter with learning rate 1e-4, batch size 20, and weight decay 0.05. The knowledge loss factor \u03bb is set to 0.5. We train the 7 and 11 layers of RoBERTa-large with domain-adapter in 10 epochs. In Stage 2, we use the Pfeiffer adapter as the default task-adapter and train 20 epochs. All the experiments are conducted on Nvidia 2080Ti GPUs. We find the best hyper-parameters through grid search and the best results are listed in Appendix A. The computation time can be found in Appendix B.\n\nExperimental Results\nWe compare the performance of MixDA with our baselines on 15 datasets. First, we train the domainadapter for each domain individually and then perform each task with its corresponding domainadapter, which shows significant improvement over our baselines. Next, we plug in several domainadapters trained on different domains parallelly to verify the scalability of our model. detect the chemical-protein interaction. For example, MixDA shows more familiarity with words associated with that field, such as \"gefitinib\" and \"tyrosine kinase inhibitor\". In contrast, MixDA falters on STS-B, falling behind Pfeiffer by 0.8%. This is because the knowledge in Stage 1 is not effectively utilized. STS-B consists of sentence pairs like \"The cat sat on the mat\" and \"The cat did not sit on the mat\", with little need for additional knowledge. Across the three task domains, MixDA has an average improvement of 4.8% over RoBERTa + Pfeiffer on out-of-domain tasks, 2.5% on indomain tasks, and 5.0% on knowledge-intensive tasks. It shows that MixDA is not only effective for out-of-domain tasks and knowledge-intensive tasks that require additional knowledge but is helpful for general-domain language tasks as well, demonstrating its ability to excel at both in-domain and out-of-domain tasks (reliability).\n\nParallel Domain Adapters\nIn the previous section, we explored using a single domain-adapter for each downstream task. Next, we show the scalability of MixDA by using parallel domain-adapters and only train the MoA layer and task-adapters in Stage 2. The training process in Stage 2 follows the previous experiments. Table 3 shows the comparison across single domainadapter, parallel domain-adapters, and RoBERTa + Pfeiffer on 7 datasets. On average, parallel domainadapters show an improvement of 0.6% over vanilla RoBERTa + Pfeiffer, even though they fall behind the single domain adapter by 1.9%. This could be attributed to the MoA gate choosing the suboptimal domain-adapter for some test data. Still, considering its improvement over Pfeiffer, the MoA gate chooses the correct domain-adapter in most cases. Therefore, MixDA demonstrates its scalability, allowing end users to train Stage 1 on different datasets and combine them later. Overall, in both single and parallel situations, MixDA significantly improves upon the vanilla RoBERTa + Pfeif- fer model with a small increase in model size. This is due to the ability of MixDA to capture knowledge and the MoA to select useful knowledge for downstream tasks.\n\nAnalysis\nIn this section, we analyze the respective contributions of each part of MixDA through detailed analysis, including the Stage 1 training, task-adapters in Stage 2, and the mixture-of-adapters gate.\n\nAblation Study\nIn this section, we conduct an ablation study to reveal the contributions of each part of the model. There are three variants: (1) We remove the MoA gate and choose the domain-adapter instead of the RoBERTa feed-forward layer (-MoA). ( 2 \n\nStructured and Unstructured Knowledge\nIn Section 5, the MixDA is only trained on unstructured knowledge. As a comparison, we also train the domain adapter on ConceptNet, a structured knowledge dataset, and then attach both the unstructured and structured to our model and train the MoA layer and the task-adapter during Stage 2.\nTable 5 shows the result of combining structured and unstructured knowledge in Stage 1. FEVER and CSQA, two knowledge-intensive tasks, have the greatest improvement: 10.3% for FEVER and 1.2% for CSQA. This is because ConceptNet stores commonsense knowledge that can help both tasks. Meanwhile, MRPC and STS-B also obtain improvement, showing that ConceptNet can benefit general language tasks as well. In conclusion, the experiment demonstrates the ability of MixDA to utilize structured knowledge, the extensibility of our model, and the possible benefits of structured knowledge.\n\nEffectiveness of Task-Adapters\nIn most experiments of this paper, we adopt Pfeiffer as the task-adapter unless otherwise specified. In this section, we test the performance of MixDA combined with other kinds of task-adapters, including Houlsby, Prefix-Tuning, LoRA, and Pfeiffer. parameters compared to Houlsby, making it the optimal choice of task-adapters in our experiment.\n\nConclusion\nIn this paper, we proposed MixDA, a mixture of adapters for domain adaptation. We first decouple the knowledge modules (i.e., FFNs) into the old-domain and domain-specific FFNs. Then we propose a two-stage adapter tuning strategy: first tuning the domain adapter on each domain and then tuning the task adapter on each task. Moreover, our model could be scaled to multiple domains easily with the introduction of the mixture-of-adapters gate. Empirically, MixDA achieved significant improvement over in-domain tasks, out-of-domain tasks, and knowledge-intensive tasks. Further analyses demonstrate the reliability, scalability, and efficiency of our method.\n"}
{"question": "What kind of manuscripts did the study date using machine learning algorithms and release publicly for other researchers?", "evidence": "  To test this hypothesis, we collected 220 manuscripts dated with an upper CE date limit (i.e., not after that date). This process led to the date estimation for 159 manuscripts, which we release publicly in our repository to assist other researchers. ", "options": ["A. Literary papyri", "B. Fragments of ancient books", "C. Greek inscriptions", "D. Manuscripts with an upper CE date limit"], "answer": "D", "content": "\nIntroduction\nAncient textual artefacts are arguably the richest source of information on the ancient world. In the Graeco-Roman world and particularly in its Greekspeaking part, the most extensive coeval texts come from inscriptions and papyri. The latter is a collective term used for all ancient manuscripts, regardless of their writing material which, apart from papyrus, may be parchment, pottery, wood, and others. To correctly evaluate and make good use of these texts, we need to determine their date, provenance and historical context of their production and use. As far as dating is concerned, the value of the relevant evidence provided by the artefacts themselves varies considerably, ranging from a direct date in the text (following, of course, the calendar and dating system of the respective historical period) to no evidence at all. In between, there are texts containing references to known historical figures and events of a certain period, papyri which have been found next to other objects that can be dated, or other indirect evidence. The presence or absence of a date depends on the type of text preserved on the papyrus and its use through time, as well as on its state of conservation. Just like in modern times, it is much more likely to include a date in an official letter than in a page torn from a novel book. At the same time, it is more probable to find a date in a fully surviving letter than in a damaged one missing, for instance, the upper part of the first page.\nGreek papyri, which mostly survive in fragments, are divided into two broad categories: books (literary and sub-literary papyri) and documents of all kinds (documentary papyri). The former ones never carry a date, whereas the latter often do, albeit not always unambiguously convertible by modern scholars. Most importantly for our study, literary papyri contain copies of works authored many years (often centuries) before the production of the actual manuscripts. On the other hand, documentary texts were usually written down as they were composed or shortly after that, making the content of their texts contemporary to their writing style or script. Therefore, any temporal indication in the text is also dating evidence regarding the production of the document. Even when there is no direct date in the text (e.g. Figure 1 ), documentary papyri can be dated securely sometimes within a short time-frame, because they may refer to known historical events or concern people known through other sources to have lived at a particular time.\nWhen neither direct or indirect dating is possible, papyrologists resort to palaeography, the study of the script. In palaeography, particular writing styles are associated with certain chronological periods. Therefore, similar writing styles point to similar dates (Mazza, 2019) . Securely dated specimens are used as a guide to chronologically place the undated ones. Growing criticism on the subjectivity of palaeographical dating (Mazza, 2019; Choat, 2019; Nongbri, 2019 Nongbri, , 2014) ) highlights the need for more reliable methods. Recent efforts for computational dating of historical manuscripts are based on the script rather than the text and, although they consider various languages, they disregard Greek (Omayio et al., 2022) .\nIn this study we focus on computational dating of Greek documentary papyri based on their transcriptions, contributing in the following three ways:\n1. We present and publicly release a machineactionable dataset of 389 documentary Greek papyri, containing texts of various aspects of daily life (e.g. contracts, receipts, letters).\n2. We draw the baseline in text regression for the tasks of dating experimenting with Monte Carlo and leave one out cross validation.\n3. We apply a committee of regressors to three papyri, which present different types of dating challenges, and on 159 manuscripts for which only the upper date limit is known.\nThis approach does not apply to literary papyri and our research involves solely documents. Apart from their texts being contemporary with the actual manuscripts (by dating the text, we date the papyrus), nonliterary papyri also include vastly more numerous objectively dated specimens than literary ones. Specific dates on our training set also allow for more accurate (narrower date-spans) predictions by our models.\n\nRelated Work\nDating historical documents with computational means has been studied for many languages (Baledent et al., 2020; Dhali et al., 2020; Li et al., 2015; Hamid et al., 2019; Adam et al., 2018) . However, very limited work has been done for Greek and no published work at all has focused on Greek papyri. The only work to our knowledge is Ithaca, a Transformer trained on ancient Greek inscriptions performing text restoration, geographical attribution, and dating (Assael et al., 2022) . Ithaca has achieved an error of 0.29 centuries in dating epigraphs. This result is by far better than an onomastic baseline using the known distribution of Greek personal names to infer the date, which scored 1.44. Inscriptions differ from papyri in many aspects (such as the genre, the length, and their geographical distribution), but in principle, this system is applicable to our data and was therefore used as a baseline. Below, given the absence of dating studies for Greek, we summarise work for other languages.\nThe studied languages are Latin (Baledent et al., 2020; Wahlberg et al., 2016 Wahlberg et al., , 2015)) , Hebrew (Dhali et al., 2020) , Dutch (Hamid et al., 2019 (Hamid et al., , 2018;; He et al., 2014 He et al., , 2016b,a),a) , Arabic (Adam et al., 2018) , Swedish (Wahlberg et al., 2016 (Wahlberg et al., , 2015)) , French (Baledent et al., 2020) and English (Li et al., 2015; Rastas et al., 2022) . A collection of 595 Dead Sea Scrolls, in Aramaic script, was the dataset with the oldest manuscripts, dated from 250 to 135 BCE, and the only one so far concerning texts written on papyri (Dhali et al., 2020) . The rest of the datasets comprised more data, ranging from less than five (Adam et al., 2018) to more than ten thousand manuscripts (Wahlberg et al., 2015) or more (Rastas et al., 2022) , while the one with the most recent manuscripts comprises historical English-language documents (Li et al., 2015) , printed between the 15th and 19th CE.\nThe employed methods usually were standard machine learning methods, such as KNN (Adam et al., 2018) , decision trees (Baledent et al., 2020) , random forests (Baledent et al., 2020) and support vector machines (Hamid et al., 2019; Dhali et al., 2020; He et al., 2014 He et al., , 2016b,a),a) . Textural features, such as Gabor filters, Uniform Local Binary Patterns and Histogram of Local Binary Patterns are extracted and then fed to the classifiers (Hamid et al., 2018) . The writing style evolution, however, has also been used as an intermediate step (Dhali et al., 2020; Adam et al., 2018) . In this case, the periods are first aligned with specific writing styles. Then, any new manuscript is dated based on the detected style.\nPre-trained convolutional neural networks have been used to extract features, which are passed to a classifier or regressor (Hamid et al., 2019; Wahlberg et al., 2016) , or used in combination with text features extracted with optical character recognition methods (Li et al., 2015) . Transfer learning has been reported to lead to human performance (Wahlberg et al., 2016) . This was deemed to be the most promising direction for the present study on Greek manuscripts, and was, hence, employed.\n\nData\nOur dataset, which we release publicly, 1 comprises the transcriptions of 389 manuscripts, dated from the 3rd century BCE to the 7th century CE, originating from Greco-Roman Egypt (with a few exceptions from the Near-East).\n\nThe source\nThe dataset was compiled mainly from PA-PYRI. INFO. 2 The documents in its collections set a reliable point of reference for scholars who aspire to study the evolution of ancient manuscripts in time. These collections incorporate full transcriptions and references to scholarly editions of the papyri, as well as a set of metadata that can also assist in dating (e.g. provenance).\n\nThe scripts and the language\nNonliterary papyri in Greek from the 3rd c. BCE to the 7th c. CE are written in a great variety of cursive hands (Harrauer, 2010) , posing an extra challenge for image classification methods and calling for other approaches. The language of the papyri, Greek of the Ptolemaic, Roman and early Byzantine periods, reflects the diversity and the diachronic changes of the Greek-speaking communities in Egypt, which is the provenance of most of our specimens.\n\nThe ground truth\nThe date of a manuscript may be found in different forms. It can be an exact date, a range of years, a starting date (not before that date), or an ending date (not after that date), or two-three alternative dates. Our dataset has been curated so that dating applies at the level of the quarter of the century, by considering manuscripts dated exactly or with a period ranging within that quarter. We did not consider manuscripts that were dated only before or after a specific date.\n\nData collection\nOur first dataset comprised 400 manuscripts, 40 samples per century. Our initial pool consisted of 77,040 items and we opted for ones that satisfy the following conditions:\n\u2022 The transcriptions must be available in machine actionable form.\n\u2022 The papyri must contain documents (not works of literature) to ensure that text and papyrus are contemporary. 3\n\u2022 The papyri must be securely and accurately dated. Many papyri do not carry a date and are, therefore, dated with subjective criteria or with a large date span (e.g. 1st-2ndCE).\n\u2022 The image is available, to allow image-based dating and potentially jointly from different modalities: text and image.\nGiven these limitations, it was the 7thCE that dictated the size per century of a balanced dataset, since there are not more than 40 securely dated papyri from 7thCE. For each of these records, the text was retrieved afterwards from PAPYRI.INFO by parsing the respective XML files. We discarded records whose extracted text was less than ten characters, which resulted in our final 389 records. From these records, we extracted the entire text from one side of the papyrus (the side that had more text than the other). In the few cases of papyri with more than one fragment, we only included the first one. This decision was based on weighing the benefit of avoiding a considerable amount of noise during automatic parsing against eliminating a portion of text, in a dataset whose nature is by definition fragmentary.\n\nNormalisation\nThe transcribed text comprises a variety of characters and symbols. We preprocessed the data by lowercasing and normalising the text (see Table 1 ). We (o) '\u1f45', '\u1f43', '\u03cc', '\u03cc', '\u1f44', '\u1f41', '\u1f40', '\u1f78' (\u03b1) '\u1f02', '\u1fb4', '\u1f03', '\u1f85', '\u03ac', '\u1f01', '\u1f04', '\u1fb6', '\u1f00', '\u1fb7', '\u1f70', '\u03ac', '\u1f05' (\u03b7) '\u1f24', '\u1f23', '\u1fc3', '\u1f22', '\u1f20', '\u03ae', '\u1f26', '\u1f25', '\u1fc6', '\u1f27', '\u1f97', '\u1f94', '\u1fc7', '\u1f21', '\u1fc4', '\u1f91', '\u1f74', '\u03ae' (\u03b9) '\u03af', '\u1fd6', '\u1f37', '\u1f31', '\u1f36', '\u1fd2', '\u03af', '\u1f30', '\u1f76', '\u0390', '\u1f34', '\u03b9', '\u03ca', '\u1f33', '\u1f35' (\u03b5) '\u03ad', '\u1f72', '\u03ad', '\u1f14', '\u1f10', '\u1f15', '\u1f11', '\u03b5' (\u03c5) '\u1f57', '\u03cd', '\u1fe6', '\u03cd', '\u1f55', '\u1f53', '\u1f7a', '\u1f51', '\u1f54', '\u1f50', '\u1f56' (\u03c1) '\u1fe5', '\u1fe4' (\u03c9) '\u03ce', '\u1ff6', '\u1f66', '\u1fa4', '\u1f60', '\u1f67', '\u1ff3', '\u1ff7', '\u1f62', '\u1f65', '\u03ce', '\u1fa7', '\u03ce', '\u1fa6', '\u1f64', '\u1fa0', '\u1f7c', '\u1f61', '\u1ff4' (\u03c3) '\u03c3', '\u03c2' Table 1 : Normalisation rules of characters in the dataset, all characters on the right have been replaced by the character on the left. also discarded any character besides the 24 Greek letters, also removing white space and all punctuation marks. We did not eliminate the editors' corrections and supplements nor edit otherwise the data, which often led to duplicate words with alternative orthography (original and normalisation).\nThe transcriptions available are not diplomatic (reflecting exactly what is written) but normalised according to modern conventions, for example as far as punctuation and word separation (or sometimes spelling) are concerned. Therefore, we chose to disregard these conventions, because they do not represent data present in our sources, but normalisation on the papyrologists' part for the purpose of scholarly editions.\nTo provide some more concrete examples, there is no capitalization of proper names or initial words in sentences in papyri. Punctuation is very scarce and sometimes completely absent. Diacritics are not meaningless, but they are extremely rare in documentary papyri (i.e., except diaresis which is used in a different way than modern conventions, to mark iota and upsilon as the first letter of a word). Breathings and accents are marked inconsistently (if at all) by different scribes. Hence, removing diacritics leads to inclusion and can help avoid multiple variations of what is in fact the same word. Regarding spelling, we kept both the original and the corrected form (if provided by the editors), because spelling mistakes reflect language evolution.\n\nExploratory analysis\nThe overall text length per quarter of century varies over time, as can be seen in Figure 2 . Although we have selected an equal number of manuscripts per century ( \u00a73.4), the number of lines within each manuscript varies, and so does the line length. Furthermore, within a century, manuscripts of a spe- cific quarter of a century may be more frequent due to random discoveries, as is the case of 7thCE, where the first quarter holds most of the support, a discrepancy deriving from the reduced number of dated papyri in this century overall.\nThe most frequent character in our dataset is '\u03b1' (35,101 occurrences), followed by '\u03bf' (33,176), '\u03b9' (30,151), and '\u03b5' (25,116) . On the other hand, the least common are '\u03b2' (2520), '\u03be' (1210), '\u03b6' (379), and '\u03c8' (334). These figures are coherent with general frequencies of letters in Ancient and Modern Greek (Mikros et al., 2005) .\nIn order to assess the quality of the ground truth, we employed the Callimachus' Conservation number (CCN), 4 which provides an educated estimation of the preservation and legibility of a papyrus. The lowest score is 0 and the highest score (i.e., 1) indicates readability and 'perfect' conservation of the text. The status of the conservation of a papyrus affects the quality of the transcription, indicating the amount of text that has not been recorded in the transcriptions (or recorded with some level of uncertainty) because of the material state of preservation of the manuscripts. Damage in papyri could affect as little as one or two letters (or even none), to as much as several lines and whole parts of the \n\nMethodology\nTo estimate the date of production of manuscripts, we opted for text regression, taking advantage of the continuous target objective. Statistical validity was established with 5-fold Monte Carlo crossvalidation. The best regression method was used to form a committee of models, which were applied on unseen data in order to analyse the predictions.\n\nBenchmarking\nWe performed Monte Carlo cross-validation, by sampling 90% for training, 10% for validation, and then re-sampling with replacement five times. We report the mean absolute error (MAE), the mean squared error (MSE), and the explained variance (R 2 ). Besides the average results across folds, we also report the best score achieved per metric.\n\nRegression methods\nFern\u00e1ndez-Delgado et al. ( 2019) surveyed 77 regression methods and undertook an experimental analysis on 83 datasets. Regression with extremely randomised trees achieved the best R 2 in many datasets but gradient boosting and random forests were also found to have a promising performance. Following these findings, we opted for extremely randomised trees, random forests, gradient boosting, and linear regression for our experiments. 5 Extremely randomised trees (XTrees) is a treebased ensemble, created with the Extra-Trees algorithm (Geurts et al., 2006) . Although simple in nature, it is both accurate and efficient Fern\u00e1ndez-Delgado et al. (2019) . Compared to other ensembles that use decision trees, XTrees splits the nodes of the tree by choosing randomly cut-off points and the trees grow by using the whole sample to learn instead of bootstrapping.\n\nThe Committee\nUsing the best-performing regression method out of the ones examined, we performed leave one out cross-validation, which allowed an evaluation using the whole dataset. Furthermore, it yielded as many regressors as the data points, which in our case is 389. We used these models to form a committee and date unseen papyri (further discussed in \u00a76).\n\nEmpirical analysis\nThis section presents our experimental results using regression on textual features to date Greek manuscripts. First, we present preliminary experiments and then we analyse the experimental findings from our regression analysis.\n\nPreliminary experiments\nPreliminary experiments comprised image classification (Hamid et al., 2018 ), text classification with Transformers trained on another domain (Assael et al., 2022) , and transferring learning from large language models (Koutsikakis et al., 2020) . Image classification was used prior to using transcribed text as our input, experimenting with using the documents' images (Hamid et al., 2018; Wahlberg et al., 2016; Paparigopoulou et al., 2022) . Vanilla convolutional neural networks were outperformed by a pre-trained one (Tan and Le, 2019), fine-tuned for our dating task. Our estimated MAE, however, was consistently more than a hundred years (Table 2 ), hence we opted for textual input. Ithaca was presented by Assael et al. (2022) , consisting of a Transformer that is trained not only in dating but also in text restoration and geographical attribution. Ithaca has achieved an error of 0.29 centuries in dating inscriptions, which is by far better than an onomastics baseline (error of 144 years).\nBy using the open-access web interface, 6 we scored all our preprocessed texts, 7 registering a MAE of approx. one century by using the maximum decade predicted or the average of the distribution (Table 2). The difference from the published result possibly stems from the fact that this is a model trained and focused on inscriptions, not papyri. Transfer learning was used with GreekBERT, a Transformer that is pre-trained in masked language modelling, among other tasks, in modern Greek (Koutsikakis et al., 2020) . GreekBERT has been further pre-trained in ancient Greek (Singh et al., 2021) . We experimented with fine-tuning both variants in predicting the date, 8 but MAE was approx. one century (Table 2 ).\n\nRegression analysis\nExperiments were undertaken with Google Colaboratory, using a 12GB NVIDIA Tesla K80 GPU. We extracted term-frequency-inverse-documentfrequency features using lower-cased text and character n-grams (from 1-to 5-grams). 9 All other parameters were set to default values. 10\n\nMonte Carlo cross validation\nLinear regression achieved a MAE of 86 years on average (Table 2 ) and a MSE of 1.33. R 2 was similar across folds, around 83. A random forest had an even better MAE of 73 years on average but a worse MSE (1.58). Its average R 2 was lower than that of linear regression, but the maximum one achieved across folds was much better. Random forest also outperformed both gradient boosting methods in MAE but GBoost achieved a better MSE and R 2 on average. XTrees achieved the best results in all metrics, with a MAE of 54 years and the best R 2 climbing up to 95.43.\n\nLeave one out cross validation\nUsing the best performing XTrees, we performed leave one out cross validation, by hiding one instance, training the algorithm on the remaining instances, and then using the model to predict the hidden record. 11 The MAE was found to be 55 years, MSE was 1.11, and R 2 was 85.89, close to the Monte Carlo evaluation scores. In order to better understand the errors, we rounded the predictions and the ground truth, evaluating as if we would in a classification setting. Predictions most often fall on or close to the diagonal (Figure 4 ), which explains the low error. The best result is 8 We used white space, to allow subword computation. 9 Preliminary experiments with centroid or trainable word embeddings before recurrent or convolutional neural networks deteriorated performance.\n10 Manual hyper-parameter tuning per regressor yielded insignificant improvements.\n11 The experiment lasted 15 hours. 3 .\nachieved for the 1st and 2nd CE, followed by the 7th CE (see Table 3 ). The overall accuracy is 60%.\n\nError analysis\nIn very few cases, our leave-one-out regression fell considerably out of its predictions (Figure 4 ). Our analysis showed that these texts happen to contain specific words typical of another period, which confused the prediction. For instance among the highest prediction error were two late texts (6-7thCE) that exceptionally contain \u03a3\u03b5\u03c1\u03b1\u03c0\u03af\u03bf\u03c5 and \u0392\u03b1\u03c3\u03b9\u03bb\u03b5\u03af\u03bf\u03c5, usually found in Ptolemaic time (3rd-1stBCE). In another case, we provided experimentally the longer version of the text, initially parsed only partially ( \u00a73.4). Using the full text led to an accurate prediction, influenced by the word 'indiction' in the additional text ( \u00a77.1).\n\nUse cases\nWe applied our 389 regressors, produced upon leave-one-out cross-validation, to three use cases, which present different types of dating challenges.\n\nPSI 8 934\nThis document 12 preserves the ca. 15 last lines of a land lease. texts from the 6th and early 7thCE c., the Dioscorus archive (Fournet, 2008) , because, among other concordant elements, it contains microtoponyms from the respective village countryside. The notary who signed the contract, Abraam, is known from other documents, which is crucial evidence for the dating of the papyrus. This notary's period of activity has been proven to span at least between 524 and 545 (Fournet, 2003) . This papyrus, therefore, is securely dated by indirect evidence, but no date is explicitly mentioned in the text (Fournet, 2008) . Our average prediction is 310 CE, dated between 260 CE (min) and 352 CE (maximum prediction).\n\nP. Basel 2 15\nThis papyrus, also shown in Figure 1 , is a private letter dated indirectly from the 1st CE. The letter is almost complete, except for a damaged word at the end of line 5. Private letters usually do not bear a date. The dating, therefore, by the editor is done on palaeographical grounds as well as on the basis of scribal habits: \"the hand [...] is more at home in the first century CE than the second, a dating that is supported by the writer's use of iota adscript...\" (Huebner et al., 2020) . Iota adscript is an expected feature in the 3rd BCE, starting to be irregularly written between the 2nd BCE and the first CE to almost completely disappear from the 2nd CE onwards (Clarysse, 1976) . Onomastics strengthen the editor's dating hypothesis: of the three personal names mentioned in the letter (Pasis, Orsenouphis, and Tithoes), the first two are attested from ca. 250 BCE to 250 CE while the last one starts appearing in the papyri only in the 1st c. CE. 13 Our models date this to 140 BCE, from 165 BCE to 112 BCE.\n\nP. Petra 1 5\nThe last manuscript 14 contains a request for transfer of taxation from 538 CE. It is a geographical outsider since it does not come from Egypt but from Petra (Jordan). We tested this manuscript since many of the words found in the text are infrequent in Egyptian manuscripts, on which our models are trained. The date mentioned in the papyrus is \"second indiction\". This refers to the second year of a repeated fifteen-year cycle (indiction) and the year 538 is relative, since it could be the second year of the previous or the next indiction (523 or 553). 538 is logically deduced by the editors in view of the whole dossier of papyri from Petra. Our models date this manuscript to 555 CE (521-575 CE), overcoming the geographical variation.\n\nDiscussion\nThe computational, quantitative method suggested in this work is intended to complement human expertise. Its main contribution lies in providing an additional dating criterion for ancient Greek documents, in addition to the ones usually employed by papyrologists (palaeography, onomastics, prosopography, toponymy, archaeological evidence, etc.). It can predict a date for those papyri that do not include one, narrow down the possible time-span of doubtful dating, or contribute to deciding on one particular date when several alternatives seem possible. Despite the fact that limitations exist (discussed in \u00a77.3), compared to traditional approaches the models trained in this study are expected to reduce biases. Their value is not limited to predicting dates for individual manuscripts, but they can be applied to any attribute of a group of papyri, e.g. the place of provenance or the text's type. At the same time, easily accessible open-source metadata exist for most published papyri ( \u00a73.1).\n\nRationale generation\nThe use of supervised learning, such as the work of Assael et al. (2022) or ours, can yield accurate estimations, which can at least help the human expert. The assistance is greater, however, when explanations are provided for the models' decisions. In our case, we used a committee of hundreds of regressors in order to estimate the date of three use cases. Therefore, we sampled models per case and generated rationales regarding their predictions, by using their Shapley values (Lundberg and Lee, 2017).\nIn the case of PSI 8 934 ( \u00a76.1), our investigation showed that the mention of the name 'Aurelios Victor' ('\u0391\u1f50\u03c1\u03ae\u03bb\u03b9\u03bf\u03c2 \u0392\u03af\u03ba\u03c4\u03c9\u03c1') influenced the decision, resulting to a more recent date than what would have been predicted otherwise. Similarly, in the case of P. Petra 1 5 ( \u00a76.3), the decision was influenced by a reference to 'indiction' ('\u1f30\u03bd\u03b4\u03b9\u03ba\u03c4\u03af\u03c9\u03bd\u03bf\u03c2'), a word that refers to a periodic reassessment of taxation in the Late Roman Empire.\n\nIn the wild\nComputational dating can facilitate a macroscopic analysis of vaguely dated or undated manuscripts. By generating estimated dates for hundreds of such manuscripts, the expert can view from distance the collection, potentially drawing useful conclu-sions or making significant remarks. To test this hypothesis, we collected 220 manuscripts dated with an upper CE date limit (i.e., not after that date). We formed a committee of regressors, 15 and we estimated the minimum, the maximum, and the average chronology of each manuscript. In 28% of them, the maximum prediction exceeded the upper threshold and was discarded to avoid doubting the expert. This process led to the date estimation for 159 manuscripts, which we release publicly in our repository to assist other researchers. As can be seen in Figure 5 , some of our estimations fall far away from the upper limit (in red) while others fall close. The estimated date from our regressors' committee should be read along with other information, which is kept in the shared corpus, such as the place settlement (Figure 6 shows frequent places). We observe, for example, that in some places the estimated dates fall closer to the upper limit (e.g. in Oxyrhynchos and Tebtynis the distance is 132 years) compared to others (e.g. in Antinoopolis and Hermopolis the distance is 283 and 384 years).\n\nChallenges and limitations\nOur experimental analysis proved that text regression is a considerably reliable and accurate tool in dating nonliterary papyri. Limitations and challenges stem mainly from the composition of our dataset, which is balanced as far as the dates of the papyri included are concerned, both at the level of the century (approx. 40 records per century) and at the level of the quarter of the century (albeit less strictly and with the exception of the 7th CE). Furthermore, although we retained a substantial text sample of each papyrus, in approximately 1/4 of the records some text was eliminated.\n\nBiases\nDespite our effort to balance the dataset in terms of dates, biases are present. Since our main concern in collecting the data was for the date distribution, no deliberate selection was made on the basis of the document types. Some types are thus over or underrepresented (e.g. private letters that do not usually bear a date; \u00a76.2). Each type of document has however distinctive linguistic characteristics, such as the level of formality or unusual constructions (e.g. accounts). This uneven typological representation probably affects the performance of the models. Other possible biases in the dataset concern the provenance of papyri, the length of their text, and the state of conservation (sizeable portions of missing text or entirely missing parts of the documents).\n\nChronological analysis of words\nChronological analysis of word occurrence is possible if we detect and collect terms only attested in the papyrological material during a limited period. The word 'denarius' only appears after the 2nd CE and before the 5th CE, its presence in a text thus means that the text must have been written during this timespan. Likewise a text containing the word 'indiction' cannot have been written before the 4th CE. The investigation should also regard the possibility that the models make a prediction for a papyrus based on typical dating formulas present in the text like the name of the ruling emperor. Although our investigation of explanations did not yield any major concerns, a bigger sample of test cases should be created and more explainability methods should be employed (Ribeiro et al., 2016) to make conclusive remarks on this front.\n\nTranscription of papyri is not optional\nTranscription of the papyri is required (at least partial, but substantial) to reach this high degree of accuracy with our method. Thus, while there are transcriptions available for most already published papyri, it is less practical for dating unpublished papyri that have not been yet transcribed to a relatively high standard. In that case, image classification on the scripts can provide a less accurate prediction of the date as starting point.\n\nConclusion\nWe presented a machine-actionable dataset of 389 Greek documentary papyri of (mostly) Egyptian provenance, dated and balanced in terms of chronological quarter-century distribution. We trained extremely randomised trees on top of character n-gram-based features, reaching a mean absolute error of 54 years and 60% in century-level classification accuracy. We then formed a committee of regressors, which we applied to three use cases: a land lease, a private letter, and a geographical outsider (not from Egypt). To assist future research, our committee dated 159 manuscripts, for which only the upper limit is known. Future endeavours for this research extend far beyond the dating of individual manuscripts. It can produce valuable data for the study of the Greek language and its evolution through a millennium, help identify and trace linguistic habits and trends, as well as the history of document production, circulation, and use (e.g. which period produces what kind of texts, which administration relied on what type of documents, etc.). It can also produce further data and resources towards the typology of ancient Greek documents, completing with computational methods the work already underway and well-advanced of the grammateus project. Last, it can in the future fruitfully be combined with computational paleography to analyse the script and content of a given text.\n"}
{"question": "What does the introduction mainly tell us?", "evidence": "  Introduction\nThe task of correcting factual errors is in high demand and requires a significant amount of human effort. The English Wikipedia serves as a notable case in point. It is continually updated by over 120K editors, with an average of around six factual edits made per minute 2 . Using machines to correct factual errors could allow the articles to be updated with the most current information automatically. This process, due to its high speed, can help retain the integrity of the content and prevent the spread of false or misleading information.\nIn addition, the hallucination issues have been shown to be a prime concern for neural models,\n ", "options": ["A. The need for correcting factual errors is substantial, and it necessitates a considerable amount of human effort.", "B.The English Wikipedia is constantly being updated by a community of over 120,000 editors, who make an average of approximately six factual edits every minute. ", "C. Machines\u2019 high speed can help keep the integrity of the content and prevent the spread of false or misleading information.", "D. The problem of hallucination has emerged as a crucial concern for neural models."], "answer": "A", "content": "\nIntroduction\nThe task of correcting factual errors is in high demand and requires a significant amount of human effort. The English Wikipedia serves as a notable case in point. It is continually updated by over 120K editors, with an average of around six factual edits made per minute 2 . Using machines to correct factual errors could allow the articles to be updated with the most current information automatically. This process, due to its high speed, can help retain the integrity of the content and prevent the spread of false or misleading information.\nIn addition, the hallucination issues have been shown to be a prime concern for neural models,\n\nEvidence\nThe novel COVID-19 is highly contagious and is transmitted mostly through respiratory droplets. But, whether its transmission can be forwarded by touching a surface (i.e., a fomite) is uncertain.... COVID-19 has a case fatality rate of below 2%.\n\nFinal Correction\nCOVID-19 is not infectious.\n\nInput Claim\nFigure 1 : An example of a factual but unfaithful correction leading to misleading information. While it is technically true that the majority of people infected with COVID-19 will recover, there is no information in the evidence that supports the final correction. Additionally, when this statement is taken out of context, it could mislead people to believe that COVID-19 is not dangerous and that there is no need for precautions, which is false. A factual and faithful correction is \"COVID-19 is highly contagious.\".\nwhere they are prone to generate content factually inconsistent with the input sources due to the unfaithful training samples (Maynez et al., 2020) and the implicit \"knowledge\" it learned during pre-training (Niven and Kao, 2019) . Factual error correction can be used in both pre-processing and post-processing steps to rectify the factual inconsistencies in training data and generated texts, respectively. This can help build trust and confidence in the reliability of language models.\nPrior work typically formulates factual error correction as a sequence-to-sequence task, either in a fully supervised or in a distantly supervised manner (Shah et al., 2020; Thorne and Vlachos, 2021) . While these approaches have made great strides in generating fluent and grammatically valid corrections, they only focus on the aspect of factuality: whether the outputs are aligned with facts. Little emphasis was placed on faithfulness: the factual consistency of the outputs with the evidence. Faithfulness is critical in this task as it indicates whether a generated correction reflects the information we intend to update. If faithfulness is not ensured, this could lead to the spread of misleading content, causing serious consequences. Figure 1 shows a concrete example. In the context of automatically updating textual knowledge bases, the topic of an unfaithful output would likely deviate much from that of the expected correction. Therefore, such an edit is not desirable, even if it is factual.\nIn this work, we present the first study on the faithfulness aspect of factual error correction. To address faithfulness, we propose a zero-shot factual error correction framework (ZEROFEC), inspired by how humans verify and correct factual errors. When humans find a piece of information suspicious, they tend to first identify potentially false information units, such as noun phrases, then ask questions about each information unit, and finally look for the correct answers in trustworthy evidence (Saeed et al., 2022; Chen et al., 2022) . Following a similar procedure, ZEROFEC breaks the factual error correction task into five sub-tasks:\n(1) claim answer generation: extracting all information units, such as noun phrases and verb phrases, from the input claim; (2) question generation: generating question given each claim answer and the original claim such that each claim answer is the answer to each generated question; (3) question answering: answering each generated question using the evidence as context; (4) QA-to-claim: converting each pair of generated question and answer to a declarative statement; (5) correction scoring: evaluating corrections based on their faithfulness to the evidence, where faithfulness is approximated by the entailment score between the evidence and each candidate correction. The highest-scoring correction is selected as the final output. An overview of our framework is shown in Figure 2 . Our method ensures the corrected information units are derived from the evidence, which helps improve the faithfulness of the generated corrections. In addition, our approach is naturally interpretable since the questions and answers generated directly reflect which information units are being compared with the evidence.\nOur contributions can be summarized as follows:\n\u2022 We propose ZEROFEC, a factual error correction framework that effectively addresses faithfulness by asking questions about the input claim, seeking answers in the evidence, and scoring the outputs by faithfulness. \u2022 Our approach outperforms all prior methods, including fully-supervised approaches trained on 58K instances, in ensuring faithfulness on two factual error correction datasets, FEVER (Thorne et al., 2018) and SCIFACT (Wadden et al., 2020) . \u2022 We analyze the correlation of human judgments with automatic metrics to provide intuition for future research on evaluating the faithfulness, factuality, and intelligibility of factual error corrections.\n\nTask\nIn Thorne and Vlachos (2021)'s setting, retrieved evidence is used, which means the model may be able to correct factual errors, even though there is no supporting information in the evidence. In this case, although the prediction is considered correct, the model is hallucinating, which is not a desired property. Additionally, due to the way data was collected, they require systems to alter the input claim even if the input claim is already faithful to the evidence. We argue that no edit is needed for claims that are faithful to the evidence.\nTo address these shortcomings, our setup aims to edit a claim using a given piece of grounded evidence that supports or refutes the original claim (see Figure 2 ). Using gold-standard evidence avoids the issue where a system outputs the correct answer by chance due to hallucinations. In our setting, a system must be faithful to the evidence to correct factual errors, allowing us to evaluate system performance more fairly. Furthermore, we require the model not to edit the original claim if it is already factually consistent with the provided evidence.\nConcretely, the input to our task is a claim C and a piece of gold-standard evidence E that supports or refutes C. The goal of factual error correction is to produce a corrected claim \u0108 that fixes factual errors in C while being faithful to E. If C is already supported by E, models should output the original claim (i.e. \u0108 = C).\n\nProposed Methods\nOur framework, ZEROFEC, faithfully corrects factual errors using question-answering and entailment.\nSpecifically, we represent the input claim C as question-answer pairs \n{(Q 1 , A C 1 ), ..., (Q n , A C n )} such that each question Q i reflects the corresponding information unit A C i ,\n\nCandidate Corrections\nNight of the Living Dead is a horror film.\n\nFinal Correction\nFigure 2 : An overview of our framework. First, given an input claim, we generate the claim answers by enumerating all information units in the input claim. Second, conditioned on each extracted answer and the input claim, a question is generated. Third, each question is then fed to a question answering model to produce an evidence answer using the given evidence as context. Fourth, using a sequence-to-sequence approach, each evidence answer and the corresponding question are transformed into a statement, which serves as a candidate correction. Finally, the final correction is produced by scoring candidate corrections based on faithfulness.\nanswer A E i in the given evidence E using a learned QA model ( \u00a73.3). Each candidate correction S i is obtained by converting the corresponding pair of Q i and A E i into a declarative statement ( \u00a73.4). This guarantees that the corrected information units we replace factual errors with are derived from the evidence and thus ensures high faithfulness. The final output of ZEROFEC is the S i with the highest faithfulness score computed by an entailment model ( \u00a73.5). An overview of our framework is shown in Figure 2 .\nOne major challenge that makes our task more difficult than prior studies on faithfulness (Wang et al., 2020; Fabbri et al., 2022a ) is that we need to handle more diverse factual errors, such as negation errors and errors that can only be abstractively corrected. For instance, in the second example of in Table 2 , the QA model should output \"Yes\" as the answer, which cannot be produced by extractive QA systems. To address this issue, we adopt abstractive QG and QA models that can handle diverse question types and train our QA-to-claim model on multiple datasets to cover cases that cannot be handled by extractive systems. The following subsections illustrate the details of each component in our framework.\n\nClaim Answer Generation\nThe goal of claim answer generation is to identify information units in the input claim that may be unfaithful to E. We aim to maximize the recall in this step since the missed candidates cannot be recovered in later steps. Therefore, we extract all noun chunks and named entities using Spacy 3 and extract nouns, verbs, adjectives, adverbs, noun phrases, verb phrases using Stanza 4 . Additionally, we also extract negation terms, such as \"not\" and \"never\", from the input claim. We name the extracted information units claim answers, denoted as\nA C = {A C 1 , A C 2 , ..., A C n }.\n\nQuestion Generation\nUpon claim answers are produced, we generate questions that will be later used to look for correct information units in the evidence. Questions are generated conditioned on the claim answers using the input claim as context. We denote the question generator as G. Each claim answer\nA C i is concatenated with the input claim C to generate a question Q i = G(A C i , C).\nWe utilize MixQG (Murakhovs 'ka et al., 2022) as our question generator G to cover the wide diversity of factual errors and candidates extracted. MixQG was trained on nine question generation datasets with various answer types, including boolean, multiple-choice, extractive, and abstractive answers.\n\nQuestion Answering\nThe question answering step identifies the correct information units A E i corresponding to each question Q i in the given evidence E. Our QA module answers questions from the question generation steps with the given evidence as context. Let F denote our QA model. We feed the concatenation of a generated question and the evidence to the QA model to produce an evidence answer (Khashabi et al., 2022) is used as our question answering model. UnifiedQA-v2 is a T5-based (Raffel et al., 2020b) abstractive QA model trained on twenty QA datasets that can handle diverse question types.\nA E i = F(Q i , E). UnifiedQA-v2\n\nQA-to-Claim\nAfter questions and answers are generated, we transform each pair of question and answer into a declarative statement, which serves as a candidate correction that will be scored in the next step. Previous studies on converting QAs to claims focus on extractive answer types only (Pan et al., 2021) . To accommodate diverse types of questions and answers, we train a sequence-to-sequence model that generates a claim given a question-answer pair on three datasets: QA2D (Demszky et al., 2018) for extractive answers, BoolQ (Clark et al., 2019) for boolean answers, and SciTail (Khot et al., 2018) for covering scientific domain QAs. Note that samples in BoolQ do not contain converted declarative statements. Using Stanza's constituency parser, we apply heuristics to transform all QAs to their declarative forms in BoolQ. Our QA-to-claim model is a T5-base fine-tuned on these three datasets. Concretely, let M denote our QA-to-claim model. M takes in a generated question Q i and an evidence answer A E i as inputs and outputs a statement\nS i = M(Q i , A E i ).\n\nCorrection Scoring\nThe final correction is produced by scoring the faithfulness of each candidate correction from the previous steps w.r.t. the evidence. We use entailment score to approximate faithfulness. Here, DocNLI (Yin et al., 2021) is used to compute such document-sentence entailment relations. Doc-NLI is more generalizable than other documentsentence entailment models, such as FactCC (Kryscinski et al., 2020) , since it was trained on five datasets of various tasks and domains. Conventional NLI models trained on sentence-level NLI datasets, such as MNLI (Williams et al., 2018) , are not applicable since previous work has found that these models are ill-suited for measuring entailment beyond the sentence level (Falke et al., 2019) . In addition, to prevent the final correction from deviating too much from the original claim, we also consider ROUGE-1 scores, motivated by Wan and Bansal (2022) . The final metric used for scoring is the sum of ROUGE-1 score 5 and DocNLI entailment score. Formally,\nEQUATION\nEQUATION\nwhere C \u2032 is the final correction produced by our framework. Furthermore, to handle cases where the input claim is already faithful to the evidence, we include the input claim in the candidate correction list to be scored.\n\nDomain Adaptation\nDuring the early stage of our experiments, we found that our proposed framework did not perform well in correcting factual errors in biomedical claims. This results from the fact that our QA and entailment models were not fine-tuned on datasets in the biomedical domain. To address this issue, we adapt UNIFIEDQA-V2 and DOCNLI on two biomedical QA datasets, PUBMEDQA (Jin et al., 2019) and BIOASQ (Tsatsaronis et al., 2015) , by further fine-tuning them for a few thousand steps. We later show that this simple domain adaptation technique successfully improves our overall factual error correction performance on a biomedical dataset without decreasing performance in the Wikipedia domain (see \u00a75.1).\n4 Experimental Setup\n\nDatasets\nWe conduct experiments on two English datasets, FEVER and SCIFACT. FEVER (Thorne and Vla-chos, 2021 ) is repurposed from the corresponding fact-checking dataset (Thorne et al., 2018 ) that consists of evidence collected from Wikipedia and claims written by humans that are supported or refuted by the evidence. Similarly, SCIFACT is another fact-checking dataset in the biomedical domain (Wadden et al., 2020) . We repurpose it for the factual error correction task using the following steps. First, we form faithful claims by taking all claims supported by evidence. Then, unfaithful claims are generated by applying Knowledge Base Informed Negations (Wright et al., 2022) , a semantic altering transformation technique guided by knowledge base, to a subset of the faithful claims. Appendix A shows detailed statistics.\n\nEvaluation Metrics\nOur evaluation focuses on faithfulness. Therefore, we adopt some recently developed metrics that have been shown to correlate well with human judgments in terms of faithfulness. BARTScore (Yuan et al., 2021) computes the semantic overlap between the input claim and the evidence by calculating the logarithmic probability of generating the evidence conditioned on the claim. FactCC (Kryscinski et al., 2020) is an entailment-based metric that predicts the faithfulness probability of a claim w.r.t. the evidence. We report the average of the COR-RECT probability across all samples. In addition, we consider QAFACTEVAL (Fabbri et al., 2022a) , a recently released QA-based metric that achieves the highest performance on the SUMMAC factual consistency evaluation benchmark (Laban et al., 2022) . Furthermore, we also report performance on SARI (Xu et al., 2016) , a lexical-based metric that has been widely used in the factual error correction task (Thorne and Vlachos, 2021; Shah et al., 2020) .\n\nBaselines\nWe compare our framework with the following baseline systems. T5-FULL (Thorne and Vlachos, 2021) is a fully-supervised model based on T5-base (Raffel et al., 2020a) that generates the correction conditioned on the input claim and the given evidence. MASKCORRECT (Shah et al., 2020) and T5-DISTANT (Thorne and Vlachos, 2021) are both distantly-supervised methods that are composed of a masker and a sequence-to-sequence (seq2seq) corrector. The masker learns to mask out information units that are possibly false based on a learned fact verifier or an explanation model (Ribeiro et al., 2016) and the seq2seq corrector learns to fill in the masks with factual information. The biggest difference is in the choice of seq2seq corrector. T5-DISTANT uses T5-base, while MASKCOR-RECT utilizes a two-encoder pointer generator. For zero-shot baselines, we selected two post-hoc editing frameworks that are trained to remove hallucinations from summaries, REVISEREF (Adams et al., 2022) and COMPEDIT (Fabbri et al., 2022b) .\nREVISEREF is trained on synthetic data where hallucinating samples are created by entity swaps.\nCOMPEDIT learns to remove factual errors with sentence compression, where training data are generated with a separate perturber that inserts entities into faithful sentences.\n\nImplementation Details\nNo training is needed for ZEROFEC. As for ZEROFEC-DA, we fine-tune UNIFIEDQA-V2 and DOCNLI on the BIOASQ and PUBMEDQA datasets for a maximum of 5,000 steps using AdamW (Loshchilov and Hutter, 2019) with a learning rate of 3e-6 and a weight decay of 1e-6.\nDuring inference time, all generative components use beam search with a beam width of 4.\n\nMain Results\nTable 1 summarizes the main results on the FEVER and SCIFACT datasets. Both ZEROFEC and ZEROFEC-DA achieve significantly better performance than the distantly-supervised and zeroshot baselines. More impressively, they surpass the performance of the fully-supervised model on most metrics, even though the fully-supervised model is trained on 58K samples in the FEVER experiment.\nThe improvements demonstrate the effectiveness of our approach in producing faithful factual error correction by combining question answering and entailment predictions. In addition, even though our domain adaptation technique is simple, it successfully boosts the performance on the SCIFACT dataset while pertaining great performance on the FEVER dataset. The first example in It is true that ZEROFEC-DA requires additional training, which is different from typical zero-shot methods. However, the key point remains that our framework does not require any task-specific training data. Hence, our approach still offers the benefits of zero-shot learning by not requiring any additional training data beyond what was already available for the question answering task, a field with much richer resources compared to the factchecking field.\n\nQualitative Analysis\nTo provide intuition for our framework's ability to produce faithful factual error corrections, we manually examined 50 correct and 50 incorrect outputs made by ZEROFEC on the FEVER dataset. The interpretability of ZEROFEC allows for insightful examinations of the outputs. Among the correct samples, our framework produces faithful corrections because all intermediate outputs are accurately produced rather than \"being correct by chance\". For the incorrect outputs, we analyze the source of mistakes, as shown in Figure 3 . The vast majority of failed cases result from DocNLI's failure to score candidate corrections faithfully. In addition to the mediocre performance of DocNLI, one primary reason is that erroneous outputs from other compo-nents would not be considered mistakes so long as the correction scoring module determines the resulting candidate corrections unfaithful to the evidence. A possible solution to improve DocNLI is to further fine-tune it on synthetic data generated by perturbing samples in FEVER and SCIFACT. Examples of correct and incorrect outputs are presented in Table 7 and Table 8 \n\nHuman Evaluation\nTo further validate the effectiveness of our proposed method, we recruited three graduate students who are not authors to conduct human evaluations on 100 and 40 claims from FEVER and SCIFACT, respectively. For each claim, human judges are presented with the ground-truth correction, the goldstandard evidence, and output produced by a factual error correction system and tasked to assess the quality of the correction with respect to three dimensions. Intelligibility evaluates the fluency of the correction. An intelligible output is free of grammatical mistakes, and its meaning must be T5-DISTANT's output: Fuller House ( TV series ) isn't airing on HBO.\nTable 2 : Example outputs from different approaches. The outputs from our framework are directly interpretable, as the generated questions and answers reflect which information units in the input claim are erroneous and which information in the evidence supports the final correction. We only show the generated answers and questions directly related to the gold correction. In the first example, ZEROFEC-DA corrects a mistake made by ZEROFEC thanks to domain adaptation. In the second example, ZEROFEC successfully produces a faithful factual error correction, whereas the output of T5-DISTANT, the distantly-supervised baseline, is factual yet unfaithful to the evidence.\nunderstandable by humans without further explanation. Factuality considers whether the input claim is aligned with facts. Systems' output can be factual and semantically different from the gold correction as long as it is consistent with the world's knowledge. Faithfulness examines whether the input is factually consistent with the given evidence. Note that a faithful output must be factual since we assume all evidence is free of factual error. To evaluate the annotation quality, we compute the inter-annotator agreement. Krippendorff's Alpha (Krippendorff, 2011 ) is 68.85%, which indicates a moderate level of agreement. Details of our human evaluation can be found in Appendix B.\nThe human evaluation results are demonstrated in Table 3 . We observe that: (1) ZEROFEC and ZEROFEC-DA achieve the best overall performance in Factuality and Faithfulness on both datasets, even when compared to the fully-supervised method, suggesting that our approach is the best in ensuring faithfulness for factual error correction.\n(2) Our domain adaptation for the biomedical domain surprisingly improves faithfulness and factuality in the Wikipedia domain (i.e. FEVER). This suggests that fine-tuning the components of our framework on more datasets helps improve robustness in terms of faithfulness.\n(3) Factual output produced by ZEROFEC and ZEROFEC-DA are always faithful to the evidence, preventing the potential spread of misleading information caused by factual but unfaithful corrections. The second example in Table 2 demonstrates an instance of factual but unfaithful correction made by baseline models. Here, the output of T5-DISTANT is unfaithful since the evidence does not mention whether Fuller House airs on HBO. In fact, although Fuller House was not on HBO when it premiered, it was later accessible on HBO Max. Therefore, the correction produced by T5-DISTANT is misleading.\n\nCorrelation with Human Judgments\nRecent efforts on faithfulness metrics have been mostly focusing on the summarization task. No prior work has studied the transferability of these metrics to the factual error correction task. We seek to bridge this gap by showing the correlation between the automatic metrics used in measure, the results are summarized in Table 4 .\nWe have the following observations. (1) SARI is the most consistent and reliable metric for evaluating Factuality and Faithfulness across two datasets. Although the other three metrics developed more recently demonstrate high correlations with human judgments of faithfulness in multiple summarization datasets, their transferability to the factual error correction task is limited due to their incompatible design for this particular task. For example, QA-based metrics like QAFACTEVAL are less reliable for evaluating faithfulness in this task due to their inability to extract a sufficient number of answers from a single-sentence input claim. In contrast, summaries in summarization datasets generally consist of multiple sentences, enabling the extraction of a greater number of answers. To validate this, we analyzed the intermediate outputs of QAFACTEVAL. Our analysis confirms that it extracts an average of only 1.95 answers on the FEVER dataset, significantly lower than the more than 10 answers typically extracted for summaries. (2) Across the two datasets, the correlations between all automatic metrics and Intelligibility are low. The extremely high proportion of intelligible outputs may explain the low correlation. (3) The correlation for learning-based metrics, including QAFACTEVAL and FACTCC, drop significantly when applied to SCIFACT. This is likely caused by the lack of fine-tuning or pre-training with biomedical data.\n6 Related Work\n\nFactual Error Correction\nAn increasing number of work began to explore factual error correction in recent years, following the rise of fact-checking (Thorne et al., 2018; Wadden et al., 2020; Gupta and Srikumar, 2021; Huang et al., 2022b) and fake news detection (Shu et al., 2020; Fung et al., 2021; Wu et al., 2022; Huang et al., 2022a) . Shah et al. (2020) propose a distant supervision learning method based on a masker-corrector architecture, which assumes access to a learned fact verifier. Thorne and Vlachos (2021) created the first factual error correction dataset by repurposing the FEVER (Thorne et al., 2018) dataset, which allows for fully-supervised training of factual error correctors. They also extended Shah et al. (2020) 's method with more advanced pre-trained sequence-to-sequence models. Most recently, Schick et al. (2022) proposed PEER, a collaborative language model that demonstrates superior text editing capabilities due to its multiple text-infilling pre-training objectives, such as planning and realizing edits as well as explaining the intention behind each edit 6 .\n\nFaithfulness\nPrevious studies addressing faithfulness are mostly in the summarization field and can be roughly divided into two categories, evaluation and enhancement. Within faithfulness evaluation, one line of work developed entailment-based metrics by training document-sentence entailment models on synthetic data (Kryscinski et al., 2020; Yin et al., 2021) or human-annotated data (Ribeiro et al., 2022; Chan et al., 2023) , or applying conventional NLI models at the sentence level (Laban et al., 2022) . Another line of work evaluates faithfulness by comparing information units extracted from summaries and input sources using QA (Wang et al., 2020; Deutsch et al., 2021) . There is a recent study that integrates QA into entailment by feeding QA outputs as features to an entailment model (Fabbri et al., 2022a) . We combine QA and entailment by using entailment to score the correction candidates produced by QA. Within faithfulness enhancement, some work improves factual consistency by incorporating auxiliary losses into the training process (Nan et al., 2021; Cao and Wang, 2021; Tang et al., 2022; Huang et al., 2023) . Some other work devises factuality-aware pre-training and fine-tuning objectives to reduce hallucinations (Wan and Bansal, 2022) . The most similar to our work are studies that utilize a separate rewriting model to fix hallucinations in summaries. For example, Cao et al. (2020) present a post-hoc corrector trained on synthetic data, where negative samples are created via perturbations. Adams et al. (2022) fix factually inconsistent information in the reference summaries to prevent the summarization from learning hallucinating examples. Fabbri et al. (2022b) propose a compression-based post-editor to correct extrinsic errors in the generated summaries. By contrast, we leverage the power of QA and entailment together to address faithfulness.\n\nConclusions and Future Work\nWe have presented ZEROFEC, a zero-shot framework that asks questions about an input claim and seeks answers from the given evidence to correct factual errors faithfully. The experimental results demonstrate the superiority of our approach over prior methods, including fully-supervised methods, as indicated by both automatic metrics and human evaluations. More importantly, the decomposability of ZEROFEC naturally offers interpretability, as the questions and answers generated directly reflect which information units in the input claim are incorrect and why. Furthermore, we reveal the most suitable metric for assessing faithfulness of factual error correction by analyzing the correlation between the reported automatic metrics and human judgments. For future work, we plan to extend our framework to faithfully correct misinformation in social media posts and news articles to inhibit the dissemination of false information. In addition, it may be meaningful to explore extending zero-shot factual error correction to multimedia task settings, such as identifying inconsistencies between chart and text (Zhou et al., 2023) .\n"}
{"question": "Which of the following is not a contribution to the article?", "evidence": " Recent work show the viability of unsupervised speech recognition. W2V2-U (Baevski et al., 2021) accomplished this by running Principal Component Analysis (PCA), k-means clustering, and mean pooling to convert W2V2 (Baevski et al., 2020) features into phoneme-granularity features, then trains a GAN model to output phoneme text from the post-processed model (Baevski et al., 2021) .  We summarize our contributions below:\n\u2022 We propose using ASR-U components to augment SSL speech encoders for generating subword tokens with semantic information.\n\u2022 The augmented SSL speech encoders can be connected with powerful LLMs seamlessly and yields state-of-the-art performance under the universal representation setup.\n\u2022 We show attention residual connections and adapters are essential to combining and aligning speech and text encoders. ", "options": ["A. We show the viability of unsupervised speech recognition by running Principal Component Analysis (PCA), k-means clustering, and mean pooling to convert W2V2 features into phoneme-granularity features, then trains a GAN model to output phoneme text from the post-processed model. ", "B. We show attention residual connections and adapters are essential to combining and aligning speech and text encoders.We show attention residual connections and adapters are essential to combining and aligning speech and text encoders.", "C. We propose using ASR-U components to augment SSL speech encoders for generating subword tokens with semantic information.", "D. The augmented SSL speech encoders can be connected with powerful LLMs seamlessly and yields state-of-the-art performance under the universal representation setup."], "answer": "A", "content": "\nIntroduction\nRealizing artificial intelligence (AI) that can understand and respond to spoken language is a north star for many speech and natural language processing (NLP) researchers. A particularly effective framework for this is the encoder-decoder architecture, where an encoder represents input audio signals as high-dimensional embeddings and a decoder converts said embeddings to outputs for different downstream tasks. Benchmarks for such systems include spoken language understanding, where intent, named entities, or slot values are predicted from input utterances (Yang et al., 2021; Bastianelli et al., 2020; Shon et al., 2022) , and spoken question answering, where the start and end frames of an input audio passage answering an input audio question are predicted (Lin et al., 2022a) .\nA particularly notable setup of the encoderdecoder framework is the universal representation setup (Yang et al., 2021) , where a shared selfsupervised speech encoder is pretrained upstream once and frozen for all downstream tasks, then a different lightweight decoder is fine-tuned on each downstream task. This setup is appealing for building speech systems as maintaining a separate large specialized model for every task is not computationally efficient. The universal representation setup has been widely adopted in other areas of research, such as computer vision (Goyal et al., 2019; Ericsson et al., 2021) and NLP (Rogers et al., 2020; Qiu et al., 2020) , and production when there are many downstream tasks or domains (Molino et al., 2019) . The current state-of-the-art speech encoders under this setup are W2V2 and HUBERT (Yang et al., 2021; Baevski et al., 2020; Hsu et al., 2021) , which are transformer-based models trained with self-supervised learning (SSL) on raw audio and have achieved impressive performance on various tasks.\nRecently, analytical works found SSL speech encoders capture primarily acoustic, not semantic, information (Pasad et al., 2021) . Thus, researchers proposed end-to-end systems (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) that introduce semantic information through large language models (LLMs), such as ROBERTA (Liu et al., 2019) or BART (Lewis et al., 2019) , which are pretrained to capture language semantics (Clark et al., 2019) . This is typically accomplished by the pipeline approach (Bastianelli et al., 2020) , which passes audio input through the SSL speech encoder, then bridge module, then LLM. The bridge module converts speech encoder embedding outputs into LLM token inputs (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) .\nUnsupervised ASR models (ASR-U) (Liu et al., 2020b; Baevski et al., 2021; Liu et al., 2022) have also seen recent success. The state-of-the-art ASR-U model uses generative adversarial networks (GANs) (Goodfellow et al., 2020) to generate text transcription from input audio (Liu et al., 2022) .\nCurrent works combining SSL speech encoders and LLMs do not satisfy the universal representation framework, as they either (1) rely on ASR data on the downstream task, which is expensive to collect and maintain, (2) are not lightweight, requiring training the whole system end-to-end, or (3) are not general, as they do not consider a wide variety of downstream tasks (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) . Similarly, ASR-U was proposed for speech recognition and the focus is not improving SSL speech encoders (Baevski et al., 2021; Liu et al., 2022) .\nWe propose introducing Semantics into Speech Encoders, SSE, a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. Concretely, SSE adopts the pipeline approach to obtain semantic embeddings, with an ASR-U bridge connector to extract information from LLMs. As ASR-U is inherently noisy, SSE introduces attention residual connection (He et al., 2016; Vaswani et al., 2017) between the speech encoder and LLM. SSE also efficiently aligns the LLM with the speech encoder through adapter modules (Houlsby et al., 2019) . SSE improves W2V2 (Baevski et al., 2020) and HUBERT (Hsu et al., 2021) on 3 SLU tasks across 3 datasets, all under the universal representation setup. SSE also outperforms state-of-the art no-ASR method, DUAL (Lin et al., 2022a) , in SQA.\nWhile recent works use ASR-U to augment existing speech encoders with phoneme-level LLMs (Feng et al., 2022; Meng et al., 2022; Shi et al., 2022; Hsu et al., 2022) , subword-level LLMs contain much more pertinent and measurable semantic information (Clark et al., 2019) . Other works in SQA rely on clustering to assign audio frames to frequent subword tokens, but this requires heavy finetuning on the downstream task (Lin et al., 2022a) .\nTo the best of our knowledge, we are the first to propose a task-agnostic SSL speech encoder which directly interfaces with subword-based LLMs, unblocking many other applications and future work in this domain. To this end, attention residual con-nections and adapters are essential to successfully extracting semantic information from noisy intermediary transcriptions. We summarize our contributions below:\n\u2022 We propose using ASR-U components to augment SSL speech encoders for generating subword tokens with semantic information.\n\u2022 The augmented SSL speech encoders can be connected with powerful LLMs seamlessly and yields state-of-the-art performance under the universal representation setup.\n\u2022 We show attention residual connections and adapters are essential to combining and aligning speech and text encoders.\n2 Related Works 2.1 Self-Supervised Speech Encoders SSL speech encoders (Liu et al., 2020a; Chung et al., 2020a; Ling and Liu, 2020; Liu et al., 2021 Liu et al., , 2020c;; Chung et al., 2019; Baevski et al., 2019; Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Qian et al., 2022; Zhang et al., 2022) are trained to learn and reconstruct pooled clustered representations of input audio from the original audio. The intuition for this objective comes from linguistics, where speech can be broken down into phoneme groups, where different chunks of input audio represent different phoneme groups.\nW2V (Schneider et al., 2019) trains a convolutional neural network model to reconstruct the quantized cluster representations. W2V2 (Baevski et al., 2020) uses transformers and a discrete codebook quantization module. HUBERT (Hsu et al., 2021) improves W2V2 by disentangling the clustering and SSL objectives and using a BERT-style encoder (Devlin et al., 2018) . The speech processing universal performance benchmark (SU-PERB) (Yang et al., 2021; Lin et al., 2022b; Tsai et al., 2022) shows SSL speech encoders are the most effective method for solving multiple downstream tasks with minimal fine-tuning. A recent analytical work finds SSL speech encoders successfully encode acoustic information, but lack semantic information (Pasad et al., 2021) . In response, CONTENTVEC (Qian et al., 2022) propose disentangling the speaker and semantic content of audio via an SSL objective. SPEECHLM (Zhang et al., 2022) propose training a multi-modal speech and text encoder.\n\nLarge Language Models\nIn contrast to speech encoders, pretrained LLMs are shown to capture rich semantic information (Clark et al., 2019) . These methods optimize variants of the masked language modeling (MLM) objective to train a large transformer model. BERT (Devlin et al., 2018) uses MLM to learn a transformer encoder. ROBERTA (Liu et al., 2019) introduces dynamic masking and a larger text corpus. BART (Lewis et al., 2019) supports generative modeling and adds a denoising objective, making it less susceptible to noisy text inputs. LONG-FORMER (Beltagy et al., 2020) is pretrained for long documents by increasing the document length limit during pretraining. LLMs have been successfully integrated with speech models for specific semantic tasks (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) , but not under the universal representation framework.\n\nTask-Specific Speech Models\nTask-specific SLU systems outperform generic SSL speech encoders typically by using a LLM. These systems rely on ASR data to reliably interface the LLM. LUGOSCH (Lugosch et al., 2019) trains a LSTM bridge module to convert audio features into phonemes then text. CTI's (Seo et al., 2022) bridge module uses ASR logits to compute a weighted average of token embeddings. In addition to improving the bridge module, other works attempt to also distill LLM embeddings into speech representations (Chung et al., 2020b; Cha et al., 2021; Kim et al., 2021; Agrawal et al., 2022) . For optimizing targeted metrics, researchers have also experimented with reinforcement learning (Rao et al., 2021) . While combinations of these methods achieve impressive performance, they do not satisfy the universal representation setup.\n\nUnsupervised ASR\nRecent work show the viability of unsupervised speech recognition. W2V2-U (Baevski et al., 2021) accomplished this by running Principal Component Analysis (PCA), k-means clustering, and mean pooling to convert W2V2 (Baevski et al., 2020) features into phoneme-granularity features, then trains a GAN model to output phoneme text from the post-processed model (Baevski et al., 2021) . The state-of-the-art method for phoneme-level unsupervised ASR is W2V2-U2.0 (Liu et al., 2022) which directly trains a CNN to output phonemes from W2V2 features and uses a reconstruction loss to tie the input audio with corresponding generated text. Both methods use WFSTs to decode the phonemes into raw text. While there have been preliminary attempts (Feng et al., 2022; Meng et al., 2022) to use W2V2-U2.0 with phoneme language models 1 , we are the first to combine it with semantically-rich subword-based LLMs.\n\nAdapters\nAdapters are intermediary layers added to a large pretrained encoder. Adapter weights are learned during fine-tuning while the rest of the pretrained model is frozen. Adapters serve the dual purpose of efficient fine-tuning and preventing overfitting. First used by computer vision researchers (Rebuffi et al., 2017) , adapters now enjoy much success in the natural language processing community by efficiently tuning LLMs (Houlsby et al., 2019) . In particular, the multilingual speech translation community found that adapters can effectively align SSL speech encoders and LLMs for spoken translation tasks (Li et al., 2020; Le et al., 2021) .\n\nProposed Method\nWe propose to introduce semantics into SSL speech encoders by using ASR-U to interface with LLMs. Section 3.2 describes how to use ASR-U to link a speech encoder with a LLM. Section 3.3 describes how to combine both acoustic and semantic information and deal with ASR transcriptions errors. Finally, Section 3.4 describes how to align LLMs with the speech encoder for downstream tasks.\n\nProblem Setting\nFollowing the universal representation framework (Yang et al., 2021) , our model consists of a large speech encoder, E : X \u2192 Z, mapping input audio, X \u2208 X , to embeddings, Z \u2208 Z, and a light-weight task decoder,\nD \u03c9 : Z \u2192 Y \u03c9 , mapping embeddings to downstream task outputs, Y \u03c9 \u2208 Y \u03c9 .\nThe speech encoder, E, is pretrained once, then shared on all downstream tasks. The task decoder, D \u03c9 , is fine-tuned on its respective task, \u03c9 \u2208 \u2126.\nDuring fine-tuning, the majority of model weights are frozen. This ensures the model can be efficiently stored and deployed.\nDuring pretraining, the speech encoder is trained on unlabelled audio, X \u2208 X , and unlabeled text, T u \u2208 T u . During finetuning, the model is trained on the labelled downstream dataset, (X, Y \u03c9 ) \u2208 X \u00d7 Y \u03c9 . Notice, costly labelled ASR data is not required during pretraining or finetuning.\n\nUnsupervised Semantic Representation as a Bridge\nTo incorporate semantic information into SSL speech encoders, E : X \u2192 Z, we wish to leverage subword-based LLMs, M : S \u2192 Z, that capture language semantics (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Beltagy et al., 2020) . The major challenge is the mismatch of input spaces. Speech encoders take raw audio as input, X \u2208 X . LLMs take subword tokens as input, S \u2208 S. SSE uses W2V2-U2.0 (Liu et al., 2022) as a bridge module (Seo et al., 2022) , B : Z \u2192 S, to convert speech encoder embedding output into LLM subword tokens in a pipelined approach,\nE SSE = E \u2022 B \u2022 M.\nFollowing W2V2-U2.0, the bridge module, B uses a GAN (Goodfellow et al., 2020) We also add an upsampling layer, U : Z \u2192 Z to make the sequence length of the LLM output match the speech encoder output, such that E and E SSE share the same output space.\nWe choose the 15th layer of the W2V2 (Baevski et al., 2020) as our speech encoder, as the last layers overfit the self-supervised training objective hence providing worse acoustic representations (Fan et al., 2020; Baevski et al., 2021; Pasad et al., 2021) . We choose BART (Lewis et al., 2019) as our LLM, as it is trained to denoise noisy input subword tokens, and we expect the bridge module to introduce some noise. We call this version of our model SSE-BASE. A depiction can be found in Figure 1a .\n\nCombining Semantics and Acoustics with Residual Attention\nWe hypothesize certain tasks may require more acoustic information than others. to implicitly transcribe parts of the input speech, a primarily acoustic task. Since the pipelined model may suffer from transcription errors introduced by ASR-U, naively using the pipelined approach introduces an information bottleneck at the bridge module. Hence, we propose adding a residual connection (He et al., 2016) between SSE-BASE and the speech encoder, E. This can be done in two ways: (1) upsampling semantic embeddings and concatenating with speech embeddings, Z = [Z E ||U(Z M )], or (2) using multihead attention (Vaswani et al., 2017) to merge the two embeddings, Z =\n[Z E ||MHA(Z E , Z M , Z M )],\nwhere Z E \u2208 Z is the output of the W2V2L15 (Baevski et al., 2020) and Z M \u2208 Z is the output of BART (Lewis et al., 2019) . The former is a simpler but more naive method. The latter is more effective as the attention layers are able to learn the alignment between speech and semantic embeddings. Notice, (2) introduces more learnable parameters to the finetuning-step, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder.\n\nAligning Pretrained Text Model with Adapters\nInspired by works from speech translation (Li et al., 2020; Le et al., 2021) , we hypothesize that the LLM can easily be adapted for speech tasks through the use of adapters. We adopt the general recipe for adapters, where an adapter (Houlsby et al., 2019) , composed of a LayerNorm and 2-layer ReLU neural network, is added to the end of each feed forward layer in the LLM and finetuned on downstream tasks. This introduces additional parameters to finetuning, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder. We call the model using both residual attention and adapters SSE-TUNE, and outline it in Figure 1b .\n\nExperiments 4.1 Dataset\nTo show the effectiveness of introducing semantics into speech encoders, we evaluate 3 SLU tasks, intent classification (IC), slot filling (SF), and named entity recognition (NER), and SQA \n\nSpoken Language Understanding\nTo show SSE improves SSL speech encoders, we augment two state-of-the art speech encoders under the universal representation setup: W2V2 and HUBERT. Following prior works that found intermediary layers of W2V2 contain better representations (Pasad et al., 2021; Baevski et al., 2021) , we consider the 15th layer and the last layer of W2V2, named W2V2L15 and W2V2L24 respectively. As mentioned in Section 3, we show 2 versions of our model, SSE-BASE and SSE-TUNE. The former uses the pipelined approach to connect W2V2L15 with BART (Lewis et al., 2019) with no additional modifications. The latter introduces an attention residual connection and learnable adapters to combine acoustics and semantics together and align the LLM with the speech encoder respectively. We either connect the residual connection to the output of W2V2L15, yielding SSE-TUNE (W2V2L15), or to the output of HU-BERT, yielding SSE-TUNE (HUBERT).\nTo show the importance of using LLMs, we compare against 2 very recent approaches for improving SSL speech encoders without LLMs, SPEECHLM (Zhang et al., 2022) and CON-TENTVEC (Qian et al., 2022) . As HUBERT-BASE was used as the base speech encoder by both baselines, we also provide results where SSE-TUNE is used to augment HUBERT-BASE.\n\nSpoken Question Answering\nTo show the effectiveness of SSE, we compare it against DUAL (Lin et al., 2022a) , the state-ofthe-art SQA model which does not use ASR data. While both SSE and DUAL obtain frame-level tokens from speech input, SSE uses ASR-U to obtain its tokens, whereas DUAL uses clustering. As a result, SSE's output tokens exists in the LLM's existing vocabulary, whereas DUAL's output tokens does not. Hence, DUAL must retrain the LLM on its output tokens.\nWe compare DUAL to the closest analogous SSE model, which is SSE-BASE but with adapter layers, SSE-BASE (ADAP). Similar to DUAL, both methods modify the LLM weights. Unlike DUAL, SSE-BASE (ADAP) is lightweight, tuning only around 10% of the total parameters. To produces framelevel predictions, we remove the upsampling layer from SSE-BASE (ADAP). We choose W2V2L15 as our speech model and BART as our LLM, as it is robust to ASR errors.\nWe also show a PIPELINE model, which trains a W2V2 model on ASR data and a LONGFORMER LLM on text-only question answering data. It is worth noting that since evaluation is based on the frame-level, SSL speech encoders are not a baseline since they operate at the audio level.\n\nDecoder Setup\nTo satisfy the universal representation setup, we adopt lightweight SLU decoders from SU-PERB (Yang et al., 2021) is sum pooling followed by a multilayer perceptron classifier trained with cross entropy loss. For the SF and NER tasks, the decoder is recursive neural network (RNN) that transcribes input audio into text. The decoder identifies named entities or slot values by surrounding them with named special tokens and is trained with connectionist temporal classification loss. For SQA, we adopt the same decoder as DUAL (Lin et al., 2022a) , which is a linear layer classifying each subword embedding as the start or end or neither of an answer span.\n\nImproving SSL Speech Encoders\nAs seen in Table 2 , SSE significantly improves the SLU performance of both W2V2 and HU-BERT, confirming that including semantic information drastically improves existing SSL speech encoder performance. Specifically, SSE-TUNE (W2V2L15) improves W2V2L15 on all tasks. SSE-TUNE (HUBERT) improves HUBERT on 3 out of 4 tasks, and is the best performing model overall. Comparing SSE-TUNE with SSE-BASE shows residual attention and adapters effectively counteracts bridge module transcription errors. The relative performance gain for IC is more than SF or NER. Unlike IC, both SF and NER require the speech encoder to transcribe identified audio snippets, and transcription is a primarily acoustic task. Hence SF and NER require less semantic information than IC. Nevertheless, combining both acoustic and semantic information, as done by SSE-TUNE, provides the most consistent performance improvement, since the skip connection can learn which type of information is more needed.\n\nImportance of LLMs\nAs seen in Table 2 , SSE-TUNE (HUBERT-BASE) outperforms alternative approaches augmenting speech encoders, SPEECHLM (HUBERT-BASE) and CONTENTVEC (HUBERT-BASE). Unlike these alternative approaches, SSE-TUNE incorporate information from LLMs, which we found to be very beneficial for capturing semantic information as they are carefully pretrained objectives on large amounts of unlabelled text data.\nIt is noteworthy that SSE-TUNE is a general framework which can augment any speech encoder of our choice, including SPEECHLM and CON-TENTVEC. Similarly, SSE-TUNE can directly integrate new LLMs without costly pretraining. We leave incorporating such encoders into SSE-TUNE as future work.\n\nSpoken Question Answering\nAs seen in Table 3 , SSE outperforms recent unsupervised clustering-based approaches, DUAL. In contrast to DUAL's HUBERT cluster tokens, SSE's ASR-U tokens are better aligned with LLMs and share the same space. Thus, SSE can better utilizes pretrained LLMs. Furthermore, SSE does not require carefully tuning the number of HUBERT cluster counts, as the vocabulary size of the LLM is fixed and consistent with ASR-U.\n\nChoice of Language Model\nWe find subword-based LLMs contain more information than phoneme-based LLMs (Clark et al., 2019) . We empirically verify this by replacing our subword-based LLM, BART (Lewis et al., 2019) , with popular character-based LLM, ByT5 (Xue et al., 2022) , and phoneme-based LLM, T5lephone (Hsu et al., 2022) in SSE-BASE. As seen in Table 4 , the subword-based LLM perform the best as each subword token is more semantically meaningful than a phoneme or character. We believe T5lephone outperforms the Byt5 as it has better robustness to ASR-U errors. Overall, subword-based LLMs are the best choice for embedding semantic information in transcribed text.\n\nResidual Attention and Adapters\nTo more carefully analyze the affect of residual attention and adapters in SSE-TUNE, we run experiments on all SLU datasets with and without each component. We denote these two design choices as (ResAtt) and (Adap) respectively. As seen in Table 4, both components provide ample performance improvement over SSE-BASE.\nWe also try the naive residual connection approach described in Section 3.3 by directly concatenating the LLM upsampled semantic embeddings to the speech embeddings. We call this approach SSE-BASE (RES). This method is less effective than SSE-BASE (RESATT) as it does not learn how to align speech and semantic embeddings, but improves SSE-BASE, further validating our hypothesis that merging acoustic and semantic information is beneficial.\nAs seen in parameter breakdown for the SSE-TUNE (W2V2L15) model in Table 1 , the number of new learnable parameters introduced by (Re-sAtt) and (Adap) are unsubstantial compared to the size of the lightweight downstream decoder. Specifically, the downstream task decoder accounts for 9.60% of the total model parameters. SSE-TUNE introduces only 10.47% more parameters than SSE-BASE during fine-tuning and 0.91% to the total model parameter count, but often provides significant performance improvement.\n\nComparison with Supervised ASR Methods\nTo quantify the effect of transcription errors introduced by the bridge module, we compute the word error rate (WER) of the bridge connector in SSE-TUNE, and compare it against standard W2V2 supervised ASR models (Baevski et al., 2020) trained on 10 minutes, 100 hours, and 960 hours of labeled ASR data. indicating the effectiveness of the bridge module.\nOn SLURP and SLUE, the relative drop in WER (> 20%) is substantially more than the relative drop in downstream performance (< 5%), verifying SSE-TUNE's tolerance to noisy transcriptions. The robustness to ASR errors come from our choice of LLM, BART, which is trained to handle noisy inputs, residual connection to acoustic embeddings, and LLM alignment with adapters.\n\nComparison to Specialized SLU Models\nTo better quantify the performance improvement introduced by SSE, we compare against 2 specialized SLU models that do not abide by the universal representation framework: Kaldi+HerMiT, which is a pipelined Kaldi ASR (Povey et al., 2011) and HerMiT NLU (Vanzo et al., 2019) model reported in the SLURP paper (Bastianelli et al., 2020) , and CTI (Seo et al., 2022) , which is an end-to-end pipelined W2V2 (Baevski et al., 2020) ASR and ROBERTA (Liu et al., 2019) NLU model. To the best of our knowledge, CTI is the state-of-the-art SLU model. In addition to unlabelled text, unlabelled audio, and downstream data, both Kaldi+HerMiT and CTI require 40 hours of downstream SLURP ASR data (Bastianelli et al., 2020) . Kaldi+HerMiT requires an additional 24,000 hours of ASR data (Povey et al., 2016) . CTI requires an additional 960 hours of ASR data (Panayotov et al., 2015) . Neither use lightweight fine-tuning. Thus, such specialized SLU models are less general, more expensive, and require much more data. As seen in Table 6 , SSE helps bridge the gap between tailormade models and more practical SSL speech encoders. We believe ASR-U errors plays a major role in the remaining gap, as the ASR-supervised Kaldi+HerMiT and CTI models have WER of 16.20% and 16.67% respectively, compared to A mix-up is when the model either misclassifies label \"A\" as \"B\" or misclassifies label \"B\" as \"A\". For each mix-up, we compute the percentage of less mistakes made by SSE-TUNE (HUBERT) than HUBERT. For example, SSE-TUNE (HUBERT) misclassifies calendar_set as calendar_query or vice-versa 20% less frequently than HUBERT. The \"general-quirky\" label is assigned to Out-of-Distribution inputs.\nSSE's ASR-U bridge with a WER of 51.51%.\n\nError Analysis\nTo better understand the semantic information captured by SSE, we study predictions made by both HUBERT and SSE-TUNE (HUBERT) on SLURP-IC's test set. We find HUBERT errors are made primarily between intents within the same or similar domains (e.g. calendar_set vs calendar_query).\nThe performance bottleneck lies with distinguishing finer-grained in-domain intents. Table 7 shows that SSE-TUNE is better at differentiating finergrained intents. SSE-TUNE's misclassifications come primarily from errors made by its ASR-U bridge component. As seen in Table 8 , the ASR-U WER of incorrect predictions made by HUBERT is much lower than that of incorrect predictions made by SSE-TUNE. When ASR-U returns resonable transcriptions (typically <50% WER), SSE-TUNE can correctly classify inputs that HUBERT cannot. Hence, the effectiveness of SSE is tightly coupled with the effectiveness of ASR-U.\n\nRepresentation Visualization\nTo better see the impact of including semantic representations, we visualize the pooled audio snippet embedding for intent classification on SLURP-IC using t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton, 2008) or incorrectly (\u2717). We denote the number of pairs belonging to each subset, in the thousands, in parentheses. denote the ground truth label of each audio snippet by the color of its pooled embedding. As seen in Figure 2 , the clusters produced by semantic embeddings are more spread out and better separated than those produced by just acoustic speech embeddings, indicating that SSE introduces new semantic information that existing speech encoders lack.\n\nConclusion\nWe presented a compelling case for introducing semantics into SSL speech encoders and an effective method of doing so. Our approach boosts the performance of existing speech encoders on multiple SLU and SQA tasks and datasets. We provide reasoning for what tasks may benefit more or less from incorporating semantics. Furthermore, our approach is task agnostic and can augment any existing SSL speech encoder. With SSE-TUNE, we show merging acoustic and semantic information and effectively aligning LLMs to the speech encoder on downstream tasks can further boost performance with minimal parameter overhead. As it can generalize to many downstream tasks, SSE provides an important step towards AI that can understand and respond to spoken language.\n"}
{"question": "How do models trained on a combination of synthetic and curated data perform compared to other models in Icelandic error correction task?", "evidence": "  The models trained on only the finetuning data perform the worst throughout The results for these models on the curated in-domain test sets are in fact mostly on par with the models finetuned on the synthetic data only ", "options": ["A. Perform the best", "B. Perform well", "C. Perform average", "D. Perform poorly "], "answer": "B", "content": "\nIntroduction\nSpelling mistakes due to typos and rushed writing, nonstandard punctuation and spelling, and grammatical and stylistic issues are common to almost everyone who writes any kind of text. This applies in any language and can distract the reader or make the communication miss its mark. This can hinder people who have difficulties writing text conforming to a particular language standard, be it due to disability, dyslexia, linguistic background, limited access to education or any other reason. Prejudice against people whose writing deviates from the standard can make some shy away from communicating with others, leaving their voices out of important discussions and restricting their opportunities (Alexander-Passe, 2015) .\nGrammatical error correction (GEC) is the task of adjusting a text's spelling, grammar, and linguistic style to conform to an approved language standard or convention (Rauf et al., 2017) . While the latest work on GEC is based on Transformer models (Vaswani et al., 2017) , the subword tokenization methods commonly used in these models are a source of problems when it comes to typos and other variants (Schmaltz et al., 2017) . Subword tokenization (Sennrich et al., 2016; Kudo, 2018) was presented as a solution to the open vocabulary problem, as it is a compromise between character encoding and whole word encoding, enabling unknown words to be represented using known subwords.\nHowever, a significant downside of subword tokenization is how it is affected by noisy input; if a word contains a typo or other spelling variants, this can completely shift its representation. In addition, in languages with rich morphology, a word can have many different surface forms, some rarer than others, that all carry the meaning of the base word, but appear in different syntactic contexts. A subword-tokenized model may struggle to capture the nuances of such a language effectively since it may need several different subwords to represent a single word, depending on spelling and context. When an unfamiliar variant of the word appears in unseen text, the model is challenged to decode it correctly, even when it results in uncommon subword units.\nOur motivation is that a byte or character-level approach should intuitively be more robust to spelling or morphology variations, as it is not constrained by the subword vocabulary. We explore using a byte-level architecture, ByT5 (Xue et al., 2022) , for correcting everything from typos to com-Figure 1 : Overview of training data and comparison of output. The Icelandic-English mBART-ENIS model and the multilingual ByT5 and T5 models are first trained on generated parallel error corpora before being adapted on curated (collected) true error corpora in Icelandic. The final models are compared on an error correction (EC) task in Icelandic. The example demonstrates how the byte-level model performs well while the subword model cannot see the individual characters in every word, leading to degraded performance. plex grammatical issues in text. The language studied is Icelandic, a highly-inflected North Germanic language. For instance, the morphological complexity in Icelandic means that nouns can have up to 16 different surface forms, and adjectives over 50. GEC for a morphologically complex language needs to go beyond correcting only single words or limited phrases; it needs to consider the syntax of the whole sentence. This is the case for Icelandic but also for other languages with rich morphology, such as Arabic, Hebrew, Polish, Basque, Lithuanian and Hungarian, to name a few.\nWe compare the performance of the byte-level architecture to two subword-based architectures; ByT5's predecessor, mT5 (Xue et al., 2021) , and an mBART (Liu et al., 2020) model that has been pretrained further on both Icelandic and English. We employ real and synthetic error data for training, and present models and a framework for error generation methods that can be adapted to other languages. For under-resourced languages such as Icelandic (R\u00f6gnvaldsson, 2022) , using synthetic training data makes neural training for GEC a viable option.\nOur main contributions include a comparison between subword tokenization and byte-level tokenization for GEC when training over a combination of curated and synthesized data. We demonstrate how byte-level models not only bypass subword-related issues, but can also correct long-range errors in text. We release our error generation framework as well as models for GEC using byte-level and subword tokens in Icelandic. While our work focuses on the Icelandic language, we have no reason to believe that similar results do not hold for other languages, particularly those similar to Icelandic in terms of morphological complexity.\n\nRelated work\nThe bulk of research on grammatical error detection and correction has been focused on English and English learner texts, due to existing training data and benchmarks, and the large market of English learners worldwide who benefit from an automatic language correction tool (N\u00e1plava and Straka, 2019) . While spelling and grammar errors appear in every language, each language has its own set of error types that are more common than others, due to different phonetic, morphological and syntactic characteristics.\n\nSynthetic data generation for GEC\nThe problem of data scarcity in GEC, when approached as a sequence-to-sequence task, is typically addressed with synthetic data generation (Stahlberg and Kumar, 2021) . One approach to cre-ating ungrammatical sentences uses random character noise and simple rules to manipulate the text. Another approach is using a spell checker in reverse to noise text (Grundkiewicz and Junczys-Dowmunt, 2019) . In contrast, others have used probabilities derived from an annotated corpus of naturally occurring errors to corrupt text (Felice and Yuan, 2014) . Recent efforts widely employ neural networks to create synthetic errors (Stahlberg and Kumar, 2021) ; many use methods derived from machine translation (Junczys-Dowmunt et al., 2018) , for example, by creating worse text using deliberately bad translation models (Xie et al., 2018; Zhou et al., 2020) or roundtrip translations between languages (Lichtarge et al., 2019) . Yet another option is to leverage available resources with edits, such as Wikipedia edit histories, to generate corrupted corpora (Grundkiewicz and Junczys-Dowmunt, 2014) .\n\nSequence segmentation for GEC\nGEC can essentially be considered the task of generating grammatical target text from an ungrammatical source, similar to machine translation. The idea of approaching GEC as a machine translation problem dates back to 2006 (Brockett et al., 2006) , and this approach has since become the most prevalent method of GEC, with the focus shifting from statistical machine translation (SMT) to neural methods as they developed (Yuan and Briscoe, 2016; Ji et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018) . However, phrase-based SMT continued to be the state-of-the-art for GEC for longer than in the field of interlingual neural machine translation (NMT) (Junczys-Dowmunt et al., 2018) . This is partly because of the data scarcity problem and partly because of the tokenization methods typically used in transformer models. Breaking words up into subword units decreases the vocabulary size while addressing the out-of-vocabulary problem (Sennrich et al., 2016) . A prominent drawback of this approach is that the fixed subword vocabulary makes the models sensitive to noise in the text (Tay et al., 2021; Eger and Benz, 2020) .\nIn a subword-based GEC model, when a word contains a typo or is spelled unconventionally, it may look like an unknown word, for which no known representation exists. The model may then segment the word differently from what was seen during training, causing mispredictions. If the subword representation for \"different\" is [_diff, er, ent] , but the word is misspelled as \"diffirent\", its subwords might be [_diffi, ren, t], and a subword-based GEC model might correct the typo by outputting a different word, [_diffi, cul, t] . This issue is highlighted in Figure 1 , also showing how a byte-based approach is not limited by this issue.\nThis is also true for unseen words that are correctly spelled, such as foreign-named entities, which can lead to the subword-based GEC model \"correcting\" a perfectly spelled word it has not seen before, by replacing it with the most likely candidate. In the sentence \"The tournament was held in Espoo, Finland.\", the place name \"Espoo\" may be represented by a single subword token Espoo. Since this token is unfamiliar, the model finds the most likely subword token for this particular sentence, Helsinki. 1 This changes the semantics of the sentence and can introduce serious errors. The result is a grammatically correct and meaningful sentence, but the semantics have drifted away from the original text.\nDue to this known shortcoming of subword tokenization (Schmaltz et al., 2017) , efforts have been made to design architectures where the characters or the underlying bytes are used directly as input tokens. Byte and character-level models inevitably result in much longer sequences than subword models, making them more costly to train, and slower in inference. Some truly token-free general-purpose architectures that are increasingly competitive to token-based models have emerged recently, including CANINE (Clark et al., 2022 ) (character-level), PIXEL (Rust et al., 2023 ) (text-to-image), and ByT5 (Xue et al., 2022) (byte-level) . The training of ByT5 is based on the subword-based multilingual mT5 (Xue et al., 2021) approach, but in comparison, the model is equipped with a heavier encoder (three times the depth of the decoder). Compared to mT5, ByT5 is more robust to noisy input, but inference is slower (1.5 to 2.6 times slower on average on a transliteration task, and up to 9 times longer on tasks with longer input sequences) (Xue et al., 2022) . Another approach to making subword models more robust is using subword regularization to produce multiple segmentations of the same word (Kudo, 2018; Provilkov et al., 2020) . This is commonly used for addressing the open vocabulary problem and noisy data, such as ungram-matical text.\nDespite the reported advantage of character or byte-based Transformer models on noisy text (Libovick\u00fd et al., 2022) , work using this approach to GEC is not very common. A notable exception is work for GEC in the Lithuanian language, where ByT5 has been used for diacritics restoration (Stankevi\u010dius et al., 2022) and limited GEC (Stankevi\u010dius and Luko\u0161evi\u010dius, 2022) . They generate synthetic data by applying noise (common typographical errors, swapping letters for similar sounding ones, other non-grammatical word-level noise) to a crawled and filtered corpus and compare results when training with T5 and ByT5. Their findings agree with ours that the byte-level model outperformed the subword model.\nOur work deviates from that of Stankevi\u010dius et al. (2022) in the following key ways: We (a) generate more sophisticated and realistic errors using grammatical information from part-of-speech (PoS) tags and using custom rules based on empirical findings; (b) combine generated error data with true error corpora from a wide range of demographic sources; and (c) explore in detail which error and text types benefit the most from finetuning on true error corpora, as opposed to training on synthetic data. As far as we are aware, no attempt at bytelevel Transformer-based GEC, trained on synthetic and real error corpora, has been published. Concurrent work using ByT5 for Icelandic is (Jasonarson et al., 2023) , where errors in the OCR output of historical Icelandic texts are corrected using a generated corpus of errors extracted from real data.\nCurrent state-of-the-art in GEC is based on sequence-tagging methods (Omelianchuk et al., 2020) , which instead of generating whole sequences, tag the erroneous sentence with its corrections, and on sequence-to-sequence methods, as has been described. Further work has explored automatic character transformations for GEC tagging (Straka et al., 2021) to better handle character-level errors. One of the current highest-scoring models on English GEC benchmarks is gT5 (Rothe et al., 2021) , which is based on the mT5 model.\n\nPrior work for Icelandic\nApart from some rule-based spell checkers that don't make use of the full context, one rule-based correction system exists for Icelandic, based around parse trees, GreynirCorrect (\u00d3lad\u00f3ttir et al., 2022) . This system is contingent on the sentence parsing according to a pre-defined context-free grammar, and can only handle issues that fit pre-defined rules. This setup is both a strength and a weakness as the system is highly configurable and capable of many things, such as detecting syntactic inconsistencies and errors, and can give the user useful information on the errors found. Still, when a text has many errors, complexity builds up and rules can start interfering. Sentences containing many issues, such as from users with dyslexia, generally have lower accuracy using this method.\nThe work presented here is the first where neural networks are used in GEC for Icelandic. Snaebjarnarson et al. (2022) use neural methods for detecting such errors, but not for correcting them.\n\nCurated dataset\nA single collection of parallel error corpora exists for Icelandic, the Icelandic Error Corpus (IceEC). The corpora are annotated and corrected by language experts (Arnard\u00f3ttir et al., 2021) . The dataset is highly granular in its categories, containing hundreds of labels, but with a limited number of highlevel groups (coherence, grammar, orthography, style and vocabulary).\nThe IceEC is split into a larger general corpus and three specialized corpora (Arnard\u00f3ttir et al., 2022) . The general one contains 58,239 sentences of student essays, online news texts and Wikipedia articles. This corpus is annotated with around 50k errors of different categories. The three specialized corpora are much smaller and contain texts from Icelandic language learners (6270 sentences), dyslexic native speakers (1362 sentences), and children (2070 sentences), volunteered by the users themselves. These smaller corpora contain more errors per sentence than the general one, and add diversity to the training data.\nThis curated error data was used for finetuning our models, and combined into one training dataset for a total of 64k input sequences (single sentences), after setting aside validation and test data. The general IceEC also includes a 5.3k sentence test set used for evaluation.\n\nSynthetic dataset\nWe applied a diverse set of methods for error generation, both using linguistic knowledge and random noising methods. This rule-based approach to synthetic data generation gave us control over the types of noise applied, and allowed us to generate evaluation data for each error type.\nAs our basis of correct text to be noised, we used the Icelandic Gigaword Corpus (IGC) (Steingr\u00edmsson et al., 2018) , a collection of Icelandic editorial texts. These are mostly news articles, published literature and legal texts. We selected from this corpus those text sources that are the most likely to have been reviewed as part of the editorial process of each publication/source (literature, journals, news, laws, adjudications and transcribed parliamentary speeches). Some of these texts still have their share of typos and other errors and inconsistencies, especially the news articles, which were deemed important training data because of their general vocabulary, not found in the more formal text sources. As a preprocessing step, we filtered out lower-quality and irrelevant sentences, by removing sentences containing mostly foreign texts, illegal characters and words with known misspellings, sourced from lists of common misspellings. The corpus was tokenized using the Greynir Tokenizer (\u00deorsteinsson et al., 2019) and PoS tagged using the GreynirSeq tagger (Snaebjarnarson et al., 2022) .\nWe generated three categories of errors: 1) noise within words; 2) noise at the sequence level; and 3) grammatical and morphological modifications. The first two resemble those used when noising backtranslation data (Edunov et al., 2018) . The third type is based on using available tools and linguistic knowledge to create errors that are unlikely to be formed randomly, but resemble those of human writers.\nIn order to explore to what extent subword and byte-level models can learn and generalize grammatically complex issues in a morphologically rich language, we go beyond naive language-agnostic noising of text. A more detailed explanation of the Icelandic-specific noise is given in Appendix C. The noise methods are shown in Table 1 . This is by no means a finite list of linguistic variants or errors found in Icelandic texts, but constitutes examples chosen for studying the model performance on these more grammatically complex challenges.\nThe error generator allows for noising levels to be configured via hyperparameters. Experiments with different noise ratios in the synthetic data showed that highly noised text provided the best training examples, without the models learning to \"overcorrect\", i.e., to introduce false positives. Instead of producing even more synthetic data, we geared up the noise to produce highly error-dense examples, setting the random and more naive error noise to appear in 80% of cases, and the rule-based error noise to be used wherever possible. 2 Examples of parallel sentences with and without synthetic errors are shown in Table 2 . A total of 35M synthetic parallel sentences were generated using these methods. 2000/4000 lines were set aside for validation/testing, respectively, and special evaluation sets were generated for each error type (see Section 4).\n\nModels\nWe compared three model architectures to evaluate the differences between using subword tokenization and a byte-level method. Comparing models with different architectures calls for defining which factors are compared. In particular, byte sequences are longer than subword sequences when counting the number of tokens, roughly 4 times longer on average in the original multilingual mT5/ByT5 training data (Xue et al., 2021) . And as Xue et al. (2022) note, the mT5 architecture tends to need less training time than ByT5. We compared the models after an equal amount of training samples (100k updates). We also continued the training of the ByT5 model, using more than five times the number of updates.\n\nmBART\nWe continued training of the pretrained multilingual BART25 model (mBART) (Liu et al., 2020) , using the original pretraining objective on texts in Icelandic and English. The training of the model is detailed in Appendix A. This model, mBART-ENIS, was then finetuned on the synthetic error data, to teach it to correct errors in Icelandic texts.\nTraining on the synthetic error data was performed with an effective batch size of 3000 input tokens (roughly 60 sequences), a learning rate of 3e-5 with an inverse square root scheduler, 0.1 dropout, 0.3 attention dropout, 1000 warmup steps, 0.1 label smoothing, no weight decay, and using the Adam optimizer, for 100k updates on an A100 card for a day. 3 In addition to the above experiments, we conducted separate experiments using segmentation regularization (Kudo, 2018) to introduce more noise to the training examples and explore alternative measures to mitigate the subword tokenization problem. The BART architecture uses unigram subword units; we applied subword regularization with \u03b1 = 0.7 and keep all other parameters unchanged.\n\nByT5\nFor the byte-level approach we employed the ByT5base model (Xue et al., 2022) , which is based on the multilingual T5 model (Raffel et al., 2020) , but operates on bytes instead of subwords. The ByT5 model is pretrained on over 100 languages, but has only seen a limited amount of Icelandic. The mC4 dataset which is used to train ByT5 and mT5 is also lacking in quality for low-resource languages, in particular for Icelandic, as shown by Snaebjarnarson et al. (2022) .\nThe pretraining task in ByT5 has been adapted\nto a byte-level model, with span infilling based on bytes, not subwords. Apart from this, the main difference between the mT5 and ByT5 model architecture is the heavier encoder of ByT5.\nSequences in byte-level architectures are long and correspond more or less to the number of characters in Icelandic, resulting in increased training time. We trained the ByT5-base model using a maximum sequence length of 512 bytes, which was found to be a reasonable compromise, as most sentences in Icelandic texts are shorter than this.\nThe ByT5-base model was finetuned on the synthetic data with an effective batch size of 32 sequences (sentences). The learning rate was set to 2e-5 using the Adam optimizer with 1000 warmup steps and no weight decay. This model was further trained for a total of 550k updates, or 13 A100 card days. 4\n\nmT5\nFor a more direct comparison of byte-level and subword-level models, we also finetuned the mT5base (Xue et al., 2021) model on the same data, with the notable difference to the mBART model that it was not further trained on Icelandic. The mT5 models were pretrained on the same data as ByT5 and have a similar architecture, as described above, and are thus as comparable as subword and byte-level models can be. As previously mentioned, mT5-base is the base for the state-of-the-art gT5 (Rothe et al., 2021) model for multilingual GEC.\nWe finetuned the mT5-base model on the synthetic data using the same parameters as in our ByT5-base finetuning and evaluated it at 100k updates. 5 \n\nFinetuning on curated corpora\nUsing the curated error corpora (IceEC), we finetuned the byte-level and subword-level models to convergence. For the mBART model, this meant training with a learning rate of 2e-6 for 53k updates (67 epochs), with attention dropout set to 0.1, weight decay to 0.001 and other parameters being the same as during the synthetic finetuning.\nThe ByT5 and mT5 models were finetuned with a learning rate of 2e-6, other parameters were the same as during finetuning on the synthetic data. The ByT5 model had converged at 120k updates (60 epochs), while the mT5 was still improving on the validation data at 200k updates (100 epochs), but with time we found it forgot too much of the synthetic error correction task. We report evaluation scores at 130k.\nFor comparison, we also finetuned the different models (mBART-ENIS, mT5 and ByT5-base) on the IceEC data only, without the synthetic finetuning phase. This was done to examine how much the models learn from the added synthetic examples, and how far we can get using a small amount of hand-corrected examples. The mT5 and ByT5 models were trained for 100k updates and the mBART-ENIS model for 10k updates.\n\nResults\nDifferent metrics exist for evaluating GEC performance, but most are language-specific, and have not been adapted to Icelandic. Here we employ a language agnostic metric for scoring our models, the GLEU score (Napoles et al., 2015 (Napoles et al., , 2016)) . GLEU 6 is a variant of the BLEU (Papineni et al., 2002) score used to evaluate machine-translation output. It has been modified to account for both the source and the reference, by rewarding overlap between the source and the target sentence, and penalizing n-grams that should have been changed, but were not.\nWhen evaluating GEC for English, ERRANT (Bryant et al., 2017 ) is commonly used. It is a spanbased correction method that uses the F 0.5 metric, where precision weighs twice as much as recall. Though this metric has not been implemented for Icelandic, we also report ERRANT scores using a language-agnostic approach, disregarding error types and only reporting the span-based F 0.5 scores for each test set. These results are shown in Table 5 in Appendix E; they align well with the GLEU results in Table 3 which are described below.\nWe consider a variety of curated and synthetic test sets to get a good overview of the differences between the byte-level and subword-level approach for GEC. For the real errors, we report scores over the IceEC.test set, the test set from the IceEC, which contains around 5000 sentences. In con-trast, the dyslexic, L2 and children test sets contain 500 held-out sentences each from the respective specialized error corpora described in section 3.1 (only 100 examples were collected for the dativitis error type, a rarer occurrence in the data). We also annotated a small dataset (163 sentences) of data from an Icelandic news outlet (news), where each sentence contains at least one error; this is further described in Appendix B.\nFor the synthetic errors, we report GLEU scores over the test.synth set, which contains around 4000 held-out sentences from the synthetic data. Furthermore, we generated test sets of synthetic examples, each containing a particular error type in each sentence (dativitis, spaces, commas, dupl-words, mood, rand-noise, noun-case). This last group of test sets was generated using source texts that, while editorial, may include other errors, just like the synthetic training examples. The models, as they get better, learn to correct these errors as well. This may paradoxically lower the GLEU score as the corrected output deviates from the erroneous reference. These generated test sets still provide valuable information about what the models learn about each error type in isolation.\nTo understand what approach is best suited for GEC we trained the models on different data combinations and using different pre-trained models. The Synth-100k models are all trained for 100k updates on the same synthetic data, and the Synth-100k-EC models are additionally finetuned on the curated IceEC error corpus. To provide a baseline for the GLEU scores, we also report no_corr scores, where the source text is not corrected. This gives some idea of the noise level of the test sets, with test.synth being the noisiest and IceEC.test containing the least noise.\nThe GreynirCorrect (\u00d3lad\u00f3ttir et al., 2022) (GC) results were produced by applying GreynirCorrect to the test sets in its two configurations; correcting single-word errors on the one hand (spell.) and all errors found on the other (all). The GC system was developed in part to focus on the error categories and error frequencies defined in the IceEC training data, and this may be reflected in the scores for the IceEC test sets (asterisked in Table 3 ). The news test set ( \u2020) was created using only sentences flagged by GC (see Appendix B) and is therefore heavily biased towards that system. Models trained only on the synthetic data (Synth 100k) generally perform best on the synthetic er- ror corpora and thus solve the more simple and systematic spelling errors such as mistakes in punctuation, missing white space and word duplication. Their performance on the curated error corpora is somewhat lacking. In contrast, models trained only on the curated error corpora (EC) generally produce somewhat better GLEU scores on the curated error corpora than the Synth 100k models, but do not generalize to the error categories presented in the synthetic test sets. They are also unable to correct multiple errors in a single sentence (test.synth) .\nTraining on the synthetic data and then finetuning on the curated error corpora (Synth100k/ Synth550k+EC) performs best on the curated errors and retains much of the performance on the synthetic test sets. In all of these experiments, we can see that the ByT5 models generally perform better than the subword counterparts. This is also reflected in the ERRANT scores in Appendix E, Table 5 , where the ByT5 models score highest overall.\n\nDiscussion & Conclusion\nOur results show that the ByT5 models are the overall high-scorers on the real-world test sets, and on most of the synthetic ones. We include finetuning results on the ByT5 model that has been trained for longer on the synthetic data (550k updates) to compare how performance improves with time. We see the GLEU scores keep going up with time, and more importantly, when taking a close look at the actual generated output, this is the model that best corrects real-world errors. This makes it the most feasible option for use in real-world scenarios. A comparison of the output of the models trained on both data sources is shown in Appendix D.\nAn example from the test data is when the subword-tokenized model mBART-ENIS-Synth100k+EC incorrectly changes the name of a person from a rare name (\"L\u00e1retta\") to a more common one (\"L\u00e1ra\"). This kind of error is not seen in the byte-level model, which is quite conservative in its corrections of unknown entities. While this means ByT5 occasionally misses actual errors, we find that it is much better suited for production than a subword-level model that makes serious semantic errors. These more nuanced error correction examples may not be fully captured by the automated metrics, but are crucial for real-world use.\nThe subword regularization experiments are included as an alternative approach for mitigating the subword tokenization problem. The results are marginally better than the model without subword regularization when trained on the synthetic data, and the model performs better than the ByT5-Synth100k model in the case of dupli-cate words, which linguistically is a quite trivial task, and in more intricate mood errors. It however doesn't do any better than the mB-ISEN-Synth100k trained without subword regularization on the curated datasets, and this also holds when the model is finetuned additionally on curated data. The model finetuned on only the curated data with subword regularization (mB-ISEN-reg-EC) however performs consistently much better than its counterpart without subword regularization, often on par with or surpassing ByT5. This model has not seen any of the highly noised synthetic data, and thus has the most to gain from the subword noise. We speculate that this is one of the reasons we don't see more gains from adding subword regularization; the training examples are already so highly noised that there is not much to be learned from the added subword noise.\nThe IceEC finetuning data contain real-world errors which have been hand-corrected. These texts are somewhat different from the highly noised training examples with synthetic errors, have fewer errors on average and are more varied as they are naturally occurring. They also include stylistic edits from the reviewers, which improve the text's fluency, but in those cases the original is not necessarily incorrect as per the language standard. With these differences in mind, we expect the models to have to forget some of the synthetic error correction task in order to adapt to this \"new\" denoising task. We see this happen in the mBART-ENIS finetuning on the curated data, and to a lesser extent in the ByT5 finetuning. The denoising task performance on the synthetic errors from the previous step has in part been lost, which is expected, since some of these errors are not particularly common in real texts.\nFor the more grammatically complex error-types in the synthetic data (dativitis and changes to noun cases and verb moods), we find that the mBART-ENIS trained on synthetic data generally does well; for some subsets even surpassing the ByT5 counterpart that was finetuned on curated corpora. We suspect that this has to do with the linguistic knowledge the model has already gained during its pretraining on Icelandic texts, as explained in Appendix A. The ByT5 model that was trained for longer however manages to surpass it on the mood error type, indicating that it is still adapting to the Icelandic language, alongside its primary denoising task.\nThe models trained on only the finetuning data perform the worst throughout. The results show that they do not manage to correct the synthetic categories much beyond the baseline, except for mBART-ENIS in some cases. We expect that this has to do with their extra knowledge of Icelandic and the denoising objective used in the synthetic error correction finetuning. The results for these models on the curated in-domain test sets are in fact mostly on par with the models finetuned on the synthetic data only. Looking at the generated output, we see that the error types these models correct are not the same as those that the syntheticonly models are able to correct, which is expected, as they are trained on different data.\nWe conclude that adopting a byte-level approach rather than a subword approach leads to best results for the task of GEC, at the very least in the case of a morphologically rich language such as Icelandic. Finally, we find that the optimal way of capturing a wide range of errors is to train on a combination of synthetic and curated data, particularly when the curated data is limited.\n"}
{"question": "Why is it necessary to find a specific balance between tasks?", "evidence": "  To illustrate our idea, we give a two-task learning example shown in Figure 1 . As shown in Figure (1a) , it is observed that Pareto optimality-based methods can generate a set of Pareto solutions for a given two-task learning problem. However, some of Pareto solutions can increase the task 1 error while decreasing task 2 error, leading to unsatisfactory overall performance for MTL model. This im-plies that not all Pareto solutions always satisfy the goal of mitigating the tasks conflicts in MTL, and thus failing to achieve a better trade-off between tasks. Therefore, it is necessary to find a specific trade-off between tasks that is beyond what only using Pareto optimality can achieve.\n ", "options": ["A.  Because it can reach the climax which only use Pareto optimality can achieve", "B. Certain Pareto solutions may result in suboptimal overall performance for a Multi-Task Learning (MTL) model.", "C. Because all Pareto solutions can both increase the task 1 error and task 2 error.", "D. Two-task learning example has low efficiency."], "answer": "B", "content": "\nIntroduction\nMulti-task Learning (MTL), which aims to learn a single model that can tackle multiple correlated but different tasks simultaneously, makes multiple tasks benefit from each other and obtain superior performance over learning each task independently (Caruana, 1997; Ruder, 2017; Liu et al., 2015; Mao et al., 2020) . By discovering shared information/structure across the tasks, it has gained attention in many areas of research and industrial communities, such as computer vision (Misra et al., 2016; Gao et al., 2019; Yogamani et al., 2019; Sun et al., 2020 ) and text classification (Liu et al., 2017; Xiao et al., 2018; Mao et al., 2021 Mao et al., , 2022)) .\nHowever, it is observed in multi-task text classification (MTC) scenarios that some tasks could conflict with each other, which may be reflected via conflicting gradients or dominating gradients (Yu et al., 2020; Vandenhende et al., 2022) , leading to the degraded performance of MTL due to poor training. How to make a proper trade-off among jointing different tasks in MTC is a difficult problem. Recently, several methods have been proposed to mitigate gradient conflicts issue via both loss balance (linear weighted scalarization) such as homoscedastic uncertainty (Kendall et al., 2018) and task variance regularization (Mao et al., 2021) , and gradient balance like Pareto optimality (Sener and Koltun, 2018; Mao et al., 2020) . Existing methods devote to finding an arbitrary Pareto optimality solution in the Pareto set, which achieve a single arbitrary trade-off among all tasks. However, they can only satisfy the improved performance on part of tasks, not all tasks simultaneously. This means that these methods can not converge to a minimum average loss of all objectives.\nTo illustrate our idea, we give a two-task learning example shown in Figure 1 . As shown in Figure (1a) , it is observed that Pareto optimality-based methods can generate a set of Pareto solutions for a given two-task learning problem. However, some of Pareto solutions can increase the task 1 error while decreasing task 2 error, leading to unsatisfactory overall performance for MTL model. This im-plies that not all Pareto solutions always satisfy the goal of mitigating the tasks conflicts in MTL, and thus failing to achieve a better trade-off between tasks. Therefore, it is necessary to find a specific trade-off between tasks that is beyond what only using Pareto optimality can achieve.\nTo address this issue, inspired by multi-objective optimization (Sener and Koltun, 2018) , we argue that a more efficient way to mitigate task conflicts is to find a gradient trade-off between tasks in the neighborhood of the average loss rather than exhaustively searching for a proper solution from the set of Pareto solutions. As shown in Figure 1b , the Pareto solutions nearby the average loss can achieve a better trade-off between task 1 and task 2, leading to better performance on both tasks at the same time. Based on it, in this paper, we propose a novel gradient trade-off multi-task learning approach, named GetMTL, to mitigate task conflicts in multi-task text classification. Specifically, the gradients of each task are utilized to derive an update vector that can minimize the conflicts among task gradients in the neighborhood of the average gradient, so as to achieve a better trade-off performance among joint training tasks. In summary, the main contributions of our work are as follows:\n\u2022 A novel multi-task learning approach based on gradient trade-off between different tasks (GetMTL) is proposed to deal with task conflict in multi-task text classification problems, so as to improve the performance of all tasks simultaneously. \u2022 We give in-depth theoretical proofs and experimental analyses on establishing converge guarantees of our GetMTL. \u2022 We extensively verify the effectiveness of our GetMTL on two real-world text classification datasets, and the results show that our GetMTL performs competitively with a variety of state-of-the-art methods under a different number of task sets.\n\nRelated Works\nMulti-task Learning methods jointly minimize all task losses based on either loss balance methods (Kendall et al., 2018; Chen et al., 2018; Mao et al., 2021 Mao et al., , 2022) ) or gradient balance methods (Sener and Koltun, 2018; Mao et al., 2020) .\nThe loss balance methods adaptively adjust the tasks weights during training based on various heuristic approaches, such as task uncertainty quan-tification (Kendall et al., 2018) , gradient normalization (Chen et al., 2018) , task difficulty prioritization (Guo et al., 2018) , dynamic weight average (Liu et al., 2019) , random loss weighting (Lin et al., 2021) , task variance regularization (Mao et al., 2021) , and meta learning-based approach (Mao et al., 2022) . These methods are mostly heuristic and can have unstable performance while ignoring the task conflicts among all tasks, leading to the bad generalization performance of MTL models.\nRecently, some gradient balance based methods have been proposed to mitigate task conflicts for improving task performance. For example, D\u00e9sid\u00e9ri (2012) leverages multiple-gradient descent algorithm (MGDA) to optimize multiple objectives. Due to the guarantee of convergence to Pareto stationary point, this is an appealing approach. Sener and Koltun (2018) cast the multi-objective problem as a multi-task problem and devote to finding an arbitrary Pareto optimal solution. Mao et al. (2020) propose a novel MTL method based Tchebycheff procedure for achieving Pareto optimal without any convex assumption. However, these methods only consider achieving an arbitrary Pareto optimal solution while it is not the main objective. Unlike these methods, we propose an MTL approach based on multi-objective optimization and seek to find a set of solutions that are Pareto optimality and nearby the main MTC objective L 0 .\n\nPreliminaries\nConsider a multi-task learning problem with T 1 tasks over an input space X and a collection of task spaces {Y t } t\u2208 [T ] , where each task contains a set of i.i.d. training samples\nD t = {x i , y t i } i\u2208[nt] ,\nT is the number of tasks, and n t is the number of training samples of task t. The goal of MTL is to find parameters {\u03b8 sh , \u03b8 1 , ..., \u03b8 T } of a model F that can achieve high average performance across all training tasks over X , defined as F(X , \u03b8 sh , \u2022 \u2022 \u2022 , \u03b8 t ) : X \u2192 Y, where \u03b8 sh denotes the parameters shared between tasks and \u03b8 t denotes the task-specific parameters of task t. In particular, we further consider a parametric taskspecific map as f t (\u2022, \u03b8 sh , \u03b8 t ) : X \u2192 Y t . We also consider task-specific loss functions t (\u2022, \u2022) : Y t \u00d7 Y t \u2192 R + . We also denote the multi-task loss as L(\u03b8) = T i i (\u03b8), and the gradients of each task as g i = \u2207 i (\u03b8) for the particular \u03b8. In this paper, we choose the average loss as main objective of MTC problem, defined as L 0 (\u03b8) = 1 T T i i (\u03b8).\n\nMTL as Multi-objective Optimization\nMTL can be formulated as a specific case of multiple-objective optimization (MOO), which optimizes a set of potentially conflicting objectives (Sener and Koltun, 2018; Mao et al., 2020) . Given objective functions of T tasks, 1 , . . . , T , we formulate the optimization objective of MTL as the vectors of objective values :\nmin \u03b8 sh ,\u03b8 1 ,...,\u03b8 T (\u03b8 sh , \u03b8 1 ), . . . , (\u03b8 sh , \u03b8 T ) (1)\nSince there is no natural linear ordering on vectors, it is not possible to compare solutions and thus no single solution can optimize all objectives simultaneously. In other words, there is no clear optimal value. Alternatively, we can achieve Pareto optimality to obtain different optimal trade-offs among all objectives to solve MOO problem.\nDefinition 1 (Pareto dominance). Given two points {\u03b8, \u03b8} in \u2126, a point \u03b8 Pareto dominates \u03b8 (\u03b8 \u03b8) for MTL if two conditions are satisfied:\n(i) No one strictly prefers \u03b8 to \u03b8, that is, \u2200i \u2208 {1, . . . , T }, i (\u03b8 sh , \u03b8 i ) \u2264 i (\u03b8 sh , \u03b8 i ).\n(ii) At least one point strictly prefers \u03b8 to \u03b8, that is, \u2203j \u2208 {1, ..., T }, j (\u03b8 sh , \u03b8 j ) < j (\u03b8 sh , \u03b8 j ).\nDefinition 2 (Pareto optimality). \u03b8 * is a Pareto optimal point and (\u03b8 * ) is a Pareto optimal objective vector if it does not exist \u03b8 \u2208 \u2126 such that \u03b8 \u03b8 * . That is, a solution that is not dominated by any other is called Pareto optimal.\nThe set of all Pareto optimal solutions is called the Pareto set, and the image of Pareto set in the loss space is called Pareto front (Lin et al., 2019) . In this paper, we focus on gradient-based multiobjective optimization to achieve an appropriate Pareto trade-off among all tasks, which can approximate the Pareto front that minimizes the average loss.\n\nGradient-based Multi-Objective Optimization\nGradient-based MOO (Sener and Koltun, 2018) aims to find a direction d that we can iteratively find the next solution \u03b8 (t+1) that dominates the previous one \u03b8 (t) ( (\u03b8 (t+1) ) \u2264 (\u03b8 (t) )) by moving against d with step size \u03b7, i.e. \u03b8 (t+1) = \u03b8 (t) \u2212 \u03b7d. D\u00e9sid\u00e9ri (2012) ; Sener and Koltun (2018) propose to use multiple gradient descent algorithm (MGDA) that converges to a local Pareto optimal by iteratively using the descent direction d, which can be obtained as follows:\nd * = arg min d\u2208R m ,\u03b1\u2208R \u03b1 + 1 2 d 2 s.t. \u2207 i (\u03b8 (t) ) T d \u2264 \u03b1, i = 1, ..., T.\n(\nEQUATION\nwhere d * is the direction that can improve all tasks. Essentially, gradient-based MOO methods minimize the loss by combining gradients with adaptive weights, and obtaining an arbitrary Pareto optimality solution, ignoring the true objective (the average loss) (Liu et al., 2021) . In this paper, we generalize this method and propose a novel gradient-based approach to achieve a gradient trade-off among tasks for mitigating task conflicts, as well as constrain the solution that can minimize the average loss (L 0 (\u03b8)).\n\nGradient Trade-offs for Multi-task Text Classification\nFollowing most MTL methods, as shown in Figure 2 , we employ the hard parameter sharing MTL architecture, which includes f sh parameterized by heavy-weight task-shared parameters \u03b8 sh and f t parameterized by light-weight task-specific parameters \u03b8 t . All tasks take the same shared intermediate feature z = f sh (x; \u03b8 sh ) as input, and the t-th taskspecific network outputs the prediction as f t (z; \u03b8 t ).\nSince task-shared parameters \u03b8 sh are shared by all tasks, the different tasks may conflict with each other, leading to the degraded performance of MTL model. In this paper, we hypothesize that one of the main reasons for task conflicts arises from gradients from different tasks competing with each other in a way that is detrimental to making progress.\nWe propose a novel gradient-based MOO optimization to find a gradient trade-off among tasks in the neighborhood of the average loss, so as to mitigate task conflicts. Note that, we omit the subscript sh of task-shared parameters \u03b8 sh for the ease of notation.\n\nGetMTL\nGiven a task i, we define its gradient as g i = \u2207 i (\u03b8) via back-propagation from the raw loss i , and g i represents the optimal update direction for task i. However, due to the inconsistency of the MOO method and our GetMTL on three gradients (g 1 , g 2 and g 3 ) in R 3 , where g i denotes the gradient (black) of i-th task, g 0 is the average gradient, and blue arrows denote the projections of update direction to each task gradient.\noptimal update direction of task-shared parameters for each task, different task gradients may conflict with each other, leading to the training of networks being stuck in the over-training of some tasks and the under-training of other tasks. Intuitively, it is desirable to find a direction that can minimize the task conflicts among different tasks as well as achieve Pareto optimality to improve the performance of MTL model. We first achieve an arbitrary Pareto optimal via finding a descent direction d des by searching for a minimum-norm point in the Convex Hull CH of gradients, defined by,\nEQUATION\ns.t. S T = \u03b2 \u2208 R T + T j=1 \u03b2 j = 1 (4)\nwhere G \u2208 R T \u00d7m = {g 1 , ..., g T } is the matrix of task gradient, S T is the T -dimensional regular simplex. We use the multiple gradient descent algorithm (MGDA) (Sener and Koltun, 2018) to obtain an arbitrary Pareto optimal by iteratively using the descent direction, defined by,\nd des = arg min d\u2208CH d 2 2\n(5)\nIn addition, the d des can be reformulated as a linear combination of all task gradients, defined by,\nd des = T i=1 \u03b2 i g i (6)\nwhere g i = \u2207 i (\u03b8) is the i-th task gradient. It implies that, when converges to an arbitrary Pareto optimal, the optimal gradient value of each task via back-propagation is \u03b2 i g i , defined as\ng \u03b2 i = \u03b2 i g i .\nHowever, moving against d des does not guarantee that the solution meets the requirements of multi-task text classification task (MTC), that is, to alleviate the gradient conflict among tasks in MTC, so as to improve the performance of all tasks. To address this issue, we seek a direction that enables us to move from a solution \u03b8 (t) to \u03b8 (t+1) such that both \u03b8 (t+1) dominates \u03b8 (t) (L(\u03b8 (t+1) ) \u2264 L(\u03b8 (t) )) and alleviate the gradient conflict among all tasks. Based on it, as shown in Figure 2 (b), we propose to search for an update direction d in the Convex Hull CH \u03b2 of back-propagation gradients such that it can improve any worst objective and converge to an optimum of MTC objective L 0 (\u03b8). We first find the worst task gradient with respect to the update direction d, that is, it has a maximum angle with d, which can be formulated via the following optimization problem,\nEQUATION\nwhere g \u03b2 i is the i-task gradient after optimizing by MGDA algorithm.\nTo improve the worst gradient of any task and achieve a trade-off between all task gradients in a neighborhood of the average gradient (defined as g 0 = 1 T T i=1 g i ), we formulate this gradient trade-off optimization problem via the following Maximin Optimization Problem (dual problem).\nProblem 1.\nmax d\u2208R m min i\u2208[T ] g \u03b2 i , d s.t. d \u2212 g 0 \u2264 \u03b5g T 0 d, \u2212 g T 0 d \u2264 0 (8)\nwhere g \u03b2 i = \u03b2 i g i is the back-propagation gradient value of i-th task via solving Eq. ( 5), \u03b5 \u2208 (0, 1] is a hyper-parameter that controls the stability of MTC model.\n\nSolving Maximin Problem\nSince the optimal direction d can also be defined in the convex hull CH \u03b2 of g \u03b2 i , we can get\nEQUATION\nwhere\nG \u03b2 \u2208 R T \u00d7m = {g \u03b2 1 , ..., g \u03b2 T } is task gradi- ent matrix, W T = { w \u2208 R T + T j=1 w j = 1} is the T -dimensional\nprobability simplex, and w = (w 1 , ..., w T ). Therefore, we can get min i g \u03b2 i , d = min w\u2208W T i w i g \u03b2 i , d and Problem 1 can be transformed into the following form.\nAlgorithm 1: GetMTL Algorithm.\nInput: The number of task T , loss functions { i } T i=1 , network parameters \u03b8 (t) at t step, the pre-specified hyper-parameter \u03b5 \u2208 (0, 1] and step size \u00b5 \u2208 R + . 1: Task Gradients:\ng i = \u2207 i (\u03b8 (t) ), i \u2208 [T ] 2: Main Objective: g 0 = T i=1 g i 3:\nObtain {\u03b2 1 , ...\u03b2 T } by solving Eq.( 5). 4: Compute g w = i w i g \u03b2 i , where g \u03b2 i = \u03b2 i g i 5: Obtain {w 1 , ..., w T } by solving Eq.( 14) 6: Find direction d * by using Eq.( 13)\nEQUATION\nwhere g w = T i=1 w i g \u03b2 i is the convex combination in CH \u03b2 . For a given vector \u03bb \u2208 R + with non-negative components, the corresponding Lagrangian associated with the Eq.( 10) is defined as\nEQUATION\n11) Since the objective for d is concave with linear constraints and w \u2208 W T is a compact set 2 , according to the Sion's minimax theorem (Kindler, 2005) , we can switch the max and min without changing the solution of Problem 2. Formally,\nmin \u03bb,w\u2208W T max d\u2208R m g T w d \u2212 \u03bb d \u2212 g 0 2 /2 + \u03bb\u03b5 2 (g T 0 d) 2 /2 (12)\nWe get the optimal solution of primal problem (Problem 1) by solving the dual problem of Eq.( 12) (See the Appendix A for a detailed derivation procedure). Then we have\nd * = g w + \u03bb * g 0 (1 \u2212 \u03b5 2 g 2 0 )\u03bb * , where \u03bb * = g w \u03b5 g 0 2 (13)\nwhere \u03bb * is the optimal Lagrange multiplier, d * is the optimal update direction of MTC model. We can reformulate the problem of Eq.( 12) as following optimization problem w.r.t. w.\nEQUATION\n2 Compact set: a set that is bounded and closed. where g w is defined as\ng w = T i=1 w i g \u03b2 i .\nThe detailed derivation is provided in Appendix A. Algorithm 1 shows all the steps of GetMTL algorithm in each iteration.\n\nTheoretical Analysis\nIn this section, we analyze the equivalence of solutions to dual problem and then give a theoretical analysis about convergence of GetMTL algorithm. We define the Lagrangian of problem in Eq.( 10), Theorem 4.2 (Convergence of GetMTL). Assume loss functions i are convex and differential, and \u2207 i (\u03b8 (t) ) is L-lipschitz continuous with L > 0. The update rule is \u03b8 (t+1) = \u03b8 (t) \u2212 \u00b5 (t) d, where d is defined in Eq.( 13) and\nL(d, \u03bb, w) = g T w d \u2212 \u03bb 2 ( d \u2212 g 0 2 \u2212 \u03b5 2 (g T 0 d) 2 ) Theorem 4.1 (Equivalence of\n\u00b5 (t) = min i\u2208[k] d\u2212g 0 c\u2022L\u2022d 2 . All the loss functions 1 (\u03b8 (t) ) \u2022 \u2022 \u2022 T (\u03b8 (t) ) converges to ( 1 (\u03b8 * ) \u2022 \u2022 \u2022 T (\u03b8 * )).\nProof. The proof is provided in Appendix C.\n\nExperimental Datasets\nWe conduct experiments on two MTC benchmarks to evaluate the proposed GetMTL. 1) Amazon Review dataset (Blitzer et al., 2007) contains product reviews from 14 domains (See Details in Appendix D), including apparel, video, books, electronics, DVDs and so on. Each domain gives rise to a binary classification task and we follow Mao et al. (2021) to treat 14 domains in the dataset as distinct tasks, creating a dataset with 14 tasks, with 22180 training instances and 5600 test instances in total. 2) Topic classification dataset, 20 Newsgroup 3 , consists of approximately 20,000 newsgroup documents, partitioned evenly across 20 different newsgroups. We follow Mao et al. (2021) to select 16 newsgroups from 20 Newsgroup dataset shown in Table 1 and then divide them into four groups. Each group gives rise to a 4-way classification task, creating a dataset with four 4-way classification tasks, which is a more challenging dataset than amazon review dataset.\n\nExperimental Implementation\nWe follow the standard MTC setting and adopt the same network architectures with the most recent baselines for fair comparisons (Mao et al., 2021) . We adopt the hard parameter sharing MTL framework shown in Figure 2 , where task-shared network is a TextCNN with kernel size of 3,5,7 and taskspecific network is a fully connected layer with a softmax function. Adam is utilized as the optimizer to train the model over 3000 epochs with a learning rate of 1e-3 for both sentiment analysis and topic classification. We set the batch size to 256. \n\nComparison Models\nWe compare the proposed GetMTL with a series of MTC baselines, including Single-Task Learning (STL): learning each task independently.\nUniform Scaling: learning tasks simultaneously with uniform task weights.\nUncertainty: using the uncertainty weighting method (Kendall et al., 2018) .\nGradNorm: learning tasks simultaneously with gradient normalization method (Chen et al., 2018) .\nTchebycheffAdv: using adversarial Tchebycheff procedure (Mao et al., 2020) .\nMGDA: using gradient-based multi-objective optimization method (Sener and Koltun, 2018) .\nBanditMTL: learning tasks simultaneously with multi-armed bandit method (Mao et al., 2021) .\nMetaWeighting: using adaptive task weighting method (Mao et al., 2022) .\n\nMain Results\nThe main comparison results of GetMTL on two benchmark datasets are shown in Figure 3 and 4 . It is clear that (See detailed numerical comparison results in Appendix D), our proposed GetMTL model performs consistently better than the all comparison methods on all tasks of both amazon review and topic classification datasets, and its average performance is superior to that of all baselines. This verifies the effectiveness of our GetMTL method in MTC problem. More concretely, in comparison with the gradient-based MOO optimization model (MGDA), our GetMTL achieves significant improvement across all datasets. This indicates that achieving a gradient trade-off nearby average loss to mitigate task conflicts can better improve all task performance and generalization ability of MTC model. \n\nEmpirical Analysis on Convergence\nIn Section 4.3, we theoretically prove the convergence of our proposed GetMTL. Furthermore, we conduct extensive experiments about the convergence to better demonstrate the advantages of GetMTL shown in Figure 5 . It is clear that the learning curve of GetMTL is constantly decreasing as the number of iterations increases and converges to the lowest loss value compared with other baselines. It indicates that GetMTL can guarantee the convergence of the objective value and obtain better performance of all learning tasks.\nIn addition, we also conduct extensive experiments to investigate how GetMTL mitigates task conflict during training. We plot the task variance (variance between the task-specific losses) of all baselines on both amazon review and topic classification datasets shown in Figure 6 . It can be observed that all MTL baselines have lower task variance than STL method, which illustrates that MTL methods can indeed boost the learning of all tasks compared with STL method. Moreover, GetMTL has the lowest task variance and smoother evolution during training than other MTL baselines. This implies that our proposed GetMTL indeed mitigates task conflicts compared with other MTL methods.\n\nThe Evolution of Task Weight w\nIn this section, we visualize the task weights of our GetMTL and two weight adaptive MTL methods (MGDA and BanditMTL) throughout the training process using the topic classification dataset shown in Figure 7 . It can be observed from these four figures that the weight adaption process of our GetMTL is different from that of MGDA and Ban-ditMTL. GetMTL can automatically learn the task weights without pre-defined heuristic constraints. The weights adaption process of GetMTL is more stable and the search space is more compact compared with other MTL baselines.\n\nImpact of the Values of \u03b5\nTo investigate the impact of using different values of \u03b5 on the performance of our GetMTL, we conduct experiments on two datasets, and the results are shown in Figure 8 . Noting that model with \u03b5 = 0.0075 and \u03b5 = 0.025 perform overall better than other values on these two datasets, respectively. The model with larger value of \u03b5 performs unsatisfactorily overall all tasks on two datasets, one possible reason is that larger \u03b5 makes d pull far away from the average loss g 0 (see the conditions in Eq. ( 9)). That is, Pareto optimality found by GetMTL is getting further and further away from MTC objective L 0 , which can be quite detrimental to some tasks' performance, leading to degraded average performance.\n\nConclusion\nIn this paper, we propose a novel gradient tradeoff multi-task learning approach to mitigate the task conflict problem, which can achieve a specific trade-off among different tasks nearby the main objective of multi-task text classification problem. Moreover, we present a series of theoretical proofs to illustrate the effectiveness and superiority of our GetMTL. Experimental results on two benchmark datasets show that our GetMTL achieves state-ofthe-art performance in Multi-task Text Classification problem.\n"}
{"question": "Which machine learning method achieved the best results for dating Greek papyri in the study?", "evidence": "  Linear regression achieved a MAE of 86 years on average (Table 2 ) and a MSE of 1.33. R 2 was similar across folds, around 83. A random forest had an even better MAE of 73 years on average but a worse MSE (1.58). Its average R 2 was lower than that of linear regression, but the maximum one achieved across folds was much better. Random forest also outperformed both gradient boosting methods in MAE but GBoost achieved a better MSE and R 2 on average. XTrees achieved the best results in all metrics, with a MAE of 54 years. ", "options": ["A. Extremely randomised trees (XTrees)", "B. Random forests", "C. Linear regression", "D. Gradient boosting"], "answer": "A", "content": "\nIntroduction\nAncient textual artefacts are arguably the richest source of information on the ancient world. In the Graeco-Roman world and particularly in its Greekspeaking part, the most extensive coeval texts come from inscriptions and papyri. The latter is a collective term used for all ancient manuscripts, regardless of their writing material which, apart from papyrus, may be parchment, pottery, wood, and others. To correctly evaluate and make good use of these texts, we need to determine their date, provenance and historical context of their production and use. As far as dating is concerned, the value of the relevant evidence provided by the artefacts themselves varies considerably, ranging from a direct date in the text (following, of course, the calendar and dating system of the respective historical period) to no evidence at all. In between, there are texts containing references to known historical figures and events of a certain period, papyri which have been found next to other objects that can be dated, or other indirect evidence. The presence or absence of a date depends on the type of text preserved on the papyrus and its use through time, as well as on its state of conservation. Just like in modern times, it is much more likely to include a date in an official letter than in a page torn from a novel book. At the same time, it is more probable to find a date in a fully surviving letter than in a damaged one missing, for instance, the upper part of the first page.\nGreek papyri, which mostly survive in fragments, are divided into two broad categories: books (literary and sub-literary papyri) and documents of all kinds (documentary papyri). The former ones never carry a date, whereas the latter often do, albeit not always unambiguously convertible by modern scholars. Most importantly for our study, literary papyri contain copies of works authored many years (often centuries) before the production of the actual manuscripts. On the other hand, documentary texts were usually written down as they were composed or shortly after that, making the content of their texts contemporary to their writing style or script. Therefore, any temporal indication in the text is also dating evidence regarding the production of the document. Even when there is no direct date in the text (e.g. Figure 1 ), documentary papyri can be dated securely sometimes within a short time-frame, because they may refer to known historical events or concern people known through other sources to have lived at a particular time.\nWhen neither direct or indirect dating is possible, papyrologists resort to palaeography, the study of the script. In palaeography, particular writing styles are associated with certain chronological periods. Therefore, similar writing styles point to similar dates (Mazza, 2019) . Securely dated specimens are used as a guide to chronologically place the undated ones. Growing criticism on the subjectivity of palaeographical dating (Mazza, 2019; Choat, 2019; Nongbri, 2019 Nongbri, , 2014) ) highlights the need for more reliable methods. Recent efforts for computational dating of historical manuscripts are based on the script rather than the text and, although they consider various languages, they disregard Greek (Omayio et al., 2022) .\nIn this study we focus on computational dating of Greek documentary papyri based on their transcriptions, contributing in the following three ways:\n1. We present and publicly release a machineactionable dataset of 389 documentary Greek papyri, containing texts of various aspects of daily life (e.g. contracts, receipts, letters).\n2. We draw the baseline in text regression for the tasks of dating experimenting with Monte Carlo and leave one out cross validation.\n3. We apply a committee of regressors to three papyri, which present different types of dating challenges, and on 159 manuscripts for which only the upper date limit is known.\nThis approach does not apply to literary papyri and our research involves solely documents. Apart from their texts being contemporary with the actual manuscripts (by dating the text, we date the papyrus), nonliterary papyri also include vastly more numerous objectively dated specimens than literary ones. Specific dates on our training set also allow for more accurate (narrower date-spans) predictions by our models.\n\nRelated Work\nDating historical documents with computational means has been studied for many languages (Baledent et al., 2020; Dhali et al., 2020; Li et al., 2015; Hamid et al., 2019; Adam et al., 2018) . However, very limited work has been done for Greek and no published work at all has focused on Greek papyri. The only work to our knowledge is Ithaca, a Transformer trained on ancient Greek inscriptions performing text restoration, geographical attribution, and dating (Assael et al., 2022) . Ithaca has achieved an error of 0.29 centuries in dating epigraphs. This result is by far better than an onomastic baseline using the known distribution of Greek personal names to infer the date, which scored 1.44. Inscriptions differ from papyri in many aspects (such as the genre, the length, and their geographical distribution), but in principle, this system is applicable to our data and was therefore used as a baseline. Below, given the absence of dating studies for Greek, we summarise work for other languages.\nThe studied languages are Latin (Baledent et al., 2020; Wahlberg et al., 2016 Wahlberg et al., , 2015)) , Hebrew (Dhali et al., 2020) , Dutch (Hamid et al., 2019 (Hamid et al., , 2018;; He et al., 2014 He et al., , 2016b,a),a) , Arabic (Adam et al., 2018) , Swedish (Wahlberg et al., 2016 (Wahlberg et al., , 2015)) , French (Baledent et al., 2020) and English (Li et al., 2015; Rastas et al., 2022) . A collection of 595 Dead Sea Scrolls, in Aramaic script, was the dataset with the oldest manuscripts, dated from 250 to 135 BCE, and the only one so far concerning texts written on papyri (Dhali et al., 2020) . The rest of the datasets comprised more data, ranging from less than five (Adam et al., 2018) to more than ten thousand manuscripts (Wahlberg et al., 2015) or more (Rastas et al., 2022) , while the one with the most recent manuscripts comprises historical English-language documents (Li et al., 2015) , printed between the 15th and 19th CE.\nThe employed methods usually were standard machine learning methods, such as KNN (Adam et al., 2018) , decision trees (Baledent et al., 2020) , random forests (Baledent et al., 2020) and support vector machines (Hamid et al., 2019; Dhali et al., 2020; He et al., 2014 He et al., , 2016b,a),a) . Textural features, such as Gabor filters, Uniform Local Binary Patterns and Histogram of Local Binary Patterns are extracted and then fed to the classifiers (Hamid et al., 2018) . The writing style evolution, however, has also been used as an intermediate step (Dhali et al., 2020; Adam et al., 2018) . In this case, the periods are first aligned with specific writing styles. Then, any new manuscript is dated based on the detected style.\nPre-trained convolutional neural networks have been used to extract features, which are passed to a classifier or regressor (Hamid et al., 2019; Wahlberg et al., 2016) , or used in combination with text features extracted with optical character recognition methods (Li et al., 2015) . Transfer learning has been reported to lead to human performance (Wahlberg et al., 2016) . This was deemed to be the most promising direction for the present study on Greek manuscripts, and was, hence, employed.\n\nData\nOur dataset, which we release publicly, 1 comprises the transcriptions of 389 manuscripts, dated from the 3rd century BCE to the 7th century CE, originating from Greco-Roman Egypt (with a few exceptions from the Near-East).\n\nThe source\nThe dataset was compiled mainly from PA-PYRI. INFO. 2 The documents in its collections set a reliable point of reference for scholars who aspire to study the evolution of ancient manuscripts in time. These collections incorporate full transcriptions and references to scholarly editions of the papyri, as well as a set of metadata that can also assist in dating (e.g. provenance).\n\nThe scripts and the language\nNonliterary papyri in Greek from the 3rd c. BCE to the 7th c. CE are written in a great variety of cursive hands (Harrauer, 2010) , posing an extra challenge for image classification methods and calling for other approaches. The language of the papyri, Greek of the Ptolemaic, Roman and early Byzantine periods, reflects the diversity and the diachronic changes of the Greek-speaking communities in Egypt, which is the provenance of most of our specimens.\n\nThe ground truth\nThe date of a manuscript may be found in different forms. It can be an exact date, a range of years, a starting date (not before that date), or an ending date (not after that date), or two-three alternative dates. Our dataset has been curated so that dating applies at the level of the quarter of the century, by considering manuscripts dated exactly or with a period ranging within that quarter. We did not consider manuscripts that were dated only before or after a specific date.\n\nData collection\nOur first dataset comprised 400 manuscripts, 40 samples per century. Our initial pool consisted of 77,040 items and we opted for ones that satisfy the following conditions:\n\u2022 The transcriptions must be available in machine actionable form.\n\u2022 The papyri must contain documents (not works of literature) to ensure that text and papyrus are contemporary. 3\n\u2022 The papyri must be securely and accurately dated. Many papyri do not carry a date and are, therefore, dated with subjective criteria or with a large date span (e.g. 1st-2ndCE).\n\u2022 The image is available, to allow image-based dating and potentially jointly from different modalities: text and image.\nGiven these limitations, it was the 7thCE that dictated the size per century of a balanced dataset, since there are not more than 40 securely dated papyri from 7thCE. For each of these records, the text was retrieved afterwards from PAPYRI.INFO by parsing the respective XML files. We discarded records whose extracted text was less than ten characters, which resulted in our final 389 records. From these records, we extracted the entire text from one side of the papyrus (the side that had more text than the other). In the few cases of papyri with more than one fragment, we only included the first one. This decision was based on weighing the benefit of avoiding a considerable amount of noise during automatic parsing against eliminating a portion of text, in a dataset whose nature is by definition fragmentary.\n\nNormalisation\nThe transcribed text comprises a variety of characters and symbols. We preprocessed the data by lowercasing and normalising the text (see Table 1 ). We (o) '\u1f45', '\u1f43', '\u03cc', '\u03cc', '\u1f44', '\u1f41', '\u1f40', '\u1f78' (\u03b1) '\u1f02', '\u1fb4', '\u1f03', '\u1f85', '\u03ac', '\u1f01', '\u1f04', '\u1fb6', '\u1f00', '\u1fb7', '\u1f70', '\u03ac', '\u1f05' (\u03b7) '\u1f24', '\u1f23', '\u1fc3', '\u1f22', '\u1f20', '\u03ae', '\u1f26', '\u1f25', '\u1fc6', '\u1f27', '\u1f97', '\u1f94', '\u1fc7', '\u1f21', '\u1fc4', '\u1f91', '\u1f74', '\u03ae' (\u03b9) '\u03af', '\u1fd6', '\u1f37', '\u1f31', '\u1f36', '\u1fd2', '\u03af', '\u1f30', '\u1f76', '\u0390', '\u1f34', '\u03b9', '\u03ca', '\u1f33', '\u1f35' (\u03b5) '\u03ad', '\u1f72', '\u03ad', '\u1f14', '\u1f10', '\u1f15', '\u1f11', '\u03b5' (\u03c5) '\u1f57', '\u03cd', '\u1fe6', '\u03cd', '\u1f55', '\u1f53', '\u1f7a', '\u1f51', '\u1f54', '\u1f50', '\u1f56' (\u03c1) '\u1fe5', '\u1fe4' (\u03c9) '\u03ce', '\u1ff6', '\u1f66', '\u1fa4', '\u1f60', '\u1f67', '\u1ff3', '\u1ff7', '\u1f62', '\u1f65', '\u03ce', '\u1fa7', '\u03ce', '\u1fa6', '\u1f64', '\u1fa0', '\u1f7c', '\u1f61', '\u1ff4' (\u03c3) '\u03c3', '\u03c2' Table 1 : Normalisation rules of characters in the dataset, all characters on the right have been replaced by the character on the left. also discarded any character besides the 24 Greek letters, also removing white space and all punctuation marks. We did not eliminate the editors' corrections and supplements nor edit otherwise the data, which often led to duplicate words with alternative orthography (original and normalisation).\nThe transcriptions available are not diplomatic (reflecting exactly what is written) but normalised according to modern conventions, for example as far as punctuation and word separation (or sometimes spelling) are concerned. Therefore, we chose to disregard these conventions, because they do not represent data present in our sources, but normalisation on the papyrologists' part for the purpose of scholarly editions.\nTo provide some more concrete examples, there is no capitalization of proper names or initial words in sentences in papyri. Punctuation is very scarce and sometimes completely absent. Diacritics are not meaningless, but they are extremely rare in documentary papyri (i.e., except diaresis which is used in a different way than modern conventions, to mark iota and upsilon as the first letter of a word). Breathings and accents are marked inconsistently (if at all) by different scribes. Hence, removing diacritics leads to inclusion and can help avoid multiple variations of what is in fact the same word. Regarding spelling, we kept both the original and the corrected form (if provided by the editors), because spelling mistakes reflect language evolution.\n\nExploratory analysis\nThe overall text length per quarter of century varies over time, as can be seen in Figure 2 . Although we have selected an equal number of manuscripts per century ( \u00a73.4), the number of lines within each manuscript varies, and so does the line length. Furthermore, within a century, manuscripts of a spe- cific quarter of a century may be more frequent due to random discoveries, as is the case of 7thCE, where the first quarter holds most of the support, a discrepancy deriving from the reduced number of dated papyri in this century overall.\nThe most frequent character in our dataset is '\u03b1' (35,101 occurrences), followed by '\u03bf' (33,176), '\u03b9' (30,151), and '\u03b5' (25,116) . On the other hand, the least common are '\u03b2' (2520), '\u03be' (1210), '\u03b6' (379), and '\u03c8' (334). These figures are coherent with general frequencies of letters in Ancient and Modern Greek (Mikros et al., 2005) .\nIn order to assess the quality of the ground truth, we employed the Callimachus' Conservation number (CCN), 4 which provides an educated estimation of the preservation and legibility of a papyrus. The lowest score is 0 and the highest score (i.e., 1) indicates readability and 'perfect' conservation of the text. The status of the conservation of a papyrus affects the quality of the transcription, indicating the amount of text that has not been recorded in the transcriptions (or recorded with some level of uncertainty) because of the material state of preservation of the manuscripts. Damage in papyri could affect as little as one or two letters (or even none), to as much as several lines and whole parts of the \n\nMethodology\nTo estimate the date of production of manuscripts, we opted for text regression, taking advantage of the continuous target objective. Statistical validity was established with 5-fold Monte Carlo crossvalidation. The best regression method was used to form a committee of models, which were applied on unseen data in order to analyse the predictions.\n\nBenchmarking\nWe performed Monte Carlo cross-validation, by sampling 90% for training, 10% for validation, and then re-sampling with replacement five times. We report the mean absolute error (MAE), the mean squared error (MSE), and the explained variance (R 2 ). Besides the average results across folds, we also report the best score achieved per metric.\n\nRegression methods\nFern\u00e1ndez-Delgado et al. ( 2019) surveyed 77 regression methods and undertook an experimental analysis on 83 datasets. Regression with extremely randomised trees achieved the best R 2 in many datasets but gradient boosting and random forests were also found to have a promising performance. Following these findings, we opted for extremely randomised trees, random forests, gradient boosting, and linear regression for our experiments. 5 Extremely randomised trees (XTrees) is a treebased ensemble, created with the Extra-Trees algorithm (Geurts et al., 2006) . Although simple in nature, it is both accurate and efficient Fern\u00e1ndez-Delgado et al. (2019) . Compared to other ensembles that use decision trees, XTrees splits the nodes of the tree by choosing randomly cut-off points and the trees grow by using the whole sample to learn instead of bootstrapping.\n\nThe Committee\nUsing the best-performing regression method out of the ones examined, we performed leave one out cross-validation, which allowed an evaluation using the whole dataset. Furthermore, it yielded as many regressors as the data points, which in our case is 389. We used these models to form a committee and date unseen papyri (further discussed in \u00a76).\n\nEmpirical analysis\nThis section presents our experimental results using regression on textual features to date Greek manuscripts. First, we present preliminary experiments and then we analyse the experimental findings from our regression analysis.\n\nPreliminary experiments\nPreliminary experiments comprised image classification (Hamid et al., 2018 ), text classification with Transformers trained on another domain (Assael et al., 2022) , and transferring learning from large language models (Koutsikakis et al., 2020) . Image classification was used prior to using transcribed text as our input, experimenting with using the documents' images (Hamid et al., 2018; Wahlberg et al., 2016; Paparigopoulou et al., 2022) . Vanilla convolutional neural networks were outperformed by a pre-trained one (Tan and Le, 2019), fine-tuned for our dating task. Our estimated MAE, however, was consistently more than a hundred years (Table 2 ), hence we opted for textual input. Ithaca was presented by Assael et al. (2022) , consisting of a Transformer that is trained not only in dating but also in text restoration and geographical attribution. Ithaca has achieved an error of 0.29 centuries in dating inscriptions, which is by far better than an onomastics baseline (error of 144 years).\nBy using the open-access web interface, 6 we scored all our preprocessed texts, 7 registering a MAE of approx. one century by using the maximum decade predicted or the average of the distribution (Table 2). The difference from the published result possibly stems from the fact that this is a model trained and focused on inscriptions, not papyri. Transfer learning was used with GreekBERT, a Transformer that is pre-trained in masked language modelling, among other tasks, in modern Greek (Koutsikakis et al., 2020) . GreekBERT has been further pre-trained in ancient Greek (Singh et al., 2021) . We experimented with fine-tuning both variants in predicting the date, 8 but MAE was approx. one century (Table 2 ).\n\nRegression analysis\nExperiments were undertaken with Google Colaboratory, using a 12GB NVIDIA Tesla K80 GPU. We extracted term-frequency-inverse-documentfrequency features using lower-cased text and character n-grams (from 1-to 5-grams). 9 All other parameters were set to default values. 10\n\nMonte Carlo cross validation\nLinear regression achieved a MAE of 86 years on average (Table 2 ) and a MSE of 1.33. R 2 was similar across folds, around 83. A random forest had an even better MAE of 73 years on average but a worse MSE (1.58). Its average R 2 was lower than that of linear regression, but the maximum one achieved across folds was much better. Random forest also outperformed both gradient boosting methods in MAE but GBoost achieved a better MSE and R 2 on average. XTrees achieved the best results in all metrics, with a MAE of 54 years and the best R 2 climbing up to 95.43.\n\nLeave one out cross validation\nUsing the best performing XTrees, we performed leave one out cross validation, by hiding one instance, training the algorithm on the remaining instances, and then using the model to predict the hidden record. 11 The MAE was found to be 55 years, MSE was 1.11, and R 2 was 85.89, close to the Monte Carlo evaluation scores. In order to better understand the errors, we rounded the predictions and the ground truth, evaluating as if we would in a classification setting. Predictions most often fall on or close to the diagonal (Figure 4 ), which explains the low error. The best result is 8 We used white space, to allow subword computation. 9 Preliminary experiments with centroid or trainable word embeddings before recurrent or convolutional neural networks deteriorated performance.\n10 Manual hyper-parameter tuning per regressor yielded insignificant improvements.\n11 The experiment lasted 15 hours. 3 .\nachieved for the 1st and 2nd CE, followed by the 7th CE (see Table 3 ). The overall accuracy is 60%.\n\nError analysis\nIn very few cases, our leave-one-out regression fell considerably out of its predictions (Figure 4 ). Our analysis showed that these texts happen to contain specific words typical of another period, which confused the prediction. For instance among the highest prediction error were two late texts (6-7thCE) that exceptionally contain \u03a3\u03b5\u03c1\u03b1\u03c0\u03af\u03bf\u03c5 and \u0392\u03b1\u03c3\u03b9\u03bb\u03b5\u03af\u03bf\u03c5, usually found in Ptolemaic time (3rd-1stBCE). In another case, we provided experimentally the longer version of the text, initially parsed only partially ( \u00a73.4). Using the full text led to an accurate prediction, influenced by the word 'indiction' in the additional text ( \u00a77.1).\n\nUse cases\nWe applied our 389 regressors, produced upon leave-one-out cross-validation, to three use cases, which present different types of dating challenges.\n\nPSI 8 934\nThis document 12 preserves the ca. 15 last lines of a land lease. texts from the 6th and early 7thCE c., the Dioscorus archive (Fournet, 2008) , because, among other concordant elements, it contains microtoponyms from the respective village countryside. The notary who signed the contract, Abraam, is known from other documents, which is crucial evidence for the dating of the papyrus. This notary's period of activity has been proven to span at least between 524 and 545 (Fournet, 2003) . This papyrus, therefore, is securely dated by indirect evidence, but no date is explicitly mentioned in the text (Fournet, 2008) . Our average prediction is 310 CE, dated between 260 CE (min) and 352 CE (maximum prediction).\n\nP. Basel 2 15\nThis papyrus, also shown in Figure 1 , is a private letter dated indirectly from the 1st CE. The letter is almost complete, except for a damaged word at the end of line 5. Private letters usually do not bear a date. The dating, therefore, by the editor is done on palaeographical grounds as well as on the basis of scribal habits: \"the hand [...] is more at home in the first century CE than the second, a dating that is supported by the writer's use of iota adscript...\" (Huebner et al., 2020) . Iota adscript is an expected feature in the 3rd BCE, starting to be irregularly written between the 2nd BCE and the first CE to almost completely disappear from the 2nd CE onwards (Clarysse, 1976) . Onomastics strengthen the editor's dating hypothesis: of the three personal names mentioned in the letter (Pasis, Orsenouphis, and Tithoes), the first two are attested from ca. 250 BCE to 250 CE while the last one starts appearing in the papyri only in the 1st c. CE. 13 Our models date this to 140 BCE, from 165 BCE to 112 BCE.\n\nP. Petra 1 5\nThe last manuscript 14 contains a request for transfer of taxation from 538 CE. It is a geographical outsider since it does not come from Egypt but from Petra (Jordan). We tested this manuscript since many of the words found in the text are infrequent in Egyptian manuscripts, on which our models are trained. The date mentioned in the papyrus is \"second indiction\". This refers to the second year of a repeated fifteen-year cycle (indiction) and the year 538 is relative, since it could be the second year of the previous or the next indiction (523 or 553). 538 is logically deduced by the editors in view of the whole dossier of papyri from Petra. Our models date this manuscript to 555 CE (521-575 CE), overcoming the geographical variation.\n\nDiscussion\nThe computational, quantitative method suggested in this work is intended to complement human expertise. Its main contribution lies in providing an additional dating criterion for ancient Greek documents, in addition to the ones usually employed by papyrologists (palaeography, onomastics, prosopography, toponymy, archaeological evidence, etc.). It can predict a date for those papyri that do not include one, narrow down the possible time-span of doubtful dating, or contribute to deciding on one particular date when several alternatives seem possible. Despite the fact that limitations exist (discussed in \u00a77.3), compared to traditional approaches the models trained in this study are expected to reduce biases. Their value is not limited to predicting dates for individual manuscripts, but they can be applied to any attribute of a group of papyri, e.g. the place of provenance or the text's type. At the same time, easily accessible open-source metadata exist for most published papyri ( \u00a73.1).\n\nRationale generation\nThe use of supervised learning, such as the work of Assael et al. (2022) or ours, can yield accurate estimations, which can at least help the human expert. The assistance is greater, however, when explanations are provided for the models' decisions. In our case, we used a committee of hundreds of regressors in order to estimate the date of three use cases. Therefore, we sampled models per case and generated rationales regarding their predictions, by using their Shapley values (Lundberg and Lee, 2017).\nIn the case of PSI 8 934 ( \u00a76.1), our investigation showed that the mention of the name 'Aurelios Victor' ('\u0391\u1f50\u03c1\u03ae\u03bb\u03b9\u03bf\u03c2 \u0392\u03af\u03ba\u03c4\u03c9\u03c1') influenced the decision, resulting to a more recent date than what would have been predicted otherwise. Similarly, in the case of P. Petra 1 5 ( \u00a76.3), the decision was influenced by a reference to 'indiction' ('\u1f30\u03bd\u03b4\u03b9\u03ba\u03c4\u03af\u03c9\u03bd\u03bf\u03c2'), a word that refers to a periodic reassessment of taxation in the Late Roman Empire.\n\nIn the wild\nComputational dating can facilitate a macroscopic analysis of vaguely dated or undated manuscripts. By generating estimated dates for hundreds of such manuscripts, the expert can view from distance the collection, potentially drawing useful conclu-sions or making significant remarks. To test this hypothesis, we collected 220 manuscripts dated with an upper CE date limit (i.e., not after that date). We formed a committee of regressors, 15 and we estimated the minimum, the maximum, and the average chronology of each manuscript. In 28% of them, the maximum prediction exceeded the upper threshold and was discarded to avoid doubting the expert. This process led to the date estimation for 159 manuscripts, which we release publicly in our repository to assist other researchers. As can be seen in Figure 5 , some of our estimations fall far away from the upper limit (in red) while others fall close. The estimated date from our regressors' committee should be read along with other information, which is kept in the shared corpus, such as the place settlement (Figure 6 shows frequent places). We observe, for example, that in some places the estimated dates fall closer to the upper limit (e.g. in Oxyrhynchos and Tebtynis the distance is 132 years) compared to others (e.g. in Antinoopolis and Hermopolis the distance is 283 and 384 years).\n\nChallenges and limitations\nOur experimental analysis proved that text regression is a considerably reliable and accurate tool in dating nonliterary papyri. Limitations and challenges stem mainly from the composition of our dataset, which is balanced as far as the dates of the papyri included are concerned, both at the level of the century (approx. 40 records per century) and at the level of the quarter of the century (albeit less strictly and with the exception of the 7th CE). Furthermore, although we retained a substantial text sample of each papyrus, in approximately 1/4 of the records some text was eliminated.\n\nBiases\nDespite our effort to balance the dataset in terms of dates, biases are present. Since our main concern in collecting the data was for the date distribution, no deliberate selection was made on the basis of the document types. Some types are thus over or underrepresented (e.g. private letters that do not usually bear a date; \u00a76.2). Each type of document has however distinctive linguistic characteristics, such as the level of formality or unusual constructions (e.g. accounts). This uneven typological representation probably affects the performance of the models. Other possible biases in the dataset concern the provenance of papyri, the length of their text, and the state of conservation (sizeable portions of missing text or entirely missing parts of the documents).\n\nChronological analysis of words\nChronological analysis of word occurrence is possible if we detect and collect terms only attested in the papyrological material during a limited period. The word 'denarius' only appears after the 2nd CE and before the 5th CE, its presence in a text thus means that the text must have been written during this timespan. Likewise a text containing the word 'indiction' cannot have been written before the 4th CE. The investigation should also regard the possibility that the models make a prediction for a papyrus based on typical dating formulas present in the text like the name of the ruling emperor. Although our investigation of explanations did not yield any major concerns, a bigger sample of test cases should be created and more explainability methods should be employed (Ribeiro et al., 2016) to make conclusive remarks on this front.\n\nTranscription of papyri is not optional\nTranscription of the papyri is required (at least partial, but substantial) to reach this high degree of accuracy with our method. Thus, while there are transcriptions available for most already published papyri, it is less practical for dating unpublished papyri that have not been yet transcribed to a relatively high standard. In that case, image classification on the scripts can provide a less accurate prediction of the date as starting point.\n\nConclusion\nWe presented a machine-actionable dataset of 389 Greek documentary papyri of (mostly) Egyptian provenance, dated and balanced in terms of chronological quarter-century distribution. We trained extremely randomised trees on top of character n-gram-based features, reaching a mean absolute error of 54 years and 60% in century-level classification accuracy. We then formed a committee of regressors, which we applied to three use cases: a land lease, a private letter, and a geographical outsider (not from Egypt). To assist future research, our committee dated 159 manuscripts, for which only the upper limit is known. Future endeavours for this research extend far beyond the dating of individual manuscripts. It can produce valuable data for the study of the Greek language and its evolution through a millennium, help identify and trace linguistic habits and trends, as well as the history of document production, circulation, and use (e.g. which period produces what kind of texts, which administration relied on what type of documents, etc.). It can also produce further data and resources towards the typology of ancient Greek documents, completing with computational methods the work already underway and well-advanced of the grammateus project. Last, it can in the future fruitfully be combined with computational paleography to analyse the script and content of a given text.\n"}
{"question": "Why do we resort to pseudo labeling?", "evidence": "   Our motivation is two-fold: first, adding instantiation after conceptualization to form a cycle can strongly benefit two conceptualization tasks simultaneously. On the one hand, instantiating conceptualized triple relies on the correctness of event conceptualization. On the other hand, properly conceptualized triples can benefit event conceptualization via instantiation by providing more context brought by (r, t). Second, to address the lack of annotations, we resort to pseudo labeling, a typical semi-supervised learning approach to automatically assign pseudo labels to the vast majority of unlabeled data using a teacher model. ", "options": ["A. To address the lack of annotations.", "B. To form a cycle.", "C. To provide more context", "D. To guarantee the correctness of event conceptualization."], "answer": "A", "content": "\nIntroduction\n\"Concepts are the glue that holds our mental world together.\"- Murphy (2004) Commonsense reasoning is a crucial ability for machines to make situational presumptions and draw inferences from the knowledge that reflects our humans' understanding of situations and common facts (Davis, 1990; Davis and Marcus, 2015) . It has gained increasing popularity in the Natural Language Processing (NLP) community with the emergence of CommonSense Knowledge Bases (CSKB) (Sap et al., 2019a; Speer et al., 2017;  Figure 1 : A demonstration of commonsense reasoning on an unknown situation, PersonX plays with his dog, with the aid of abstract commonsense knowledge. Decontextualized conceptualization, such as observe, may yield wrong abstract commonsense knowledge that cannot be instantiated within the corresponding context. Hwang et al., 2021) and large language models (Bosselut et al., 2019; Rajani et al., 2019; Liu et al., 2022b; Su et al., 2022; Yu et al., 2022b) . However, when encountering situations beyond the data given, more abstract background knowledge must be acquired and generalized to assist the reasoning (Tenenbaum et al., 2011) , and language models trained with an autoregressive language modeling objective do not explicitly leverage such abstract knowledge during inference.\nInstead, humans rely on conceptual induction and deduction (Murphy, 2004) to make inferences on novel situations without the need to memorize all special cases. As shown in Figure 1 , humans can derive conceptualizations based on the assertion that \"PersonX watches a football game, as a result, he feels relaxed\" to infer that \"relaxing events can make someone feel relaxed,\" where the acquired abstract commonsense knowledge can be further used as general knowledge to perform reasoning on similar or associated situations. A new commonsense knowledge \"PersonX plays with his dog, as a result, he feels happy and relaxed\" can be deduced by instantiating relaxing events to playing with his dog.\nAs the cornerstone of generalizable commonsense reasoning, such a process is extremely challenging for machines to replicate due to the absence of contextualized conceptualizations and abstract commonsense knowledge in CSKBs and a lack of relevant methodologies.\nYet, existing works address the process of induction and deduction separately via conceptualization and instantiation. Several methods performing conceptualization are proposed with a specific focus on entity-level (Durme et al., 2009; Song et al., 2011; Gong et al., 2016; He et al., 2020; Peng et al., 2022; Song et al., 2015) and event-level (Chen et al., 2020; He et al., 2022) semantics. Instantiation (Allaway et al., 2023) , as the process that simulates conceptual deduction, is tackled separately and not leveraged by these methods. Though abstract commonsense knowledge can be derived by using existing conceptualization methods to abstract a certain instance from factual commonsense knowledge, several limitations still exist.\nFirst, the plausibility of abstract commonsense knowledge banks on both the correctness of conceptualization and proper contextualization under specific assertions. The latter one, which is an essential step for the deduction of abstract knowledge, is missing from current methodologies. Take Figure 1 as an example, the concept observe will not necessarily lead to the result of \"feeling relaxed\", as observe omits the entertaining property of the original instance as a cost of abstraction. Second, instantiating abstract commonsense knowledge can yield much more and diverse concrete commonsense knowledge that can serve as an augmentation of the training dataset, while current methods undervalue such a process and only focus on conceptualization. Finally, the complex contextualization and conceptualization of commonsense knowledge can easily bring more than two orders of magnitude of data on top of the original dataset. This makes current labeled data scarce and infeasible for practitioners to annotate all of them, leaving a large amount of unlabeled data.\nTo fill in these research gaps, we propose CAT (Contextualized ConceptuAlization and InsTantiation), a semi-supervised learning framework that unites event conceptualization and instantiation in cascade to conceptualize CSKBs and acquire abstract commonsense knowledge to aid commonsense reasoning. Inspired by how humans learn with concepts (Carey, 2004) , we design a novel bootstrapping 1 method to enhance conceptualizations and abstract commonsense knowledge verification with the help of similar conceptualizations and instantiations as a reference. We demonstrate the effectiveness of CAT by using the acquired abstract commonsense knowledge to train COMET (Bosselut et al., 2019) , a commonsense inference language model that generates if-then commonsense knowledge, and showing that our derived abstract commonsense knowledge can significantly improve commonsense inference modeling.\nOur contributions are three-fold: (1) We introduce a semi-supervised learning framework, CAT, to conceptualize CSKBs with the assistance of progressively bootstrapping similar abstract concepts or instantiations in the conceptualization process.\n(2) We use CAT to acquire abstract commonsense knowledge at scale with high quality, which can be used for commonsense inference modeling. (3) We demonstrate the effectiveness of our framework by achieving state-of-the-art performance on two CSKB conceptualization tasks and remarkably improving commonsense inference modeling with our derived abstract commonsense knowledge.\n\nRelated Works\nConceptualization and Instantiation. Many existing works have studied conceptualization and instantiation separately. Durme et al. (2009) first attempted to derive more general knowledge by abstracting over large sets of factoids obtained from WordNet (Miller, 1995) synsets. Song et al. (2011 Song et al. ( , 2015) ) and Gong et al. (2016) proposed to turn instances in a sentence into concepts via weight matching from Probase (Wu et al., 2012) . Recently, Liu et al. (2022c) proposed a taxonomy-guided induction method to mine verb-oriented commonsense knowledge from verb phrases. Peng et al. (2022) constructed a conceptual knowledge benchmark to evaluate language models with three zeroshot probing tasks. While these works focus on the conceptualization of entities, He et al. (2022) constructed an event conceptualization benchmark based on ATOMIC (Sap et al., 2019a ) by combining syntactic parsing, semantically heuristic matching, and human annotation. Besides, the line of works focusing on ultra-fine entity typing (Choi et al., 2018; Dai et al., 2021; Li et al., 2022) similar objectives of typing named entities, nominal nouns, and pronouns into a set of free-form phrases. Instantiation was attempted by Allaway et al. (2023) , who proposed a controllable generative framework to probe valid instantiations for abstract knowledge automatically. Though Porada et al. (2021) and Peng et al. (2022) both proved that existing pretrained language models lack conceptual knowledge, none of existing works explicitly combine both techniques to derive abstract knowledge that is context-sensitive and generalizable.\nCommonsense Reasoning. Endowing NLP systems with the ability to perform commonsense reasoning is an elusive goal of artificial intelligence (Sap et al., 2020) . A diverse collection of commonsense reasoning tasks have been proposed as evaluation benchmarks (Talmor et al., 2019; Omura et al., 2020; Ponti et al., 2020; Fang et al., 2021a) . Among them, Bosselut et al. (2019) proposed a generative model, COMET, to learn to produce if-then commonsense knowledge as an effective approach toward modeling commonsense inference that can be applied in various commonsense reasoning tasks (Talmor et al., 2019) .\nSemi-Supervised Learning. Semi-supervised learning (SSL) aims at taking advantage of unlabeled data to equip models with stronger generalization ability (van Engelen and Hoos, 2020) . The most common approach is using pseudo labels (Iscen et al., 2019; Wang et al., 2022) to expose more unseen data to the student model. It has been applied in various machine learning tasks such as image classification (Liu et al., 2022a; Hu et al., 2021 ), text classification (Li et al., 2021; Meng et al., 2019; Xiao et al., 2019) , commonsense knowledge base population (Fang et al., 2022) , and named entity recognition (Liu et al., 2021; Chen et al., 2021) .\n\nProblem Definition\nDefinition. Conceptualizing an event-centric CSKB to derive abstract commonsense knowledge comprises two steps (He et al., 2022) : event conceptualization and triple conceptualization.\n\nDenote the triples in the original CSKB as\nD o = {(h o , r, t)|h o \u2208 H o , r \u2208 R, t \u2208 T }, where H o , R,\nand T are the set of heads, relations, and tails in the original CSKB. The first step only operates on head events without considering the context in r and t. The goal of event conceptualization is to produce conceptualized head event h a from the original head h o to represent an abstraction of h o . In the second step, the task is to verify whether the conceptualized head h a still makes sense in the context of r and t, as r and t will further restrict the level of abstractness in h a . As shown in Figure 1 , conceptualizing watch football game to observe is wrong within the context of having feel relaxed as a result. Plausible (h a , r, t) triples will be considered as valid abstract commonsense knowledge. Specifically, in the first step, there are two ways of conceptualizing head events alone: a retrievalbased discriminative way and a generative way. The retrieval-based discriminative paradigm identifies and links a component i in h o to a concept c in a concept taxonomy C to form a conceptualization h a by replacing i with c. The model needs to verify whether h a is a valid conceptualization of h o . The generative paradigm aims to generate a h a directly given h o and the designated component i in h o .\nFormally, denote the annotated dataset in the first step, event conceptualization, as\nD l h = {(h o , h a , y)|h o \u2208 H o , h a \u2208 H a , y \u2208 {0, 1}},\nwhere h o is an original head event without conceptualization, h a is a corresponding conceptualization of h o , and y is the human-annotated label indicating whether such a conceptualization is plausible or not. The labeled dataset in the second step, triple conceptualization, is denoted as\nD l t = {(h, r, t, y)|h \u2208 H a , r \u2208 R, t \u2208 T, y \u2208 {0, 1}},\nwhere h is a conceptualized head event from the first step, r and t are a relation and a tail from the original CSKB accompanied with the corresponding original head h o , and y is the human-annotated label indicating whether such abstract commonsense knowledge, in the form of a conceptualized triple, is plausible or not. Besides labeled datasets, unlabeled datasets are defined similarly as D u h and D u t only with the difference that labels y are missing. Thus, the task objective for discriminative event conceptualization is to determine whether a h o can be properly abstracted using h a , where h a is derived by replacing a component i \u2282 h o with its linked concept c from a concept taxonomy C. The task objective for generative event conceptualization is to generate h a directly from h o with text generation models. For the triple conceptualization task, the objective is to distinguish whether a conceptualized triple (h a , r, t), representing abstract commonsense knowledge, is plausible or not.\nDataset. To study conceptualization over CSKBs, we use the AbstractATOMIC dataset provided by He et al. (2022) as the benchmark. In Ab-stractATOMIC, ATOMIC is used as the original CSKB. And the event conceptualization adopts a discriminative way, where a syntactic parsing schema is defined to identify the components i in h o to be heuristically linked to concept taxonomies Probase (Wu et al., 2012) and WordNet (Miller, 1995) to form conceptualized h a . Such a heuristic can produce over 32 times more candidate conceptualized head events and over 10 times more conceptualized triples compared with the original ATOMIC, as the number of retrieved concepts from the concept taxonomy C can be manually controlled to acquire a large number of conceptualizations. Triple conceptualization is defined as predicting the plausibility of the triples whose head is conceptualized. Only 131K (26%) conceptualizations of 7K (45%) ATOMIC head events and 81K (1.3%) conceptualized triples are manually anno-tated as D l h and D l t , while others remain unlabeled D u h and D u t . The trn/dev/tst partition follows the same split as in the original ATOMIC. Statistics and more detailed explanations of AbstractATOMIC are shown in Table 1 and Appendix A.\n\nCAT Framework\nThis section introduces our proposed Contextualized ConceptualizAtion and InsTantiation (CAT) framework for conceptualizing commonsense knowledge bases and acquiring abstract commonsense knowledge. An overview is presented in Figure 2 . Our motivation is two-fold: first, adding instantiation after conceptualization to form a cycle can strongly benefit two conceptualization tasks simultaneously. On the one hand, instantiating conceptualized triple relies on the correctness of event conceptualization. On the other hand, properly conceptualized triples can benefit event conceptualization via instantiation by providing more context brought by (r, t). Second, to address the lack of annotations, we resort to pseudo labeling, a typical semi-supervised learning approach to automatically assign pseudo labels to the vast majority of unlabeled data using a teacher model.\nFollowing He et al. (2022) , we study the retrieval-based discriminative paradigm of event conceptualization and leave the generative paradigm as an intrinsic evaluation. In CAT, we unify event conceptualization and triple conceptualization into one cycle and make them mutually benefit each other through instantiation and conceptualization. Our framework can be summarized into four steps:\n(1) Train teacher models for both event conceptualization and triple conceptualization on the labeled dataset D l h and D l t , respectively. Use the two teachers to assign pseudo labels to unlabeled datasets.\n(2) Conduct alternative conceptualization or instantiation on labeled and pseudo-labeled data.\n(3) Bootstrap (aggregate) the alternative concepts and instances in the second step using natural language prompt templates and train student models on both labeled and pseudo-labeled data. (4) Use the student models to refine the pseudo labels and then re-train the student models.\n\nTeacher Model Training\nTwo teacher models on both event and triple conceptualization tasks are trained separately on the labeled dataset D l h and D l t . As both tasks are inherently text/triple classification, we adopt KG-BERT (Yao et al., 2019) as the skeleton of our models. The event conceptualization model determines whether h a is a valid conceptualization of h o , and the triple conceptualization model determines whether a conceptualized triple (h a , r, t) is plausible or not. The two models \u03b8 are trained on annotated examples x i with a cross-entropy loss (Eq. 1) and used to provide pseudo labels to instances from the unlabeled datasets D u h and D u t . Two thresholds, T + and T \u2212 , are set to determine the pseudo labels of unlabeled examples with high confidence. Examples with a pseudo-labeled score higher than T + will be labeled y i = 1, and those lower than T \u2212 will be labeled y i = 0. The rest will be discarded.\nL(x i , \u03b8) = \u2212 |x| i=1 y i log(\u03b8(x i ))\n(1)\n\nAlternative Conceptualization and Instantiation\nAccording to Murphy (2004) , when humans learn a new concept, we pre-extract similar known concepts in our minds and infer possibly equivalent unknown concepts on the fly. Inspired by this theory, we retrieve additional abstract concepts or instantiated events to help discriminate conceptualizations and abstract commonsense knowledge. For event conceptualization, we retrieve some alternative possible conceptualizations of h o to accompany the learning of h a . Additional conceptualizations of h o from both labeled and pseudo-labeled examples are predicted again by the teacher model and ranked according to their plausibility score prediction. And top m conceptualizations are retrieved with m being a hyperparameter to control the number of retrievals. For triple conceptualization, we perform instantiation in cascade to instantiate c to some concrete instances to assist the learning process.\nPossible instantiations of c are extracted from annotated and pseudo-labeled event conceptualizations by searching for conceptualized events h \u2032 a \u2208 H a other than h a with c as the concept and extracting their corresponding instances i \u2282 h \u2032 a . Similarly, the instances are then scored by the teacher model, and the top n of them are retrieved. Intuitively, alternative event conceptualizations can serve as hints for discriminating the correctness of the target conceptualization, and instantiations can carry additional contextualized information to help verify the plausibility of a conceptualized triple, which meets the objective of deriving abstract commonsense knowledge that is context-sensitive.\n\nPrompt Aggregation\nWe then bootstrap the retrieved alternative conceptualizations/instantiations via natural language prompts. Here bootstrap (Carey, 2004 ) can be understood as binding the alternative retrievals and the target concept/triple together to strengthen the discrimination of the target concept/triple. As shown in Figure 2 step (3), the initially given input and retrieved concepts/instances are concatenated via human-defined prompts for both conceptualization tasks. Alternative concepts/instances are sorted in the order of their plausibility score ranking. Two student models S h and S t for both tasks are trained using the modified text with such prompts as inputs. They are expected to learn the bootstrapping connectionism between the target and the additional retrievals we provided. More detail about the prompt design is in Appendix B.\n\nPseudo-Label Refinement\nAll pseudo labels, initially derived by a teacher model trained on the original labeled dataset, are relabeled according to the plausibility score predicted by our newly enhanced student models S h and S t . Similar to the teacher model, two thresholds, T + and T \u2212 , are applied to distinguish positive and negative examples for both tasks. In addition, negative Table 2 : Performance (%) by our CAT framework on the discriminative event conceptualization and triple conceptualization tasks. We report the average AUC score and standard deviation across experiments with three random seeds. The best performances within each framework are underlined, and the best among all models are bold-faced. labels are assigned to triples whose conceptualized head events are predicted as wrong conceptualizations by S h , as wrong conceptualizations will not yield plausible abstract commonsense knowledge.\n\nApplication and Evaluation of CAT\nThe resulting models of CAT include an event conceptualization model and a triple conceptualization model, both fine-tuned on the refined pseudo labels and the labeled data. These two models can be used to conceptualize ATOMIC to a larger commonsense knowledge base on a more abstract level. We further conduct intrinsic evaluations on the acquired event conceptualization model under a generative event conceptualization paradigm and extrinsic evaluations on the resulting conceptualized CSKB with commonsense inference modeling task (COMET; Bosselut et al. (2019) ) in Section 5. Here we select COMET as the representative because it is a general commonsense model that can be applied to various downstream commonsense reason-ing tasks such as SocialIQA (Sap et al., 2019b) , self-talk (Shwartz et al., 2020) , and CSKB completion (Malaviya et al., 2020) . Meanwhile, generative event conceptualization enables performing automatic conceptualization scalably. Both are important applications and evaluations of CAT.\n\nExperiments\nWe conduct conceptualization experiments using CAT in Section 5.1 and generative experiments as evaluations in Section 5.2. These experiments demonstrate that CAT has a strong capability in conceptualizing CSKBs, and better conceptualization modeling can help populate more novel and diverse commonsense knowledge and thus help commonsense modeling (COMET).\n\nCSKB Conceptualization\nBaselines. We collectively introduce the baselines for both event and triple conceptualization tasks, as they are inherently classification tasks. Table 3 : Performance (%) of GPT2 (XL) on the generative event conceptualization task. D l h stands for annotated labeled data, and D u stands for the data acquired by CAT. The underfoot value indicates the threshold for selecting plausible pseudo labels. The best performances are bold-faced, and the second-best ones are underlined.\nAUC is used as the evaluation metric. Under a supervised learning setting, we apply KG-BERT (Yao et al., 2019) model with BERT (Devlin et al., 2019) , BART (Lewis et al., 2020) , RoBERTa (Liu et al., 2019 ), DeBERTa (He et al., 2021 , 2023 ), and ELECTRA (Clark et al., 2020) as the backbone language models. We also attempt to leverage supervised generative language models as baselines. GPT2 (Radford et al., 2019) models are trained with a text generation objective only on positive examples, and we use perplexity as the prediction scores to calculate AUC. For the semi-supervised learning baselines, we leverage UDA (Xie et al., 2020a) , NoisyStudent (Xie et al., 2020b), and Pseu-doReasoner (Fang et al., 2022) with RoBERTalarge being the backbone model. Additional explanations can be found in Appendix C.1.1.\nDiscriminative Results. The results for both tasks are presented in Table 2 . Under a supervised learning setting, KG-BERT family mostly performs better on both tasks than GPT2 due to the fact that GPT2 is only fine-tuned on positive examples and thus cannot learn from negative examples that contain wrong conceptualizations and implausible abstract commonsense knowledge. As for the semi-supervised learning setting, previous SSL baselines are rather limited in improving the performance against supervised learning. The best Pseu-doReasoner only improves by 0.5% and 0.3% on the test set for both tasks compared with supervised RoBERTa-large models. Instead, models trained with CAT can outperform all other training methodologies. Comparing the test set performance with PseudoReasoner, small backbone models (BERTbase) can improve by 3.4% and 2.2%, and large models (RoBERTa-large) can be improved by 2.1% and 2.2%. This shows pipelining two-step concep-tualizations as a loop and leveraging our proposed bootstrapping-based method can yield a larger performance gain compared with simply applying a semi-supervised learning strategy. Due to limited space, ablation studies on framework components and the semi-supervised learning paradigm of CAT are conducted in Appendix C.1.4. For example, the results indicate that bootstrapping alternative conceptualization and instantiation plays the most important role in assisting learning conceptualization among all components of CAT. Additional results and a computational cost study can be found in Appendix C.1.3 and Appendix D.\n\nApplication and Evaluation of CAT\nAs CAT is a framework for acquiring conceptualized commonsense knowledge, including both conceptualized head events (from h o to h a ) and abstract commonsense triples (h a , r, t), we assess these pseudo-labeled outcomes via two generative tasks with various threshold tuning as evaluations.\nGenerative Event Conceptualization. To intrinsically evaluate the effectiveness of CAT's event conceptualization, we use the acquired conceptualized head events as training data to learn a generative event conceptualizer. Specifically, the models are trained with instance-conceptualizations pairs in the format of \"<instance> is an instance of <concept>\". At the evaluation phase, the model is prompted with \"<instance> is an instance of [GEN]\" where <instance> is the instance to be conceptualized and [GEN] is the generation token. We then retrieve the top-1 generation and compare it against the target set from the evaluation dataset to compute four NLG metrics, as listed in Appendix C.2.1. These scores can be regarded as an approximation of the top-1 generations' recall. 40.0 40.3 27.1 27.8 20.0 20.8 16.5 17.5 16.1 16.3 35.3 35.7 31.6 31 Additionally, we uniformly sample 500 generations from each evaluation split and conduct expert annotations on the plausibility of each conceptualization to ensure that out-of-domain concepts can be properly evaluated. The experts are asked to determine whether each top-1 generation is indeed a plausible conceptualization or not, such that the top-1 generations' precision is reflected. Thus, current evaluation measures jointly evaluate the top-1 generations' precision and recall, which makes it robust and non-easy to be impacted by repetition problems (Li et al., 2020) . Zero-shot GPT2 and GPT2 fine-tuned on the originally labeled event conceptualizations in D l h are used as baselines. We also study the effect of the threshold T + that selects plausible conceptualized heads, where higher thresholds indicate higher plausibility regarded by CAT. The results are presented in Table 3 . With a relatively high threshold, generators trained on a mixture of pseudo-labeled data by CAT and annotated concepts significantly outperform the baselines in every automated metric. A plausible rate of 93.3% is maximally achieved on the test set, which is 11.8% higher than the baseline. Gradually reducing the threshold also decreases the performance, indicating abstract heads with lower plausibility scores can be of poorer quality. Such results indicate that CAT can produce high-quality event conceptualizations for generative models to learn better conceptualizers without the need to annotate a large number of data.\n\nCommonsense Inference Modeling (COMET).\nThe second component of CAT produces triple-level abstract commonsense knowledge. We evaluate these abstract commonsense triples with a commonsense inference task that generates commonsense tails given heads and relations as inputs, as in COMET (Bosselut et al., 2019) . Following He et al. ( 2022), we apply the same training and evaluation process to the models. The base training data we use are a subset of ATOMIC triples corresponding to those annotated abstract triples in D l t , which contains 17K (3.7%) among the original ATOMIC. We derive abstract commonsense knowledge using CAT from a subset of D u t where the heads correspond to those in the ATOMIC subset to ensure no data leakage, denoted as D u CAT . GPT2 is fine-tuned on the ATOMIC subset, the annotated abstract triples D l t , the abstract knowledge verified by CAT, or their combinations. The commonsense generation results are presented in Table 4 . Similar to COMET (Bosselut et al., 2019) , all models are evaluated on the original ATOMIC's full validation and testing sets. The best result is achieved using a mixture of the ATOMIC subset and abstract triples pseudo-labeled by our framework, with 0.95 as the threshold for selecting plausible triples. This indicates high-quality abstract commonsense triples can indeed provide a more general view of the original commonsense knowledge, thus helping commonsense inference. Additionally, training with our pseudo-labeled examples outperforms training with those annotated triples in AbstractATOMIC, which also validates the effectiveness of our model that leverages a large amount of unlabeled data. To further investigate how conceptual knowledge 5HWULHYDO1XPEHU (YHQW&RQFHSWXDOL]DWLRQ$8& improves commonsense inference modeling, we conduct more empirical analysis in Section 5.4. Additional experiment results with other thresholds and case studies can be found in Appendix C.2.3 and Appendix E, respectively.\n\nNumber of Retrieved Alternative\nConceptualizations and Instantiations.\nWe then study the ablation of bootstrapping different numbers of alternative conceptualizations/instantiations (denoted as #retrieval) in our CAT framework. For simplicity, when tuning the #retrieval for one task, the #retrieval of the other task is fixed at the best value we acquired. We plot the test AUC score with #retrieval from 0 to 11 using BERT-base as the backbone model in Figure 3 . #retrieval=0 refers to training with a simple student-teacher framework without bootstrapping alternative conceptualizations and instantiations. For event conceptualization, the performance generally positively correlates with the number of retrievals, while it starts dropping after 9. A reversed trend is observed for triple conceptualization, where using only two instances achieves the best performance. One possible reason is that in triple conceptualization, the retrieved instances are events and much longer than the retrieved concepts in event conceptualization, and aggregating various alternative events for a triple will cause language models to be less sensitive to the semantics of the original triple (Holtzman et al., 2020) .\n\nThe Effect of Abstract Knowledge\nWe finally study the effect of abstract commonsense knowledge acquired by CAT by studying the semantic overlaps between training and testing data. We sort the test set by the BERTScore (Zhang \n\nConclusion\nIn conclusion, this paper proposes CAT, a semisupervised learning framework for commonsense reasoning, by leveraging the power of abstract commonsense knowledge. By achieving state-of-theart performances in CSKB conceptualization tasks, we remarkably improve modeling commonsense inference, as an important cornerstone of many commonsense reasoning tasks. Our analysis also demonstrates that high-quality abstract commonsense knowledge can benefit commonsense inference modeling by providing more generalizability on hard commonsense knowledge. We hope this work can draw insights toward commonsense reasoning from a conceptualization perspective.\n"}
{"question": "What is the main finding regarding LLMs (Large Language Models) in the paper's evaluation?", "evidence": "  we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation... the implications of our findings can become even more critical with the progress of LLMs trained with human preference feedback... ", "options": ["A. LLMs adjusted by human feedback consistently outperform traditional metrics", "B. LLMs adjusted by human feedback perform better under reference-based evaluation", "C. LLMs adjusted by human feedback are less affected by annotators' preferences", "D. LLMs adjusted by human feedback may overfit unconstrained human evaluation "], "answer": "D", "content": "\nIntroduction\nHuman evaluation plays an essential role in both assessing the rapid development of summarization systems in recent years (Lewis et al., 2020a; Zhang et al., 2020a; Brown et al., 2020; Sanh et al., 2022; He et al., 2022) and in assessing the ability of automatic metrics to evaluate such systems as a proxy * Equal contribution for manual evaluation (Bhandari et al., 2020; Fabbri et al., 2022a; Gao and Wan, 2022) . However, while human evaluation is regarded as the gold standard for evaluating both summarization systems and automatic metrics, as suggested by Clark et al. (2021) an evaluation study does not become \"gold\" automatically without proper practices. For example, achieving a high inter-annotator agreement among annotators can be difficult (Goyal et al., 2022) , and there can be a near-zero correlation between the annotations of crowd-workers and expert annotators (Fabbri et al., 2022a) . Also, a human evaluation study without a large enough sample size can fail to find statistically significant results due to insufficient statistical power (Card et al., 2020) .\nTherefore, we believe it is important to ensure that human evaluation can indeed serve as a solid foundation for evaluating summarization systems and automatic metrics. For this, we propose using a robust human evaluation protocol for evaluating the salience of summaries that is more objective by dissecting the summaries into finegrained content units and defining the annotation task based on those units. Specifically, we introduce the Atomic Content Unit (ACU) protocol for summary salience evaluation ( \u00a73), which is modified from the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. We demonstrate that with the ACU protocol, a high inter-annotator agreement can be established among crowd-workers, which leads to more stable system evaluation results and better reproducibility.\nWe then collect, through both in-house annotation and crowdsourcing, RoSE, a large human evaluation benchmark of human-annotated summaries with the ACU evaluation protocol on recent state-of-the-art summarization systems, which yields higher statistical power ( \u00a74). To support evaluation across datasets and domains, our benchmark consists of test sets over three summarization datasets, CNN/DailyMail (CNNDM) (Nalla-\n\nStatistical Power\n\u2212 High statistical power is difficult to reach for human evaluation of similar-performing systems. \u00a74.1 \u2212 Increasing the sample size of human evaluation effectively raises statistical power.\n\nSummary Length\n\u2212 Summaries from different summarization systems show a large difference in average length. \u00a74.2 \u2212 Difference in summary length is not well-reflected by automatic evaluation metrics.\n\u2212 Reference-free and reference-based human evaluation results have a near-zero correlation. Evaluation \u2212 Reference-free human evaluation strongly correlates with input-agnostic, annotator preference. Protocol Comparison \u2212 Annotator's input-agnostic preference has a strong positive correlation with summary lengths.\n\u00a75.2 \u2212 Annotator's input-agnostic preference does not favor reference summaries. \u2212 Compared to smaller, fine-tuned models, zero-shot large language models (e.g. GPT-3) perform better under reference-free evaluation, but worse under reference-based evaluation.\nEvaluating \u2212 A higher-powered human evaluation dataset can lead to a more robust automatic metric evaluation, as shown by a tighter confidence interval and higher statistical power of metric evaluation. Automatic Metrics \u2212 Automatic metric performance differs greatly under different human evaluation protocols.\n\u00a76.1 & \u00a76.2 \u2212 Automatic metrics show relatively strong system-level correlation and moderate summary-level correlation with our robust human evaluation protocol.\nTable 1 : Summary of the key findings in our work. pati et al., 2016) , XSum (Narayan et al., 2018), and SamSum (Gliwa et al., 2019) , and annotations on the validation set of CNNDM to facilitate automatic metric training. To gain further insights into the characteristics of different evaluation protocols, we conduct human evaluation with three other protocols ( \u00a75). Specifically, we analyze protocol differences in the context of both fine-tuned models and large language models (LLMs) in a zero-shot setting such as GPT-3 (Brown et al., 2020) . We find that different protocols can lead to drastically different results, which can be affected by annotators' prior preferences, highlighting the importance of aligning the protocol with the summary quality intended to be evaluated. We note that our benchmark enables a more trustworthy evaluation of automatic metrics ( \u00a76), as shown by statistical characteristics such as tighter confidence intervals and more statistically significant comparisons ( \u00a76.2).\nOur evaluation includes recent methods based on LLMs (Fu et al., 2023; Liu et al., 2023) , and we found that they cannot outperform traditional metrics despite their successes on related benchmarks such as SummEval (Fabbri et al., 2022a) . We summarize our key findings in Tab. 1. Our contributions are the following: (1) We propose the ACU protocol for high-agreement human evaluation of summary salience. (2) We curate the RoSE benchmark, consisting of 22000 summary-level annotations and requiring over 150 hours of in-house annotation, across three summarization datasets, which can lay a solid foundation for training and evaluating automatic metrics. 1 (3) We compare four human evaluation protocols for summarization and show how they can lead to drastically different model preferences. (4) We evaluate automatic metrics across different human evaluation protocols and call for human evaluation to be conducted with a clear evaluation target aligned with the evaluated systems or metrics, such that task-specific qualities can be evaluated without the impact of general, input-agnostic preferences of annotators. We note that the implications of our findings can become even more critical with the progress of LLMs trained with human preference feedback (Ouyang et al., 2022) and call for a more rigorous human evaluation of LLM performance.\n\nRelated Work\nHuman Evaluation Benchmarks Human annotations are essential to the analysis of summarization research progress. Thus, recent efforts have focused on aggregating model outputs and annotating them according to specific quality dimensions (Huang et al., 2020; Bhandari et al., 2020; Stiennon et al., 2020; Zhang and Bansal, 2021; Fabbri et al., 2022a; Gao and Wan, 2022) . The most relevant work to ours is Bhandari et al. (2020) , which annotates summaries according to semantic content units, motivated by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. However, this benchmark only covers a single dataset (CNNDM) without a focus on similarly-performing state-of-the-art systems, which may skew metric analysis (Tang et al., 2022a) and not fully reflect realistic scenarios (Deutsch et al., 2022) . In contrast, our benchmark consists only of outputs from recently-introduced models over three datasets.\nSummarization Meta-Evaluation With a human evaluation dataset, there exist many directions of meta-evaluation, or re-evaluation of the current state of evaluation, such as metric performance analyses, understanding model strengths, and human evaluation protocol comparisons.\nWithin metric meta-analysis, several studies have focused on the analysis of ROUGE (Lin, 2004b) , and its variations (Rankel et al., 2013; Graham, 2015) , across domains such as news (Lin, 2004a) , meeting summarization (Liu and Liu, 2008) , and scientific articles (Cohan and Goharian, 2016) . Other studies analyze a broader set of metrics (Peyrard, 2019; Bhandari et al., 2020; Deutsch and Roth, 2020; Fabbri et al., 2022a; Gabriel et al., 2021; Kasai et al., 2022b) , including those specific to factual consistency evaluation (Kryscinski et al., 2020; Durmus et al., 2020; Wang et al., 2020; Maynez et al., 2020; Laban et al., 20d; Fabbri et al., 2022b; Honovich et al., 2022; Tam et al., 2022) .\nRegarding re-evaluating model performance, a recent line of work has focused on evaluating zeroshot large language models (Goyal et al., 2022; Liang et al., 2022; Tam et al., 2022) , noting their high performance compared to smaller models.\nAs for the further understanding of human evaluation, prior work has compared approaches to human evaluation (Hardy et al., 2019) , studied annotation protocols for quality dimensions such as linguistic quality (Steen and Markert, 2021) and factual consistency (Tang et al., 2022b) , and noted the effects of human annotation inconsistencies on system rankings (Owczarzak et al., 2012) . The unreliability and cost of human evaluation in certain settings have been emphasized (Chaganty et al., 2018; Clark et al., 2021) , with some work noting that thousands of costly data points may need to be collected in order to draw statistically significant conclusions (Wei and Jia, 2021). Our meta-analysis focuses on this latter aspect, and we further analyze potential confounding factors in evaluation such as length and protocol design, with respect to both small and large zero-shot language models.\n\nAtomic Content Units for Summarization Evaluation\nWe now describe our Atomic Content Unit (ACU) annotation protocol for reference-based summary salience evaluation, including the procedure of writing ACUs based on reference summaries and matching the written ACUs with system outputs.\n\nPreliminaries\nIn this work, we focus on a specific summarization meta-evaluation study on summary salience. Salience is a desired summary quality that requires the summary to include all and only important information of the input article. The human evaluation of summary salience can be conducted in either reference-free or reference-based manners. The former asks the annotators to assess the summary directly based on the input article (Fabbri et al., 2022a) , while the latter requires the annotators to assess the information overlap between the system output and reference summary (Bhandari et al., 2020) , under the assumption that the reference summary is the gold standard of summary salience. 2 Given that reference-based protocols are more constrained, we focus on reference-based evaluation for our human judgment dataset collection, and we conduct a comparison of protocols in \u00a75.\n\nACU Annotation Protocol\nInspired by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols and subsequent annotation collection efforts (Bhandari et al., 2020; Zhang and Bansal, 2021) , the ACU protocol is designed to reduce the subjectivity of reference-based human evaluation by simplifying the basic annotation unit -the annotators only need to decide on the presence of a single fact, extracted from one text sequence, in another text sequence, to which a binary label can be assigned with more objectivity. Specifically, the evaluation process is decomposed into two steps: (1) ACU Writing -extracting facts from one text sequence, and (2) ACU Matching -checking for the presence of the extracted facts in another sequence. We formulate the ACU protocol as a recall-based protocol, such that the first step only needs to be performed once for the reference summary, allowing for reproducibility and reuse of these units when performing matching on new system outputs. ACU Writing While the LitePyramid approach defines its basic content unit as a sentence containing a brief fact, we follow Bhandari et al. (2020) to emphasize a shorter, more fine-grained information unit. Specifically, we define the ACU protocol with the concept of atomic facts -elementary information units in the reference summaries, which no\nThe clash occurred inside the box.\nOscar is Brazilian.\nOscar was taken off at half time.\nDidier Drogba replaced Oscar. longer need to be further split for the purpose of reducing ambiguity in human evaluation. 3 Then, ACUs are constructed based on one atomic fact and other minimal, necessary information. Fig. 1 shows an example of the written ACUs. To ensure annotation quality, we (the authors) write all the ACUs used in this work. We define guidelines to standardize the annotation process; for each summary sentence the annotator creates an ACU constituting the main information from the subject of the main clause (e.g., root), followed by additional ACUs for other facts while including the minimal necessary information from the root. We provide rules for dealing with quotations, extraneous adjectives, noisy summaries, and additional cases. We note that there can still be inherent subjectivity in the written ACUs among different annotators even with the provided guidelines. However, such subjectivity should be unbiased in summary comparison because all the candidate summaries are evaluated by the same set of written ACUs. ACU Matching Given ACUs written for a set of reference summaries, our protocol evaluates summarization system performance by checking the presence of the ACUs in the system-generated summaries as illustrated in Fig. 1 . For this step, we recruit annotators on Amazon Mechanical Turk 4 (MTurk). The annotators must pass a qualification test, and additional requirements are specified in Appendix A. Besides displaying the ACUs and the system outputs, we also provide the reference summaries to be used as context for the ACUs. Scoring Summaries with ACU ACU matching annotations can be aggregated into summary scores. We first define an un-normalized ACU score f of a candidate summary s given a set of ACUs A as: where A s is a subset of A that is matched with s.\n\nOscar collided with\nEQUATION\nWe note that f by default is a recall based score with respect to the reference summary r. Therefore, we also define a normalized ACU score f as:\nf\u03b1(s, A, r) = e min (0,\nEQUATION\nwhere |s|, |r| are the length (i.e., number of words) of the candidate summary s and the reference summary r respectively, and \u03b1 is a positive number controlling the strength of the normalization. This normalization is in effect a redundancy penalty, which penalizes the summaries longer than the reference and resembles the brevity penalty in BLEU (Papineni et al., 2002) . In practice, we set the value of \u03b1 by de-correlating f with the summary length using the collected ACU annotations.\n\nACU Annotation Collection\nWe collect ACU annotations on three summarization datasets: CNNDM (Nallapati et al., 2016 ), XSum (Narayan et al., 2018 ), and SamSum (Gliwa et al., 2019) . To reflect the latest progress in text summarization, we collect and annotate the generated summaries of pre-trained summarization systems proposed in recent years. 5 Detailed informa-tion about the summarization systems we used can be found in Appendix A.2. Table 2 shows the statistics of the collected annotations. The annotations are collected from the test set of the above datasets, and additionally from the validation set of CNNDM to facilitate the training of automatic evaluation metrics. In total, we collect around 21.8k ACU-level annotations and around 22k summary-level annotations, aggregated over around 50k individual summary-level judgments.\nTo calculate inter-annotator agreement, we use Krippendorff's alpha (Krippendorff, 2011) . The aggregated summary-level agreement score of ACU matching is 0.7571, and the ACU-level agreement score is 0.7528. These agreement scores are higher than prior collections, such as RealSumm (Bhandari et al., 2020) and SummEval (Fabbri et al., 2022a) , which have an average agreement score of crowd-workers 0.66 and 0.49, respectively.\n\nRoSE Benchmark Analysis\nWe first analyze the robustness of our collected annotations and a case study on the system outputs.\n\nPower Analysis\nWe analyze the statistical power of our collected human annotations to study whether it can yield stable and trustworthy results (Card et al., 2020) . Statistical power is the probability that the null hypothesis of a statistical significance test is rejected given there is a real effect. For example, for a human evaluation study that compares the performance of two genuinely different systems, a statistical power of 0.80 means there is an 80% chance that a significant difference will be observed. Further details can be found in Appendix B.1.\nWe conduct the power analysis for pair-wise system comparisons with ACU scores (Eq. 1) focusing on two factors, the number of test examples and the observed system difference. Specifically, we run the power analysis with varying sample sizes, and group the system pairs into buckets according to their performance difference, as determined by ROUGE1 recall scores (Fig. 2 ). 6 We observe the following: (1) A high statistical power 7 is difficult to reach when the system performance is similar. 6 We note that these scores are proxies of the true system differences, and the power analysis is based on the assumption that the systems have significantly different performance. 7 An experiment is usually considered sufficiently powered if the statistical power is over 0.80. Notably, while the sample size of the human evaluation performed in recent work is typically around 50-100, 8 such sample size can only reach a power of 0.80 when the ROUGE1 recall score difference is above 5. (2) Increasing the sample size can effectively raise the statistical power. For example, when the system performance difference is within the range of 1-2 points, the power of a 500-sample set is around 0.50 while a 100-sample set only has a power of around 0.20. The results of power analysis on three datasets with both ROUGE and ACU score differences are provided in Appendix B.2 with the same patterns, which indicates that our dataset can provide more stable summarization system evaluation thanks to its higher statistical power.\n\nSummarization System Analysis\nAs a case study, in Tab. summaries. Meanwhile, the systems that generate longer summaries may be favored by users who prefer more informative summaries. Therefore, we join the previous work (Sun et al., 2019; Song et al., 2021; Gehrmann et al., 2022; Goyal et al., 2022) in advocating treating summary lengths as a separate aspect of summary quality in evaluation, as in earlier work in summarization research. 9\n\nEvaluating Annotation Protocols\nApart from ACU annotations, we collect human annotations with three different protocols to better understand their characteristics. Specifically, two reference-free protocols are investigated: Prior protocol evaluates the annotators' preferences of summaries without the input document, while Ref-free protocol evaluates if summaries cover the salient information of the input document. We also consider one reference-based protocol, Ref-based, which evaluates the content similarity between the generated and reference summaries. Appendix D.1 provides detailed instructions for each protocol.\n\nAnnotation Collection\nWe collected three annotations per summary on a 100-example subset of the above CNNDM test set using the same pool of workers from our ACU qualification. Except for ACU, all of the summaries from different systems are evaluated within a single task with a score from 1 (worst) to 5 (best), similar (2) annotations for summaries from GPT-3 (Brown et al., 2020), 10 T0 (Sanh et al., 2022), BRIO, and BART to better understand annotation protocols with respect to recently introduced large language models applied to zero-shot summarization.\n\nResults Analysis\nWe investigate both the summary-level and systemlevel correlations of evaluation results of different protocols to study their inherent similarity. Details of correlation calculation are in Appendix C.\nResults on Fine-tuned Models We show the system-level protocol correlation when evaluating the fine-tuned models in Tab. 4, and the summarylevel correlation can be found in Appendix D.2. We use the normalized ACU score (Eq. 2) because the other evaluation protocols are supposed to resemble an F1 score, while the ACU score is by definition recall-based. We have the following observations:\n(1) The Ref-free protocol has a strong correlation with the Prior protocol, suggesting that the latter may have a large impact on the annotator's document-based judgments.\n(2) Both the Prior and Ref-free protocols have a strong correlation with summary length, showing that annotators may favor longer summaries.\n(3) The Ref-free protocol and the Ref-based protocol have a negative correlation while ideally they are supposed to measure similar quality aspects.\nWe perform power analysis on the results following the procedure in \u00a74.1 and found that ACU protocol can yield higher statistical power than the Ref-based protocol, suggesting that the ACU protocol leads to more robust evaluation results. We also found that the reference-free Prior and Ref-free Table 6 : The Kendall's correlation between the automatic metric scores and ACU scores of system outputs on CNNDM, XSum, and SamSum datasets. The correlation is calculated at both the system level and the summary level. We use the recall score of the automatic metrics when available to align with the ACU scores.\nautomatic metric variants. We focus the metric evaluation on ACU annotations because of two insights from \u00a75: (1) Reference-based metrics should be evaluated with reference-based human evaluation.\n(2) ACU protocol provides higher statistical power than the summary-level Ref-based protocol.\n\nMetric Evaluation with ACU Annotations\nWe use the correlations between automatic metric scores and ACU annotation scores of system outputs to analyze and compare automatic metric performance. The following metrics are evaluated:\n(1) lexical overlap based metrics, ROUGE (Lin, 2004b), METEOR (Lavie and Agarwal, 2007) 5) evaluation methods based on large language models, GPTScore (Fu et al., 2023) and G-Eval (Liu et al., 2023) , with two variants that are based on GPT-3.5 11 (G-Eval-3.5) and GPT-4 12 (OpenAI, 2023) (G-Eval-4) respectively. We note that for LLM-based evaluation we require the metric to calculate the recall score. For G- -3.5 -.091 -.273 -.091 .818 1.00 1.00 G-Eval-3.5-S -.091 -.273 -.273 .818 1.00 1.00 G-Eval-4\n.091 .818 .636 1.00 1.00 1.00\nTable 7 : The system-level Kendall's correlation between the automatic metric and ACU scores on different system pairs grouped by their ACU score differences on the CNNDM dataset, into six equal-sized buckets. We use the recall score of the automatic metrics when available.\nEval-3.5 we report two variants that are based on greedy decoding (G-Eval-3.5) and sampling (G-Eval-3.5-S) respectively, Details of the LLM-based evaluation are in Appendix E.2. Tab. 6 shows the results, with additional results of more metrics in Appendix E.3. We note:\n(1) Several automatic metrics from the different families of methods (e.g., ROUGE, BARTScore) are all able to achieve a relatively high correlation with the ACU scores, especially at the system level.\n(2) Metric performance varies across different datasets. In particular, metrics tend to have stronger correlations on the SamSum dataset and weaker correlations on the XSum dataset. We hypothesize that one reason is that the reference summaries of the XSum dataset contain more complex structures.\n(3) Despite their successes (Fu et al., 2023; Liu et al., 2023) in other human evaluation benchmarks such as SummEval, LLM-based automatic evaluation cannot outperform traditional methods such as ROUGE on RoSE. Moreover, their low summarylevel correlation with ACU scores suggests that their predicted scores may not be well-calibrated.\nFollowing Deutsch et al. ( 2022), we further investigate metric performance when evaluating system pairs with varying performance differences. Specifically, we group the system pairs based on the difference of their ACU scores into different buckets and calculate the modified Kendall's correlation (Deutsch et al., 2022) on each bucket. The system pairs in each bucket are provided in Appendix E.4. Tab. 7 shows that the automatic metrics generally perform worse when they are used to evaluate similar-performing systems. \n\nAnalysis of Metric Evaluation\nWe analyze the metric evaluation with respect to the statistical characteristics and the impact of different human evaluation protocols on metric evaluation.\nConfidence Interval We select several representative automatic metrics and calculate the confidence intervals of their system-level correlations with the ACU scores using bootstrapping. Similar to Deutsch et al. ( 2021b), we find that the confidence intervals are large. However, we found that having a larger sample size can effectively reduce the confidence interval, which further shows the importance of increasing the statistical power of the human evaluation dataset as discussed in \u00a74.1. We provide further details in Appendix E.5.\n\nPower Analysis of Metric Comparison\nWe conduct a power analysis of pair-wise metric comparison with around 200 pairs, which corresponds to the chance of a statistical significance result being found. More details can be found in Appendix E.6. The results are in Fig. 3 , showing similar patterns as in the power analysis of summarization system comparison ( \u00a74.1):\n(1) Significant results are difficult to find when the metric performance is similar;\n(2) Increasing the sample size can effectively increase the chance of finding significant results. automatic metrics generally perform better under reference-based evaluation protocols, but can have negative correlations with reference-free protocols.\n\nConclusion and Implications\nWe introduce RoSE, a benchmark whose underlying protocol and scale allow for more robust summarization evaluation across three datasets. With our benchmark, we re-evaluate the current state of human evaluation and its implications for both summarization system and automatic metric development, and we suggest the following:\n(1) Alignment in metric evaluation. To evaluate automatic metrics, it is important to use an appropriate human evaluation protocol that captures the intended quality dimension to be measured. For example, reference-based automatic metrics should be evaluated by reference-based human evaluation, which disentangles metric performance from the impact of reference summaries.\n(2) Alignment in system evaluation. We advocate for targeted evaluation, which clearly defines the intended evaluation quality. Specifically, text summarization, as a conditional generation task, should be defined by both the source and target texts along with pre-specified, desired characteristics. Clearly specifying characteristics to be measured can lead to more reliable and objective evaluation results. This will be even more important for LLMs pretrained with human preference feedback for disentangling annotators' prior preferences for LLMs with the task-specific summary quality.\n(3) Alignment between NLP datasets and tasks.\nHuman judgments for summary quality can be diverse and affected by various factors such as summary lengths, and reference summaries are not al-ways favored. Therefore, existing summarization datasets (e.g. CNNDM) should only be used for the appropriate tasks. For example, they can be used to define a summarization task with specific requirements (e.g. maximum summary lengths), and be important for studying reference-based metrics.\n"}
{"question": "Which of the following scenarios do not refer to or use the PRIMERA model?", "evidence": "  Similar to the PRIMERA model (Xiao et al., 2022) , when concatenating the documents and the question, we add a special document separator token (<doc-sep>) between the documents to signal to the model to be aware of the document boundaries.  To follow our pre-training scheme, we append the question to the context and fine-tune the model to generate the correct answer. We use the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) and PRIMERA (Xiao et al., 2022) as the baselines, for assesing the contribution of our pre-trainig format. We compare QAMDEN (447M parameters) against a set of strong long-context transformer baselines, including LED (447M parameters) (Beltagy et al., 2020) , PRIMERA (447M parameters) (Xiao et al., 2022) , 4 and LongT5-xl (3B parameters) 5 (Guo et al., 2022 ) (see \u00a72).   ", "options": ["Additionally, we assess the choice of QASEM as our questionanswer generation module by using the generators from Jia et al. ( 2022) and Khashabi et al. (2022) ."], "answer": "D", "content": "\nIntroduction\nAmong recent NLP research, multi-document processing is gaining increasing attention, due to the need to handle and process an increasing amount of textual data and available documents online. A * Work partly done as an intern at AI2. 1 Our code is available at https://github.com/ aviclu/peekacross. which we split into context documents (2) and a held-out document (3), we select the most salient sentence (4) that is used for generating a question-answer pair (5).\nThen, we pre-train a model by generating the proper answer and the salient sentence, given the question and the context documents (6).\nnumber of prominent applications that are concerned with aggregating information from multiple texts are multi-document summarization (Fabbri et al., 2019; Zhao et al., 2020) , query-focused multidocument summarization (Xu and Lapata, 2020; Pasunuru et al., 2021a) , and multi-hop question answering (Yang et al., 2018; Welbl et al., 2018) . These tasks remain challenging mostly since existing NLP models are designed to handle single texts, rather than processing multiple documents at once (Caciularu et al., 2021) .\nEarly solutions for multi-text processing were task-specific and used complex architectures that were difficult to generalize across different multidocument tasks (Liu and Lapata, 2019; Wang et al., 2020; Ginzburg et al., 2021) . Efficient LMs (Tay et al., 2021; Beltagy et al., 2020) recently demonstrated that by simply concatenating multiple documents into a single sequence, the transformer can offload the goal of identifying and connecting relevant information between the documents. Recently, it was suggested that these long-context LMs can be equipped with new pre-training objectives to enable them to process multiple documents more effectively (Caciularu et al., 2021; Xiao et al., 2022; Yasunaga et al., 2022) .\nThese pre-trained models demonstrated state-ofthe-art performance on a variety of multi-document downstream tasks, and outperformed underlying LMs and task-specific architectures. Such models are often pre-trained using a dataset where each instance is a set of related documents (e.g., news articles all discussing a specific event), which facilitates modeling of cross-text relationships. Existing multi-document pre-training objectives involve unmasking tokens in a document (Caciularu et al., 2021) , or generating a salient masked sentence (Zhang et al., 2020; Xiao et al., 2022) , encouraging the model to recover missing information using other documents. While successful, these models are either limited to classification tasks (Caciularu et al., 2021) or primarily designed for summarization (Zhang et al., 2020; Xiao et al., 2022) .\nIn this work, we propose a novel pre-training objective that supports both short and long text generation, resulting in a versatile and general multidocument language model. In particular, we hypothesize that using questions and answers involving multiple documents can encourage the model to better learn and incorporate both fine-grained information (by asking questions about core information units in a specific sentence) as well as coarsegrained cross-document relationships required to generate a long text such as a summary. We show that this approach holds not only for summarization, but for other multi-document downstream tasks as well.\nDuring the pre-training of existing multidocument language models, the goal is to unmask spans (for encoder-only models) or generate masked textual spans (for encoder-decoder models) under a multi-document context. To that end, multiple concatenated sequences of related documents are fed during pre-training, thus requiring a large number of sets of related documents for an effective pre-training phase (Hoffmann et al., 2022) . In a variety of existing multi-document benchmarks, such as multi-document summarization, only small to medium-scale document clusters are readily available. These are acquired either automatically with lexical similarity and retrieval (Fabbri et al., 2019) or semi-automatically (Gu et al., 2020) , but generally, this process requires a substantial amount of human effort for filtering instances and generating high quality corpora.\nBy employing a novel multi-document question-answer generation procedure, we propose an effective method for expanding the multi-document pre-training corpora. Our approach allows us to provide multiple views for every single cluster of documents, thereby artificially increasing the pretraining data size (in terms of number of instances) via augmentation. To expose the model to a variety of contexts and diversify the pre-training data, we propose to generate multiple pairs of questions and answers and condition them on a subset of the documents' cluster. We select a salient sentence in one held-out document and then employ a recent parser to generate a high-quality question-answer pair about one predicate in the selected sentence, using a systematic semantically-oriented approach (Klein et al., 2022) . This new multi-document pre-training objective challenges the model to generate both the answer to the question as well as the salient sentence, while discarding the held-out document or parts of it (see Figures 1, 2 for illustration). This procedure exposes the model to a variety of contexts -a question and a different subset of the documents in the cluster per instance, in contrast to prior methods that provide only a single view of the cluster. Our contributions are summarized below:\n\u2022 A new pre-training approach for multidocument modeling, formulated as a crossdocument question answering task, further directing the LM to model cross-text relationships, focusing on both fine-and coarsegrained information. \n\nRelated Work\nLong-context efficient text generation transformers (Tay et al., 2021 (Tay et al., , 2022) ) extend earlier transformer models (Vaswani et al., 2017) for processing long sequences, often using a sparse self-attention architecture. Examples include the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) , and LongT5 (Guo et al., 2022) . These models demonstrated that single-text approaches be can adapted to multi-document tasks by concatenat-ing multiple documents into a single sequence and processing them using their sparse attention patterns. They sparsify the full self-attention matrix of transformers by using a combination of a localized sliding window (called local attention), as well as a global attention pattern on a few specific input locations. LED is build upon the BART model (Lewis et al., 2020) by using additional positional embeddings and global attention weights, and introduces the global attention mode that operates over pre-selected tokens. LongT5 extends the T5 model (Raffel et al., 2020 ) by using a similar technique introduced in the ETC and BIGBIRD models (Ainslie et al., 2020; Zaheer et al., 2020) , relieving the requirement to manually select global tokens by automatically globalizing the aggregated representations of groups of tokens.\nFurther strategies have been proposed for increasing these models' abilities in multi-document tasks. The Cross-Document Language Model (CDLM) (Caciularu et al., 2021) suggested pretraining a Longformer-encoder (Beltagy et al., 2020) over sets of related documents, and showed superior performance results over several multidocument tasks. Following this methodology, the authors of LinkBERT (Yasunaga et al., 2022 ) used a similar approach, but utilized Wikipedia's hyperlinks in order to curate informative pairs of linked documents for LM pre-training.\nIn order to adopt the multi-document pretraining approach for sequence-to-sequence tasks, PRIMERA (Xiao et al., 2022) , which is built on top of the Longformer encoder-decoder model (LED), selected salient sentences within clusters of related documents using a pyramid estimation approach, resembling the method presented for pre-training the single-document PEGASUS model (Zhang et al., 2020) . While this work is the closest to ours, it was pre-trained to generate masked salient sentences without any control, which makes the model potentially hallucinate while generating text, while our model uses a controlled QA-based objective. Furthermore, unlike these works, our method generates significantly more data then used to pre-train PRIMERA, which is possible to obtain by the singledocument QA generation approach. Our QA pretraining formulation allows us to generate multiple contexts per document cluster.\nAnother related line of work includes methods that incorporate large-scale QA-generated data for pre-training LMs (He et al., 2020; Jia et al., 2022 ;\n\n(a) The held-out document is discarded from the context (c) The held-out document is included in the context, but the answer in the anchor sentence is masked (b) The held-out document is included in the context, but the anchor sentence is masked\nFigure 2 : A schematic of our pretraining data modes. The salient sentence which is used for QA generation is colored in yellow. (a) The context does not include the held-out document, therefore this mode is the most challenging. (b) The held-out document is present in the context, but the salient sentence used for the QA generation is masked (red). (c) The held-out document is present in the context, but the answer span within the salient sentence is masked (red). Huber et al., 2022) . These works hypothesize and show that pre-training by utilizing generated QA data can encourage contextual representations to encode useful semantic information for other non-QA downstream tasks. Inspired by that, we conjecture that LMs can strongly benefit from infusing QA during pre-training in the multi-document setup, for adding an additional signal for modelling cross-text relationships.\n\nAugmenting the Multi-Document Pre-training objective\nIn this section, we provide the required steps for compiling the pre-training dataset for QAMDEN.\nWe next elaborate on the details of the data creation and provide analysis of the resulted corpus.\nRecent works have shown that for text summarization, pre-training LMs to generate a \"summarylike\" sequence, termed pseudo summary, inherently provides gains over general-purpose pre-trained LMs (PEGASUS, PRIMERA; Zhang et al., 2020; Xiao et al., 2022) . The data in which the PEGASUS and PRIMERA models were pre-trained on was constructed using the Gap Sentence Generation (GSG) method, which suggests masking highly-ranked salient sentences, where salience is pre-determined by a sentence-scoring method of interest. Particularly, in PEGASUS, GSG has been adopted as its pre-training objective, where some sentences in a single document are masked in the input and the model is tasked to generate them.\nFormally, for each sentence s i in a given input document D, PEGASUS computes its salience score based on its ROUGE score (Lin, 2004) w.r.t the rest of the sentences within the document (D/{s i }), i.e. Score(s i ) = ROUGE(s i , D/{s i }). Intuitively, \u2026Pokemon Sword and Shield might have already been announced, but we now know there's another new Pokemon game on the way from DeNA\u2026 QASem QA generation (Klein et al., 2022) Q1: What might been announced? A1: Pokemon Sword and Shield. (Answer length: 4) Q3: Who knows something? A: We. (Answer length: 1) Q2: Where does someone know something? A: On the way from DeNA. (Answer length: 5) Contextualization (Pyatkin et al., 2022) Q: Where did we know there's another new Pokemon game? A: On the way from DeNA.\n\nSelected\nFigure 3 : A schematic of the process of QA generation using QASEM (Klein et al., 2022) and the contextualization model from Pyatkin et al. (2021) . This is an actual sample that was created and used for pre-training QAMDEN, where the document is taken from New-SHead (Gu et al., 2020) .\nthis metric assigns a high score to the sentences that have a high overlap and share more lexical information with the rest of the sentences in the document, thus assigning high scores to prominent sentences. PRIMERA has generalized this notion to support the multi-document setup, by applying a GSG variant over a cluster of related documents.\nCross-Document GSG. We propose augmenting the GSG technique to formulate a cross-document question answering pre-training objective for multidocument tasks, instead of the existing pseudo summary generation methods. Our approach supports identification of both fine-and coarse-grained information as we describe below, and results in a substantially larger amount of pre-training examples compared to the preceding methods.\nFormally, we are given a cluster of related documents S = D 1 , D 2 , . . . , D |S| in a corpus C. Our cross-document (CD) GSG salience score for the i th sentence within the k th document in the set (s i k ), is defined by its ROUGE score w.r.t the rest of the sentences within the document (D k /{s i k }) as well as the other documents (S/D k ), i.e. CD-GSG-Score(s i k ) = ROUGE(s i k , S/{s i k }). Then, for every document k, following Zhang et al. (2020) ; Xiao et al. (2022) we select the top-scored sentence s * k , and then we use this sentence to generate a pair of a question and an answer.\nGenerating Cross-Document QAs. For generating the cross-document questions and their answers, we employ QASEM, a recent semantic parsing framework for question generation (Klein et \nfor k \u2190 1 to |Sn| do 4 s * k \u2190 arg max i CD-GSG-Score(s i k ); 5 (q * k , a * k ) \u2190 QASEM(s * k ); 6 t * k = [a * k , s * k ] # target text; 7 D \u2190 D \u222a {([Sn/D k , q * k ] , t * k )} # (a); 8 D \u2190 D \u222a {([Sn/ {s * k } , q * k ] , t * k )} # (b); 9 D \u2190 D \u222a {([Sn/ {a * k } , q * k ] , t * k )} # (c); 10 Return D;\n2022). 2 QASEM intended soliciting a manageable, discrete account of information in a text for the sake of building natural language semantic representations. It automatically labels each verbal predicate-argument relation with a questionanswer pair, where a natural language question represents a semantic role, while the answers correspond to the arguments that appear in the input text. QASEM is thus an appealing approach since it is capable of generating multiple high-quality questions given a sentence. We apply QASEM over the sentences withing the pre-training data in order to generate question-answer pairs, and then apply the model from Pyatkin et al. (2021) which transforms the question into a more natural and clear form, with contextualized arguments (see example in Figure 3 ). In order to resemble a summarization task where the generated text is typically long, we select the question-answer pair with the longest argument produced by QASEM. Formally, QASEM(\u2022) receives a sentence s * k as an input, and produces question-answer pair (q * k , a * k ), where a * k is the longest among the generated answers. See a detailed example and full description in App. A.1.\nConsidering the question-answer pair, our goal is to encourage the LM to generate the correct answer as well as the salient sentence in a multi-document context in order to learn cross-text relationships.\nData Generation Process. In order to facilitate the construction of a multi-document context, we propose three different modes, each one is responsible for uncovering information by using different contexts. For all the modes, we first generate a QA pair out of the most salient sentence in the held-out document.\n(a) Excluding the source document. In this mode we disregard the held-out document D k from the context S n given to the model, i.e, S n /D k . Hence, the model is tasked to predict the answer without having access to the source document at all, and is restricted to observe only the other documents in the set. Thus, this mode is considered as the most challenging one.\n(b) Masking the salient sentence. In this mode, the source salient sentence is masked, i.e, S n / {s * k }. The model has access to the surrounding context of the masked sentence in the held-out document, as well as the other documents in the set.\n(c) Masking the answer. In this mode, only the answer span within the salient sentence is masked, i.e, S n / {a * k }. The model has access to the surrounding salient sentence, as well as all the documents in the set.\nAs part of the new pre-training process of our novel multi-document model, we append the question after the context and instruct the model to generate an answer followed by its salient sentence, i.e., output = \u27e8answer\u27e9, \u27e8sentence\u27e9, inspired by Bohnet et al. (2022) . Generating the salient sentence introduces a copying mechanism (allows the model to also learn to copy information from the source directly) as well as allowing longtext generation, which is crucial for summarization downstream tasks (Zhang et al., 2020) , as well as outperforming a model which was pre-trained for generating the answer solely -according to the ablations study, this setup yields the best performance results ( \u00a74.4). In the pre-training evaluation phase, the held-out set was split and the loss was measured separately for each mode of the data. As expected, we observed that the loss for (a) was significantly higher than those for the other modes, with (a)\u227b(b)\u227b(c) ranking highest. The procedure for generating the pre-training data is summarized in Algorithm 1 and Figure 2 .\nThe resulted pre-training corpus. We applied our procedure over the NewSHead corpus (Gu et al., 2020) , which consists of a set of related documents per instance. This is the exact same pre-training corpus used also by our main baseline PRIMERA (Xiao et al., 2022) \n\nExperimental Setup and Results\nThis section presents experiments conducted to evaluate QAMDEN, as well as the the ablations and baselines we used. For the intrinsic evaluation we evaluated the models over multi-document QA tasks. For extrinsic evaluations we considered the multi-document abstractive summarization task.\nModel Implementation Details Following Xiao et al. ( 2022), we use the large-sized Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) for our model initialization. The length limits of input and output are 4096 and 1024, respectively. 3 Following the Huggingface implementation (Wolf et al., 2020) , we set the sliding window size to 1024 for local attention in the encoder part.\nSimilar to the PRIMERA model (Xiao et al., 2022) , when concatenating the documents and the question, we add a special document separator token (<doc-sep>) between the documents to signal to the model to be aware of the document boundaries. We also assign the global attention mode to these tokens which enables the model to share information across documents (Caciularu et al., 2021) . For further hyperparameter and pre-training execution details, see App. B.\n\nMulti-Document Question Answering\nMulti-document QA is the task of generating the correct answer, given a set of related multiple documents. For several multi-document QA benchmarks, models are often tasked to implicitly solve multiple sub-tasks or follow intermediate steps, such as comprehending the question, filtering out distracting documents in the context, and stitching pieces of information across the relevant documents (Geva et al., 2021; Caciularu et al., 2022) . Recall that QAMDEN was pre-trained over a automatically generated multi-document QA dataset. Hence, as a preliminary assessment, we first investigate QAMDEN's performance over two multi-document QA benchmarks, HopotQAdistractor (Yang et al., 2018) and WikiHop (Welbl et al., 2018) (see more details of the datasets in App. C.1), and compare to other models that were pre-trained using underling un-masking objectives.\nFine-Tuning Format. To follow our pre-training scheme, we append the question to the context and fine-tune the model to generate the correct answer. We use the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) and PRIMERA (Xiao et al., 2022) as the baselines, for assesing the contribution of our pre-trainig format. Confirmed by Beltagy et al. (2020) , we found out that appending the question: and context: prefixes before the question and the context tokens, respectively, resulted in better performance.\nBaselines. We compare QAMDEN (447M parameters) against a set of strong long-context transformer baselines, including LED (447M parameters) (Beltagy et al., 2020) , PRIMERA (447M parameters) (Xiao et al., 2022) , 4 and LongT5-xl (3B parameters) 5 (Guo et al., 2022 ) (see \u00a72). 6 Results. The results on multi-document QA are shown in Table 2 . We adopted the F1 and Exact Match (EM) evaluation metrics corresponding to the original works. Our QAMDEN outperforms both PRIMERA, LED, and LongT5, confirming that our pre-training data and input format are beneficial for both capturing cross-document relationships (QAMDEN\u227bLED) as well as exploiting both context and question (QAMDEN\u227bPRIMERA).\n\nMulti-Document Summarization (MDS)\nThis task aims at generating a summary for a given set of topically-related documents. Inherently, end-Model F1 EM HotpotQA LED (Beltagy et al., 2020) 65.8 50.6 LongT5-xl (Guo et al., 2022) 66.1 50.9 PRIMERA (Xiao et al., 2022) 65 Results. Tables 3 and 4 present the evaluation results over the Multi-News and Multi-XScience datasets, respectively. Following previous MDS works, we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.2 for details). For a fair comparison, we include the results of PRIMERA as well as the results of the previous state-of-the-art methods (Pasunuru et al. (2021b) and Lu et al. (2020) , for Multi-News and for Multi-XScience, respectively), and LED (Beltagy et al., 2020) . As shown in the results tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, especially on the Multi-News dataset, clearly demonstrating its consistent advan- (Beltagy et al., 2020) 47.4 20.7 23.7 LongT5-xl (Guo et al., 2022) 47.4 20.7 23.7 PRIMERA (Xiao et al., 2022) 49.9 21.1 25.9 QAMDEN 50.9 23.1 27.2 tage. This excludes the results for Multi-XScience where QAMDEN slightly underperforms the prior work and LongT5. An explanation which Xiao et al. (2022) points refers to the fact that the clusters in Multi-XScience have less overlapping information compared to the corpus we used, attributed to the use of abstracts as the input documents in Multi-XScience. In addition, LongT5 advantage over QAMDEN is attributed to significantly larger number of parameters of LongT5-xl.\n\nQuery-Focused Multi-Document Abstractive Summarization\nThe task of Query-focused Multi-Document Summarization (QMDS) aims at generating a summary from a set of documents, that answers a specific given query. Unlike MDS, QMDS tries to solve more realistic query-based scenarios, since it suggests summarizing only predefined salient information of interest that best answers the query. Since we proposed pre-trainng under the multi-document question answering setup, we posit that QAMDEN might be effective for QMDS.\nWe consider the datasets constructed by Pasunuru et al. (2021a), QMDSCNN and QMDSIR (see more details of the datasets in App. C.3) as well as their strong baseline, and include also the results of PRIMERA and LED.\nBaselines. Similar to the previous experiments, we compare QAMDEN against LED, PRIMERA, LongT5-xl. In addition, we consider also the baseline from Pasunuru et al. (2021a) . 37.9 16.4 35.2 LED (Beltagy et al., 2020) 32.3 14.3 30.9 LongT5-xl (Guo et al., 2022) 35.5 15.9 34.3 PRIMERA (Xiao et al., 2022) 36 Results. Tables 5 and 6 present the evaluation results over the QMDSCNN and QMDSIR datasets, respectively. Following MDS tasks and Pasunuru et al. (2021a) , we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.3 for details). As shown in the tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, clearly demonstrating its consistent advantage over the baselines.\n\nAblation Study\nData Generation. We next turn to a broad ablation study, for assessing our configuration and design choices across our suggested pipeline. First, we show the advantage of combining the three proposed data modes, rather than using a subset of them. We evaluate all the resulted models by fine-tuning them over HopotQA-distractor ( \u00a74.1), Multi-XScience ( \u00a74.2), and QMDSIR ( \u00a74.3). For HopotQA-distractor we report the Exact Match (EM) score, and for the summarization tasks we report the ROUGE-1 (R-1) score.\nBaselines. We pre-train QAMDEN for 100k steps, for using every subset of the set of the set (superset) of modes {(a), (b), (c)} (all its possible combinations) of the generated pre-training data modes presented in \u00a73. Note that our QAMDEN model is referred to as using all the modes, i.e., For QA we used the EM score, and for MDS and QMDS we used the ROUGE-1 score.\nResults. Figure 4 shows the ablation results. In all tasks, pre-training using all modes yields the best results. Among all modes, mode (c) appears to be the most effective for QA, since this is an extractive QA task, and mode (c) provides data in this format. Mode (a) excels at the summarization tasks, attributed to their abstractive nature as well as the requirement of all the documents for generating appropriate summaries.\nInput Format We repeat the previous experiment and ablate the pre-training input format according to the multiple different formats, and compare to the model pre-training format described in \u00a73 (with the same pre-training data): without questions, with random question, with random context document, with prefixes, placing the question before the context, with question filtering, and without generating the salient sentence. Additionally, we assess the choice of QASEM as our questionanswer generation module by using the generators from Jia et al. ( 2022) and Khashabi et al. (2022) . Finally, we also include the results of PRIMERA, which was further pre-trained for additional 300k steps (fine-tuning LED for 400k steps in total), for a fair comparison to QAMDEN ablated models. See full details regarding all the ablations in App. D.\nResults. Overall, our QAMDEN model outperforms the ablation models on most of the tasks, which a significant margin.\nPre-training the model without any questions during or using random questions, negatively impacts the results of downstream tasks. An impor- tant function of the question is to facilitate the model's ability to generate the appropriate answer and the source sentence. This aligns with the findings from Caciularu et al. (2021) , who showed that pre-training with random documents rather than related ones is sub-optimal. The use of question and context prefixes for positioning input appears to be helpful for QA, but is inferior when applied to summarization tasks due to its unique format, which is well suited for QA but seems to generalize harder for other setups. When the question is placed before the context, performance slightly decreases over query-based tasks, while maintaining the same results for summarization (where the question location is irrelevant).\nUsing question filtering is found to harm the downstream results of QAMDEN, in accordance to other QA-based pre-training prior works (Jia et al., 2022) .\nPre-training without generating the attributed source sentence introduces a significant flow to the model, particularly for the summarization downstream tasks. As mentioned before, generating longer sequences, as well as teaching the model to copy text, is beneficial for summarization tasks.\nApplying a different question generator rather then QASEM yields inferior results overall, since the other generators produce open-ended questions and answers which are more prone to errors, while QASEM utilizes an existing span in the context as the answer. In addition, QASEM generated local questions, which allows QAMDEN to focus on the fine-grained details, and not only the coarsegrained information in the multi-document context.\nWhen PRIMERA is pre-trained with 400k steps (to match QAMDEN's number of further pretraining steps), it underperforms QAMDEN and even fails to add any significant improvements over its 100K checkpoint, possibly due to the small amount of pre-training data it contains. \n\nComparison with Large Language Models\nIn order to get insights into how QAMDEN compares with state-of-the-art Generalist Large Language Models (LLMs), we provide a small comparison with two capable models, GPT-3.5 turbo (Ouyang et al., 2022) and GPT-4 8 (OpenAI, 2023) (including the 8k input length version) evaluated on the zero-shot setting.\nFor a fair comparison, we used the same context window size of 4K tokens for all models (and up to 8k for GPT-4 8k). Due to the fact that multidocument tasks involve processing long sequences, the cost of API calls is significant for a comprehensive evaluation across all datasets. Therefore, we only evaluate on a sample of 200 instances from the multi-news dataset (see prompting details in App. E). Table 8 depicts the results. We observe that QAMDEN significantly outperforms both GPT-3.5 and GPT-4 models, though the performance of GPT-4 and GPT-3.5 is comparable. We leave more comprehensive comparisons with LLMs to future work.\nWe further assessed QAMDEN through manual comparison against PRIMERA, GPT-3.5, and GPT-4 8k. NLP graduate students were shown summaries for a given topic from the three systems and QAMDEN in arbitrary order, along with a corresponding reference summary. Following (Ernst et al., 2022) , participants were asked to rank the systems based on Content (overlap with the reference), Readability (the readability of a summary), Grammaticality (avoiding grammar errors), and Non-Redundancy (avoiding repetitions), and we extract the pairwise results out of the rankings (see (Ernst et al., 2022) for further details). In App. F, we provide several examples to system summaries and their corresponding reference summaries.\nThe results of this study are presented in Table 9 . Under each evaluation criterion, it indicates the percentage of cases where QAMDEN was preferred over both baselines. QAMDEN was favored in all cases except for grammatical errors and readability (which corresponds to the Reinforcement Learning from Human Feedback phase of the GPT models).\n\nConclusions\nIn this work, we present a novel pre-training scheme for multi-document tasks. First, our approach suggests to augment the existing multidocument pre-training objectives into a crossdocument question answering task. Second, we generate high-quality large-scale QA pre-training data using a controlled generation approach, in which each QA pair originates from a salient sentence in one of the documents in the set.\nDuring pre-training, we task the the Longformer Encoder-Decoder (LED) model to generate the answer and the salient sentence on the basis of the remaining context. This objective encourages the LED model to elicit cross-document relationships, and stitch pieces of information across the input documents, which are relevant for performing multi-document tasks. The resulted model QAMDEN shows significant performance improvements compared to prior models under extensive experimentation over multiple challenging multidocument summarization and QA datasets.\nFuture work can extend the ideas in this work for equipping decoder-only large LMs with crossdocument modeling using our proposed method, also in the setup of in-context learning and prompt tuning. We foresee that our method should be significant specifically for retrieval-augmented language modeling setups (Izacard et al., 2022) , where there is a use of related documents as an outsourced external non-parametric knowledge source. Finally, the use of a single document in order to trigger cross-document relationships, as firstly introduced in this work, might be further investigated.\n"}
{"question": "Which model demonstrates the best performance on ideology detection across various political actor modeling tasks?", "evidence": "  Results of ideology detection tasks indicate that our model can not only characterize the ideology of legislators but is also good at modeling other roles like journalists in the celeb dataset and cabinet in the PEM dataset, demonstrating the transferability of using languages to represent characters.  The compared general PLMs include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) . We also compare our model with available PLMs for social science texts-SsciBERT (Shen et al., 2022) , and for the political domain: POLI-TICS (Liu et al., 2022) and PoliBERTweet (Kawintiranon and Singh, 2022). Nevertheless, we still outperform the majority of baselines. ", "options": ["A. UPPAM", "B. BERT", "C. RoBERTa", "D. SsciBERT"], "answer": "A", "content": "\nIntroduction\nPolitical actors are shaping our attitudes, opinions, and decisions toward public issues. For instance, on social platforms, politicians can select and emphasize certain aspects of content to bias the discussion, through which they can derive an opinion climate from user engagement and acquire direct feedback from potential voters and opinion leaders (Bene, 2017; Heiss et al., 2019) . Political actor modeling is essential for quantitative political science and has applications in various downstream tasks such as roll call vote prediction (Yang et al., 2020) , frame detection (Johnson et al., 2017) and bias detection (Baly et al., 2020) .\nData-driven approaches utilize different kinds of information to profile political actors, including public statements, legislative behaviors and social Figure 1 : An illustration of political actors. They not only participate in legislative activities, but also form relationships with others, and convey opinions through tweets, speeches and etc. We propose to represent political actors based on their statements and learn the mapping from language to their representations using social networks and behaviors as self-constructed supervision.\nnetworks (Figure 1 ). Early research analyzes roll call data to estimate the ideology of political actors. Ideal point model (Clinton et al., 2004 ) is one of the most widely used approaches for votebased analysis that reveals how cleavages between legislators reflect partisan affiliation. Researchers further incorporate texts of bills to enhance the ideal point model (Gerrish and Blei, 2011, 2012; Kraft et al., 2016) and develop multidimensional vectors to replace one-dimension points. Recently, more abundant information has been considered to learn effective representations for political actors, such as co-sponsorship network (Yang et al., 2020) , relations of contributors (Davoodi et al., 2020) , stakeholders (Davoodi et al., 2022) , mention in documents (Pujari and Goldwasser, 2021) , and expert knowledge (Feng et al., 2021 (Feng et al., , 2022)) .\nGenerally speaking, previous research aims to learn representations for a certain group of political actors using supervision from specific downstream tasks as objectives. Although they report positive results on target tasks, their models lack generalization ability in two aspects. (1) Representations are learned on labeled data from specific tasks, e.g., state-level vote prediction, therefore they cannot be easily transferred to other tasks or scenarios. (2) The model is limited to the training setting and can not be adapted to dynamic social contexts. In other words, it's hard for the model to estimate new legislators, non-voting candidates and other political actors unseen.\nRecently, large-scale pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019b; Brown et al., 2020) have demonstrated a strong generalization ability and achieved excellent performance in many language modeling tasks. Motivated by PLMs, we explore representing political actors based on their statements and propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM) 1 . We employ a two-stage training procedure following the fashion of PLMs. Firstly, we pre-train our model to learn the mapping from statements to actor representation. We propose a multigranular method to represent political actors based on language, and information of political scenarios is further injected into our model via proposed structure-aware contrastive learning and behaviordriven contrastive learning tasks. Secondly, we fine-tune the model for downstream tasks using the corresponding supervised objectives.\nUPPAM is novel in three points. (1) We learn the mapping from statements to the representation of political actors, instead of directly learning actor representations. By doing so, the mapping parameters can be transferred to any downstream tasks easily, learning representations for unseen political actors based on their statements. (2) We propose several self-training tasks to inject general knowledge in the political scenarios into mapping parameters in the pre-training stage. (3) We propose a multigranular actor representation model, that can capture nuances of both general ideology and specific preferences between different political actors. We evaluate our approach on three types of tasks in quantitative political science, i.e., profile of actors, prediction of behaviors and analysis of languages. UPPAM outperforms general PLMs and other political domain-specific PLMs on these tasks. Our task-agnostic model also achieved competitive results compared to the task-specific models that employ architectures crafted for the 1 We have made our code publicly available at https:// github.com/xymou/UPPAM. vote prediction task. Further analysis shows the effectiveness and robustness of UPPAM in few-shot settings and different aggregation settings.\n\nMultigranular Actor Representation\nPolitical actors manifest themselves in political activities in multiple granularities. On the one hand, they hold a general ideology or bias, which is long-term and stable. On the other hand, when discussing or taking action on different issues, they hold specific positions (Gerrish and Blei, 2012) , which are the result of long-term bias and shorttime interests (Spell et al., 2020) . Based on this, we propose to represent political actors in two granularities to model both broad ideology and specific preferences for various downstream scenarios.\n\nGeneral and Specific Statements Collection\nIn practice, we use all statements a political actor has posted to get his general representation, characterizing the broad political leaning. Furthermore, issue-related content is adopted to help capture specific attitudes. Concretely, we use a handcrafted information retriever (see more details in Appendix A.2), to collect statements related to the queried policy area as input to encode the specific representation.\nStatements Aggregator Since a political actor can post thousands of statements, the first challenge is how to aggregate one's statements to get his representation. It is too expensive in time and computation cost to combine full sentences. Instead, we identify indicator words from statements for information aggregation. According to the framing theory (Entman, 1993) , entities and subjective content an author uses can implicitly reflect his political leaning. Following this, we identify entities, frame and sentiment words as indicators. We sort them by TFIDF (Jones, 1972) scores and keep indicators with the highest values to form an indicator sequence. In this case, for each political actor, we can get two kinds of indicator sequences, given a query about policy area j:\nS g i = w g 1 , w g 2 , ...w g N (1) S p j i = w p j 1 , w p j 2 , ...w p j M (2)\nwhere S g i is calculated from all the statements made by political actor i, S related to policy area j, and we reserve top N and M indicators with highest TFIDF value, where N and M are pre-defined hyper-parameters.\nIn subsequent pre-training and downstream tasks, we use general sequences as input when the goal is to profile the characters broadly, e.g., estimating ideology. And we input both sequences and average the representation when specific attitudes are required in tasks, as shown in Figure 2 . Note that even if the issue-related content can not be retrieved, we can use the general sequence as a substitute, to ensure input compatibility.\n\nMultidimensional Pre-training for Political Actor Modeling\nTo inject general knowledge of the political landscape into the mapping from statements to representation, we construct self-supervised tasks based on structural and behavioral information.\n\nStructure-aware Contrastive Learning (SCL)\nIn terms of structural information, we mainly focus on the relationship formed between political actors. Previous studies have revealed that homophily exists in political communities, where people with similar ideologies form a link with each other (Barber\u00e1, 2015) . We use two parts of links, namely party affiliation and co-sponsorship in voting. We treat party affiliation as a coarse relationship and cosponsorship as a fine relationship respectively. By doing this, the model can further capture nuances across parties as well as inside the same party.\nParty Affiliation Link We compare statements of legislators from different parties. We choose a legislator as the anchor, and then take another legislator with the same party affiliation as the positive sample, while those from the opposite party are regarded as negative samples. By comparing general statement sequences of legislators from different parties, the model can learn the differences in the languages of different ideologies.\nCo-sponsorship Link In the legislative process, a bill is initialized by a sponsor and several cosponsors. We assume that the more two legislators collaborate, the more they are alike since they reach agreements on many occasions (Yang et al., 2020; Mou et al., 2021) . Given an anchor legislator, other legislators are divided into three categories based on the number of times they co-sponsored with the anchor legislator: G 1 (the co-sponsorship times are above the average); G 2 (the co-sponsorship times are below the average); G 3 (they have never cosponsored). And we further sample positive and negative samples with the rule of\nG 1 < G 2 < G 3 .\nBased on the triplets constructed in the above two ways, the structure-aware contrastive objective is formulated as follows:\nLSCL = t\u2208T SCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4SCL + (3)\nwhere T SCL is the set of legislator triplets, t (a) , t (p) and t (n) are actor representation encoded by general sequences of anchor, positive and negative sample in triplet t, \u03b4 SCL is a hyperparameter and\n[\u2022] + is max(\u2022, 0).\nNotably, this task endows the model to capture general ideology of speakers from their languages.\n\nBehavior-driven Contrastive Learning (BCL)\nWhen it comes to behavioral information, we pay attention to the most common and important actions, i.e., voting. Specifically, we sample triplets consisting of an anchor bill and a pair of legislators, where the positive legislator p votes yea on the given bill and the negative one n votes nay.\nDifferent from the ideology cleavages modeled in Sec 2.2.1, the divergence of specific preferences is supposed to be reflected in the languages here. Thus, for each legislator, we extract statements about the policy area of the anchor bill as the specific sequence, input with the general sequence, as we mentioned in Sec 2.1. In this way, the behaviordriven contrastive objective is as follows:\nLBCL = t\u2208T BCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4BCL + (4)\nwhere T BCL contains all vote triplets, and \u03b4 BCL is a hyperparameter. t (a) is the bill representation, t (p) and t (n) are the average of representation of the general sequence and the specific sequence, for the positive and negative legislators respectively.\nIt's noticeable that this pattern is not limited to the roll-call vote scenarios, instead, it can be applied to model the preferences towards any bills, events, or targets with a text description.\n3 Pre-training Process\n\nLanguage Model Co-training\nAs mentioned in Sec 2.2.2, modeling political actors in political scenarios inevitably requires encoding textual information of the bills and issues they interact with, e.g., Equation 4. Meanwhile, it is important to understand their opinions in a single discourse without context. Thus, we incorporate additional modules to model political texts. Specifically, as shown in Figure 2 , we have two FFN layers in parallel in each transformer layer, to handle text and actor sequences separately. Given a sequence of input x = {x 1 , ..., x n }, the model first performs multi-head self-attention and then the corresponding module FNN k obtains the required representation:\nh k = FNN k ( Self-Attention ({x 1 , . . . , x n }))\n(5) where k \u2208 {0, 1} indicates the modules of actor and text respectively.\nWe adopt a masked language model objective to pre-train the language model. As mentioned before, political bias and framing effect are often reflected in the selection and mention of specific entities, subjective content, and emphasized frames. Thus, we take a masking strategy that upsamples entity tokens, sentiment words (Wilson et al., 2005) and frame indicators (Roy and Goldwasser, 2020) to be masked for the MLM objectives, with a 30% probability. More details can be found in Appendix B.\n\nOverall Pre-training\nSince the indicator sequence is not a normal sentence, we don't train the MLM task with contrastive learning together. Instead, the pre-training process is divided into two stages. In the first stage, we adopt the MLM task on the original statement sentences and activate text modules, to urge the model to understand the political text. Then, based on this checkpoint, we further conduct the multidimensional pre-training for political actor modeling by combining the objectives:\nEQUATION\n)\nwhere \u03b1 is hyperparameters.\n\nExperiment Setup\nWe fine-tune our model on different kinds of downstream tasks in quantitative political science. We then compare it with prior general PLMs and political domain-specific PLMs.\n\nPre-training Datasets\nCompared to other political actors, congress legislators are more typical and they generate massive content every day. Thus, we start with legislators to construct our pre-training datasets. Overall, we get 887 legislators and delete the meaningless tweets including self-promotion advertisements, notifications, etc., using regular expressions. Finally, the cleaned data contains 2,020,938 tweets, covering discussions of events in various areas. We keep 10K held-out tweets as the validation set.\n\nLegislative Context\nWe collect the party affiliation, sponsorship lists of bills, bills, and corresponding voting records from VoteView 2 and the website of U.S. Congress 3 . Each bill belongs to a specific policy area and has textual information of title and description. We get bills of 112th and 113th for pre-training and reserve those of 114th and 115th for the formulation of downstream tasks. In the pre-training stage, 1,045 bills and 375,440 voting records are involved.\nTo correlate legislators' votes to their statements in the related policy area, we filtered each legislator's tweets in each policy area by the handcrafted information retriever mentioned in Sec 2.1. We finally acquire 1,142,587 tweets, and the details can be found in Appendix A.2. The distribution of the policy agenda of bills and the percentage of legislators whose related tweets can be retrieved in each policy area are shown in Figure 3a and Figure 3b . Over 90% of legislators can be retrieved with relevant statements in most policy areas.\n\nImplementation Details\nUPPAM is produced via continued pre-training on RoBERTa-base model (Liu et al., 2019b) , where we add parallel FFN modules in each transformer layer with the same initialization as the original one. In the first stage, the model is trained on tweets, to minimize the MLM loss with AdamW (Loshchilov and Hutter, 2018) optimizer. In the second stage, the model is further trained on indicator sequences and bill texts, to minimize the L CL . We evaluate the model every 200 training steps on the validation set and keep the best checkpoint. The pre-training procedure takes around 96 hours on 4 Tesla V100-SXM2 GPUs. More details and hyperparameters can be found in Appendix B.\n\nDownstream Tasks and Datasets\nWe evaluate the models on three types of tasks, namely actor profiling, behavior prediction and language analysis. Notably, datasets include not only congress legislators but also other political actors such as journalists, news media, and even anonymous users, to validate the model's generalization capability.\n\nActor Profiling\nThis type of task can be formulated as a user-level classification task, where we aggregate multiple statements to predict the speaker's attribute.\nIdeology Detection is the main task to profile actors broadly, aiming to predict political leaning. Models are evaluated on the following datasets.\n\u2022 CongS (Gentzkow et al., 2018) collects speeches from US congressional records. \u2022 celeb (Wojcieszak et al., 2022) contains tweets of celebrities (journalists, politicians and media). We convert the ideology scores into labels according to the signs. \u2022 Reddit (Kitchener et al., 2022) collects comments of common users in non-political subreddits, and labels the users with ideology in the economic dimension. \u2022 PEM (Xiao et al., 2022) collects tweets of legislators, news outlets and cabinet of President Obama and President Trump. \u2022 TIMME (Xiao et al., 2020) includes Twitter accounts with location information and selfidentified political-polarity labels. These accounts are not run by politicians.\n\nBehavior Prediction\nThis type of task can be regarded as a relation prediction task, where we predict a political actor's attitude or action towards a given target with a piece of text description.\nVote Prediction tasks aim to predict votes of legislators towards bills with stances of yea or nay. We follow two configurations in (Mou et al., 2021) . \u2022 VoteIn refers to the in-session setup, where we randomly split the bills in the same congress session, i.e., the 114th session. \u2022 VoteOut refers to the more challenging outof-session setup, where we use data in the 114th session for training and validation while testing on the 115th session.\nGrade Prediction tasks are designed as classification tasks for ratings in a certain issue, given a politician's statements and background description of the given issue. We include datasets as follows:\n\u2022 NRA Grades (Pujari and Goldwasser, 2021) provides politicians' grades {A, B, C, D & F} assigned by National Rifle Association and their statements on guns, as well as background information of guns from ontheissues.org. \u2022 LCV Grades (Pujari and Goldwasser, 2021) is similar to NRA Grades, but it's about the scores in the environment area.\n\nLanguage Analysis\nIn addition to the overall characterization of political actors, we also test models' ability to understand individual discourses. We apply stance detection and frame detection as downstream tasks, which can be formulated as sentence-level classification tasks.\nStance detection tasks aim to predict one's stance towards a given target. The tasks take a 3-way label (favor, against, and neutral) or binary label (favor, against). We test on these datasets.\n\u2022 poldeb (Somasundaran and Wiebe, 2010) provides opinion-target pairs from several debating platforms covering different domains. \u2022 election (Kawintiranon and Singh, 2021) contains tweets related to the 2020 US presidential election, expressing stances towards President Trump and Biden.\n\u2022 SEval (Mohammad et al., 2016 ) is a shared task to detect stances in public tweets.\nFrame detection tasks aim to detect which frame dimensions are employed in a piece of text. It's a multi-label classification task with a pre-defined label set. We test on these datasets.\n\u2022 twitter (Johnson et al., 2017) annotates tweets of politicians with 17 general frames. \u2022 gvfc (Liu et al., 2019a) collects news headlines about gun violence, and annotates them with 9 issue-specific frame dimensions. \u2022 immi (Mendelsohn et al., 2021) collects immigration-related tweets posted by the public, annotated with 14 general frames.\n5 Experiment Results\n\nMain Results\nThe compared general PLMs include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) . We also compare our model with available PLMs for social science texts-SsciBERT (Shen et al., 2022) , and for the political domain: POLI-TICS (Liu et al., 2022) and PoliBERTweet (Kawintiranon and Singh, 2022). We fine-tune all the PLMs in the same settings, and we select the best fine-tuned model on validation sets using macro F1.\nThe implementation details and hyperparameters can be found in Appendix C.2. Table 1 presents macro F1 scores on the downstream tasks.\nActor Profiling Our model shows superior performance on various political actor modeling tasks.\nResults of ideology detection tasks indicate that our model can not only characterize the ideology of legislators but is also good at modeling other roles like journalists in the celeb dataset and cabinet in the PEM dataset, demonstrating the transferability of using languages to represent characters. The reason for not performing best on the Reddit dataset may be the gap between the expression habits of common users and that of politicians. Nevertheless, we still outperform the majority of baselines.\nBehavior Prediction All the models show excellent performance on vote prediction and grade prediction tasks, using languages to represent political actors. It indicates that it's a feasible scheme to infer political actors' behaviors from their languages. Among all the PLMs, our model is the best. We attribute the performance gain to our proposed behavior-driven pre-training task.\nLanguage Analysis Moreover, our model also achieves competitive performance on tasks of analyzing individual text including stance detection and frame detection, indicating that the ability to understand political languages is preserved while the model is learning to profile actors, benefiting from the co-training process in Sec 3.1.\n\nAblation Study\nTo explore the effects of different components, we conduct ablation studies and results are reported in Table 2 . Removing SCL or BCL mainly hurts the performance of actor profiling tasks. Removing the text modules results in the most loss in language analysis tasks, especially the frame detection task. This demonstrates the necessity of separate modules to guarantee the ability to model political text.\n\nFurther Analysis\nFew-shot Learning We fine-tune PLMs on different numbers of samples. Figure 4 tasks. Benefiting from the pre-training stages, our model can better capture ideology and preference differences, even when using only 16 samples.\nCompare with Task-specific Models Taking the vote prediction task as an example, we compare our model with previous task-specific models, where particular meta-data and structural information is crafted for the task. Table 3 shows that UPPAM achieves competitive results, indicating that we can deduce political actors' votes from languages. Additionally, our method can be used to analyze nonvoting actors, relieving the cold-start problem.\n\nMethods of Statements Aggregation\nWe show the impact of statements aggregation methods on ideology detection in fine-tuning. We mainly compare our method with concat (Table 4 ) and mean pooling (Table 5 ). concat means to concatenate each speaker's political statements into a flat sequence and then encode it. mean pooling encodes each sentence individually and uses the averaged representation as the final representation. We further discuss the impact of the number of aggregated sentences in Appendix C.2.2. Results illustrate that our model shows robustness in several settings and our aggregator is more effective and efficient.\n\nVisualization\nGeneral Ideology We perform Principle Component Analysis (PCA) on political actor representation generated by our model for the CongS dataset. As shown in Figure 5a , our method can well separate politicians of different ideologies.\nIndividual Specific Preferences We also visualize specific representation in different policy areas for individuals. Figure 5b shows the representation in several highly-discussed policy areas, learned by different models from the tweets of Rep. Rooney. We can observe that Rep. Rooney behaves conservatively in immigration, but expresses left-wing views on environment (Pujari and Goldwasser, 2021) . While most of our baselines fail to capture this nuance, UPPAM can well compare the relative polarity in each area.\n\nRelated Work\nPolitical Actor Modeling focuses on modeling attributes and behaviors of political actors, with special attention to estimating the ideology. Because of the publicity and typicality, politicians like legislators have been the research subject for most work.\nThe most widely used approach to estimate the ideology of legislators is ideal point model (Clinton et al., 2004 ) that represents legislators and bills as points in a one-dimension latent space from the rollcall data. After that, researchers further incorporate texts of bills (Gerrish and Blei, 2011; Gu et al., 2014) to enhance the model, solving the problem of prediction on new bills. Some embedding methods are also proposed to promote learning of legislators (Kraft et al., 2016; Kornilova et al., 2018) . More recently, external information including cosponsorship (Yang et al., 2020) , donors (Davoodi et al., 2020) , relevant stakeholders (Davoodi et al., 2022) and expert knowledge (Feng et al., 2021 (Feng et al., , 2022) ) is used to better learn legislator representation. They follow a mixed structure of textual encoder and graph encoder, to explicitly combine textual and structural information. Despite outstanding performance on target tasks, these methods are limited to certain settings or data, behaving inefficient in dynamic political scenarios. Thus they are hard to be transferred to all actors. By contrast, methods relying on texts (Vafa et al., 2020) provide more possibility for generalization.\n\nDomain-specific Pre-training\nBased on continued pre-training on domain-specific data, domainspecific Pre-trained Language Models have shown superiority on many NLP tasks. Domain-specific PLMs have been investigated in many areas including medical (Zhang et al., 2021) and financial (Araci, 2019) \n\nConclusion\nIn this paper, we propose to learn political actors from languages and inject multidimensional domain knowledge into the PLMs through structureaware contrastive learning and behavior-driven con-trastive learning. Experimental results validate the effectiveness and generalization capability of our approach.\n"}
{"question": "what is Symbolic Chain-of-Thought Distillation (SCoTD)?", "evidence": "  We show that ordersof-magnitude smaller models (125M-1.3B parameters) can still benefit from chain-ofthought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. ", "options": ["A.method for training large language models. ", "B. An algorithm for generating coherent chains of inference. ", "C. A method for training smaller language models to have reasoning abilities. ", "D.A metric for evaluating the performance of teacher models."], "answer": "C", "content": "\nIntroduction\nEmpirical scaling laws suggest that the accuracy of Large Language Models (LLMs) on benchmark tasks can be improved by increasing model size and pre-training data volume (Hoffmann et al., 2022) . Beyond these training-time improvements, however, an inference-time strategy dubbed \"chain-ofthought\" (CoT) prompting, 1 i.e., eliciting verbalizations of predictive processes via key-phrases like \"Let's think step-by-step\" (Kojima et al., 2022) , can *Work done during an internship at AI2. Figure 1 : Symbolic Chain-of-thought Distillation (SCoTD) applied to a student model, ranging in size from 125M-1.3B parameters. We show that fine-tuning on a (diverse and potentially filtered) corpus of expert chain-of-thought demonstrations from a teacher model is an effective strategy to make smaller models capable of chain-of-thought reasoning.\nsimilarly improve performance, e.g., Suzgun et al. (2022) demonstrate additional performance gains on a hard subset of the BigBench tasks (BIG-bench collaboration, 2022) using chain-of-thought.\nHowever, chain-of-thought prompting has only been shown to be beneficial for models of sufficient scale (e.g., with more than 60B parameters (Wei et al., 2022b) ). In this work, we study whether small language models can be \"taught\" the capacity for chain-of-thought reasoning by larger language models. We adopt a simple strategy, which we call Symbolic Chain-of-thought Distillation (SCoTD): first, we sample chain-of-thought rationales from large language model given (unlabeled) input instances from a dataset; then, we train a smaller language model to predict the sampled rationale CommonsenseQA: Q: The man working in the attic swatted away a bee, but soon the single bee was an entire what? (a) swarm; (b) pack; (c) countryside; (d) soft drink; (e) field of flowers GPT-3: The answer must be related to bees, but also connected with being numerous. Of the above answers, only swarm fits the scenario. So the answer is: (a) OPT-1.3B: The answer must be a swarm of bees. Of the above choices, only soft drink is used to describe a swarm of bees. So the answer is: (c) OPT-1.3B + SCoTD: The answer must be the name of something that has bees. Of the above choices, only swarms have bees. So the answer is: (a) QuaRel: Q: Steve is waiting at the gym ... He starts hitting around a hockey puck with his stick. The gym has a carpeted floor and more friction than the ice rink he goes to. Which surface is rougher? (A) carpet (B) ice rink GPT-3: When something is smoother, it is easier to slide on and easier to pass through. So the carpet is rougher. So the answer is: (A) OPT-1.3B: _________ is rougher than carpet. Thus, the gym floor is rougher than the ice rink. So the answer is: (A) OPT-1.3B + SCoTD: When something is rougher, it has more friction. Thus, the gym has more friction than the ice rink he goes to. So the answer is: (A)\nOpenBookQA: Q: Magnets may be attracted to (a) clay pots (b) roofing nails (c) paper plates (d) plastic cutlery GPT-3: Magnets are attracted to metal objects. These objects include roofing nails. So the answer is: (b) OPT-1.3B: Magnets are attracted to clay pots, roofing nails, paper plates, plastic cutlery. So the answer is: (d) OPT-1.3B + SCoTD: Magnets may be attracted to some metals, but not to clay pots, roofing nails, paper plates or plastic cutlery. So the answer is: (b) Table 1 : Few-shot chain-of-thoughts produced by GPT-3 (code-davinci-002, the teacher model), OPT-1.3B (the un-distilled student model), and OPT-1.3B + SCoTD (ours), the student model trained using Symbolic Chainof-thought Distillation. \u00a73 shows this process significantly improves the task-accuracy of the student model in a variety of settings, and in \u00a73.1.1, human evaluations show that, even when the un-distilled student model happens to get the multiple choice question correct (see QuaRel example), humans tend to prefer OPT-1.3B + SCoTD. and sampled label. This process follows the \"symbolic knowledge distillation\" paradigm as in West et al. (2022) , wherein corpora are sampled from a larger language model to serve as training data for a smaller one.\nWe find that through SCoTD, smaller language models learn to self-rationalize and perform significantly better on 3 commonsense QA tasks compared to learning without rationalizations. This result holds for both supervised and few-shot settings, and across student models of varying scales (125M-1.3B parameters). Performance gains are especially pronounced when applying distilled chain-ofthought models to difficult scenarios like: contrast sets (Gardner et al., 2020) ( \u00a73.4 ; SCoTD significantly outperforms supervised learning on labels) and fully held-out tasks ( \u00a73.5; few-shot SCoTD significantly outperforms in-context learning).\nKey to the success of this process is sampling a relatively large number of rationales per example from the teacher model (e.g., 30 rationales/example) (Figure 2 ). This is different from many prior practices that train with one rationale per example (Camburu et al., 2018; Li et al., 2022a) . In ablation studies, we investigate several competing hypotheses for what are the most important factors within the corpus: we filter the corpus to CoTs that are assigned high probability by GPT-3 vs. filtering to CoTs that are diverse vs. filtering to CoTs that explain more open-ended input instances.\nWhile diversity and high probability are reasonable filters that on average perform well, the \"null hypothesis\" of random downsampling performs well, suggesting that the sheer volume of the rationales is also a key contributing factor.\nWe will release code and the corpus of sampled chain-of-thoughts at https://github.com/ allenai/cot_distillation.\n\nSymbolic Chain-of-Thought Distillation\nOur primary goal is to improve the accuracy of a (relatively small) student language model S on a target classification 2 task D Test = {(x i , y i )}. 3 We assume access to 1) (an unlabeled) training set D Train = {(x i )}; and 2) a large teacher language model T (e.g., GPT-3 (Brown et al., 2020) ), capable of generating chain-of-thoughts in a few-shot fashion.\nOur first step is to curate a set of labeled chainof-thoughts to serve as few-shot Prompts for T . For each target task, we sample a small number (e.g., 10) of examples x i from D Train , provide a gold classification label y i , and manually author a chain-of-thought z i for each to form the prompt set P = {(x i , y i , z i )} 4 . Then, for each x i in D Train , we sample N chainof-thoughts zi along with the resulting prediction \u1ef9i from the teacher model, i.e.,\n(\u1ef9 k i , zk i ) \u223c N T (y i , z i |x i , P).\nThe result of this sampling is a corpus\nC = {(x i , {(\u1ef9 k i , zk i )} N k=1\n)}, which contain teacherpredicted chain-of-thoughts/labels. Depending on the experimental setting (details in \u00a7 3), we sometimes filter the entries of C, e.g., in the fully supervised case where D Train instances have associated labels, we discard samples for which the sample the teacher model predicted an incorrect label. Next, we train the student model using the standard language modeling loss, i.e., we maximize\nE (x,\u1ef9,z) \u223c C [S(\u1ef9, z|x)].\nAfter fine-tuning the student model on the corpus sampled from the teacher, to evaluate the model on a test instance (x test , y test ) from the target task, we decode both a chain-of-thought ztest and a predicted label \u1ef9test from the student and evaluate \u1ef9test versus the true label y test . We consider two strategies for decoding. (1) Predict the most likely chain-of-thought and the label ztest , \u1ef9test = argmax z,y S(z, y|x test ). This can be approximated by greedy decoding or beam search. (2) There may be different valid chainof-thoughts for a given question and as a result, large language models distribute probability mass for a certain label across many diverse chain-of-thoughts (Wang et al., 2022b) . Thus, it is beneficial to marginalize out the reasoning paths to find the most consistent answer: \u1ef9test = argmax y E z\u223cS(z|xtest) S(y|z, x test ). This can be approximated by sampling multiple reasoning paths and take a majority vote among the predicted answers, dubbed \"self-consistency\" (Wang et al., 2022b) . We experiment with both approaches and conduct a discussion in \u00a73.2.\n\nExperiments\nWe evaluate primarily on 3 target tasks: 1) Com-monsenseQA (CSQA) (Talmor et al., 2019) , a 5way multi-choice dataset; 2) OpenBookQA (Mihaylov et al., 2018) , and 3) QuaRel (Tafjord et al., 2019) . While any model capable of few-shot chain-of-thought could be substituted, we use the thought prompts from prior work (Wei et al., 2022b; Wang et al., 2022b) code-davinci-002 version of GPT-3 5 (Brown et al., 2020) as our teacher model T . We use OPT (Zhang et al., 2022) as our student model S. Our standard student model is OPT-1.3B (though we explore a range of student model sizes in \u00a73.3).\nWe sample from GPT-3 with a temperature of T = 1.0. For each training example, we sample N = 30 rationales. OPT is fine-tuned with a batch size of 32 and a learning rate of 2 \u00d7 10 \u22125 . We use HuggingFace transformers (Wolf et al., 2019) , Pytorch (Paszke et al., 2019) , and Accelerate 6 for the implementation. Main experiments can be reproduced on one GPU with 48GB of memory.\n\nResults in Default SCoTD Setting\nWe first consider both a few-shot learning setting and a supervised setting. For the few-shot setting, the only labeled examples available to our teacher/student models are contained in the prompt set P (but we use the unlabeled examples and teacher-generated chain-of-thoughts/labels for training). 7 We also consider the supervised setting, where we assume access to labels in D Train . Supervised SCoTD involves simply discarding the samples within C that do not have the correct label prior to fine-tuning the student: for Common-\nCommonsenseQA QuaRel OpenBookQA\nFigure 2 : For three commonsense QA tasks, accuracy (y-axis) improves significantly as the student is trained on more chain-of-thoughts sampled from the teacher (x-axis). Oversampling chain-of-thoughts is sometimes required to improve student performance beyond the supervised label-only baseline, e.g., as in OpenbookQA.\nsenseQA, OpenBookQA, and QuaRel, this results in discarding 40.4%, 45.0%, 34.2% of chain-ofthoughts. For the few-shot setting, we decode with the self-consistency approach; for the supervised setting, we decode with greedy decoding (introduced in \u00a7 2; see an discussion in \u00a7 3.2). We compare SCoTD to 2 baselines: 1) Label-Only, the student is fine-tuned on just the label (in the few-shot setting, the label comes from the teacher and could be wrong; in the supervised setting, we use the gold label), instead of also with CoT; 2) Greedy-CoT, we decode a single-CoT per example (instead of N = 30 samples) from T for each training example instead of sampling. For additional reference, Table 2 (a) reports the performance of the student (and teacher) in a variety of few-shot settings prior to applying any distillation: No CoT = few shot prompting with labeled instances from P but no z i , Greedy and Self-Consistency are prompting with CoT but with different decoding strategies ( \u00a7 2).\nTable 2 (b) gives the performance of the student model after distillation in the supervised and fewshot settings. In all cases, distillation significantly improves the student model, and in all-but-one case, learning with CoT outperforms the label-only distillation baseline. While the student model initially fails to perform CoT through prompting (Table 2 (a)) it learns to do so through distillation.\nThe number of samples. In our default setting, to serve as our distillation corpus C, we sample N = 30 rationales from the teacher T for each (unlabelled) training instance. Figure 2 shows the performance of the student model when it is trained on corpora with fewer sampled CoT per instance: results suggest that learning with multiple sampled (albeit nosier) rationales/chain-of-thoughts per example is more beneficial than learning with one (most likely) rationale. Will more rationales bring more performance improvement? We sampled more rationales from GPT-3 to train the student model; however, this does not bring more performance gains. When N = 50, the performance is similar to N = 30: the model achieves 67.0 in accuracy on OpenBookQA (v.s. 67.0), 67.2 on CommonsenseQA (v.s. 67.0), 84.9 on QuaRel (v.s. 83.8).\n\nHuman Evaluations\nWhile SCoTD improves task accuracy significantly, we additionally conduct human evaluations to assess the generated chain-of-thoughts themselves (see Table 1 for samples). We sample instances from the CommonsenseQA, OpenBookQA, and QuaRel validation sets (300 instances per dataset), and conduct head-to-head human evaluations 8 to assess:\nQ1: Does SCoTD result in higher-quality chainof-thoughts? Test: OPT-1.3B versus OPT-1.3B + SCoTD. Result: Yes. We assess this hypothesis on two subsets of instances: 1) a pure random sample (N=900); and 2) a set of instances for which both models eventually predicted the correct label (N=654). The second setting focuses more closely on the chain-of-thoughts themselves rather than the predictive accuracy of the model. SCoTD is superior in both settings: for the random sample setting, SCoTD won in 59% of cases (p<.001), whereas in the correctness controlled setting, SCoTD won in 61% of cases (p<.001). Results hold with p < .05 for each QA dataset individually.\nQ2: Does a SCoTD student surpass the much larger teacher? Test: OPT-1.3B + SCoTD versus text-davinci-002. While the task accuracy of the teacher is still higher in most cases, the studentgenerated CoT are comparable. 9 We again evaluate on: 1) a pure random sample (N=900); and 2) a correctness-controlled setting (N=659). The 100x smaller SCoTD's generations are competitive in both cases; we can't reject the null hypothesis of the crowd having equal preferences (OPT-1.3B + SCoTD wins in 47% and 51% of cases respectively, p > .01). Results hold for each dataset individually, as well.\n\nSelf-Consistency for the Student\nWang et al. (2022b) find that, for chain-of-thought prompted models, taking a majority vote over a large set of sample of predicted labels (resulting from a diverse range of CoTs) can improve performance. Our results regarding the effectiveness of sampling N = 30 rationales from the teacher during SCoTD are similar-in-spirit: i.e., we also show performance gains from sampling multiple rationalization chains per instance.\n9 See \u00a76 for more discussion about the disparity between CoT-quality and task accuracy. A natural question is, does the student model S exhibit the same phenomenon, i.e., can we sample multiple chain-of-thoughts from it and take a majority vote? We find that the student model can benefit from \"self-consistency,\" but not in all cases. In Table 3 , we report the performance with/without self-consistency (majority vote among 30 sampled reasoning paths with a temperature of 0.7). When training with filtered CoTs (Table 3 (a) bottom rows) or training with few CoTs per example (Table 3 (b), when #CoTs/Example is small), the student model does not benefit from self-consistency. Only when we train with multiple rationales per example without filtering (the few-shot setting), self-consistency is beneficial on CSQA and Open-BookQA. Overall, the results show that student models benefit from being shown a diverse/noisy set of rationales, and that self-consistency can be effectively applied after distillation.\n\nSCoTD across Model and Dataset Sizes\nWe also verify the effectiveness of SCoTD across model and dataset sizes; in these experiments, we consider the supervised setting. Data scaling. Figure 3 shows the effect of varying the size of D Train (for simplicity, we show only performance on CSQA as an example). Learning with CoTs is beneficial under all data scales. Interestingly, SCoTD, trained with access to only 40% of the labelled data, can surpass the direct supervised label-only model with 100% of the labelled corpus; this result aligns with the argument in Zaidan et al. (2007) -providing more explanations from the teacher model could be more beneficial than providing more labels.\nStudent model size scaling. Figure 4 presents results when varying the size of the student model from 125M to 1.3B parameters for CSQA. For all model three model sizes, SCoTD outperforms the standard supervised fine-tuning baseline (Label Only). Sampling multiple rationales per input instance is an effective strategy for all model sizes.\n\nSCoTD on Challenging Contrast Sets\nCan learning with explanations help generalization, as hypothesized by (Zaidan et al., 2007) ? As a preliminary study, we show that SCoTD enables better generalization to contrast sets. Contrast sets (Gardner et al., 2020) are proposed to evaluate a model's robustness to perturbations around the decision boundary, by asking annotators to modify the original test instances in small but meaningful ways that (typically) change the gold label.\nWe experiment on the IMDB (Maas et al., 2011 ) sentiment analysis task in the supervised setting; we consider the corresponding contrast set of IMDB proposed by Gardner et al. (2020) . We train two models on the training set of IMDB: Label-Only and SCoTD. For efficiency, we sub-sample 100K examples from the training set of IMDB and truncate input sequences to 700 tokens. As shown in Figure 5 , while both models with/without SCoTD achieve high performance on the original IMDB test set (96.1% v.s. 95.5%, with the Label-Only model performing slightly better), the model with SCoTD achieves significantly higher performance on the contrast set: 92.0% vs. 81.6%. This result supports the hypothesis of (Zaidan et al., 2007) ; that explanations can support more robust generalization.\n\nSCoTD on Unseen, Out-of-domain Tasks\nLarge language models can perform few-shot, incontext learning with chain-of-thought prompting, i.e., generating reasonable chain-of-thoughts on unseen tasks with a few demonstrations (Suzgun et al., 2022) . We conduct a preliminary experiment, inspired by Min et al. (2021) 's MetaICL, to test whether student models trained with SCoTD acquire the same ability. We train a supervised SCoTD model on ANLI, CommonsenseQA, and OpenBookQA, and evaluate it on SST-2 (Socher et al., 2013) , a sentiment analysis task.\nThe SCoTD model achieves a few-shot accuracy of 79.6% on the validation set (an example prediction is shown in Figure 6 ). 10 Compared to a baseline model that learns with no CoT(i.e., a re-implementation of MetaICL trained on 3 source tasks); the baseline fails to recognize the input/output format of the new task and predicts answers out of the desired label set. It achieves (an effective) 0% accuracy on SST-2. This suggests the potential of including CoTs during instruction/incontext tuning (Wei et al., 2022a; Min et al., 2021) .\n\nWhat Factors are Important for\nDistillation?\nAn important factor underlying the performance gains highlighted in \u00a73 was the number of chain-ofthoughts we sampled from the teacher model perinstance (more samples = better; Figure 2 ). Here we ask: is data volume the key contributing factor to the performance improvement? Or, are specific aspects of chain-of-thought samples key for the performance improvements?\nWe design several filters to identify potentially important examples/CoTs among the correct rationales. We apply designed filters (to be introduced) to C \u2032 , the corpus sampled from the teacher (with wrong CoTs dropped), that operationalize different hypotheses about what factors are important to distill. We control for dataset size when filtering, i.e., \n\nLabel Only SCoTD\nThe author said that they love this movie and they are never tired of watching it.\nThey say that the movie is wonderful and they are grateful to see such an outstanding picture. So the answer is: positive\nThis was a wonderfully clever and entertaining movie that I shall never tire of watching many, many times\u2026 I can only be grateful when I see such an outstanding picture for most of the motion pictures made more\nThis was a wonderfully thick as two short planks and soul-destroying movie that I shall never watch any number of times\u2026 I can only be sorry when I see such an abysmal picture just as most of the motion pictures \u2026\n\nIMDB Dataset\nThe author said that the movie was 'thick as two short planks and souldestroying', implying that the movie is bad. So the answer is: negative all filtered corpora have the same number of training CoTs. We downsample with a budget of 5 CoT per instance on average 11 . Then, we train the same student model on each of the filtered corpora, and compare on downstream tasks. If a student model trained on filtered corpus A tends to outperform the student model trained on filtered corpus B, then we argue that the property that produced corpus A is more important. The hypotheses we consider are:\nNull hypothesis: data volume. As a null hypothesis, we randomly sub-sample 5 CoT per instance; this filter operationalizes the assumption that an arbitrary set of samples is sufficient.\n\nDiversity.\nFor each instance, we compute S-BERT (Reimers and Gurevych, 2019) embed-11 In rare cases, we may end up with less as there are less than 5 correct CoTs for the instance. dings 12 of each of the chain-of-thoughts, and cluster the resulting embeddings using hierarchical clustering into k = 5 clusters. Then, we randomly sample a single instance from each cluster: the resulting sample covers all clusters, and thus represents a diverse+representative sample.\nTeacher likelihood. For each instance, we keep the 5 CoT samples with the highest per-token loglikelihood according to the teacher model.\nOpen-endedness. Some instances in each dataset lead to a broader range of chain-of-thought samples than others. For example, on CommonsenseQA, the question \"What form of alcohol is made from grapes?\" leads to a narrower range of rationalizations vs. \"Why might someone purposefully be going into trance?\"\nWe hypothesize that openended instances could benefit from relatively more sampled rationales. We sort instances into quintiles based on the unique bi-grams in their corresponding 30 CoTs; for high-ranking instances (more unique CoT bi-grams, like the \"trance\" example above), we keep more rationales and for low-ranking instances, we keep less rationales. We keep 1, 3, 5, 7, 9 rationales for instances of different bins (thus controlling for the total number of CoT).\nResults Figure 7 reports the accuracy of the student model when fine-tuned on the different subsampled corpora for the three tasks we consider. Overall, random subsampling is a strong baseline, but, we see some evidence that diversity among the rationales is important. None of the models trained on the sub-sampled data could approach the model trained on the full 30x/instance CoT set. This suggests that the sheer volume of the CoTs is a key driving force for the performance improvement.\n\nRelated Work\nChain-of-thought prompting. As an extension of few-shot prompting (Brown et al., 2020) Learning with explanations. Hase and Bansal (2022) discuss how explanations can serve as inputs (Talmor et al., 2020) , targets (Hendricks et al., 2016; Fidler et al., 2017; Camburu et al., 2018; Zhou et al., 2020; Narang et al., 2020; Kayser et al., 2021; Wiegreffe et al., 2022) , and priors (Zhang et al., 2016; Srivastava et al., 2018) for machine learning models. Chain-of-thought extends earlier efforts which treat explanations as intermediate structures, generated at inference time (Rajani et al., 2019) . Most related to our work is Li et al. (2022a) , who do also learn with GPT-3 generated explanations; we show multiple samples improve significantly over their single-sample method, and also use chain-of-thought prompting at inference time vs. predicting explanations+labels via independent multitasking.\nKnowledge distillation. Recent work, inspired by Knowledge Distillation (Hinton et al., 2015) , has considered symbolic knowledge distillation, (West et al., 2022) , i.e., instead of distilling from soft representations like logits, large language model serve as training data generators (Xiong et al., 2019; Petroni et al., 2019; Schick and Sch\u00fctze, 2021; West et al., 2022; Liu et al., 2022; Meng et al., 2022; Bhagavatula et al., 2022) ; this paper continues this line of work.\nContemporaneous work. There are several contemporaneous papers: Huang et al. (2022) , Magister et al. (2022), and Ho et al. (2022) all show that smaller models can benefit from large models' chains of thought. We contributes beyond these by: 1) showing that sampling a large number of chain-of-thoughts is paramount; 2) exploring transfer performance to challenge sets/unseen tasks; and 3) analysis that address what factors are important in the teacher corpus.\n\nConclusion\nWe demonstrate the effectiveness of Symbolic Chain-of-thought Distillation (SCoTD): a method that enables smaller language models to effectively use chain-of-thought-style reasoning. We demonstrate the method's effectiveness across several downstream tasks, different student model sizes, different levels of supervision, and in difficult settings (challenge sets, unseen tasks). Our ablations shed light on what factors are particularly important to distill in these chain-of-thoughts. Our concrete recommendations are: 1) sampling multiple and diverse CoTs for each input instance, and 2) performing self-consistency when the teacher CoTs are noisy. Several promising av-enues for future work include:\n1. Exploring SCoTD for generation tasks in addition to classification tasks;\n2. Scaling up the number of source tasks in \u00a7 3.5 to generalize to more tasks;\n3. Using the down-sampling setup introduced in \u00a74 to explore additional hypotheses about what other factors may be of importance in CoTs.\n"}
{"question": "Which formalism does the paper primarily focus on for modeling discontinuous language phenomena?", "evidence": "  Using the probabilistic linear context-free rewriting system (LCFRS) formalism, our approach fixes the rule structure in advance and focuses on parameter learning with maximum likelihood. We focus on the LCFRS formalism as it has previously been successfully employed for supervised discontinuous constituency parsing... ", "options": ["A. Probabilistic context-free grammars (PCFG)", "B. Tree adjoining grammars", "C. Linear context-free rewriting systems (LCFRS)", "D. Combinatory categorial grammars"], "answer": "C", "content": "\nIntroduction\nUnsupervised parsing aims to induce hierarchical linguistic structures given only the strings in a language. A classic approach to unsupervised parsing is through probabilistic grammar induction (Lari and Young, 1990) , which learns a probabilistic grammar (i.e., a set of rewrite rules and their probabilities) from raw text. Recent work has shown that neural parameterizations of probabilistic contextfree grammars (PCFG), wherein the grammar's rule probabilities are given by a neural network over shared symbol embeddings, can achieve promising results on unsupervised constituency parsing (Kim et al., 2019; Jin et al., 2019 Jin et al., , 2021;; Yang et al., 2021b Yang et al., , 2022)) .\nHowever, context-free rules are not natural for modeling discontinuous language phenomena such as extrapositions, cross-serial dependencies, and Code: https://github.com/sustcsonglin/TN-LCFRS. wh-movements. Mildly context-sensitive grammars (Joshi, 1985) , which sit between context-free and context-sensitive grammars in the classic Chomsky-Sch\u00fctzenberger hierarchy (Chomsky, 1959; Chomsky and Sch\u00fctzenberger, 1963 ), 1 are powerful enough to model richer aspects of natural language including discontinuous and non-local phenomena. And despite their expressivity they enjoy polynomial-time inference algorithms, making them attractive both as cognitively plausible models of human language processing and as targets for unsupervised learning.\nThere are several weakly equivalent formalisms for generating the mildly context-sensitive languages which might serve as potential targets for grammar induction: tree adjoining grammars (Joshi, 1975) , head grammars (Pollard, 1985) , combinatory categorial grammars (Steedman, 1987) , and linear indexed grammars (Gazdar, 1988) . In this paper we work with linear context-free rewriting systems (LCFGS, Vijay-Shanker et al., 1987) , which generalize the above formalisms and are weakly equivalent to multiple context-free grammars (Seki et al., 1991) . Derivation trees in an LCFRS directly correspond to discontinuous constituency trees where each node can dominate a non-contiguous sequence of words in the yield, as shown in Fig. 1 .\nWe focus on the LCFRS formalism as it has previously been successfully employed for supervised discontinuous constituency parsing (Levy, 2005; Maier, 2010; van Cranenburgh et al., 2016) . The complexity of parsing in a LCFRS is O(\u2113 3k |G|), where \u2113 is the sentence length, k is the fan-out (the maximum number of contiguous blocks of text that can be dominated by a nonterminal), and |G| is the grammar size. While polynomial, this is too high to be practical for unsupervised learning on real-world data. We thus restrict ourselves to LCFRS-2, i.e., binary LCFRS with fan-out two, which has been shown to have high coverage on discontinuous treebanks (Maier et al., 2012) . Even with this restriction LCFRS-2 remains difficult to induce from raw text due to the O(\u2113 6 |G|) dynamic program for parsing and marginalization. However Corro (2020) observe that a O(\u2113 5 |G|) variant of the grammar that discards certain rules can still recover 98% of real world treebank constituents. Our approach uses with this restricted variant of LCFRS-2 (see Sec 2.2). Finally, following recent work which finds that that overparameterizing deep latent variable models is beneficial for unsupervised learning (Buhai et al., 2020; Yang et al., 2021b; Chiu and Rush, 2020; Chiu et al., 2021) , we scale LCFRS-2 to a large number of nonterminals by adapting tensor-decomposition-based inference techniques-originally developed for PCFGs (Cohen et al., 2013; Yang et al., 2021b Yang et al., , 2022)) -to the LCFRS case.\nWe conduct experiments German and Dutchboth of which have frequent discontinuous and non-local language phenomena and have available discontinuous treebanks-and observe that our approach is able to induce grammars with nontrivial performance on discontinuous constituents. Rabanser et al., 2017) to decompose the 3D binary rule probability tensor T \u2208 R m\u00d7m\u00d7m as,\n\nApproach\nT = r q=1 u q \u2297 v q \u2297 w q ,\nwhere u q , v q , w q \u2208 R m , r is the tensor rank (a hyperparameter), and \u2297 is the outer product. Letting U, V, W \u2208 R r\u00d7m be the matrices resulting from stacking all u q , v q , w q , Cohen et al. ( 2013) give the following recursive formula for calculating the inside tensor \u03b1 \u2208 R (\u2113+1)\u00d7(\u2113+1)\u00d7m for a sentence of length \u2113:\n\u03b1 L i,j = V \u03b1 i,k , \u03b1 R j,k = W \u03b1 k,j , \u03b1 i,j = U T j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k .\nHere 2021b) and further pre-compute matrices J = V U T , K = W U T to rewrite the above recursive formula as:\n\u03b1 L , \u03b1 R \u2208 R (\u2113+1)\u00d7(\u2113+1\n\u03b1 L i,j = J\u03b1 \u2032 i,j ,\u03b1 R i,j = K\u03b1 \u2032 i,j \u03b1 \u2032 i,j = j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k\nwhere \u03b1 \u2032 \u2208 R (n+1)\u00d7(n+1)\u00d7r is an auxiliary inside score tensor. The resulting complexity of this approach is O(\u2113 3 r + \u2113 2 r 2 ), which is smaller than O(\u2113 3 r + \u2113 2 mr) when r \u226a m, i.e., in the setting with a large number of nonterminals whose probability tensor is of low rank. In this paper we adapt this low rank neural parameterization to the LCFRS case to scale to a large number of nonterminals.\n\nRestricted LCFRS\nIn an LCFRS, a single nonterminal node can dominate a tuple of strings that need not be adjacent in the yield. The tuple size is referred to as the fanout. We mark the fan-out of each non-leaf node in Fig. 1 . The fan-out of an LCFRS is defined as the maximal fan-out among all its nonterminals, and influences expressiveness and parsing complexity. For a binary LCFRS (i.e., LCFRS with derivation rules that have at most two nonterminals on the right hand side) with fan-out k, the parsing complexity for a sentence of length \u2113 is O(\u2113 3k ). 2 In this paper we work with binary LCFRS with fanout 2 (Stanojevi\u0107 and Steedman, 2020, LCFRS-2) , which is expressive enough to model discontinuous constituents but still efficient enough to enable practical grammar induction from natural language data. This choice is also motivated by Maier et al. (2012) who observe that restricting the fan-out to two suffices for capturing a large proportion of discontinuous constituents in standard treebanks. 3 However, LCFRS-2's inference complexity of O(\u2113 6 |G|) is still too expensive for practical unsupervised learning. We thus follow Corro (2020) and discard all rules that require O(\u2113 6 ) time to parse, which reduces parsing complexity to O(\u2113 5 |G|). 4 Formally, this restricted LCFRS-2 is a 6-tuple G = (S, N 1 , N 2 , P, \u03a3, R) where: S is the start symbol; N 1 , N 2 are a finite set of nonterminal symbols of fan-out one and two, respectively; P is a finite set of preterminal symbols; \u03a3 is a finite set of terminal symbols; and R is a set of rules of the following form (where M \u225c N 1 \u222a P):\nS(x) \u2192 A(x) A \u2208 N 1 A(xy) \u2192 B(x)C(y) A \u2208 N 1 , B, C \u2208 M A(yxz) \u2192 B(x)C(y, z) A \u2208 N 1 , B \u2208 M, C \u2208 N 2 A(x, y) \u2192 B(x)C(y) A \u2208 N 2 , B, C \u2208 M\n2 A binary CFG is thus a special case of a binary LCFRS with fan-out one, and parsing in this case reduces to the classic CKY algorithm.\n3 For instance, Stanojevi\u0107 and Steedman (2020) report that LCFRS-2 can cover up to 87% of the gold discontinuous constituents in the NEGRA treebank. We refer readers to Table 1 of Corro (2020) for more details. 4 These correspond to rules (d), (i), (j), (k), and (l) in Figure 3 of Corro (2020) .\n\nItem form:\n[A, i, j]: fan-out-1 node A spanning [i, j)\n[A, i, j, k, n]: fan-out-2 node A spanning [i, j), [k, n) Axioms: [A, i, i + 1], 0 \u2264 i < \u2113 + 1, A \u2208 N 1 Goals: [S, 0, n] Deductive rules: [B, i, k] [C, k, j] [A, i, j] A(xy) \u2192 B(x)C(y) i < k < j 1a [B, i, j] [C, m, n] [A, i, j, m, n] A(x, y) \u2192 B(x)C(y) i < j < m < n 1b [B, m, n] [C, i, m, n, j] [A, i, j] A(yxz) \u2192 B(x)C(y, z) i < m < n < j 2a [B, i, k] [C, k, j, m, n] [A, i, j, m, n] A(xy, z) \u2192 B(x)C(y, z) i < k < j < m < n 2b [B, k, j] [C, i, k, m, n] [A, i, j, m, n] A(yx, z) \u2192 B(x)C(y, z) i < k < j < m < n 2c [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, xz) \u2192 B(x)C(y, z) i < j < m < k < n 2d [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, zx) \u2192 B(x)C(y, z) i < j < m < k < n 2e\nTable 1 : Chart parsing algorithm described in the parsing-asdeduction framework. Here \u2113 is the sentence length and we use interstice indices (not word indices) as in Corro (2020) .\nA(xy, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(yx, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, xz) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, zx) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M T (w) \u2192 w, T \u2208 P, w \u2208 \u03a3.\nHere A(x) indicates that A has a fan-out 1; A(x, y) indicates that A has a fan-out 2 and x and y are nonadjacent contiguous strings in the yield of A. \n\u2192 B(x)C(y, z) above. B is a fan-out-1 node whose yield is x = w i \u2022 \u2022 \u2022 w k\u22121 and C is a fan-out-2 node whose first span is y = w k \u2022 \u2022 \u2022 w j\u22121 and whose second span is z = w m \u2022 \u2022 \u2022 w n\u22121 .\nA is the parent node of B, C, and inherits the yields of B and C, where x is concatenated with y to form a contiguous span and z is a standalone span.\nParsing. Table 1 gives the parsing-asdeduction (Pereira and Warren, 1983 ) description of the CKY-style chart parsing algorithm of our restricted LCFRS-2.\n\nTensor decomposition-based neural parameterization\nWe now describe a parameterization of LCFRS-2 that combines a neural parameterization with tensor decomposition, which makes it possible to scale LCFRS-2 to thousands of nonterminals.\nLet\nm 1 = |N 1 |, m 2 = |N 2 |, p = |P|, and m = m 1 + p.\nThe rules involving A \u2208 N 1 on the left hand side are 1a and 2a , whose probabilities can be represented by 3D tensors\nC 1 \u2208 R m 1 \u00d7m\u00d7m and D 1 \u2208 R m 1 \u00d7m\u00d7m 2 . For A \u2208 N 2 , the relevant rules are 1b , 2b , 2c , 2d , 2e , whose proba- bilities can be represented by 3D tensors C 2 \u2208 R m 2 \u00d7m\u00d7m and D 3 , D 4 , D 5 , D 6 \u2208 R m 2 \u00d7m\u00d7m 2 . We stack D 3 , D 4 , D 5 , D 6 into a single 4D tensor D 2 \u2208 R m 2 \u00d7m\u00d7m 2 \u00d74\nto leverage the structural similarity of these rules. Since these tensors are probabilities, we must have\nEQUATION\nEQUATION\nTensor decomposition. To scale up the LCFRS-2 to a large number of nonterminals, we first apply CPD on all the binary rule probability tensors,\nC 1 = r 1 \u22121 q=0 U 1 :,q \u2297 V 1 :,q \u2297 W 1 :,q C 2 = r 2 \u22121 q=0 U 2 :,q \u2297 V 2 :,q \u2297 W 2 :,q D 1 = r 3 \u22121 q=0 U 3 :,q \u2297 V 3 :,q \u2297 W 3 :,q D 2 = r 4 \u22121 q=0 U 4 :,q \u2297 V 4 :,q \u2297 W 4 :,q \u2297 P :,q\nwhere U :,q denotes the q-th column of U . The dimensions of these tensors are\nU 1 \u2208 R m 1 \u00d7r 1 , V 1 , W 1 \u2208 R m\u00d7r 1 , U 2 \u2208 R m 1 \u00d7r 2 , V 2 \u2208 R m\u00d7r 2 , W 2 \u2208 R m 2 \u00d7r 2 , U 3 , W 3 \u2208 R m 2 \u00d7r 3 , U 4 , W 4 \u2208 R m 2 \u00d7r 4 , V 3 \u2208 R m\u00d7r 3 , V 4 \u2208 R m\u00d7r 4\n, and P \u2208 R 4\u00d7r 4 . Here r 1 , r 2 , r 3 , r 4 are the ranks of the tensors that control inference complexity. To ensure these factorizations lead to valid probability tensors, 1), we additionally impose the following restrictions: (1) all decomposed matrices are non-negative; (2) P, V i , W i are column-wise normalized where i \u2208 {1, 2, 3, 4};\n(3) \u2200i, j U 1 ij + k U 2 ik = 1; and (4) \u2200i, j U 3 ij + k U 4 ik = 1.\nIt is easy to verify that Eq. 1 and 2 are satisfied if the above requirements are satisfied.\nRank-space dynamic programming. For unsupervised learning, we need to compute the marginal likelihood of a sentence p(w 1 w 2 \u2022 \u2022 \u2022 w \u2113 ). We give the rank-space dynamic program (i.e., the inside algorithm) for computing p(w\n1 w 2 \u2022 \u2022 \u2022 w \u2113 ) in this tensor decomposition-based LCFRS-2 in App. A.\nThe resulting complexity is dominated by O(\u2113 5 r 4 + \u2113 4 (r 3 +r 4 )(r 2 +r 4 )). We thus set r 4 to a very small value, which greatly improves runtime.\nParameterization. Following prior work on neural parameterizations of grammars (Jiang et al., 2016; Kim et al., 2019) , we parameterize the component matrices to be the output of neural networks over shared embeddings.\nThe symbol embeddings are given by: E 1 \u2208 R m\u00d7d where the first m 1 rows correspond to fanout-1 nonterminal embeddings and the last p rows are the preterminal embeddings; E 2 \u2208 R m 2 \u00d7d for the fan-out-2 nonterminal embedding matrix; r \u2208 R d for the start symbol embedding. We also have four sets of \"rank embeddings\"\nR 1 \u2208 R r 1 \u00d7d , R 2 \u2208 R r 2 \u00d7d , R 3 \u2208 R r 3 \u00d7d\n, and R 4 \u2208 R r 4 \u00d7d . Given this, the entries of the U, V, W matrices are given by,\nU o ij \u221d exp{(R o j ) \u22a4 f o U (E 1 i )}, o \u2208 {1, 2} U o ij \u221d exp{(R o j ) \u22a4 f o U (E 2 i )}, o \u2208 {3, 4} V o ij \u221d exp{(R o j ) \u22a4 f o V (E 1 i )}, o \u2208 {1, 2, 3, 4} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 1 i )}, o \u2208 {1, 2} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 2 i )}, o \u2208 {3, 4} where f o U , f o V , f o W are one-layer ReLU MLPs with output size d. U o , V o , W o\nare normalized according to the requirements described in the previous subsection. We share the parameters of the following MLP pairs:\n(f 1 U , f 2 U ), (f 3 U , f 4 U ), (f 1 V , f 3 V ), (f 2 V , f 4 V ), (f 1 W , f 3 W ), (f 2 W , f 4 W )\nas they play similar roles (e.g., f 1 V and f 3 V are both applied to left children). For the D 2 tensor we also require the matrix P \u2208 R 4\u00d7r 4 , and this is given by P \u22a4 = f P (R 4 ), where f P is a one-layer residual network with output size 4 that is normalized via a softmax across the last dimension.\nFinally, for the starting and the terminal distributions we have\ns = f s (r), Q = f Q (E 1 m 1 :\n), which results in s \u2208 R m 1 (i.e., the probability vector for rules of the form S \u2192 A) and Q \u2208 R p\u00d7v (i.e., probability matrix for rules of the form T (w) \u2192 w). Here E 1 m 1 : is the last p rows of E 1 , and f s and f Q are residual MLPs with softmax applied in the last layer to ensure that s and Q are valid probabilities.\nDecoding. While the rank-space inside algorithm enables efficient computation of sentence likelihoods, direct CKY-style argmax decoding in this grammar requires instantiating the full probability tensors and is thus computationally intractable. We follow Yang et al. (2021b) and use Minimal Bayes Risk (MBR) decoding (Goodman, 1996) . This procedure first obtains the posterior probability of each span's being a constituent via the inside-outside algorithm (which has the same complexity as the inside algorithm). Then, these posterior probabilities are used as input into CKY in a grammar that only has a single nonterminal. The complexity of this approach is thus independent of the number of nonterminals in the original grammar, and takes O(\u2113 5 ). This strategy can be seen as finding the tree that has the largest number of expected constituents (Smith and Eisner, 2006) . See App. A for details.\n\nEmpirical Study\nData. We conduct experiments with our Tensor decomposition-based Neural LCFRS (TN-LCFRS) on German and Dutch, where discontinuous phenomena are more common (than in English). For German we concatenate TIGER (Brants et al., 2001) and NEGRA (Skut et al., 1997) as our training set, while for Dutch we use the LASSY Small Corpus treebank (van Noord et al., 2013) . The data split can be found in App. B.1. For processing we use disco-dop 5 (van Cranenburgh et al., 2016) and discard all punctuation marks. We further take the most frequent 10,000 words for each language as the vocabulary, similar to the standard setup in unsupervised constituency parsing on PTB (Shen et al., 2018 (Shen et al., , 2019;; Kim et al., 2019) .\nGrammar size. To investigate the importance of using a large number of latent variables (which has previously been shown to be helpful for structure induction (Buhai et al., 2020; Yang et al., 2021b )), we train TN-LCFRSs of varying sizes. We first choose the number of preterminals |P| \u2208 {45, 450, 4500} and set the number of fan-out one and fan-out two nonterminals to be\n|N 1 | = |N 2 | = 1 3 |P|.\nThe rank of the probability tensors are set to r 1 = r 3 = 400, r 2 = r 4 = 4, and the dimensionality of the Baselines. Our baselines include: the neural PCFG (N-PCFG) and the compound PCFG (C-PCFG) (Kim et al., 2019) , which cannot directly predict discontinuous constituents 6 but still serve as strong baselines for overall F1 since the majority of spans in these treebanks are continuous; and their direct extensions, neural LCFRS (N-LCFRS) and compound LCFRS (C-LCFRS), which do not employ the tensor-based low-rank factorization. These non-low-rank models have high computational complexity and hence we set |P| = 45 for these models. When |P| = 4500, we also compare against the tensor decompositional-based neural PCFG (TN-PCFG) from Yang et al. (2021b) .\nEvaluation. We use unlabeled corpus-level F1 to evaluate unsupervised parsing performance, reporting both overall F1 and discontinuous F1 (DF1). For all experiments, we report the mean results and standard deviations over four runs with different random seeds. See App. B.2 for further details.\n\nMain results\nTable 2 shows the main results. With smaller grammars (|P| = 45), we find that both neural/compound LCFRSs have lower F1 than their PCFG counterparts, despite being able to predict discontinuous constituent spans. On the other hand, TN-LCFRS achieves better F1 than N-LCFRS even though it is a more restricted model (since it assumes that the rule probability tensors are of low rank), showing the benefits of parameter sharing through low rank factorizations. As we scale up TN-LCFRSs with |P| \u2208 {45, 450, 4500} we observe continuous improvements in performance, with TN-LCFRS 4500 achieving the best F1 and DF1 on all three datasets. These results all outperform trivial (left branching, right branching, and random tree) baselines.\nAs an upper bound we also train a supervised model with TN-LCFRS 4500 . 7 We also show the maximum possible performance with oracle binary trees with this optimal binarization. While the discontinuous F1 of our unsupervised parsers are nontrivial, there is still a large gap between the unsupervised and supervised scores (and also between the supervised and the oracle scores), indicating opportunities for further work in this area.\n\nAnalysis\nRecall by constituent label. Table 3 shows the recall by constituent tag for the different models averaged over four independent runs. Overall the unsupervised methods do well on noun phrases (NP), prepositional phrases (PP) and proper nouns (PN), with some of the models approach the supervised baselines. Verb phrases (VP) and adjective dynamic programming to sum out all possible nonterminals for each node, resulting in the joint log probability of unlabeled binarized tree and sentence. This was then maximized during training. As for the oracle bound, we emphasize that the gold trees are nonbinary while our model can only predict binary trees. Approximation error. Approximation error in the context of unsupervised learning arises due to the mismatch between the EM objective (i.e., log marginal likelihood) and structure recovery (i.e., F1), and is related to model misspecification (Liang and Klein, 2008) . Figure 2 (left column) plots the training/dev perplexity as well as the dev F1/DF1 as a function of the number of epochs. We find that larger grammars result in better performance in terms of both perplexity and structure recovery, which ostensibly indicates that the unsupervised objective is positively correlated with structure induction performance. However, when we first perform supervised learning on the log joint likelihood and then switch to unsupervised learning with log marginal likelihood (Figure 2 , right), we find that while perplexity improves when we switch to the unsupervised objective, structure induction performance deteriorates. 8 Still, the difference in F1 before and after switching to the unsupervised objective is less for larger models, confirming the benefits of using larger grammars.\n\nEven more restricted LCFRS formalisms.\nThere are even more restricted versions of LCFRSs which have faster parsing (e.g. O(\u2113 3 ), O(\u2113 4 )) but 8 It is worth noting that the phenomenon of mismatch between log marginal likelihood objective and parsing accuracy is quite common in unsupervised grammar induction (and latent variable modeling approaches to structured induction more generally). Many previous works have observed this phenomenon, e.g., Merialdo (1994) in the context of HMMs, and Johnson et al. (2007) and Liang and Klein (2008) in the context of PCFGs. This is partially attributed to the fact that generative grammars often make some unreasonable independence assumptions to make the training process tractable, which does not fully comply with the true generative process of human languages and their underlying structures. 45.4 0.9 44.5 0.5\n\nModel\nTable 5 : Ablation studies on the German (TIGER) treebank.\ncan still model discontinuous constituents. In the supervised case, these restricted variants have been shown to perform almost as well as the more expressive O(\u2113 5 ) and O(\u2113 6 ) variants (Corro, 2020) .\nIn the unsupervised case however, we observe in Table 5 that disallowing O(\u2113 5 ) rules ( 2b , 2c , 2d , 2e ) significantly degrades discontinuous F1 scores. We posit that this phenomena is again related to empirical benefits of latent variable overparameterization-while in theory it is possible to model most discontinuous phenomena with more restricted rules, making the generative model more expressive via \"overparameterizing\" in rule expressivity space (i.e., using more flexible rules than is necessariy) empirically leads to better performance.\nParameter sharing. As shown in Table 5 , it was important to share the symbol embeddings across the different rules. Sharing the parameters of the MLPs as described in Sec. 2.3 was also found to be helpful. This highlights the benefits of working with neural parameterizations of grammars which enable easy parameter sharing across rules that share symbols and/or have similar shapes.\nQualitative analysis. In Fig. 3 , we show some examples trees in German. For each sentence, we show the gold, TN-LCFRS 4500 , and TN-PCFG 4500 trees. In the first sentence, the crossing dependency occurs due to the initial adverb (\"So\")'s being analyzed as a dependent of the non-finite verb phrase at the end of the sentence which occurs due to German V2 word order. Our parser correctly predicts this dependency, although the subject NP (which itself is correctly identified) has the wrong internal structure. relative clause a part of the non-finite verb complex, which does not conform to the annotation guidelines but resembles an alternative analysis that has been proposed for extraposed relative clauses (Baltin, 1983) . Sentence initial adverbs in the context of auxiliary verb constructions and right-extraposed relative clauses describe two common instances of discontinuous phenomena in German. Wh-questions constitute another potential class of discontinuous phenomena; however, these are not treated as discontinuous in TIGER/NEGRA. See App. D for more examples trees (including on Dutch).\n\nRelated work\nMildly context-sensitive grammars. Given the evidence against the context-freeness of natural language (Shieber, 1985) , mildly context-sensitive grammars such as tree adjoining grammars were thought to be just flexible (but still constrained) enough to model natural language (Joshi, 1985) . Prior work on inducing mildly context-sensitive grammars has generally focused on combinatory categorial grammars (Bisk and Hockenmaier, 2012, 2013) , and we are unaware of any work on in-ducing LCFRSs from observed yields alone. Our work is also related to the rich line of work on supervised discontinuous parsing (Kallmeyer and Maier, 2010; Maier et al., 2012; Maier, 2015; Corro, 2020; Vilares and G\u00f3mez-Rodr\u00edguez, 2020; Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez, 2020 , 2021 , 2023) , though we are unaware of any prior work on unsupervised discontinuous parsing.\nNeural grammars. Early work on probabilistic approaches to grammar induction was largely negative (Lari and Young, 1990; Carroll and Charniak, 1992) . However, recent work has shown that neural parameterizations of classic grammars can greatly improve structure induction. Our work adds to the line of work on neural parameterizations of dependency models (Jiang et al., 2016; Han et al., 2017; He et al., 2018; Yang et al., 2020) , context-free grammars (Kim et al., 2019; Jin et al., 2019; Zhu et al., 2020; Yang et al., 2021a) , and synchronous grammars (Kim, 2021; Wang et al., 2022; Friedman et al., 2022) . Neural parameterizations make it easy to share parameters and condition on additional side information (images/audio/video) which has shown to be particularly useful for multimodal grammar induction (Zhao and Titov, 2020; Jin and Schuler, 2020; Su et al., 2021; Hong et al., 2021; Zhang et al., 2021) .\nScaling latent variable models. Buhai et al. (2020) study the empirical benefits of overparameterization in learning latent variable models. Other works have explored parameterizations of latent variable models that make it especially amenable to scaling (Chiu and Rush, 2020; Chiu et al., 2021; Yang et al., 2021b Yang et al., , 2022)) . Relatedly, Peharz et al. (2020) and Liu et al. (2022) show the benefits of scaling probabilistic circuits (Choi et al., 2020) .\n\nConclusion\nThis work studied unsupervised discontinuous constituency parsing with mildly context-sensitive grammars, focusing on the formalism of linear context-free rewriting systems. By using a tensor decomposition-based neural parameterization of linear context-free rewriting systems, our approach was able to induce grammars that had nontrivial discontinuous parsing performance on German and Dutch. Whether even more expressive grammars will eventually lead to models learn linguistically meaningful structures and are at the same time competitive with pure neural language models (as a language model) remains an open question.\n"}
{"question": "Which is not the strategy  we propose for improving the training of our approach?", "evidence": "   In this part, we further propose several strategies for improving the training of our approach.\nQuery Augmentation. Generating pseudo queries is proven to be effective in improving the performance of model-based retrieval (Wang et al., 2022; Zhuang et al., 2022) . Here, we utilize query generation for constructing the training data for passage generation. Specifically, we take the passage collection as the corpus, and use an existing query generation model (i.e., DocT5query (Nogueira et al., 2019) ) trained on the labeled dataset to generate multiple pseudo queries for each passage in the corpus. Following DSI-QG (Zhuang et al., 2022) , we use the top-k sampling strategy for query generation, and set k up to 20. The generated pseudo queries and their corresponding passages are then used to construct query-passage pairs as the training data for the passage generation model. Such a query augmentation method can significantly increase the availability of training data, and also enhance the generalization capability of the model for different queries.\nReducing the Passage Length. Since passages are much longer than URLs, passage generation is more complicated than URL generation. In the generation task, a more extensive generation target results in larger search space, which typically leads to a decrease in efficiency and effectiveness. ", "options": ["A. Manual input data", "B. Reducing the Passage Length.", "C. Query Augmentation.", "D. top-k sampling strategy"], "answer": "A", "content": "\nIntroduction\nInformation retrieval systems have undergone continuous development over the past few decades, with the aim of obtaining relevant resources, such as documents, in response to a user query from a vast collection. With the recent success of Pretrained Language Models (PLMs) (Devlin et al., 2019; Raffel et al., 2020; Zhao et al., 2023) , researchers have developed PLM-based dense retrievers (Lin et al., 2021; Zhao et al., 2022) , which utilize dual-encoders and nearest neighbor search index for retrieval and achieve significant improvements over sparse retrievers.\nMore recently, a new retrieval paradigm, known as model-based retrieval (Tay et al., 2022; Zhou et al., 2022c) , has been introduced by developing an alternative architecture for retrieval. In contrast to traditional retrieval methods, it does not explicitly maintain a corpus index, thereby simplifying the classic index-retrieve-rerank process. Typically, a model-based retrieval system is built based on a sequence-to-sequence generation model with an encoder-decoder architecture, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) . It accepts a query as input and directly generates the corresponding document identifier via the generation model.\nDespite its attractive benefits in simplifying the retrieval pipeline, model-based retrieval still faces following major challenges.\n\u2022 Firstly, since the retrieval task is framed as a prediction task of document identifiers, making it crucial to design document identifiers that are well-suited to the underlying generative PLM. However, this issue is rarely discussed in prior research, and most existing approaches employ manually or randomly constructed identifiers (i.e., docids) as generation targets. Such docids are not adequately captured in the pretraining stage of the generative PLM, thus limiting PLM's capabilities for generative prediction (e.g., unseen docids during pre-training). This creates a discrepancy between the pre-training and fine-tuning phases.\n\u2022 Secondly, there is a discrepancy between training and inference in the single-model generative architecture. While most existing studies incorporate multi-task learning (Tay et al., 2022) and auxiliary pre-training tasks (Zhou et al., 2022b) to model both documents and queries during training, the model only processes queries dur- ing inference, resulting in a gap between the training and inference stages.\nTo this end, in this paper, we propose a novel TwO-stage Model-based rEtrieval approach, TOME (as illustrated in Figure 1 ), which makes two major technical contributions.\n\u2022 Firstly, we suggest using tokenized URLs (or URIs) as text identifiers, which are widely available for web pages or Wikipedia pages 1 . By using URL-based identifiers, the tokenized symbols are well aligned with the vocabulary of the generative PLM, thereby enhancing the generative capacity of the PLM. URLs are typically comprised of normal text, as opposed to manually or randomly constructed identifiers. As a result, such an identifier design can be used to help alleviate the gap between pre-training and fine-tuning.\n\u2022 Secondly, our approach decomposes the prediction task into two consecutive stages, namely passage generation and URL generation, which are fulfilled by two separate T5-based generation models, respectively. The first stage aims to generate a relevant passage in the corpus based on the query, while the second stage aims to generate the corresponding URL of the generated passage from the first stage. This two-stage architecture can reduce the discrepancy between training and inference. In addition, the entire generation process is progressive. Consequently, the second stage is capable of tolerating errors that may be introduced by the preceding stage and generates correct URLs.\nMoreover, we discover that optimizing modelbased retrieval becomes a challenging task when dealing with a vast corpus. As a result, we propose a number of improved training strategies to optimize the generation models, including query augmentation, passage length reduction, and model scaling.\nTo verify the effectiveness of TOME, we conduct extensive experiments on the publicly available MS MARCO and NQ datasets. Experimental results demonstrate the effectiveness of the proposed method, including the URL identifier design and the two-stage generation process. Additionally, case studies indicate that the second stage can tolerate errors induced by the first stage. Furthermore, we investigate the scaling laws of TOME by examining different model sizes, corpus sizes, and text lengths. We anticipate that these experimental results will facilitate further research on model-based retrieval.\n\nRelated Works\nText Retrieval. Text retrieval endeavors to find textual information related to a query from a large candidate corpus. Early studies on sparse retrieval focused on term matching by utilizing sparse representations and inverted indices, such as BM25 (Robertson et al., 2009) . In recent years, with the resurgence of neural networks and the emergence of pre-trained language models (PLMs) (Devlin et al., 2019; Raffel et al., 2020) , dense retrieval achieves better performance beyond traditional sparse retrieval on multiple tasks (Khattab and Zaharia, 2020; Karpukhin et al., 2020; Xiong et al., 2021; Qu et al., 2021) . The dense retrieval and the technique of approximate nearest neighbor search have been widely adopted in various applications (Oguz et al., 2020; Ren et al., 2021a,b; Asai et al., 2021; Ren et al., 2022; Zhou et al., 2022a) . Recently, Zhao et al. (2022) have made a very comprehensive survey about the recent progress of dense retrieval based on PLMs, and we refer the readers to this survey paper for more details.\nModel-based Retrieval. Both sparse retrieval and dense retrieval rely on explicit indices. Recently, researchers have proposed model-based retrieval (a.k.a., generative retrieval) models (Metzler et al., 2021; Tay et al., 2022) . These methods consider model parameters as retrieval indices and directly generate the identifiers of related documents. Such an idea is initially proposed for entity retrieval (Cao et al., 2021) , which autoregressively generates unique entity identifiers. Following this approach, researchers have introduced sequenceto-sequence encoder-decoder architecture for document retrieval (Zhou et al., 2022c; Bevilacqua et al., 2022; Zhuang et al., 2022; Wang et al., 2022; Lee et al., 2022; Chen et al., 2022; Zhou et al., 2022b) . As discussed in the previous section, there still remain issues with model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. Our work tries to deal with these issues with a two-stage generation architecture with URL identifiers.\n\nApproach\nIn this section, we first introduce the task formulation, followed by the description of the proposed two-stage generation approach TOME.\n\nTask Formulation\nIn this work, we consider the task of text retrieval, which aims to find relevant text resources (e.g., documents) related to a query from a large corpus. We further assume that these texts can be accessed by an associated URL 2 (or URI).\nTo develop our approach, we adopt the recently proposed model-based paradigm for text retrieval (Tay et al., 2022; Zhuang et al., 2022) . For retrieval, a model-based retrieval model takes a query q as input and uses the text-to-text model to generate the identifier y (length n) of the relevant document in an autoregressive manner, with the conditional probability:\nEQUATION\nwhere y i denotes the i-th output token in the identifier y, y <i denotes the previous tokens y 1 , . . . , y i\u22121 , and M represents the PLM. The identifier can be an atomic token or a string (Tay et al., 2022) . In our setting, it is assigned to an associated URL of a text (refer to Section 3.2.1). Typically, a generative pre-trained language model (PLM) with an encoder-decoder architecture is employed to implement the text-to-text model (e.g., T5), which is typically optimized by a cross-entropy loss as follows:\nL(M) = \u2212 log Pr M (y|q) = \u2212 n i=1 log Pr M (y i |y <i , q) . (2)\nThe key to model-based retrieval is to design a generative architecture that employs suitable document identifiers, and to develop effective training methods that can effectively associate queries to the identifiers of documents. Next, we expound our approach in detail.\n\nModel Architecture\nIn this section, we first introduce the design of document identifiers, and then present the two-stage generation architecture.\n\nIdentifier Design\nExisting studies typically use docids to represent a document (Tay et al., 2022; Zhuang et al., 2022) . These docids are often randomly generated or manually constructed, which may not exist in realworld text corpora. However, the generative PLM is pre-trained based on large-scale text corpora, leading to a discrepancy between pre-training and fine-tuning.\nDifferent from previous approaches, we consider a tokenized form of URLs as the docids. We directly treat the URL as a text string and tokenize it into a sequence of tokens using a T5 tokenizer. For instance, a sample URL 'https://en.wikipedia.org/wiki/Nevada' can be tokenized to {'https', '://', 'en', '.', 'wikipedia', '.', 'org', '/', 'wiki', '/', 'N', 'e', 'vada'}. We use the token sequence as the prediction target of the generative PLM, following the generation formula of Equation (1). It is worth noting that Ultron (Zhou et al., 2022b ) also uses URLs as identifiers, where a URL is reversed and only used as part of an identifier (also involving titles and domains). As a comparison, we solely utilize tokenized URLs as the identifier, without any additional processing.\nCompared to non-linguistic docids, URLs typically contain more meaningful tokens in the form normal text and widely exist in real-world text corpora, making them more suitable to modeling and prediction using generative PLMs. During decoding, we can directly adopt the general text decoding method to generate the URL, without resorting to limited search strategies such as constrained beam search (Tay et al., 2022; Bevilacqua et al., 2022) . Since these tokenized symbols often overlap among different URLs (e.g., web pages from the same domains), they naturally derives semantic strings as the clustering method in DSI (Tay et al., 2022) .\n\nTwo-stage Generation Architecture\nThe objective of the generative model for retrieval is to establish a correlation between a query and its corresponding docid (i.e., URL). However, owing to the scarcity of annotated data, various improved strategies such as multi-task learning (Tay et al., 2022) or pre-training (Zhou et al., 2022b) have been proposed. Typically, a model processes both documents and queries during training, while it processes only queries during inference, resulting in the discrepancy between training and inference. To tackle this issue, we propose a two-stage generation approach with two different generation models: one for passage generation and the other for URL generation, as shown in Figure 1 .\nPassage Generation. In the first stage, we employ a T5-based passage generation model to map an input query to the passage content according to Equation (1). The generated passage is anticipated as a relevant passage in the corpus that can provide an answer to the query. The objective of the passage generation model is to memorize the passages in the corpus, so as to generate the passages with utmost precision. It is trained with query-passage pairs, where each pair comprises a query and a passage from the document, along with the corresponding labeled URL. Different from existing methods (Tay et al., 2022; Bevilacqua et al., 2022) , we do not utilize any data structure to restrict the decoding process and simply use greedy search to generate an individual result for a query in an autoregressive manner, which has a high decoding efficiency. By incorporating the intermediate passage generation, our approach can mitigate the training-inference discrepancy that the query encoder also needs to process documents (Tay et al., 2022) . URL Generation. In the second stage, another T5-based PLM is employed to predict the corresponding URL as the retrieval result, utilizing the passage generated by the passage generation model as input. The URL is generated by means of greedy search decoding in a similar manner as in Equa-tion (1). The URL generation model is trained with passage-URL pairs, where each pair comprises a passage and its corresponding URL. The objective of the URL generation model is to memorize all the URLs in the corpus, so as to map a generated passage related to a query to a corresponding URL. Meanwhile, even if the generated passages contain some irrelevant content or noise, this stage can still make reliable predictions since it can employ long passages as the context, rather than short queries.\nOverall, such a two-stage generation approach can more effectively capture the semantic relatedness between queries and identifiers by both reducing the training-inference discrepancy and enriching the generation context, which is specifically tailored for model-based retrieval.\n\nTraining\nFor both the passage generation model and the URL generation model, we optimize them independently by utilizing the cross-entropy loss for optimizing standard T5 models, as shown in Equation (2). Nevertheless, optimizing model-based retrieval approaches (Zhuang et al., 2022; Wang et al., 2022 ) is a challenging task as they essentially require memorizing the corpus information, and generating long text also poses challenges in model convergence. In this part, we further propose several strategies for improving the training of our approach.\nQuery Augmentation. Generating pseudo queries is proven to be effective in improving the performance of model-based retrieval (Wang et al., 2022; Zhuang et al., 2022) . Here, we utilize query generation for constructing the training data for passage generation. Specifically, we take the passage collection as the corpus, and use an existing query generation model (i.e., DocT5query (Nogueira et al., 2019) ) trained on the labeled dataset to generate multiple pseudo queries for each passage in the corpus. Following DSI-QG (Zhuang et al., 2022) , we use the top-k sampling strategy for query generation, and set k up to 20. The generated pseudo queries and their corresponding passages are then used to construct query-passage pairs as the training data for the passage generation model. Such a query augmentation method can significantly increase the availability of training data, and also enhance the generalization capability of the model for different queries.\nReducing the Passage Length. Since passages are much longer than URLs, passage generation is more complicated than URL generation. In the generation task, a more extensive generation target results in larger search space, which typically leads to a decrease in efficiency and effectiveness. While, in our approach, passage generation serves as an indirect step for predicting the URL, so that we consider reducing the passage length for improving the training efficiency. For this purpose, we shorten the maximum truncation length of the passage, from 128 to 32. However, reducing the passage length will probably results in a information loss, thus hurting the generation performance. As the solution, we concatenate the title (a short text) and the shortened passage for enhancing the contained semantics. We also add prompts before titles and passage contents like \"title:\" or \"passage:\" for better generation performance.\nIncreasing Model Scale. Model-based retrieval requires a strong memorization capacity from the generative PLM, especially for our approach that involves a passage generation stage. Besides, scaling up the text corpus will significantly increase the difficulty of corpus memorization, and the PLM with a small parameter scale will have a limited memorization capacity when the data scale reaches a certain level. Considering the two aspects, we scale the model size accordingly and employ a larger PLM when necessary. Specifically, we use T5-large (the first stage is more difficult) and T5base for the two stages of our approach on a small corpus (e.g., subsets of MS MARCO), respectively. Further, we increase them to T5-3B and T5-large accordingly on a large corpus (e.g., the full set of MS MARCO). Besides the improved capacity, we find that using a larger model size is also useful in improving the convergence rate (as detailed in Section 5.4).\n\nExperimental Settings\nThis section describes the major experimental settings, including datasets, evaluation metrics, baselines and implementation details.\n\nDatasets and Evaluation Metrics\nDatasets. We conduct experiments on two public available datasets, namely MS MARCO (Nguyen et al., 2016) Passage Ranking and Natural Questions (NQ) (Kwiatkowski et al., 2019) . (1) MS MARCO contains Bing search queries as well as passages from web documents, making it one of the largest web search datasets to date, with a full corpus of over 8.8 million passages. In addition, we also consider two subsets, each containing 100K and 1M passages, by following (Tay et al., 2022; Zhuang et al., 2022) . Based on the MS MARCO Question Answering dataset, we extract the URLs associated with the passages, selecting a random URL if a passage contains multiple URLs (2) The NQ dataset is a question answering dataset where the query data is collected from Google search logs, and the document data is from Wikipedia. We use the NQ320K version by following NCI (Wang et al., 2022) , which contains 320K labeled querydocument pairs and 100K documents. We collect abstracts of documents as intermediate-generated passages.\nEvaluation Metric. Following previous works, we adopt Hits@1 as the evaluation metric. This metric is calculated as the percentage of queries to which the top-1 generation result is positive. Since the outputs of models at different stages are either passage texts or URL texts, unlike the conventional MS MARCO evaluation by determining whether the retrieved identifiers are in the identifier label list, we evaluate the results by determining whether it is an exact match to the label text.\n\nBaselines\nFor comparison, we chose the following baselines including sparse retrieval, dense retrieval, and model-based retrieval.\nBM25 (Robertson et al., 2009 ) is a classical sparse retriever that uses the inverted index to find relevant passages by term overlap. DPR (Karpukhin et al., 2020) and ANCE (Xiong et al., 2021) are two representative dense retrievers that adopts dual-encoder architecture. For modelbased retrievers, DSI (Tay et al., 2022 ) is a pioneer work for model-based retrieval that uses a sequence-to-sequence model to map the input query to the relevant docid. We use the open-source code released by DSI-QG for reproducing DSI baseline on MS MARCO. SEAL (Bevilacqua et al., 2022) is proposed to generate multiple ngrams for a query with an auxiliary Ferragina Manzini index. DSI-QG (Zhuang et al., 2022) proposes to improve DSI with augmented data constructed by query generation. NCI (Wang et al., 2022 ) also utilizes pseudo queries for improving model-based retrieval with tailored architecture. Due to the different experimental settings of different methods, we copy the performance values for some baselines on NQ in NCI and reproduce all of the baselines on MS MARCO under the same evaluation strategy. All the model-based retrieval baselines adopt the \"large\" version of PLMs.\n\nImplementation Details\nWe conduct our experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) and natural language processing toolkit PaddleNLP (Contributors, 2021) on up to 32 NVIDIA Tesla A100 GPUs (with up to 80G RAM).\nPLM. The generation models adopted in our work are initialized with different parameter scales of T5 (Raffel et al., 2020) . In the passage generation model, we use T5-3B for initialization on MS MARCO Full, and other models are initialized with T5-large. In the URL generation model, we use T5large for initialization on MS MARCO Full, and other models are initialized with T5-base.\nHyper-parameters. We adopt Adam optimizer with a learning rate of 5e-5, and train the models for a maximum of 3M steps with bf16 mixed precision strategy. The batchsize is set up to 128, 384 and 80 for T5-base, T5-large and T5-3B, respectively. The maximal length of queries, passages and URLs are set as 32, 32 and 80, respectively. The warm-up step is set as 100K and 10K for passage and URL generation task, respectively. Query Augmentation. We adopt the existing docT5query-large (Nogueira et al., 2019) model that trained on MS MARCO training set, and generate 20 and 15 queries per passage for MS MARCO and NQ, respectively. For training data, we only use pseudo-labeled data constructed by query generation on MS MARCO, and use both pseudolabeled data and labeled data on NQ.\n\nExperimental Results and Analysis\nIn this section, we report the experimental results of our proposed approach and conduct comprehensive empirical analysis. (Karpukhin et al., 2020) 71.84 52.52 29.54 DSI (Tay et al., 2022) 11.75 --DSI-QG (Zhuang et al., 2022) Table 1 : The Hits@1 results of different methods on variant corpus scales of MSMARCO.\n\nMethods\nHits@1 BM25 (Yang et al., 2017) 15.11 ANCE (Xiong et al., 2021) 52.63 DSI (Tay et al., 2022) 35.60 SEAL (Bevilacqua et al., 2022) 59.93 NCI (Wang et al., 2022) 66.23 DSI-QG (Zhuang et al., 2022) 61.34 Comparison with Model-based Retrievers. We observe that TOME consistently outperforms model-based retrievers on three subsets of MS MARCO and NQ320K datasets, thereby demonstrating the effectiveness of the proposed method. Moreover, NCI is a competitive baseline on NQ320K, which uses tailored decoder architecture, preprocessed semantic docid, and regularization on top of DSI-QG, while our method is simply trained with the standard T5 configuration without any additional processing. We also discover that DSI-QG is unable to effectively converge when trained on the MS MARCO Full. We speculate that random non-linguistic docids become a bottleneck as the corpus scales up, while the loss can normally converge when using normal text (e.g., URL) as a generation target.\nEffect of Two-stage Generation Architecture. By simply substituting the generation target of DSI-QG from random string docids to URLs (singlestage of our method), the performance has been improved (refer to DSI-QG and TOME single-stage in Table 1 and 2 ), indicating that natural language identifiers are more suitable for model-based retrieval tasks than non-linguistic docids. Furthermore, if we employ the two-stage generation that includes an intermediate step to generate passages before generating URLs, the performance will be further improved (refer to TOME single-stage and TOME two-stage in Table 1 and 2 tion demonstrates that integrating passage generation in the process of model-based retrieval leads to better performance.\nComparison with Dense Retrievers. By adopting a series of training strategies, we successfully train TOME on large-scale corpora. However, although TOME outperforms dense retrieval methods on MS MARCO 100K and NQ320K, there still remains a performance gap when compared to DPR on larger corpora such as MS MARCO 1M and Full. This indicates that our method still has gaps compared to advanced dense retrieval methods when the corpus scales up. Since the model-based method necessitates complete memorization of the entire corpus, it inherently possesses a disadvantage in larger-scale corpora when compared to dense retrievers, which needs to be further explored.\n\nAblation Study\nIn this section, we conduct an ablation study to examine the effectiveness of strategies in TOME.\nWe report the results on MS MARCO 100K and NQ320K. Here, we consider three variants based on TOME for comparison: (a) w/o prompt removes the prompts before titles and passages; (b) w/ increased maxlen increases the maximum truncated length of passage from 32 to 128; (c) w/ reduced pseudo query reduces the amount of pseudo query to 10 per passage. Table 3 presents the results for variants of TOME. We can observe the following findings: (a) The performance drops in w/o prompt, demonstrating that adding prompts for identifying the title and passage is helpful for generating better results. (b) The performance drops in w/ increased maxlen, demonstrating that due to various training strategies, shortening the maximum truncated passage length does not bring performance loss but reduces the difficulty of training. (c) The performance drops in w/ reduced pseudo query, demonstrating the effectiveness of generating a large number of pseudo queries for data augmentation.\n\nAnalysis on Two-stage Generation\nIn this section, we investigate the generation results of the passage generation model quantitatively and qualitatively to showcase the superiority of the proposed two-stage generation approach.\n\nQuantitative Analysis\nWe quantitatively analyze the generation results on MSMARCO dev set with the passage generation models trained on MS MARCO 100K.\nFirst, we are surprised to find that on the entire dev set, the proportion of generated passages are the passages exist in the corpus is about 95%. In cases where the model failed to generate labels correctly, about 85% of the generated passages still exist in the corpus. This result indicates that the model is capable of memorizing the corpus precisely and is able to generate a retrieval-like result. Moreover, previous studies of dense retrieval reveal that there are a lot of false negatives in MS-MARCO (Qu et al., 2021) . We also observe that approximately 80% of the generation results that are not labeled as positives but appear in the corpus are false negatives, showing that model-based retrieval suffers from the same issue of false negatives as dense retrieval. Despite this, the passage generation model actually has strong generation capability.\n\nQualitative Analysis\nTo explore the generative capabilities of TOME, we conduct a case study on MSMARCO 100K, utilizing a maximum truncation length of 128 for better illustration.\nTable 4 gives two sampled queries, along with their corresponding label passages, evidence passages (if available) and generated passages. With respect to the first query, the generated passage is not exactly the same as the labeled passage. In comparison with the labeled positive passage, the second half of the generated passage is altered. Despite the alteration in the generation passage, the URL generation model is still able to accurately map it to the correct URL, indicating that the URL generation model can tolerate changes introduced by the passage generation model. In the second example, the model extracts relevant content from both the label passage and the evidence passage, and then combines the contents to create the generated passage. It is interesting Ginger is an analgesic (a pain-killer) that may alleviate the pain associated with a sore throat. It is also a good antibacterial and antifungal and can help fight the infection causing your sore throat. . . . Table 4 : The comparison of the labeled passages and generated passages. The evidence passages are not manually labeled but contain relevant content. The italic words with underline represents the different parts of two passages, the ::::::::::::::::::::::::::\nitalic words with wavy underline and bold words with underline in different passages represent the reference parts. to observe that the passage generation model is capable of summarizing multiple passages.\n\nAnalysis on Scaling\nWe observe that long text generation poses a challenge to the convergence of loss, so we investigate the training efficiency and capability of the model under varying conditions. In particular, we use the same computing resource and conduct training on the passage generation stage (i.e., the first stage) of TOME. Considering that the trend is similar in the second stage, it has been omitted here due to limited space.\nEffect on Data Scale. We investigate the impact of expanding the corpus on model training and examine whether the model capacity is insufficient when dealing with a large corpus. We fix the T5-large model and conduct training on MSMARCO 100K, 1M and Full datasets, respectively, without shortening the length of passages. We use perplexity (PPL) to estimate the model capacity and monitor how perplexity changes as training steps increase. The results are shown in Figure 2 (a). It can be observed that the perplexity of the T5-large model fails to converge to a lower level after corpus scale expansion, which illustrates that under this task, a certain amount of data will lead to the capacity bottleneck of the model. In addition, the decline rate of perplexity slows down on larger corpora, indicating that models with the same parameter size have low learning efficiency on a large-scale corpus.. Effect on Passage Length. In order to investi-gate the effect of reducing the length of generated passages, we fixed the model as T5-large, and conducted experiments on passages with different maximum truncated lengths as generation targets on MSMARCO 1M. Figure 2 shows that after reducing the maximum truncated length of the generated passage, the perplexity significantly decreases, indicating that such a strategy is beneficial to mitigate the difficulty of the passage generation task. Moreover, the model exhibited enhanced efficiency when generating shorter passages.\n\nConclusion\nIn this paper, we introduce TOME, a innovative two-stage model-based retrieval approach. To implement our approach, we make two major technical contributions in the design of the identifier and the architecture of two-stage generation. Moreover, we also employ a number of training strategies to better optimize our proposed architecture, especially on large-scale corpora. Extensive results demonstrate the effectiveness of TOME. Furthermore, we perform a thorough analysis and summarize the scaling law for the proposed method. We believe such an idea itself is worthwhile for exploring in designing new model-based retrieval architecture.\n"}
{"question": "In the study on Performance Analysis and Ablation Studies, what conclusion can be drawn about the importance of different components in the CMIN model's performance?", "evidence": "  Finally, better performance achieved when causality matrix and multi-directional interactions are introduced into the network. The ablation studies show that every component makes an important contribution to CMIN, and as a result the full model with all components achieves the best performance.  ", "options": ["A. CMIN-TE outperforms all other variants.", "B. CMIN-PR has the highest MCC among all variants.", "C. CMIN-CM performs equally well as CMIN-PR.", "D. The full CMIN model with all components can get best results."], "answer": "D", "content": "\nIntroduction\nFinancial services, known for their competitiveness, have always been at the forefront of adopting data science techniques to drive investment decisions. Quantitative trading, a specific field within it, has drawn immense interest from both academia and industry over the last few decades. With the rapid advancements in deep learning recently, computer scientists and quantitative researchers have joined forces to apply AI techniques to tackle the challenges within this domain.\nAmong various tasks, one of the most prominent is stock price movement prediction (Bhardwaj, 2021) . The reason for its popularity is selfevident: once a model is able to predict future movement with considerable accuracy, numerous trading strategies can be easily built around it.\nRecent studies have shown that deep neural networks are ideal candidates for such prediction models (Yoo et al., 2021; Gunduz, 2021) . Supporters of the efficient-market hypothesis (EMH), which posits that asset prices reflect all available information, tackle the task with price information alone (Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . However, an alternative perspective suggests that additional insights can be gained from analyzing news articles and social media posts, which may hold valuable clues about the future (Hu et al., 2018; Xu and Cohen, 2018; Wang et al., 2019b; Tang et al., 2020) .\nAnother intriguing approach analyzes the relationships between different stocks. Clearly positive and negative correlations, or even non-correlations can be immensely useful in constructing a diversified stock portfolio (Borodin et al., 2003) . Several studies even empirically demonstrate that exploiting correlations can improve the accuracy of stock price movement prediction (Long et al., 2020; Yoo et al., 2021) . However, their correlations are often realized by acquiring industry sector and calculating correlation matrices or attention scores, which are bidirectional and symmetrical, leading to excessive attention on spurious correlations. Due to the lag problem widely existed between two time series, we are more concerned about the dominance of information flow between stocks, specifically, the direction of causality.\nAdditionally, we have observed that the situation can significantly change when incorporating text information. Let's consider two highly correlated companies (A and B) and there is promising news specifically about company A. In such a scenario, it's fairly easy to infer that the current news might still have a substantial impact on company B, despite there being no direct connection between the two companies on paper. However, it's impossible to reach this conclusion by just examining the news about company A or the correlation between A and B alone, which highlights the limitations of relying solely on individual pieces of textual information or traditional correlations between stocks.\nInspired by observations above, we propose the Causality-guided Multi-memory Interaction Network (CMIN), a novel end-to-end deep neural network which captures both financial news as well as the causality-enhanced correlations between stocks for better stock price movement prediction.\nTo achieve this goal, CMIN incorporates two key components: the Text Memory Network and the Stock Correlation Memory Network. Both networks utilize a recurrent neural network with nonlinear combination of memory attentions to generate a global memory abstraction. And we introduce a global causality matrix according to the transfer entropy between stock price time series to guide the abstraction process, forming a Causal Attention mechanism to capture the asymmetric correlations. By considering causality, CMIN goes beyond traditional symmetric correlations and captures the true inter-dependencies between stocks. Furthermore, we employ an attention-based fusion mechanism between the two networks, introducing multi-directional interactions through which CMIN learns not only the self-influence within each network but also the interactive influence between them. It captures the interrelationship between textual information and correlations, enhancing the overall predictive power of CMIN.\nWe further demonstrate the effectiveness of CMIN with experiments conducted on 3 real-world datasets collected from both the U.S. and Chinese markets, where CMIN achieves state-of-the-art prediction accuracy, surpassing existing models in terms of performance.\nTo summarize, our main contributions are:\n\u2022 Proposal of a causality-guided multi-memory interaction network for stock movement prediction which is to our best knowledge the first attempt to simultaneously consider causalityenhanced correlations and textual information to achieve higher prediction accuracy;\n\u2022 Introduction of the attention-based multidirectional interactions, so that CMIN captures not only the self-influence of temporal movements and textual information but also the interactions between these two types of information flows;\n\u2022 Collection and release of two new datasets: one for the U.S. market and another for the Chinese market.\nBoth datasets include comprehensive financial texts and stock price time series data, which are publicly available at https://github.com/ BigRoddy/CMIN-Dataset, facilitating further research and benchmarking in the field.\n\nStock Movement Prediction\nIn traditional trading practices, two main frameworks are commonly used to make predictions on future stock prices (Ferreira et al., 2021) . The first is fundamental analysis, which aims to assess the intrinsic value of a stock by considering various factors related to it as a whole, such as financial statements, industry trends and economic conditions. The other is technical analysis, which operates under the assumption that the market is efficient (i.e., the Efficient Market Hypothesis holds true) and focuses on analyzing only historical and current price patterns in order to predict future movements.\nAlthough both frameworks have been widely adopted by top hedge funds and investment firms, technical analysis has gained more popularity among AI practitioners, many of whom focus on employing long short-term memory networks and other innovative architectures to model stock price history alongside technical analysis indicators (Nelson et al., 2017; Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . This is primarily because processing a single stream of price data is relatively simpler than analyzing and synthesizing a range of diverse data sources with varying frequencies and characteristics.\n\nPredicting with the Help of Text Data\nThe recent advancement of natural language processing (NLP) techniques has opened up new possibilities for analyzing large volumes of text data in the context of stock movement prediction. Many researchers have recognized the potential value of incorporating news articles, analysis, commentaries and even social media posts (Xu and Cohen, 2018) , which are believed to provide valuable insights about the future. Some studies focus solely on textual information. For example, (Hu et al., 2018) leverages attention mechanism at multiple levels within a deep structure to identify the most important news articles and predict price trends. Others adopt a two-step approach. First, they extract features (e.g. investor sentiment) from financial texts. Then they fuse these features with price information to make predictions such as (Li et al., 2017) and (Jin et al., 2020) . This integration of text analysis with quantitative techniques holds promise for enhancing the accuracy and effectiveness of stock movement prediction models.\n\nExploiting the Relations between Stocks\nAnother important trading framework takes advantage of the correlations between different stocks. Portfolio selection, particularly pairs trading, is a well-known and successful trading strategy that exploits the correlated nature of stocks, whether positive or negative. In fact, as early as (Borodin et al., 2003) pointed out that stock correlations based portfolio selection could beat any strategy that relied on predicting trends or specific targets.\nThe incorporation of correlations in stock movement prediction has gained attention in recent years, drawing inspiration from several existing works. For example, (Yoo et al., 2021) utilizes transformer to learn dynamic correlations between stocks in an end-to-end manner. (Long et al., 2020) employs knowledge graphs and graph embedding techniques to model the relationships between stocks. These studies have achieved admirable results, potentially due to effective feature engineering however, because the direct benefit of stock correlations in predicting future prices lacks fundamental logic.\nIn this paper, we propose constructing a single model to handle both textual data and stock correlations simultaneously, aiming to shed light on the success of correlation-based approaches with the help of financial texts. We also introduce a novel causal attention mechanism to interpret the underlying logic behind stock correlations, leveraging transfer entropy to provide insights. We further model the multi-directional interactions between texts and correlations so that we could uncover not only relevant texts for prediction through correlations, but also the hidden stock correlations through texts. By integrating text data and stock correla-tions within a unified model, we aim to provide a comprehensive understanding of the relationship between the two and discover valuable insights for stock movement prediction.\n\nProblem Formulation\nThis paper is dedicated to predict the price movement of a target stock. To this end, we leverage both the correlations between stocks and textual information to make prediction.\nConsider a target stock with numerical features denoted as P target \u2208 R k\u00d7d , where k represents the number of time steps in the monitoring window and d represents the dimension of price features, such as the highest and the closing prices. The prices of n other relevant stocks are denoted as:\nP = {P 1 , P 2 , \u2022 \u2022 \u2022 , P n } \u2208 R n\u00d7k\u00d7d .\nBesides, we have financial documents associated with the target stock, which are represented as\nM = {M 1 , M 2 , \u2022 \u2022 \u2022 , M k } \u2208 R k\u00d7l\u00d7w ,\nwhere l denotes the number of documents in a time step and w is the maximum number of words in a document. In cases where a specific stock has fewer than l documents at a given time step, zero padding values are added to align the lengths. Similarly, if a document contains fewer than w words, zero padding is applied to ensure uniform length across all documents (Ang and Lim, 2022) .\nWe formulate the task as a binary classification problem whose goal is to predict the movement of the target stock at the next time step, denoted as \u0177target . Here, \u0177target = 1 indicates a rise in the price while \u0177target = 0 indicates a fall. (1) The feature embedding module includes two encoders, one for embedding the textual information and another for embedding the price time series. Additionally, a global causality matrix is introduced to capture the asymmetric correlations using transfer entropy, which then guides the calculation of attention weights in the multi-memory networks.\n(2) The multi-memory networks consist of the Text Memory Network and Stock Correlation Memory Network, which are designed to select and re- tain the most relevant and influential information (textual and correlational) for the target stock.\n(3) The multi-directional interaction module facilitates the interaction between the textual and correlational information. This interaction allows the two types of information to reinforce each other and leverage the advantages of different information flows for better prediction performance, enhancing the predictive capabilities of the CMIN.\n\nFeature Embedding\nSelf-attention mechanisms have proven to be effective in capturing long-term dependencies and modeling complex sequential patterns, particularly in the Transformer architecture (Vaswani et al., 2017) . Given the significance of historical information in financial documents and stock prices for stock price movement prediction, we employ attention mechanisms to summarize this information.\n\nText Encoder\nThe Text Encoder focuses on processing the financial documents M to extract meaningful representations for stock movement prediction. We firstly use a popular word representation tool Glove (Li et al., 2018) to generate the word embedding tensor M word \u2208 R k\u00d7l\u00d7w\u00d7dw , where d w is the size of word embeddings. Each word in the financial documents is represented as a d w -dimensional vector.\nThen the word embeddings are passed through a text embedding layer. Here we adopt the bidirectional Gated Recurrent Unit (Bi-GRU) (Li et al., 2022) to capture both preceding and succeeding contexts within each document. The average of the last hidden vectors is taken as the text embeddings M text \u2208 R k\u00d7l\u00d7dm , or equivalently M text \u2208 R s\u00d7dm , where s is the total number of documents in the monitoring window.\nAfter that, the text attention mechanism is applied to summarize all historical documents across time steps. The text embedding of the last time step M text,\u22121 \u2208 R l\u00d7dm , serves as the query matrix, while the entire text embeddings M text \u2208 R s\u00d7dm acts as both the key and value matrices. Soft scaled dot-product attention is used to compute the attention weights, which are then applied to the text embedding to obtain a representation E text \u2208 R l\u00d7dm enhanced by the history state attention:\nE text = softmax( M text,\u22121 M T text \u221a d m )M text . (1)\nThe resulting E text is the textual embedding that contains highly concentrated information from the stock's related texts. This embedding serves as a summary of the historical text data and is used for further processing in the multi-memory networks and multi-directional interaction module of CMIN.\n\nPrice Encoder\nThe Price Encoder is introduced to utilize multivariate features from historical prices and capture their temporal interrelationships. Firstly we employ a feature mapping layer to project them into a latent space of dimension d p , aiming to improve the learning capacity (Yoo et al., 2021) . For target stock price P target \u2208 R k\u00d7d , the historical price embeddings Ptarget \u2208 R k\u00d7dp can be formulated as:\nEQUATION\nwhere\nW t \u2208 R d\u00d7dp , b t \u2208 R dp are parameters.\nMoreover, recognizing that historical patterns can repeat themselves sometimes, we incorporate a multi-head price attention layer to capture each stock's distinctive changing patterns. The price embedding of the target stock at the last time step is donated as P\u22121 target \u2208 R dp . Then we employ the multi-head attention mechanism with the query P\u22121 target and the key/value Ptarget as follows:\nv target = MultiheadAtt( Ptarget , P\u22121 target ) (3)\nv target is a key vector that serves as the initial hidden state for the two memory networks, playing a crucial role in the final prediction. Similarly, we process the remaining stocks and obtain the correlational embedding E corr \u2208 R n\u00d7dp . Notably, the shared parameters across all stocks ensure the stability and generality of the extracted features (Wang et al., 2019a) .\n\nCausality Matrix\nWhen it comes to detecting causal relationships and conducting predictive analysis, transfer entropy, a non-linear generalization of Granger causality (Seth, 2007) , serves as a conceptually neat and mathematically rigorous method. It has been considered as an important tool for causality analysis and successfully applied in diverse domains including financial markets (Sandoval Junior et al., 2015) .\nTransfer entropy is derived from Shannon Entropy: H = \u2212 N i=1 p i log p i . In this context, considering the time series of a stock, we can partition the possible values into different bins and calculate the probabilities at each time step. Transfer entropy from series X to another series Y can be defined as the average amount of information contained in the source X but not contained in Y's past:\nEQUATION\nBased on this principle, for each monitoring window, we calculate the transfer entropy between all stocks using their historical closing prices and generate a transfer entropy matrix, referred to as the Causality Matrix C \u2208 R n\u00d7n , which illustrates the asymmetric flow of information from one stock to another. Specifically, C[i, j] represents the transfer entropy from stock i to stock j, and C[i, j] > C [j, i] indicates that stock i provides more predictive information about the movement of stock j than j to i. This Causality Matrix will next serve as a guide for the memory networks, enabling the identification of causal dependencies between multivariate stocks.\n\nMulti-memory Networks\nWe introduce a Text Memory Network and a Stock Correlation Memory Network (Sukhbaatar et al., 2015) to manage the textual and correlational information separately. They each maintain a continuous representation and update it iteratively using multiple computational steps (hops), ultimately producing a global memory abstraction.\nAs shown in Figure 1 , each layer of the memory network comprises an attention unit and a GRU unit, which receive textual or correlational embeddings as inputs and are supervised by the continuous representation generated in the previous layer. To initialize the continuous representations of each network, we use the target stock vector v target (generated from Eq.3):\nv (0) text = v (0) corr = v target .\n(5)\n\nText Memory Network\nIn each layer h \u2208 [1, H] of the Text Memory Network, we input the textual embeddings E text (Eq.1) and the continuous representation from the previous layer v\n(h\u22121)\ntext . We utilize an attention unit (Eq.3) to identify important information within the textual embeddings. Subsequently, a non-linear GRU cell unit (Xu et al., 2019) acts as an information aggregator, determining the amount of text information to retain:\nv Att(h) text = MultiheadAtt(E text , v (h\u22121) text ), (6)\nwhere v (h\u22121) text is the query matrix and E text represents the raw form of the key and value matrices.\nThen the GRU cell unit updates the current hidden state into the next hidden state and outputs it to the next layer as the new continuous representation:\nv (h) text = GRU (v Att(h) text , v (h\u22121) text ).\n(7)\n\nStock Correlation Memory Network\nThe Stock Correlation Memory Network is employed to dynamically identify stock relationships and update the continuous representation of stock correlations in an intuitive and asymmetric manner. However, the use of unsupervised attention weights in previous models can be problematic as they may be inevitably misled by the dataset bias, resulting in excessive attention on spurious stock correlations. To address this, we introduce extra knowledge in the form of Transfer Entropybased causality to guide the attention weights and mitigate potential confounding effects.\nFor each target stock, we extract a causal vector v causal = C[:, target] from the pre-calculated causality matrix, which quantifies the degree of information flow from other stocks to it. Then we modify the traditional attention mechanism into Causal Attention by incorporating causal guidance:\nS = softmax( QK T \u221a d ), S = f (S, v causal ). (8)\nHere, f is a function that aggregates the attention weight S and the causal vector v causal to produce a causality-guided attention weight S. We use the average aggregation method for simplicity (i.e., f (S, v causal ) = (S + v causal )/2). To better balance them, one can introduce a hyperparameter \u03bb \u2208 [0, 1]. Then f() updates to f (S, v causal ) = \u03bbS + (1 \u2212 \u03bb)v causal . We believe that different degrees of causal attention can impact the model's performance, and leave it for future exploration. The continuous representation is gradually updated through the Causal Attention, indicating the influence of causal relationships on movement prediction and the self-influence on the flow of correlation information:\nEQUATION\nIt is important to note that although we design multiple layers within each memory network to learn deep representations, different layers of the same memory network share the same unit. This enables the network to focus on crucial information that affects the movement of the target stock, thereby enhancing the continuous representation.\n\nMulti-directional Interactions\nIn reality, textual information and correlations have an impact on each other when it comes to stock price movement prediction. For instance, news about a technological breakthrough in the new energy sector may uplift the prices of most stocks in that industry, thereby affecting the correlations among those stocks.\nTo simulate this phenomenon and enhance the synergy between textual and correlational information, we introduce a multi-directional interaction module. This module allows textual and correlational information to reinforce each other and amplify the advantages of different information flows for better prediction performance.\nTake the Text Memory Network as an example, in each layer we firstly calculate the self-influence by using v (h\u22121) text as the query:\nv Att(h) text\u2212>text = MultiheadAtt(E text , v (h\u22121) text ) (11)\nNext we consider the interactive influences from correlations to texts using v (h\u22121) corr as the query:\nv Att(h) corr\u2212>text = MultiheadAtt(E text , v (h\u22121) corr ) (12)\nFinally, we produce a new attentional continuous representation by averaging these two influences:\nEQUATION\nwhich means that we replace Eqs. 6 with Eqs. 11-13 to obtain the new attention-aggregated vector.\nThe workings of Stock Correlation Memory Network are quite similar.\nConsequently, the fusion of different information flows is promoted due to the multi-directional interaction mechanism in which CMIN learns not only the influences from text/correlation to movement prediction within each information flow but also the interactive influences between different information flows, representing the interrelationship between text and correlations.\n\nLearning Objective\nWith the continuous representations v (H) text and v (H) corr from the last layer of each memory network, along with the target stock representation v target , we concatenate them and apply a softmax function to generate the final prediction vector \u0177:\n\u0177 = softmax(W y [v (H) text , v target , v (H) corr ] + b y ). (14)\nThe objective is to minimize the cross entropy loss:\nL(y, \u0177) = \u2212 n i=1 (yi log (\u0177i) + (1\u2212yi) log (1\u2212 \u0177i)) (15)\nwhere n is the size of the training set.\n\nExperiments\nIn this section, we empirically evaluate our CMIN model with three real-world datasets collected from the U.S. and Chinese stock markets.\n\nDatasets\nIn our experiments we have used three datasets, namely ACL18, CMIN-US and CMIN-CN, spanning different time periods to evaluate our proposed model CMIN against other baselines.\nACL18 (Xu and Cohen, 2018 ) is a classic dataset with tweets from Twitter as financial texts in the task of text-enhanced stock movement prediction. As there are few existing high-quality datasets containing both texts and price, we are also making available two new benchmark datasets along with this paper from 2018-01-01 to 2021-12-31 in the U.S. and Chinese market named CMIN-US and CMIN-CN. These two datasets are available at https://github.com/BigRoddy/ CMIN-Dataset to facilitate further research and enable reproducibility. More details and statistics of those three datasets are in Appendix A.\n\nBaselines\nWe compare CMIN against the following four baselines, all of which are high-performing stock movement prediction models proposed by recent studies:\n\u2022ALSTM (Qin et al., 2017 ) is a dual-stage attention-based recurrent neural network, which selects relevant time series across all time steps.\n\u2022Adv-LSTM (Feng et al., 2019) uses adversarial training to improve the generalization of ALSTM.\n\u2022Stocknet (Xu and Cohen, 2018) introduces recurrent continuous latent variables and uses variational inference to address the posterior inference.\n\u2022DTML (Yoo et al., 2021) is a newly published attention-based model that exploits the correlations between stocks to improve the prediction accuracy.\n\nEvaluation metrics\nAs we have formulated stock price movement prediction as a classification problem, we choose two classic metrics: Accuracy (Acc.) and Matthews Correlation Coefficient (MCC), similar to the previous work (Xu and Cohen, 2018; Yoo et al., 2021) . \nEQUATION\n\nImplementation details\nWe set our model for daily price prediction, with a history market window size k = 5 and the number of price features d p = d = 3, namely the highest, the lowest and the closing prices. We limit the maximum number of financial texts in one single day to be l = 20 , and the maximum length of a text document w = 30. Within the Text Encoder, we set the size of word embedding vector d w = 50 and the hidden state of Bi-GRU network d m = 50.\nWe implement the CMIN with Pytorch on a NVIDIA Tesla V100 and train it with an Adam optimizer (Kingma and Ba, 2015) . All parameters of our model are initialized with Xavier Initialization (Glorot and Bengio, 2010) . We search the hyperparameters of CMIN as follows: number of layers of each memory network H in {1, 2, 3, 4, 5}, dropout rate in {0.1, 0.2, 0.3}, number of epochs in {10, 20, 50}, and size of the price hidden state d p in {3, 10, 50}. For baselines, we use their default parameters and fine-tune them to fit our data.\n\nPerformance Analysis\nThe results are summarized in Table 1 .\nAmong all models, ALSTM and Adv-LSTM performed poorly with little improvement over random prediction. This could be attributed to the fact that these models rely solely on stock prices as the basis for decision-making. The Stocknet and DTML incorporate additional information beyond stock prices, demonstrated significant improvements over ALSTM and Adv-LSTM, which highlights the importance of utilizing financial texts and stock correlations for this challenging task. CMIN outperformed all baselines and achieved state-of-the-art performance on both two metrics across all datasets, showing its excellent capabilities to leverage both financial texts and stock correlations, as well as capture their interrelationship. \n\nAblation Studies\nTo evaluate the contribution of CMIN's different components, we compare against several variants:\n\u2022CMIN-TE: CMIN without the Text (TE), which makes decisions just based on stock prices.\n\u2022CMIN-PR: CMIN without the Price (PR), which makes decisions just based on related texts.\n\u2022CMIN-CM: CMIN without the guide of causality matrix (CM).\n\u2022CMIN-MI: CMIN without multi-directional interactions (MI) between memory networks.\nThe results are summarized in Table 2 . CMIN-TE only achieves a level of prediction accuracy on par with ALSTM and Adv-LSTM, and is worst among all the variants, again indicating the importance of text data. Similar to the performance of Stocknet, CMIN-PR has a relatively high Acc. but a low MCC, suggesting texts are particularly helpful to predict on one side of the binary classification. By modeling both text data and stock relationships, CMIN-CM reaches a good result. Finally, better performance achieved when causality matrix and multi-directional interactions are introduced into the network. Overall, the ablation studies show that every component makes an important contribution to CMIN, and as a result the full model with all components achieves the best performance.\n\nAnalysis of Memory Network Depth\nAs introduced before, we propose two memory networks to retain vital features of texts and correlations with multiple computational layers. And we want to understand what would be the ideal number of depths to achieve the best prediction results.\nWe change the number of layers H of each memory network to find out how the performance fluctuates with it. The results are summarized in Figure 2 . When we only have one memory layer, there is no multi-directional information flows between the two memory networks and as a result they only try to identify the vital information in the embeddings related to or having an impact on the movement of the target stock under the supervision of v target . As the number of memory layers increases, the interactions between two memory networks also intensifies. It is intuitive that the performance of CMIN reaches its peak when it has three memory layers. With further increase the number of memory layers, CMIN is prone to overfit.\n\nCase Study\nHere we present an example to illustrate how CMIN considers both financial texts and stock correlations to avoid random noises in time series.\nWe visualized the causality matrix of ACL18 using a heat map as shown in Figure 3 . Stocks are sequenced by their industry sector. The black box on the left shows weak causality, representing weak information flow from Utilities to Materials. On the other hand, the yellow box on the right indicates the relative strong information flow from Materials to Finance and within the Finance industry.\nThe target stock is Bank Of America (BAC) with a monitor window spanning from 13/11/2015 to 19/11/2015. We employ CMIN to predict BAC's next movement direction on the day of 20/11/2015 and then output the attention scores of texts and causality-guided correlation. The most focused stock by CMIN is Berkshire Hathaway Inc. (BRK-A). It's interesting to note that both are in the same industry sector: Finance, and they do appear to follow a very similar movement pattern in the trading days leading to 20/11/2015, which demonstrates the ability of CMIN to find the dynamic stock correlations with the guidance of Causality Matrix.\nThe financial text of BAC that obtains the highest attention score is \"Beer, Credit Card Debt And Other Positives For Bank Of America\", the title of an news article 1 which reports the rapidlyimproving banking landscape in the U.S.. This text is clearly highly relevant to BAC's subsequent stock performance, which demonstrates that CMIN is able to identify highly relevant texts having a impact on the target stock movement.\nFurthermore, it also illustrates the underlying interrelationship between financial texts and stock correlations. Except expressing an optimistic sentiment towards BAC, the news also shows a rapidly improving state of affairs for the wider financial industry. Therefore, through the Multi-directional Interactions mechanism, the text strengthens the model's attention stocks in the same sector. These two aspects mutually reinforce and complement each other to help the model make the best judgment that BAC's stock price will rise on the next day.\n\nConclusions\nIn this paper, we proposed CMIN, a causalityguided multi-memory interaction network that simultaneously models financial documents, causality-enhanced stock correlations and the interactions between the two, and recurrently learns a global memory representation for movement prediction. This multi-modality network was designed to enable the concurrent discovery of texts and stock correlations relevant to future price change and we demonstrated, through experiments on three datasets across two distinct markets, that each component of the proposed architecture made significant contributions to the model, leading CMIN to achieve state-of-the-art accuracy.\n"}
{"question": "What is the implication for automatic metric evaluation in RoSE?", "evidence": "  To evaluate automatic metrics, it is important to use an appropriate human evaluation protocol that captures the intended quality dimension to be measured. Reference-based automatic metrics should be evaluated by reference-based human evaluation, which disentangles metric performance from the impact of reference summaries. ", "options": ["A. Automatic metrics consistently outperform human evaluation in all cases.", "B. Metrics perform better under reference-free protocols.", "C. Metrics perform better when the system performance is similar.", "D. Automatic metrics should be evaluated using an appropriate human evaluation protocol. "], "answer": "D", "content": "\nIntroduction\nHuman evaluation plays an essential role in both assessing the rapid development of summarization systems in recent years (Lewis et al., 2020a; Zhang et al., 2020a; Brown et al., 2020; Sanh et al., 2022; He et al., 2022) and in assessing the ability of automatic metrics to evaluate such systems as a proxy * Equal contribution for manual evaluation (Bhandari et al., 2020; Fabbri et al., 2022a; Gao and Wan, 2022) . However, while human evaluation is regarded as the gold standard for evaluating both summarization systems and automatic metrics, as suggested by Clark et al. (2021) an evaluation study does not become \"gold\" automatically without proper practices. For example, achieving a high inter-annotator agreement among annotators can be difficult (Goyal et al., 2022) , and there can be a near-zero correlation between the annotations of crowd-workers and expert annotators (Fabbri et al., 2022a) . Also, a human evaluation study without a large enough sample size can fail to find statistically significant results due to insufficient statistical power (Card et al., 2020) .\nTherefore, we believe it is important to ensure that human evaluation can indeed serve as a solid foundation for evaluating summarization systems and automatic metrics. For this, we propose using a robust human evaluation protocol for evaluating the salience of summaries that is more objective by dissecting the summaries into finegrained content units and defining the annotation task based on those units. Specifically, we introduce the Atomic Content Unit (ACU) protocol for summary salience evaluation ( \u00a73), which is modified from the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. We demonstrate that with the ACU protocol, a high inter-annotator agreement can be established among crowd-workers, which leads to more stable system evaluation results and better reproducibility.\nWe then collect, through both in-house annotation and crowdsourcing, RoSE, a large human evaluation benchmark of human-annotated summaries with the ACU evaluation protocol on recent state-of-the-art summarization systems, which yields higher statistical power ( \u00a74). To support evaluation across datasets and domains, our benchmark consists of test sets over three summarization datasets, CNN/DailyMail (CNNDM) (Nalla-\n\nStatistical Power\n\u2212 High statistical power is difficult to reach for human evaluation of similar-performing systems. \u00a74.1 \u2212 Increasing the sample size of human evaluation effectively raises statistical power.\n\nSummary Length\n\u2212 Summaries from different summarization systems show a large difference in average length. \u00a74.2 \u2212 Difference in summary length is not well-reflected by automatic evaluation metrics.\n\u2212 Reference-free and reference-based human evaluation results have a near-zero correlation. Evaluation \u2212 Reference-free human evaluation strongly correlates with input-agnostic, annotator preference. Protocol Comparison \u2212 Annotator's input-agnostic preference has a strong positive correlation with summary lengths.\n\u00a75.2 \u2212 Annotator's input-agnostic preference does not favor reference summaries. \u2212 Compared to smaller, fine-tuned models, zero-shot large language models (e.g. GPT-3) perform better under reference-free evaluation, but worse under reference-based evaluation.\nEvaluating \u2212 A higher-powered human evaluation dataset can lead to a more robust automatic metric evaluation, as shown by a tighter confidence interval and higher statistical power of metric evaluation. Automatic Metrics \u2212 Automatic metric performance differs greatly under different human evaluation protocols.\n\u00a76.1 & \u00a76.2 \u2212 Automatic metrics show relatively strong system-level correlation and moderate summary-level correlation with our robust human evaluation protocol.\nTable 1 : Summary of the key findings in our work. pati et al., 2016) , XSum (Narayan et al., 2018), and SamSum (Gliwa et al., 2019) , and annotations on the validation set of CNNDM to facilitate automatic metric training. To gain further insights into the characteristics of different evaluation protocols, we conduct human evaluation with three other protocols ( \u00a75). Specifically, we analyze protocol differences in the context of both fine-tuned models and large language models (LLMs) in a zero-shot setting such as GPT-3 (Brown et al., 2020) . We find that different protocols can lead to drastically different results, which can be affected by annotators' prior preferences, highlighting the importance of aligning the protocol with the summary quality intended to be evaluated. We note that our benchmark enables a more trustworthy evaluation of automatic metrics ( \u00a76), as shown by statistical characteristics such as tighter confidence intervals and more statistically significant comparisons ( \u00a76.2).\nOur evaluation includes recent methods based on LLMs (Fu et al., 2023; Liu et al., 2023) , and we found that they cannot outperform traditional metrics despite their successes on related benchmarks such as SummEval (Fabbri et al., 2022a) . We summarize our key findings in Tab. 1. Our contributions are the following: (1) We propose the ACU protocol for high-agreement human evaluation of summary salience. (2) We curate the RoSE benchmark, consisting of 22000 summary-level annotations and requiring over 150 hours of in-house annotation, across three summarization datasets, which can lay a solid foundation for training and evaluating automatic metrics. 1 (3) We compare four human evaluation protocols for summarization and show how they can lead to drastically different model preferences. (4) We evaluate automatic metrics across different human evaluation protocols and call for human evaluation to be conducted with a clear evaluation target aligned with the evaluated systems or metrics, such that task-specific qualities can be evaluated without the impact of general, input-agnostic preferences of annotators. We note that the implications of our findings can become even more critical with the progress of LLMs trained with human preference feedback (Ouyang et al., 2022) and call for a more rigorous human evaluation of LLM performance.\n\nRelated Work\nHuman Evaluation Benchmarks Human annotations are essential to the analysis of summarization research progress. Thus, recent efforts have focused on aggregating model outputs and annotating them according to specific quality dimensions (Huang et al., 2020; Bhandari et al., 2020; Stiennon et al., 2020; Zhang and Bansal, 2021; Fabbri et al., 2022a; Gao and Wan, 2022) . The most relevant work to ours is Bhandari et al. (2020) , which annotates summaries according to semantic content units, motivated by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. However, this benchmark only covers a single dataset (CNNDM) without a focus on similarly-performing state-of-the-art systems, which may skew metric analysis (Tang et al., 2022a) and not fully reflect realistic scenarios (Deutsch et al., 2022) . In contrast, our benchmark consists only of outputs from recently-introduced models over three datasets.\nSummarization Meta-Evaluation With a human evaluation dataset, there exist many directions of meta-evaluation, or re-evaluation of the current state of evaluation, such as metric performance analyses, understanding model strengths, and human evaluation protocol comparisons.\nWithin metric meta-analysis, several studies have focused on the analysis of ROUGE (Lin, 2004b) , and its variations (Rankel et al., 2013; Graham, 2015) , across domains such as news (Lin, 2004a) , meeting summarization (Liu and Liu, 2008) , and scientific articles (Cohan and Goharian, 2016) . Other studies analyze a broader set of metrics (Peyrard, 2019; Bhandari et al., 2020; Deutsch and Roth, 2020; Fabbri et al., 2022a; Gabriel et al., 2021; Kasai et al., 2022b) , including those specific to factual consistency evaluation (Kryscinski et al., 2020; Durmus et al., 2020; Wang et al., 2020; Maynez et al., 2020; Laban et al., 20d; Fabbri et al., 2022b; Honovich et al., 2022; Tam et al., 2022) .\nRegarding re-evaluating model performance, a recent line of work has focused on evaluating zeroshot large language models (Goyal et al., 2022; Liang et al., 2022; Tam et al., 2022) , noting their high performance compared to smaller models.\nAs for the further understanding of human evaluation, prior work has compared approaches to human evaluation (Hardy et al., 2019) , studied annotation protocols for quality dimensions such as linguistic quality (Steen and Markert, 2021) and factual consistency (Tang et al., 2022b) , and noted the effects of human annotation inconsistencies on system rankings (Owczarzak et al., 2012) . The unreliability and cost of human evaluation in certain settings have been emphasized (Chaganty et al., 2018; Clark et al., 2021) , with some work noting that thousands of costly data points may need to be collected in order to draw statistically significant conclusions (Wei and Jia, 2021). Our meta-analysis focuses on this latter aspect, and we further analyze potential confounding factors in evaluation such as length and protocol design, with respect to both small and large zero-shot language models.\n\nAtomic Content Units for Summarization Evaluation\nWe now describe our Atomic Content Unit (ACU) annotation protocol for reference-based summary salience evaluation, including the procedure of writing ACUs based on reference summaries and matching the written ACUs with system outputs.\n\nPreliminaries\nIn this work, we focus on a specific summarization meta-evaluation study on summary salience. Salience is a desired summary quality that requires the summary to include all and only important information of the input article. The human evaluation of summary salience can be conducted in either reference-free or reference-based manners. The former asks the annotators to assess the summary directly based on the input article (Fabbri et al., 2022a) , while the latter requires the annotators to assess the information overlap between the system output and reference summary (Bhandari et al., 2020) , under the assumption that the reference summary is the gold standard of summary salience. 2 Given that reference-based protocols are more constrained, we focus on reference-based evaluation for our human judgment dataset collection, and we conduct a comparison of protocols in \u00a75.\n\nACU Annotation Protocol\nInspired by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols and subsequent annotation collection efforts (Bhandari et al., 2020; Zhang and Bansal, 2021) , the ACU protocol is designed to reduce the subjectivity of reference-based human evaluation by simplifying the basic annotation unit -the annotators only need to decide on the presence of a single fact, extracted from one text sequence, in another text sequence, to which a binary label can be assigned with more objectivity. Specifically, the evaluation process is decomposed into two steps: (1) ACU Writing -extracting facts from one text sequence, and (2) ACU Matching -checking for the presence of the extracted facts in another sequence. We formulate the ACU protocol as a recall-based protocol, such that the first step only needs to be performed once for the reference summary, allowing for reproducibility and reuse of these units when performing matching on new system outputs. ACU Writing While the LitePyramid approach defines its basic content unit as a sentence containing a brief fact, we follow Bhandari et al. (2020) to emphasize a shorter, more fine-grained information unit. Specifically, we define the ACU protocol with the concept of atomic facts -elementary information units in the reference summaries, which no\nThe clash occurred inside the box.\nOscar is Brazilian.\nOscar was taken off at half time.\nDidier Drogba replaced Oscar. longer need to be further split for the purpose of reducing ambiguity in human evaluation. 3 Then, ACUs are constructed based on one atomic fact and other minimal, necessary information. Fig. 1 shows an example of the written ACUs. To ensure annotation quality, we (the authors) write all the ACUs used in this work. We define guidelines to standardize the annotation process; for each summary sentence the annotator creates an ACU constituting the main information from the subject of the main clause (e.g., root), followed by additional ACUs for other facts while including the minimal necessary information from the root. We provide rules for dealing with quotations, extraneous adjectives, noisy summaries, and additional cases. We note that there can still be inherent subjectivity in the written ACUs among different annotators even with the provided guidelines. However, such subjectivity should be unbiased in summary comparison because all the candidate summaries are evaluated by the same set of written ACUs. ACU Matching Given ACUs written for a set of reference summaries, our protocol evaluates summarization system performance by checking the presence of the ACUs in the system-generated summaries as illustrated in Fig. 1 . For this step, we recruit annotators on Amazon Mechanical Turk 4 (MTurk). The annotators must pass a qualification test, and additional requirements are specified in Appendix A. Besides displaying the ACUs and the system outputs, we also provide the reference summaries to be used as context for the ACUs. Scoring Summaries with ACU ACU matching annotations can be aggregated into summary scores. We first define an un-normalized ACU score f of a candidate summary s given a set of ACUs A as: where A s is a subset of A that is matched with s.\n\nOscar collided with\nEQUATION\nWe note that f by default is a recall based score with respect to the reference summary r. Therefore, we also define a normalized ACU score f as:\nf\u03b1(s, A, r) = e min (0,\nEQUATION\nwhere |s|, |r| are the length (i.e., number of words) of the candidate summary s and the reference summary r respectively, and \u03b1 is a positive number controlling the strength of the normalization. This normalization is in effect a redundancy penalty, which penalizes the summaries longer than the reference and resembles the brevity penalty in BLEU (Papineni et al., 2002) . In practice, we set the value of \u03b1 by de-correlating f with the summary length using the collected ACU annotations.\n\nACU Annotation Collection\nWe collect ACU annotations on three summarization datasets: CNNDM (Nallapati et al., 2016 ), XSum (Narayan et al., 2018 ), and SamSum (Gliwa et al., 2019) . To reflect the latest progress in text summarization, we collect and annotate the generated summaries of pre-trained summarization systems proposed in recent years. 5 Detailed informa-tion about the summarization systems we used can be found in Appendix A.2. Table 2 shows the statistics of the collected annotations. The annotations are collected from the test set of the above datasets, and additionally from the validation set of CNNDM to facilitate the training of automatic evaluation metrics. In total, we collect around 21.8k ACU-level annotations and around 22k summary-level annotations, aggregated over around 50k individual summary-level judgments.\nTo calculate inter-annotator agreement, we use Krippendorff's alpha (Krippendorff, 2011) . The aggregated summary-level agreement score of ACU matching is 0.7571, and the ACU-level agreement score is 0.7528. These agreement scores are higher than prior collections, such as RealSumm (Bhandari et al., 2020) and SummEval (Fabbri et al., 2022a) , which have an average agreement score of crowd-workers 0.66 and 0.49, respectively.\n\nRoSE Benchmark Analysis\nWe first analyze the robustness of our collected annotations and a case study on the system outputs.\n\nPower Analysis\nWe analyze the statistical power of our collected human annotations to study whether it can yield stable and trustworthy results (Card et al., 2020) . Statistical power is the probability that the null hypothesis of a statistical significance test is rejected given there is a real effect. For example, for a human evaluation study that compares the performance of two genuinely different systems, a statistical power of 0.80 means there is an 80% chance that a significant difference will be observed. Further details can be found in Appendix B.1.\nWe conduct the power analysis for pair-wise system comparisons with ACU scores (Eq. 1) focusing on two factors, the number of test examples and the observed system difference. Specifically, we run the power analysis with varying sample sizes, and group the system pairs into buckets according to their performance difference, as determined by ROUGE1 recall scores (Fig. 2 ). 6 We observe the following: (1) A high statistical power 7 is difficult to reach when the system performance is similar. 6 We note that these scores are proxies of the true system differences, and the power analysis is based on the assumption that the systems have significantly different performance. 7 An experiment is usually considered sufficiently powered if the statistical power is over 0.80. Notably, while the sample size of the human evaluation performed in recent work is typically around 50-100, 8 such sample size can only reach a power of 0.80 when the ROUGE1 recall score difference is above 5. (2) Increasing the sample size can effectively raise the statistical power. For example, when the system performance difference is within the range of 1-2 points, the power of a 500-sample set is around 0.50 while a 100-sample set only has a power of around 0.20. The results of power analysis on three datasets with both ROUGE and ACU score differences are provided in Appendix B.2 with the same patterns, which indicates that our dataset can provide more stable summarization system evaluation thanks to its higher statistical power.\n\nSummarization System Analysis\nAs a case study, in Tab. summaries. Meanwhile, the systems that generate longer summaries may be favored by users who prefer more informative summaries. Therefore, we join the previous work (Sun et al., 2019; Song et al., 2021; Gehrmann et al., 2022; Goyal et al., 2022) in advocating treating summary lengths as a separate aspect of summary quality in evaluation, as in earlier work in summarization research. 9\n\nEvaluating Annotation Protocols\nApart from ACU annotations, we collect human annotations with three different protocols to better understand their characteristics. Specifically, two reference-free protocols are investigated: Prior protocol evaluates the annotators' preferences of summaries without the input document, while Ref-free protocol evaluates if summaries cover the salient information of the input document. We also consider one reference-based protocol, Ref-based, which evaluates the content similarity between the generated and reference summaries. Appendix D.1 provides detailed instructions for each protocol.\n\nAnnotation Collection\nWe collected three annotations per summary on a 100-example subset of the above CNNDM test set using the same pool of workers from our ACU qualification. Except for ACU, all of the summaries from different systems are evaluated within a single task with a score from 1 (worst) to 5 (best), similar (2) annotations for summaries from GPT-3 (Brown et al., 2020), 10 T0 (Sanh et al., 2022), BRIO, and BART to better understand annotation protocols with respect to recently introduced large language models applied to zero-shot summarization.\n\nResults Analysis\nWe investigate both the summary-level and systemlevel correlations of evaluation results of different protocols to study their inherent similarity. Details of correlation calculation are in Appendix C.\nResults on Fine-tuned Models We show the system-level protocol correlation when evaluating the fine-tuned models in Tab. 4, and the summarylevel correlation can be found in Appendix D.2. We use the normalized ACU score (Eq. 2) because the other evaluation protocols are supposed to resemble an F1 score, while the ACU score is by definition recall-based. We have the following observations:\n(1) The Ref-free protocol has a strong correlation with the Prior protocol, suggesting that the latter may have a large impact on the annotator's document-based judgments.\n(2) Both the Prior and Ref-free protocols have a strong correlation with summary length, showing that annotators may favor longer summaries.\n(3) The Ref-free protocol and the Ref-based protocol have a negative correlation while ideally they are supposed to measure similar quality aspects.\nWe perform power analysis on the results following the procedure in \u00a74.1 and found that ACU protocol can yield higher statistical power than the Ref-based protocol, suggesting that the ACU protocol leads to more robust evaluation results. We also found that the reference-free Prior and Ref-free Table 6 : The Kendall's correlation between the automatic metric scores and ACU scores of system outputs on CNNDM, XSum, and SamSum datasets. The correlation is calculated at both the system level and the summary level. We use the recall score of the automatic metrics when available to align with the ACU scores.\nautomatic metric variants. We focus the metric evaluation on ACU annotations because of two insights from \u00a75: (1) Reference-based metrics should be evaluated with reference-based human evaluation.\n(2) ACU protocol provides higher statistical power than the summary-level Ref-based protocol.\n\nMetric Evaluation with ACU Annotations\nWe use the correlations between automatic metric scores and ACU annotation scores of system outputs to analyze and compare automatic metric performance. The following metrics are evaluated:\n(1) lexical overlap based metrics, ROUGE (Lin, 2004b), METEOR (Lavie and Agarwal, 2007) 5) evaluation methods based on large language models, GPTScore (Fu et al., 2023) and G-Eval (Liu et al., 2023) , with two variants that are based on GPT-3.5 11 (G-Eval-3.5) and GPT-4 12 (OpenAI, 2023) (G-Eval-4) respectively. We note that for LLM-based evaluation we require the metric to calculate the recall score. For G- -3.5 -.091 -.273 -.091 .818 1.00 1.00 G-Eval-3.5-S -.091 -.273 -.273 .818 1.00 1.00 G-Eval-4\n.091 .818 .636 1.00 1.00 1.00\nTable 7 : The system-level Kendall's correlation between the automatic metric and ACU scores on different system pairs grouped by their ACU score differences on the CNNDM dataset, into six equal-sized buckets. We use the recall score of the automatic metrics when available.\nEval-3.5 we report two variants that are based on greedy decoding (G-Eval-3.5) and sampling (G-Eval-3.5-S) respectively, Details of the LLM-based evaluation are in Appendix E.2. Tab. 6 shows the results, with additional results of more metrics in Appendix E.3. We note:\n(1) Several automatic metrics from the different families of methods (e.g., ROUGE, BARTScore) are all able to achieve a relatively high correlation with the ACU scores, especially at the system level.\n(2) Metric performance varies across different datasets. In particular, metrics tend to have stronger correlations on the SamSum dataset and weaker correlations on the XSum dataset. We hypothesize that one reason is that the reference summaries of the XSum dataset contain more complex structures.\n(3) Despite their successes (Fu et al., 2023; Liu et al., 2023) in other human evaluation benchmarks such as SummEval, LLM-based automatic evaluation cannot outperform traditional methods such as ROUGE on RoSE. Moreover, their low summarylevel correlation with ACU scores suggests that their predicted scores may not be well-calibrated.\nFollowing Deutsch et al. ( 2022), we further investigate metric performance when evaluating system pairs with varying performance differences. Specifically, we group the system pairs based on the difference of their ACU scores into different buckets and calculate the modified Kendall's correlation (Deutsch et al., 2022) on each bucket. The system pairs in each bucket are provided in Appendix E.4. Tab. 7 shows that the automatic metrics generally perform worse when they are used to evaluate similar-performing systems. \n\nAnalysis of Metric Evaluation\nWe analyze the metric evaluation with respect to the statistical characteristics and the impact of different human evaluation protocols on metric evaluation.\nConfidence Interval We select several representative automatic metrics and calculate the confidence intervals of their system-level correlations with the ACU scores using bootstrapping. Similar to Deutsch et al. ( 2021b), we find that the confidence intervals are large. However, we found that having a larger sample size can effectively reduce the confidence interval, which further shows the importance of increasing the statistical power of the human evaluation dataset as discussed in \u00a74.1. We provide further details in Appendix E.5.\n\nPower Analysis of Metric Comparison\nWe conduct a power analysis of pair-wise metric comparison with around 200 pairs, which corresponds to the chance of a statistical significance result being found. More details can be found in Appendix E.6. The results are in Fig. 3 , showing similar patterns as in the power analysis of summarization system comparison ( \u00a74.1):\n(1) Significant results are difficult to find when the metric performance is similar;\n(2) Increasing the sample size can effectively increase the chance of finding significant results. automatic metrics generally perform better under reference-based evaluation protocols, but can have negative correlations with reference-free protocols.\n\nConclusion and Implications\nWe introduce RoSE, a benchmark whose underlying protocol and scale allow for more robust summarization evaluation across three datasets. With our benchmark, we re-evaluate the current state of human evaluation and its implications for both summarization system and automatic metric development, and we suggest the following:\n(1) Alignment in metric evaluation. To evaluate automatic metrics, it is important to use an appropriate human evaluation protocol that captures the intended quality dimension to be measured. For example, reference-based automatic metrics should be evaluated by reference-based human evaluation, which disentangles metric performance from the impact of reference summaries.\n(2) Alignment in system evaluation. We advocate for targeted evaluation, which clearly defines the intended evaluation quality. Specifically, text summarization, as a conditional generation task, should be defined by both the source and target texts along with pre-specified, desired characteristics. Clearly specifying characteristics to be measured can lead to more reliable and objective evaluation results. This will be even more important for LLMs pretrained with human preference feedback for disentangling annotators' prior preferences for LLMs with the task-specific summary quality.\n(3) Alignment between NLP datasets and tasks.\nHuman judgments for summary quality can be diverse and affected by various factors such as summary lengths, and reference summaries are not al-ways favored. Therefore, existing summarization datasets (e.g. CNNDM) should only be used for the appropriate tasks. For example, they can be used to define a summarization task with specific requirements (e.g. maximum summary lengths), and be important for studying reference-based metrics.\n"}
{"question": "Among all the previous research,which model is most related to our model?", "evidence": "  Information retrieval systems have undergone continuous development over the past few decades, with the aim of obtaining relevant resources, such as documents, in response to a user query from a vast collection. With the recent success of Pretrained Language Models (PLMs) (Devlin et al., 2019; Raffel et al., 2020; Zhao et al., 2023) , researchers have developed PLM-based dense retrievers (Lin et al., 2021; Zhao et al., 2022) , which utilize dual-encoders and nearest neighbor search index for retrieval and achieve significant improvements over sparse retrievers. More recently, a new retrieval paradigm, known as model-based retrieval (Tay et al., 2022; Zhou et al., 2022c) , has been introduced by developing an alternative architecture for retrieval. In contrast to traditional retrieval methods, it does not explicitly maintain a corpus index, thereby simplifying the classic index-retrieve-rerank process.   ", "options": ["A. PLM-based dense retriever", "B. Pretrained Language Models (PLMs) ", "C. Dual-encoders ", "D. model-based retrieval "], "answer": "D", "content": "\nIntroduction\nInformation retrieval systems have undergone continuous development over the past few decades, with the aim of obtaining relevant resources, such as documents, in response to a user query from a vast collection. With the recent success of Pretrained Language Models (PLMs) (Devlin et al., 2019; Raffel et al., 2020; Zhao et al., 2023) , researchers have developed PLM-based dense retrievers (Lin et al., 2021; Zhao et al., 2022) , which utilize dual-encoders and nearest neighbor search index for retrieval and achieve significant improvements over sparse retrievers.\nMore recently, a new retrieval paradigm, known as model-based retrieval (Tay et al., 2022; Zhou et al., 2022c) , has been introduced by developing an alternative architecture for retrieval. In contrast to traditional retrieval methods, it does not explicitly maintain a corpus index, thereby simplifying the classic index-retrieve-rerank process. Typically, a model-based retrieval system is built based on a sequence-to-sequence generation model with an encoder-decoder architecture, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) . It accepts a query as input and directly generates the corresponding document identifier via the generation model.\nDespite its attractive benefits in simplifying the retrieval pipeline, model-based retrieval still faces following major challenges.\n\u2022 Firstly, since the retrieval task is framed as a prediction task of document identifiers, making it crucial to design document identifiers that are well-suited to the underlying generative PLM. However, this issue is rarely discussed in prior research, and most existing approaches employ manually or randomly constructed identifiers (i.e., docids) as generation targets. Such docids are not adequately captured in the pretraining stage of the generative PLM, thus limiting PLM's capabilities for generative prediction (e.g., unseen docids during pre-training). This creates a discrepancy between the pre-training and fine-tuning phases.\n\u2022 Secondly, there is a discrepancy between training and inference in the single-model generative architecture. While most existing studies incorporate multi-task learning (Tay et al., 2022) and auxiliary pre-training tasks (Zhou et al., 2022b) to model both documents and queries during training, the model only processes queries dur- ing inference, resulting in a gap between the training and inference stages.\nTo this end, in this paper, we propose a novel TwO-stage Model-based rEtrieval approach, TOME (as illustrated in Figure 1 ), which makes two major technical contributions.\n\u2022 Firstly, we suggest using tokenized URLs (or URIs) as text identifiers, which are widely available for web pages or Wikipedia pages 1 . By using URL-based identifiers, the tokenized symbols are well aligned with the vocabulary of the generative PLM, thereby enhancing the generative capacity of the PLM. URLs are typically comprised of normal text, as opposed to manually or randomly constructed identifiers. As a result, such an identifier design can be used to help alleviate the gap between pre-training and fine-tuning.\n\u2022 Secondly, our approach decomposes the prediction task into two consecutive stages, namely passage generation and URL generation, which are fulfilled by two separate T5-based generation models, respectively. The first stage aims to generate a relevant passage in the corpus based on the query, while the second stage aims to generate the corresponding URL of the generated passage from the first stage. This two-stage architecture can reduce the discrepancy between training and inference. In addition, the entire generation process is progressive. Consequently, the second stage is capable of tolerating errors that may be introduced by the preceding stage and generates correct URLs.\nMoreover, we discover that optimizing modelbased retrieval becomes a challenging task when dealing with a vast corpus. As a result, we propose a number of improved training strategies to optimize the generation models, including query augmentation, passage length reduction, and model scaling.\nTo verify the effectiveness of TOME, we conduct extensive experiments on the publicly available MS MARCO and NQ datasets. Experimental results demonstrate the effectiveness of the proposed method, including the URL identifier design and the two-stage generation process. Additionally, case studies indicate that the second stage can tolerate errors induced by the first stage. Furthermore, we investigate the scaling laws of TOME by examining different model sizes, corpus sizes, and text lengths. We anticipate that these experimental results will facilitate further research on model-based retrieval.\n\nRelated Works\nText Retrieval. Text retrieval endeavors to find textual information related to a query from a large candidate corpus. Early studies on sparse retrieval focused on term matching by utilizing sparse representations and inverted indices, such as BM25 (Robertson et al., 2009) . In recent years, with the resurgence of neural networks and the emergence of pre-trained language models (PLMs) (Devlin et al., 2019; Raffel et al., 2020) , dense retrieval achieves better performance beyond traditional sparse retrieval on multiple tasks (Khattab and Zaharia, 2020; Karpukhin et al., 2020; Xiong et al., 2021; Qu et al., 2021) . The dense retrieval and the technique of approximate nearest neighbor search have been widely adopted in various applications (Oguz et al., 2020; Ren et al., 2021a,b; Asai et al., 2021; Ren et al., 2022; Zhou et al., 2022a) . Recently, Zhao et al. (2022) have made a very comprehensive survey about the recent progress of dense retrieval based on PLMs, and we refer the readers to this survey paper for more details.\nModel-based Retrieval. Both sparse retrieval and dense retrieval rely on explicit indices. Recently, researchers have proposed model-based retrieval (a.k.a., generative retrieval) models (Metzler et al., 2021; Tay et al., 2022) . These methods consider model parameters as retrieval indices and directly generate the identifiers of related documents. Such an idea is initially proposed for entity retrieval (Cao et al., 2021) , which autoregressively generates unique entity identifiers. Following this approach, researchers have introduced sequenceto-sequence encoder-decoder architecture for document retrieval (Zhou et al., 2022c; Bevilacqua et al., 2022; Zhuang et al., 2022; Wang et al., 2022; Lee et al., 2022; Chen et al., 2022; Zhou et al., 2022b) . As discussed in the previous section, there still remain issues with model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. Our work tries to deal with these issues with a two-stage generation architecture with URL identifiers.\n\nApproach\nIn this section, we first introduce the task formulation, followed by the description of the proposed two-stage generation approach TOME.\n\nTask Formulation\nIn this work, we consider the task of text retrieval, which aims to find relevant text resources (e.g., documents) related to a query from a large corpus. We further assume that these texts can be accessed by an associated URL 2 (or URI).\nTo develop our approach, we adopt the recently proposed model-based paradigm for text retrieval (Tay et al., 2022; Zhuang et al., 2022) . For retrieval, a model-based retrieval model takes a query q as input and uses the text-to-text model to generate the identifier y (length n) of the relevant document in an autoregressive manner, with the conditional probability:\nEQUATION\nwhere y i denotes the i-th output token in the identifier y, y <i denotes the previous tokens y 1 , . . . , y i\u22121 , and M represents the PLM. The identifier can be an atomic token or a string (Tay et al., 2022) . In our setting, it is assigned to an associated URL of a text (refer to Section 3.2.1). Typically, a generative pre-trained language model (PLM) with an encoder-decoder architecture is employed to implement the text-to-text model (e.g., T5), which is typically optimized by a cross-entropy loss as follows:\nL(M) = \u2212 log Pr M (y|q) = \u2212 n i=1 log Pr M (y i |y <i , q) . (2)\nThe key to model-based retrieval is to design a generative architecture that employs suitable document identifiers, and to develop effective training methods that can effectively associate queries to the identifiers of documents. Next, we expound our approach in detail.\n\nModel Architecture\nIn this section, we first introduce the design of document identifiers, and then present the two-stage generation architecture.\n\nIdentifier Design\nExisting studies typically use docids to represent a document (Tay et al., 2022; Zhuang et al., 2022) . These docids are often randomly generated or manually constructed, which may not exist in realworld text corpora. However, the generative PLM is pre-trained based on large-scale text corpora, leading to a discrepancy between pre-training and fine-tuning.\nDifferent from previous approaches, we consider a tokenized form of URLs as the docids. We directly treat the URL as a text string and tokenize it into a sequence of tokens using a T5 tokenizer. For instance, a sample URL 'https://en.wikipedia.org/wiki/Nevada' can be tokenized to {'https', '://', 'en', '.', 'wikipedia', '.', 'org', '/', 'wiki', '/', 'N', 'e', 'vada'}. We use the token sequence as the prediction target of the generative PLM, following the generation formula of Equation (1). It is worth noting that Ultron (Zhou et al., 2022b ) also uses URLs as identifiers, where a URL is reversed and only used as part of an identifier (also involving titles and domains). As a comparison, we solely utilize tokenized URLs as the identifier, without any additional processing.\nCompared to non-linguistic docids, URLs typically contain more meaningful tokens in the form normal text and widely exist in real-world text corpora, making them more suitable to modeling and prediction using generative PLMs. During decoding, we can directly adopt the general text decoding method to generate the URL, without resorting to limited search strategies such as constrained beam search (Tay et al., 2022; Bevilacqua et al., 2022) . Since these tokenized symbols often overlap among different URLs (e.g., web pages from the same domains), they naturally derives semantic strings as the clustering method in DSI (Tay et al., 2022) .\n\nTwo-stage Generation Architecture\nThe objective of the generative model for retrieval is to establish a correlation between a query and its corresponding docid (i.e., URL). However, owing to the scarcity of annotated data, various improved strategies such as multi-task learning (Tay et al., 2022) or pre-training (Zhou et al., 2022b) have been proposed. Typically, a model processes both documents and queries during training, while it processes only queries during inference, resulting in the discrepancy between training and inference. To tackle this issue, we propose a two-stage generation approach with two different generation models: one for passage generation and the other for URL generation, as shown in Figure 1 .\nPassage Generation. In the first stage, we employ a T5-based passage generation model to map an input query to the passage content according to Equation (1). The generated passage is anticipated as a relevant passage in the corpus that can provide an answer to the query. The objective of the passage generation model is to memorize the passages in the corpus, so as to generate the passages with utmost precision. It is trained with query-passage pairs, where each pair comprises a query and a passage from the document, along with the corresponding labeled URL. Different from existing methods (Tay et al., 2022; Bevilacqua et al., 2022) , we do not utilize any data structure to restrict the decoding process and simply use greedy search to generate an individual result for a query in an autoregressive manner, which has a high decoding efficiency. By incorporating the intermediate passage generation, our approach can mitigate the training-inference discrepancy that the query encoder also needs to process documents (Tay et al., 2022) . URL Generation. In the second stage, another T5-based PLM is employed to predict the corresponding URL as the retrieval result, utilizing the passage generated by the passage generation model as input. The URL is generated by means of greedy search decoding in a similar manner as in Equa-tion (1). The URL generation model is trained with passage-URL pairs, where each pair comprises a passage and its corresponding URL. The objective of the URL generation model is to memorize all the URLs in the corpus, so as to map a generated passage related to a query to a corresponding URL. Meanwhile, even if the generated passages contain some irrelevant content or noise, this stage can still make reliable predictions since it can employ long passages as the context, rather than short queries.\nOverall, such a two-stage generation approach can more effectively capture the semantic relatedness between queries and identifiers by both reducing the training-inference discrepancy and enriching the generation context, which is specifically tailored for model-based retrieval.\n\nTraining\nFor both the passage generation model and the URL generation model, we optimize them independently by utilizing the cross-entropy loss for optimizing standard T5 models, as shown in Equation (2). Nevertheless, optimizing model-based retrieval approaches (Zhuang et al., 2022; Wang et al., 2022 ) is a challenging task as they essentially require memorizing the corpus information, and generating long text also poses challenges in model convergence. In this part, we further propose several strategies for improving the training of our approach.\nQuery Augmentation. Generating pseudo queries is proven to be effective in improving the performance of model-based retrieval (Wang et al., 2022; Zhuang et al., 2022) . Here, we utilize query generation for constructing the training data for passage generation. Specifically, we take the passage collection as the corpus, and use an existing query generation model (i.e., DocT5query (Nogueira et al., 2019) ) trained on the labeled dataset to generate multiple pseudo queries for each passage in the corpus. Following DSI-QG (Zhuang et al., 2022) , we use the top-k sampling strategy for query generation, and set k up to 20. The generated pseudo queries and their corresponding passages are then used to construct query-passage pairs as the training data for the passage generation model. Such a query augmentation method can significantly increase the availability of training data, and also enhance the generalization capability of the model for different queries.\nReducing the Passage Length. Since passages are much longer than URLs, passage generation is more complicated than URL generation. In the generation task, a more extensive generation target results in larger search space, which typically leads to a decrease in efficiency and effectiveness. While, in our approach, passage generation serves as an indirect step for predicting the URL, so that we consider reducing the passage length for improving the training efficiency. For this purpose, we shorten the maximum truncation length of the passage, from 128 to 32. However, reducing the passage length will probably results in a information loss, thus hurting the generation performance. As the solution, we concatenate the title (a short text) and the shortened passage for enhancing the contained semantics. We also add prompts before titles and passage contents like \"title:\" or \"passage:\" for better generation performance.\nIncreasing Model Scale. Model-based retrieval requires a strong memorization capacity from the generative PLM, especially for our approach that involves a passage generation stage. Besides, scaling up the text corpus will significantly increase the difficulty of corpus memorization, and the PLM with a small parameter scale will have a limited memorization capacity when the data scale reaches a certain level. Considering the two aspects, we scale the model size accordingly and employ a larger PLM when necessary. Specifically, we use T5-large (the first stage is more difficult) and T5base for the two stages of our approach on a small corpus (e.g., subsets of MS MARCO), respectively. Further, we increase them to T5-3B and T5-large accordingly on a large corpus (e.g., the full set of MS MARCO). Besides the improved capacity, we find that using a larger model size is also useful in improving the convergence rate (as detailed in Section 5.4).\n\nExperimental Settings\nThis section describes the major experimental settings, including datasets, evaluation metrics, baselines and implementation details.\n\nDatasets and Evaluation Metrics\nDatasets. We conduct experiments on two public available datasets, namely MS MARCO (Nguyen et al., 2016) Passage Ranking and Natural Questions (NQ) (Kwiatkowski et al., 2019) . (1) MS MARCO contains Bing search queries as well as passages from web documents, making it one of the largest web search datasets to date, with a full corpus of over 8.8 million passages. In addition, we also consider two subsets, each containing 100K and 1M passages, by following (Tay et al., 2022; Zhuang et al., 2022) . Based on the MS MARCO Question Answering dataset, we extract the URLs associated with the passages, selecting a random URL if a passage contains multiple URLs (2) The NQ dataset is a question answering dataset where the query data is collected from Google search logs, and the document data is from Wikipedia. We use the NQ320K version by following NCI (Wang et al., 2022) , which contains 320K labeled querydocument pairs and 100K documents. We collect abstracts of documents as intermediate-generated passages.\nEvaluation Metric. Following previous works, we adopt Hits@1 as the evaluation metric. This metric is calculated as the percentage of queries to which the top-1 generation result is positive. Since the outputs of models at different stages are either passage texts or URL texts, unlike the conventional MS MARCO evaluation by determining whether the retrieved identifiers are in the identifier label list, we evaluate the results by determining whether it is an exact match to the label text.\n\nBaselines\nFor comparison, we chose the following baselines including sparse retrieval, dense retrieval, and model-based retrieval.\nBM25 (Robertson et al., 2009 ) is a classical sparse retriever that uses the inverted index to find relevant passages by term overlap. DPR (Karpukhin et al., 2020) and ANCE (Xiong et al., 2021) are two representative dense retrievers that adopts dual-encoder architecture. For modelbased retrievers, DSI (Tay et al., 2022 ) is a pioneer work for model-based retrieval that uses a sequence-to-sequence model to map the input query to the relevant docid. We use the open-source code released by DSI-QG for reproducing DSI baseline on MS MARCO. SEAL (Bevilacqua et al., 2022) is proposed to generate multiple ngrams for a query with an auxiliary Ferragina Manzini index. DSI-QG (Zhuang et al., 2022) proposes to improve DSI with augmented data constructed by query generation. NCI (Wang et al., 2022 ) also utilizes pseudo queries for improving model-based retrieval with tailored architecture. Due to the different experimental settings of different methods, we copy the performance values for some baselines on NQ in NCI and reproduce all of the baselines on MS MARCO under the same evaluation strategy. All the model-based retrieval baselines adopt the \"large\" version of PLMs.\n\nImplementation Details\nWe conduct our experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) and natural language processing toolkit PaddleNLP (Contributors, 2021) on up to 32 NVIDIA Tesla A100 GPUs (with up to 80G RAM).\nPLM. The generation models adopted in our work are initialized with different parameter scales of T5 (Raffel et al., 2020) . In the passage generation model, we use T5-3B for initialization on MS MARCO Full, and other models are initialized with T5-large. In the URL generation model, we use T5large for initialization on MS MARCO Full, and other models are initialized with T5-base.\nHyper-parameters. We adopt Adam optimizer with a learning rate of 5e-5, and train the models for a maximum of 3M steps with bf16 mixed precision strategy. The batchsize is set up to 128, 384 and 80 for T5-base, T5-large and T5-3B, respectively. The maximal length of queries, passages and URLs are set as 32, 32 and 80, respectively. The warm-up step is set as 100K and 10K for passage and URL generation task, respectively. Query Augmentation. We adopt the existing docT5query-large (Nogueira et al., 2019) model that trained on MS MARCO training set, and generate 20 and 15 queries per passage for MS MARCO and NQ, respectively. For training data, we only use pseudo-labeled data constructed by query generation on MS MARCO, and use both pseudolabeled data and labeled data on NQ.\n\nExperimental Results and Analysis\nIn this section, we report the experimental results of our proposed approach and conduct comprehensive empirical analysis. (Karpukhin et al., 2020) 71.84 52.52 29.54 DSI (Tay et al., 2022) 11.75 --DSI-QG (Zhuang et al., 2022) Table 1 : The Hits@1 results of different methods on variant corpus scales of MSMARCO.\n\nMethods\nHits@1 BM25 (Yang et al., 2017) 15.11 ANCE (Xiong et al., 2021) 52.63 DSI (Tay et al., 2022) 35.60 SEAL (Bevilacqua et al., 2022) 59.93 NCI (Wang et al., 2022) 66.23 DSI-QG (Zhuang et al., 2022) 61.34 Comparison with Model-based Retrievers. We observe that TOME consistently outperforms model-based retrievers on three subsets of MS MARCO and NQ320K datasets, thereby demonstrating the effectiveness of the proposed method. Moreover, NCI is a competitive baseline on NQ320K, which uses tailored decoder architecture, preprocessed semantic docid, and regularization on top of DSI-QG, while our method is simply trained with the standard T5 configuration without any additional processing. We also discover that DSI-QG is unable to effectively converge when trained on the MS MARCO Full. We speculate that random non-linguistic docids become a bottleneck as the corpus scales up, while the loss can normally converge when using normal text (e.g., URL) as a generation target.\nEffect of Two-stage Generation Architecture. By simply substituting the generation target of DSI-QG from random string docids to URLs (singlestage of our method), the performance has been improved (refer to DSI-QG and TOME single-stage in Table 1 and 2 ), indicating that natural language identifiers are more suitable for model-based retrieval tasks than non-linguistic docids. Furthermore, if we employ the two-stage generation that includes an intermediate step to generate passages before generating URLs, the performance will be further improved (refer to TOME single-stage and TOME two-stage in Table 1 and 2 tion demonstrates that integrating passage generation in the process of model-based retrieval leads to better performance.\nComparison with Dense Retrievers. By adopting a series of training strategies, we successfully train TOME on large-scale corpora. However, although TOME outperforms dense retrieval methods on MS MARCO 100K and NQ320K, there still remains a performance gap when compared to DPR on larger corpora such as MS MARCO 1M and Full. This indicates that our method still has gaps compared to advanced dense retrieval methods when the corpus scales up. Since the model-based method necessitates complete memorization of the entire corpus, it inherently possesses a disadvantage in larger-scale corpora when compared to dense retrievers, which needs to be further explored.\n\nAblation Study\nIn this section, we conduct an ablation study to examine the effectiveness of strategies in TOME.\nWe report the results on MS MARCO 100K and NQ320K. Here, we consider three variants based on TOME for comparison: (a) w/o prompt removes the prompts before titles and passages; (b) w/ increased maxlen increases the maximum truncated length of passage from 32 to 128; (c) w/ reduced pseudo query reduces the amount of pseudo query to 10 per passage. Table 3 presents the results for variants of TOME. We can observe the following findings: (a) The performance drops in w/o prompt, demonstrating that adding prompts for identifying the title and passage is helpful for generating better results. (b) The performance drops in w/ increased maxlen, demonstrating that due to various training strategies, shortening the maximum truncated passage length does not bring performance loss but reduces the difficulty of training. (c) The performance drops in w/ reduced pseudo query, demonstrating the effectiveness of generating a large number of pseudo queries for data augmentation.\n\nAnalysis on Two-stage Generation\nIn this section, we investigate the generation results of the passage generation model quantitatively and qualitatively to showcase the superiority of the proposed two-stage generation approach.\n\nQuantitative Analysis\nWe quantitatively analyze the generation results on MSMARCO dev set with the passage generation models trained on MS MARCO 100K.\nFirst, we are surprised to find that on the entire dev set, the proportion of generated passages are the passages exist in the corpus is about 95%. In cases where the model failed to generate labels correctly, about 85% of the generated passages still exist in the corpus. This result indicates that the model is capable of memorizing the corpus precisely and is able to generate a retrieval-like result. Moreover, previous studies of dense retrieval reveal that there are a lot of false negatives in MS-MARCO (Qu et al., 2021) . We also observe that approximately 80% of the generation results that are not labeled as positives but appear in the corpus are false negatives, showing that model-based retrieval suffers from the same issue of false negatives as dense retrieval. Despite this, the passage generation model actually has strong generation capability.\n\nQualitative Analysis\nTo explore the generative capabilities of TOME, we conduct a case study on MSMARCO 100K, utilizing a maximum truncation length of 128 for better illustration.\nTable 4 gives two sampled queries, along with their corresponding label passages, evidence passages (if available) and generated passages. With respect to the first query, the generated passage is not exactly the same as the labeled passage. In comparison with the labeled positive passage, the second half of the generated passage is altered. Despite the alteration in the generation passage, the URL generation model is still able to accurately map it to the correct URL, indicating that the URL generation model can tolerate changes introduced by the passage generation model. In the second example, the model extracts relevant content from both the label passage and the evidence passage, and then combines the contents to create the generated passage. It is interesting Ginger is an analgesic (a pain-killer) that may alleviate the pain associated with a sore throat. It is also a good antibacterial and antifungal and can help fight the infection causing your sore throat. . . . Table 4 : The comparison of the labeled passages and generated passages. The evidence passages are not manually labeled but contain relevant content. The italic words with underline represents the different parts of two passages, the ::::::::::::::::::::::::::\nitalic words with wavy underline and bold words with underline in different passages represent the reference parts. to observe that the passage generation model is capable of summarizing multiple passages.\n\nAnalysis on Scaling\nWe observe that long text generation poses a challenge to the convergence of loss, so we investigate the training efficiency and capability of the model under varying conditions. In particular, we use the same computing resource and conduct training on the passage generation stage (i.e., the first stage) of TOME. Considering that the trend is similar in the second stage, it has been omitted here due to limited space.\nEffect on Data Scale. We investigate the impact of expanding the corpus on model training and examine whether the model capacity is insufficient when dealing with a large corpus. We fix the T5-large model and conduct training on MSMARCO 100K, 1M and Full datasets, respectively, without shortening the length of passages. We use perplexity (PPL) to estimate the model capacity and monitor how perplexity changes as training steps increase. The results are shown in Figure 2 (a). It can be observed that the perplexity of the T5-large model fails to converge to a lower level after corpus scale expansion, which illustrates that under this task, a certain amount of data will lead to the capacity bottleneck of the model. In addition, the decline rate of perplexity slows down on larger corpora, indicating that models with the same parameter size have low learning efficiency on a large-scale corpus.. Effect on Passage Length. In order to investi-gate the effect of reducing the length of generated passages, we fixed the model as T5-large, and conducted experiments on passages with different maximum truncated lengths as generation targets on MSMARCO 1M. Figure 2 shows that after reducing the maximum truncated length of the generated passage, the perplexity significantly decreases, indicating that such a strategy is beneficial to mitigate the difficulty of the passage generation task. Moreover, the model exhibited enhanced efficiency when generating shorter passages.\n\nConclusion\nIn this paper, we introduce TOME, a innovative two-stage model-based retrieval approach. To implement our approach, we make two major technical contributions in the design of the identifier and the architecture of two-stage generation. Moreover, we also employ a number of training strategies to better optimize our proposed architecture, especially on large-scale corpora. Extensive results demonstrate the effectiveness of TOME. Furthermore, we perform a thorough analysis and summarize the scaling law for the proposed method. We believe such an idea itself is worthwhile for exploring in designing new model-based retrieval architecture.\n"}
{"question": "What did the experiments show about the number of rationales sampled from the teacher model?", "evidence": "  Figure 2 shows the performance of the student model when it is trained on corpora with fewer sampled CoT per instance: results suggest that learning with multiple sampled (albeit nosier) rationales/chain-of-thoughts per example is more beneficial than learning with one (most likely) rationale. ", "options": ["A. More rationales per instance do not improve performance.", "B. Fewer rationales per instance result in better performance.", "C. Sampling multiple rationales per instance is beneficial.", "D. Only one rationale per instance is needed for improvement. "], "answer": "C", "content": "\nIntroduction\nEmpirical scaling laws suggest that the accuracy of Large Language Models (LLMs) on benchmark tasks can be improved by increasing model size and pre-training data volume (Hoffmann et al., 2022) . Beyond these training-time improvements, however, an inference-time strategy dubbed \"chain-ofthought\" (CoT) prompting, 1 i.e., eliciting verbalizations of predictive processes via key-phrases like \"Let's think step-by-step\" (Kojima et al., 2022) , can *Work done during an internship at AI2. Figure 1 : Symbolic Chain-of-thought Distillation (SCoTD) applied to a student model, ranging in size from 125M-1.3B parameters. We show that fine-tuning on a (diverse and potentially filtered) corpus of expert chain-of-thought demonstrations from a teacher model is an effective strategy to make smaller models capable of chain-of-thought reasoning.\nsimilarly improve performance, e.g., Suzgun et al. (2022) demonstrate additional performance gains on a hard subset of the BigBench tasks (BIG-bench collaboration, 2022) using chain-of-thought.\nHowever, chain-of-thought prompting has only been shown to be beneficial for models of sufficient scale (e.g., with more than 60B parameters (Wei et al., 2022b) ). In this work, we study whether small language models can be \"taught\" the capacity for chain-of-thought reasoning by larger language models. We adopt a simple strategy, which we call Symbolic Chain-of-thought Distillation (SCoTD): first, we sample chain-of-thought rationales from large language model given (unlabeled) input instances from a dataset; then, we train a smaller language model to predict the sampled rationale CommonsenseQA: Q: The man working in the attic swatted away a bee, but soon the single bee was an entire what? (a) swarm; (b) pack; (c) countryside; (d) soft drink; (e) field of flowers GPT-3: The answer must be related to bees, but also connected with being numerous. Of the above answers, only swarm fits the scenario. So the answer is: (a) OPT-1.3B: The answer must be a swarm of bees. Of the above choices, only soft drink is used to describe a swarm of bees. So the answer is: (c) OPT-1.3B + SCoTD: The answer must be the name of something that has bees. Of the above choices, only swarms have bees. So the answer is: (a) QuaRel: Q: Steve is waiting at the gym ... He starts hitting around a hockey puck with his stick. The gym has a carpeted floor and more friction than the ice rink he goes to. Which surface is rougher? (A) carpet (B) ice rink GPT-3: When something is smoother, it is easier to slide on and easier to pass through. So the carpet is rougher. So the answer is: (A) OPT-1.3B: _________ is rougher than carpet. Thus, the gym floor is rougher than the ice rink. So the answer is: (A) OPT-1.3B + SCoTD: When something is rougher, it has more friction. Thus, the gym has more friction than the ice rink he goes to. So the answer is: (A)\nOpenBookQA: Q: Magnets may be attracted to (a) clay pots (b) roofing nails (c) paper plates (d) plastic cutlery GPT-3: Magnets are attracted to metal objects. These objects include roofing nails. So the answer is: (b) OPT-1.3B: Magnets are attracted to clay pots, roofing nails, paper plates, plastic cutlery. So the answer is: (d) OPT-1.3B + SCoTD: Magnets may be attracted to some metals, but not to clay pots, roofing nails, paper plates or plastic cutlery. So the answer is: (b) Table 1 : Few-shot chain-of-thoughts produced by GPT-3 (code-davinci-002, the teacher model), OPT-1.3B (the un-distilled student model), and OPT-1.3B + SCoTD (ours), the student model trained using Symbolic Chainof-thought Distillation. \u00a73 shows this process significantly improves the task-accuracy of the student model in a variety of settings, and in \u00a73.1.1, human evaluations show that, even when the un-distilled student model happens to get the multiple choice question correct (see QuaRel example), humans tend to prefer OPT-1.3B + SCoTD. and sampled label. This process follows the \"symbolic knowledge distillation\" paradigm as in West et al. (2022) , wherein corpora are sampled from a larger language model to serve as training data for a smaller one.\nWe find that through SCoTD, smaller language models learn to self-rationalize and perform significantly better on 3 commonsense QA tasks compared to learning without rationalizations. This result holds for both supervised and few-shot settings, and across student models of varying scales (125M-1.3B parameters). Performance gains are especially pronounced when applying distilled chain-ofthought models to difficult scenarios like: contrast sets (Gardner et al., 2020) ( \u00a73.4 ; SCoTD significantly outperforms supervised learning on labels) and fully held-out tasks ( \u00a73.5; few-shot SCoTD significantly outperforms in-context learning).\nKey to the success of this process is sampling a relatively large number of rationales per example from the teacher model (e.g., 30 rationales/example) (Figure 2 ). This is different from many prior practices that train with one rationale per example (Camburu et al., 2018; Li et al., 2022a) . In ablation studies, we investigate several competing hypotheses for what are the most important factors within the corpus: we filter the corpus to CoTs that are assigned high probability by GPT-3 vs. filtering to CoTs that are diverse vs. filtering to CoTs that explain more open-ended input instances.\nWhile diversity and high probability are reasonable filters that on average perform well, the \"null hypothesis\" of random downsampling performs well, suggesting that the sheer volume of the rationales is also a key contributing factor.\nWe will release code and the corpus of sampled chain-of-thoughts at https://github.com/ allenai/cot_distillation.\n\nSymbolic Chain-of-Thought Distillation\nOur primary goal is to improve the accuracy of a (relatively small) student language model S on a target classification 2 task D Test = {(x i , y i )}. 3 We assume access to 1) (an unlabeled) training set D Train = {(x i )}; and 2) a large teacher language model T (e.g., GPT-3 (Brown et al., 2020) ), capable of generating chain-of-thoughts in a few-shot fashion.\nOur first step is to curate a set of labeled chainof-thoughts to serve as few-shot Prompts for T . For each target task, we sample a small number (e.g., 10) of examples x i from D Train , provide a gold classification label y i , and manually author a chain-of-thought z i for each to form the prompt set P = {(x i , y i , z i )} 4 . Then, for each x i in D Train , we sample N chainof-thoughts zi along with the resulting prediction \u1ef9i from the teacher model, i.e.,\n(\u1ef9 k i , zk i ) \u223c N T (y i , z i |x i , P).\nThe result of this sampling is a corpus\nC = {(x i , {(\u1ef9 k i , zk i )} N k=1\n)}, which contain teacherpredicted chain-of-thoughts/labels. Depending on the experimental setting (details in \u00a7 3), we sometimes filter the entries of C, e.g., in the fully supervised case where D Train instances have associated labels, we discard samples for which the sample the teacher model predicted an incorrect label. Next, we train the student model using the standard language modeling loss, i.e., we maximize\nE (x,\u1ef9,z) \u223c C [S(\u1ef9, z|x)].\nAfter fine-tuning the student model on the corpus sampled from the teacher, to evaluate the model on a test instance (x test , y test ) from the target task, we decode both a chain-of-thought ztest and a predicted label \u1ef9test from the student and evaluate \u1ef9test versus the true label y test . We consider two strategies for decoding. (1) Predict the most likely chain-of-thought and the label ztest , \u1ef9test = argmax z,y S(z, y|x test ). This can be approximated by greedy decoding or beam search. (2) There may be different valid chainof-thoughts for a given question and as a result, large language models distribute probability mass for a certain label across many diverse chain-of-thoughts (Wang et al., 2022b) . Thus, it is beneficial to marginalize out the reasoning paths to find the most consistent answer: \u1ef9test = argmax y E z\u223cS(z|xtest) S(y|z, x test ). This can be approximated by sampling multiple reasoning paths and take a majority vote among the predicted answers, dubbed \"self-consistency\" (Wang et al., 2022b) . We experiment with both approaches and conduct a discussion in \u00a73.2.\n\nExperiments\nWe evaluate primarily on 3 target tasks: 1) Com-monsenseQA (CSQA) (Talmor et al., 2019) , a 5way multi-choice dataset; 2) OpenBookQA (Mihaylov et al., 2018) , and 3) QuaRel (Tafjord et al., 2019) . While any model capable of few-shot chain-of-thought could be substituted, we use the thought prompts from prior work (Wei et al., 2022b; Wang et al., 2022b) code-davinci-002 version of GPT-3 5 (Brown et al., 2020) as our teacher model T . We use OPT (Zhang et al., 2022) as our student model S. Our standard student model is OPT-1.3B (though we explore a range of student model sizes in \u00a73.3).\nWe sample from GPT-3 with a temperature of T = 1.0. For each training example, we sample N = 30 rationales. OPT is fine-tuned with a batch size of 32 and a learning rate of 2 \u00d7 10 \u22125 . We use HuggingFace transformers (Wolf et al., 2019) , Pytorch (Paszke et al., 2019) , and Accelerate 6 for the implementation. Main experiments can be reproduced on one GPU with 48GB of memory.\n\nResults in Default SCoTD Setting\nWe first consider both a few-shot learning setting and a supervised setting. For the few-shot setting, the only labeled examples available to our teacher/student models are contained in the prompt set P (but we use the unlabeled examples and teacher-generated chain-of-thoughts/labels for training). 7 We also consider the supervised setting, where we assume access to labels in D Train . Supervised SCoTD involves simply discarding the samples within C that do not have the correct label prior to fine-tuning the student: for Common-\nCommonsenseQA QuaRel OpenBookQA\nFigure 2 : For three commonsense QA tasks, accuracy (y-axis) improves significantly as the student is trained on more chain-of-thoughts sampled from the teacher (x-axis). Oversampling chain-of-thoughts is sometimes required to improve student performance beyond the supervised label-only baseline, e.g., as in OpenbookQA.\nsenseQA, OpenBookQA, and QuaRel, this results in discarding 40.4%, 45.0%, 34.2% of chain-ofthoughts. For the few-shot setting, we decode with the self-consistency approach; for the supervised setting, we decode with greedy decoding (introduced in \u00a7 2; see an discussion in \u00a7 3.2). We compare SCoTD to 2 baselines: 1) Label-Only, the student is fine-tuned on just the label (in the few-shot setting, the label comes from the teacher and could be wrong; in the supervised setting, we use the gold label), instead of also with CoT; 2) Greedy-CoT, we decode a single-CoT per example (instead of N = 30 samples) from T for each training example instead of sampling. For additional reference, Table 2 (a) reports the performance of the student (and teacher) in a variety of few-shot settings prior to applying any distillation: No CoT = few shot prompting with labeled instances from P but no z i , Greedy and Self-Consistency are prompting with CoT but with different decoding strategies ( \u00a7 2).\nTable 2 (b) gives the performance of the student model after distillation in the supervised and fewshot settings. In all cases, distillation significantly improves the student model, and in all-but-one case, learning with CoT outperforms the label-only distillation baseline. While the student model initially fails to perform CoT through prompting (Table 2 (a)) it learns to do so through distillation.\nThe number of samples. In our default setting, to serve as our distillation corpus C, we sample N = 30 rationales from the teacher T for each (unlabelled) training instance. Figure 2 shows the performance of the student model when it is trained on corpora with fewer sampled CoT per instance: results suggest that learning with multiple sampled (albeit nosier) rationales/chain-of-thoughts per example is more beneficial than learning with one (most likely) rationale. Will more rationales bring more performance improvement? We sampled more rationales from GPT-3 to train the student model; however, this does not bring more performance gains. When N = 50, the performance is similar to N = 30: the model achieves 67.0 in accuracy on OpenBookQA (v.s. 67.0), 67.2 on CommonsenseQA (v.s. 67.0), 84.9 on QuaRel (v.s. 83.8).\n\nHuman Evaluations\nWhile SCoTD improves task accuracy significantly, we additionally conduct human evaluations to assess the generated chain-of-thoughts themselves (see Table 1 for samples). We sample instances from the CommonsenseQA, OpenBookQA, and QuaRel validation sets (300 instances per dataset), and conduct head-to-head human evaluations 8 to assess:\nQ1: Does SCoTD result in higher-quality chainof-thoughts? Test: OPT-1.3B versus OPT-1.3B + SCoTD. Result: Yes. We assess this hypothesis on two subsets of instances: 1) a pure random sample (N=900); and 2) a set of instances for which both models eventually predicted the correct label (N=654). The second setting focuses more closely on the chain-of-thoughts themselves rather than the predictive accuracy of the model. SCoTD is superior in both settings: for the random sample setting, SCoTD won in 59% of cases (p<.001), whereas in the correctness controlled setting, SCoTD won in 61% of cases (p<.001). Results hold with p < .05 for each QA dataset individually.\nQ2: Does a SCoTD student surpass the much larger teacher? Test: OPT-1.3B + SCoTD versus text-davinci-002. While the task accuracy of the teacher is still higher in most cases, the studentgenerated CoT are comparable. 9 We again evaluate on: 1) a pure random sample (N=900); and 2) a correctness-controlled setting (N=659). The 100x smaller SCoTD's generations are competitive in both cases; we can't reject the null hypothesis of the crowd having equal preferences (OPT-1.3B + SCoTD wins in 47% and 51% of cases respectively, p > .01). Results hold for each dataset individually, as well.\n\nSelf-Consistency for the Student\nWang et al. (2022b) find that, for chain-of-thought prompted models, taking a majority vote over a large set of sample of predicted labels (resulting from a diverse range of CoTs) can improve performance. Our results regarding the effectiveness of sampling N = 30 rationales from the teacher during SCoTD are similar-in-spirit: i.e., we also show performance gains from sampling multiple rationalization chains per instance.\n9 See \u00a76 for more discussion about the disparity between CoT-quality and task accuracy. A natural question is, does the student model S exhibit the same phenomenon, i.e., can we sample multiple chain-of-thoughts from it and take a majority vote? We find that the student model can benefit from \"self-consistency,\" but not in all cases. In Table 3 , we report the performance with/without self-consistency (majority vote among 30 sampled reasoning paths with a temperature of 0.7). When training with filtered CoTs (Table 3 (a) bottom rows) or training with few CoTs per example (Table 3 (b), when #CoTs/Example is small), the student model does not benefit from self-consistency. Only when we train with multiple rationales per example without filtering (the few-shot setting), self-consistency is beneficial on CSQA and Open-BookQA. Overall, the results show that student models benefit from being shown a diverse/noisy set of rationales, and that self-consistency can be effectively applied after distillation.\n\nSCoTD across Model and Dataset Sizes\nWe also verify the effectiveness of SCoTD across model and dataset sizes; in these experiments, we consider the supervised setting. Data scaling. Figure 3 shows the effect of varying the size of D Train (for simplicity, we show only performance on CSQA as an example). Learning with CoTs is beneficial under all data scales. Interestingly, SCoTD, trained with access to only 40% of the labelled data, can surpass the direct supervised label-only model with 100% of the labelled corpus; this result aligns with the argument in Zaidan et al. (2007) -providing more explanations from the teacher model could be more beneficial than providing more labels.\nStudent model size scaling. Figure 4 presents results when varying the size of the student model from 125M to 1.3B parameters for CSQA. For all model three model sizes, SCoTD outperforms the standard supervised fine-tuning baseline (Label Only). Sampling multiple rationales per input instance is an effective strategy for all model sizes.\n\nSCoTD on Challenging Contrast Sets\nCan learning with explanations help generalization, as hypothesized by (Zaidan et al., 2007) ? As a preliminary study, we show that SCoTD enables better generalization to contrast sets. Contrast sets (Gardner et al., 2020) are proposed to evaluate a model's robustness to perturbations around the decision boundary, by asking annotators to modify the original test instances in small but meaningful ways that (typically) change the gold label.\nWe experiment on the IMDB (Maas et al., 2011 ) sentiment analysis task in the supervised setting; we consider the corresponding contrast set of IMDB proposed by Gardner et al. (2020) . We train two models on the training set of IMDB: Label-Only and SCoTD. For efficiency, we sub-sample 100K examples from the training set of IMDB and truncate input sequences to 700 tokens. As shown in Figure 5 , while both models with/without SCoTD achieve high performance on the original IMDB test set (96.1% v.s. 95.5%, with the Label-Only model performing slightly better), the model with SCoTD achieves significantly higher performance on the contrast set: 92.0% vs. 81.6%. This result supports the hypothesis of (Zaidan et al., 2007) ; that explanations can support more robust generalization.\n\nSCoTD on Unseen, Out-of-domain Tasks\nLarge language models can perform few-shot, incontext learning with chain-of-thought prompting, i.e., generating reasonable chain-of-thoughts on unseen tasks with a few demonstrations (Suzgun et al., 2022) . We conduct a preliminary experiment, inspired by Min et al. (2021) 's MetaICL, to test whether student models trained with SCoTD acquire the same ability. We train a supervised SCoTD model on ANLI, CommonsenseQA, and OpenBookQA, and evaluate it on SST-2 (Socher et al., 2013) , a sentiment analysis task.\nThe SCoTD model achieves a few-shot accuracy of 79.6% on the validation set (an example prediction is shown in Figure 6 ). 10 Compared to a baseline model that learns with no CoT(i.e., a re-implementation of MetaICL trained on 3 source tasks); the baseline fails to recognize the input/output format of the new task and predicts answers out of the desired label set. It achieves (an effective) 0% accuracy on SST-2. This suggests the potential of including CoTs during instruction/incontext tuning (Wei et al., 2022a; Min et al., 2021) .\n\nWhat Factors are Important for\nDistillation?\nAn important factor underlying the performance gains highlighted in \u00a73 was the number of chain-ofthoughts we sampled from the teacher model perinstance (more samples = better; Figure 2 ). Here we ask: is data volume the key contributing factor to the performance improvement? Or, are specific aspects of chain-of-thought samples key for the performance improvements?\nWe design several filters to identify potentially important examples/CoTs among the correct rationales. We apply designed filters (to be introduced) to C \u2032 , the corpus sampled from the teacher (with wrong CoTs dropped), that operationalize different hypotheses about what factors are important to distill. We control for dataset size when filtering, i.e., \n\nLabel Only SCoTD\nThe author said that they love this movie and they are never tired of watching it.\nThey say that the movie is wonderful and they are grateful to see such an outstanding picture. So the answer is: positive\nThis was a wonderfully clever and entertaining movie that I shall never tire of watching many, many times\u2026 I can only be grateful when I see such an outstanding picture for most of the motion pictures made more\nThis was a wonderfully thick as two short planks and soul-destroying movie that I shall never watch any number of times\u2026 I can only be sorry when I see such an abysmal picture just as most of the motion pictures \u2026\n\nIMDB Dataset\nThe author said that the movie was 'thick as two short planks and souldestroying', implying that the movie is bad. So the answer is: negative all filtered corpora have the same number of training CoTs. We downsample with a budget of 5 CoT per instance on average 11 . Then, we train the same student model on each of the filtered corpora, and compare on downstream tasks. If a student model trained on filtered corpus A tends to outperform the student model trained on filtered corpus B, then we argue that the property that produced corpus A is more important. The hypotheses we consider are:\nNull hypothesis: data volume. As a null hypothesis, we randomly sub-sample 5 CoT per instance; this filter operationalizes the assumption that an arbitrary set of samples is sufficient.\n\nDiversity.\nFor each instance, we compute S-BERT (Reimers and Gurevych, 2019) embed-11 In rare cases, we may end up with less as there are less than 5 correct CoTs for the instance. dings 12 of each of the chain-of-thoughts, and cluster the resulting embeddings using hierarchical clustering into k = 5 clusters. Then, we randomly sample a single instance from each cluster: the resulting sample covers all clusters, and thus represents a diverse+representative sample.\nTeacher likelihood. For each instance, we keep the 5 CoT samples with the highest per-token loglikelihood according to the teacher model.\nOpen-endedness. Some instances in each dataset lead to a broader range of chain-of-thought samples than others. For example, on CommonsenseQA, the question \"What form of alcohol is made from grapes?\" leads to a narrower range of rationalizations vs. \"Why might someone purposefully be going into trance?\"\nWe hypothesize that openended instances could benefit from relatively more sampled rationales. We sort instances into quintiles based on the unique bi-grams in their corresponding 30 CoTs; for high-ranking instances (more unique CoT bi-grams, like the \"trance\" example above), we keep more rationales and for low-ranking instances, we keep less rationales. We keep 1, 3, 5, 7, 9 rationales for instances of different bins (thus controlling for the total number of CoT).\nResults Figure 7 reports the accuracy of the student model when fine-tuned on the different subsampled corpora for the three tasks we consider. Overall, random subsampling is a strong baseline, but, we see some evidence that diversity among the rationales is important. None of the models trained on the sub-sampled data could approach the model trained on the full 30x/instance CoT set. This suggests that the sheer volume of the CoTs is a key driving force for the performance improvement.\n\nRelated Work\nChain-of-thought prompting. As an extension of few-shot prompting (Brown et al., 2020) Learning with explanations. Hase and Bansal (2022) discuss how explanations can serve as inputs (Talmor et al., 2020) , targets (Hendricks et al., 2016; Fidler et al., 2017; Camburu et al., 2018; Zhou et al., 2020; Narang et al., 2020; Kayser et al., 2021; Wiegreffe et al., 2022) , and priors (Zhang et al., 2016; Srivastava et al., 2018) for machine learning models. Chain-of-thought extends earlier efforts which treat explanations as intermediate structures, generated at inference time (Rajani et al., 2019) . Most related to our work is Li et al. (2022a) , who do also learn with GPT-3 generated explanations; we show multiple samples improve significantly over their single-sample method, and also use chain-of-thought prompting at inference time vs. predicting explanations+labels via independent multitasking.\nKnowledge distillation. Recent work, inspired by Knowledge Distillation (Hinton et al., 2015) , has considered symbolic knowledge distillation, (West et al., 2022) , i.e., instead of distilling from soft representations like logits, large language model serve as training data generators (Xiong et al., 2019; Petroni et al., 2019; Schick and Sch\u00fctze, 2021; West et al., 2022; Liu et al., 2022; Meng et al., 2022; Bhagavatula et al., 2022) ; this paper continues this line of work.\nContemporaneous work. There are several contemporaneous papers: Huang et al. (2022) , Magister et al. (2022), and Ho et al. (2022) all show that smaller models can benefit from large models' chains of thought. We contributes beyond these by: 1) showing that sampling a large number of chain-of-thoughts is paramount; 2) exploring transfer performance to challenge sets/unseen tasks; and 3) analysis that address what factors are important in the teacher corpus.\n\nConclusion\nWe demonstrate the effectiveness of Symbolic Chain-of-thought Distillation (SCoTD): a method that enables smaller language models to effectively use chain-of-thought-style reasoning. We demonstrate the method's effectiveness across several downstream tasks, different student model sizes, different levels of supervision, and in difficult settings (challenge sets, unseen tasks). Our ablations shed light on what factors are particularly important to distill in these chain-of-thoughts. Our concrete recommendations are: 1) sampling multiple and diverse CoTs for each input instance, and 2) performing self-consistency when the teacher CoTs are noisy. Several promising av-enues for future work include:\n1. Exploring SCoTD for generation tasks in addition to classification tasks;\n2. Scaling up the number of source tasks in \u00a7 3.5 to generalize to more tasks;\n3. Using the down-sampling setup introduced in \u00a74 to explore additional hypotheses about what other factors may be of importance in CoTs.\n"}
{"question": "In the context of Named Entity Recognition (NER) using diffusion models, what distinguishes DIFFUSIONNER from previous generation-based methods?", "evidence": "  our DIFFUSIONNER can also decode entities in a nonautoregressive manner, and thus result in a faster inference speed with better performance. Other works (Zhang et al., 2022) focus on the disorder of the entities and mitigate incorrect decoding bias from a causal inference perspective. In addition, autoregressive generative NER works (Athiwaratkun et al., 2020; De Cao et al., 2021; Yan et al., 2021b; Lu et al., 2022) linearize structured named entities into a sequence, relying on sequence-to-sequence language models (Lewis et al., 2020; Raffel et al., 2020) to decode entities. ", "options": ["A. DIFFUSIONNER linearizes structured named entities into a sequence.", "B. DIFFUSIONNER relies on autoregressive generative NER works.", "C. DIFFUSIONNER explores the utilization of generative diffusion models for NER.", "D. DIFFUSIONNER focuses on mitigating incorrect decoding bias.", "Different from previous works, our proposed DIFFUSIONNER is the first one to explore the utilization of the generative diffusion model on NER"], "answer": "C", "content": "\nIntroduction\nNamed Entity Recognition (NER) is a basic task of information extraction (Tjong Kim Sang and De Meulder, 2003) , which aims to locate entity mentions and label specific entity types such as person, location, and organization. It is fundamental to many structured information extraction tasks, such as relation extraction (Li and Ji, 2014; Miwa and Bansal, 2016) and event extraction (McClosky et al., 2011; Wadden et al., 2019) .\nMost traditional methods (Chiu and Nichols, 2016) formulate the NER task into a sequence labeling task by assigning a single label to each token. To accommodate the nested structure between entities, some methods (Ju et al., 2018; Wang et al., + + \u00b2 \u00bb N (0; 1) + \u00b2 \u00bb N (0; 1)\nFigure 1 : Boundary diffusion in named entity recognition. The fixed forward diffusion process adds Gaussian noise to the entity boundaries at each timestep, and the noisy boundaries recover their original state by denoising with the learnable reverse diffusion process. For inference, the reverse diffusion process generates entity boundaries and performs entity typing based on the noisy spans sampled from the Gaussian distribution. 2020) further devise cascaded or stacked tagging strategies. Another class of methods treat NER as a classification task on text spans (Sohrab and Miwa, 2018; Eberts and Ulges, 2020) , and assign labels to word pairs (Yu et al., 2020; Li et al., 2022a) or potential spans (Lin et al., 2019; Shen et al., 2021a) . In contrast to the above works, some pioneer works (Paolini et al., 2021; Yan et al., 2021b; Lu et al., 2022) propose generative NER methods that formulate NER as a sequence generation task by translating structured entities into a linearized text sequence. However, due to the autoregressive manner, the generation-based methods suffer from inefficient decoding. In addition, the discrepancy between training and evaluation leads to exposure bias that impairs the model performance.\nWe move to another powerful generative model for NER, namely the diffusion model. As a class of deep latent generative models, diffusion models have achieved impressive results on image, audio and text generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021; Li et al., 2022b; Gong et al., 2022) . The core idea of diffusion models is to systematically perturb the data through a forward diffusion process, and then recover the data by learning a reverse diffusion process.\nInspired by this, we present DIFFUSIONNER, a new generative framework for named entity recognition, which formulates NER as a denoising diffusion process (Sohl-Dickstein et al., 2015; Ho et al., 2020) on entity boundaries and generates entities from noisy spans. As shown in Figure 1 , during training, we add Gaussian noise to the entity boundaries step by step in the forward diffusion process, and the noisy spans are progressively denoised by a reverse diffusion process to recover the original entity boundaries. The forward process is fixed and determined by the variance schedule of the Gaussian Markov chains, while the reverse process requires learning a denoising network that progressively refines the entity boundaries. For inference, we first sample noisy spans from a prior Gaussian distribution and then generate entity boundaries using the learned reverse diffusion process.\nEmpowered by the diffusion model, DIFFUSION-NER presents three advantages. First, the iterative denoising process of the diffusion model gives DIFFUSIONNER the ability to progressively refine the entity boundaries, thus improve performance. Second, independent of the predefined number of noisy spans in the training stage, DIF-FUSIONNER can sample a different number of noisy spans to decode entities during evaluation. Such dynamic entity sampling makes more sense in real scenarios where the number of entities is arbitrary. Third, different from the autoregressive manner in generation-based methods, DIFFUSION-NER can generate all entities in parallel within several denoising timesteps. In addition, the shared encoder across timesteps can further speed up inference. We will further analyze these advantages of DIFFUSIONNER in \u00a7 6.2. In summary, our main contributions are as follows:\n\u2022 DIFFUSIONNER is the first to use the diffusion model for NER, an extractive task on discrete text sequences. Our exploration provides a new perspective on diffusion models in natural language understanding tasks.\n\u2022 DIFFUSIONNER formulates named entity recognition as a boundary denoising diffusion process from the noisy spans. DIFFUSION-NER is a novel generative NER method that generates entities by progressive boundary refinement over the noisy spans.\n\u2022 We conduct experiments on both nested and flat NER to show the generality of DIFFU-SIONNER. Experimental results show that our model achieves better or competitive performance against the previous SOTA models.\n2 Related Work\n\nNamed Entity Recognition\nNamed entity recognition is a long-standing study in natural language processing. Traditional methods can be divided into two folders: tagging-based and span-based. For tagging-based methods (Chiu and Nichols, 2016; Ju et al., 2018; Wang et al., 2020) , they usually perform sequence labeling at the token level and then translate into predictions at the span level. Meanwhile, the span-based methods (Sohrab and Miwa, 2018; Eberts and Ulges, 2020; Shen et al., 2021a,b; Li et al., 2022a) directly perform entity classification on potential spans for prediction. Besides, some methods attempt to formulate NER as sequence-to-set (Tan et al., 2021 (Tan et al., , 2022;; Wu et al., 2022) or reading comprehension (Li et al., 2020; Shen et al., 2022) tasks for prediction. In addition, autoregressive generative NER works (Athiwaratkun et al., 2020; De Cao et al., 2021; Yan et al., 2021b; Lu et al., 2022) linearize structured named entities into a sequence, relying on sequence-to-sequence language models (Lewis et al., 2020; Raffel et al., 2020) to decode entities. These works designed various translation schemas, including from word index sequence to entities (Yan et al., 2021b) and from label-enhanced sequence to entities (Paolini et al., 2021) , to unify NER to the text generation task and achieved promising performance and generalizability. Other works (Zhang et al., 2022) focus on the disorder of the entities and mitigate incorrect decoding bias from a causal inference perspective. Different from previous works, our proposed DIFFUSIONNER is the first one to explore the utilization of the generative diffusion model on NER, which enables progressive refinement and dynamic sampling of entities. Furthermore, compared with previous generation-based methods, our DIFFUSIONNER can also decode entities in a nonautoregressive manner, and thus result in a faster inference speed with better performance.\n\nDiffusion Model\nDiffusion model is a deep latent generative model proposed by (Sohl-Dickstein et al., 2015) . With the development of recent work (Ho et al., 2020) , diffusion model has achieved impressive results on image and audio generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021) . Diffusion model consists of the forward diffusion process and the reverse diffusion process. The former progressively disturbs the data distribution by adding noise with a fixed variance schedule (Ho et al., 2020) , and the latter learns to recover the data structure. Despite the success of the diffusion model in continuous state spaces (image or waveform), the application to natural language still remains some open challenges due to the discrete nature of text (Austin et al., 2021; Hoogeboom et al., 2022; Strudel et al., 2022; He et al., 2022) . Diffusion-LM (Li et al., 2022b) models discrete text in continuous space through embedding and rounding operations and proposes an extra classifier as a guidance to impose constraints on controllable text generation. DiffuSeq (Gong et al., 2022) and SeqDiffuSeq (Yuan et al., 2022a) extend diffusionbased text generation to a more generalized setting. They propose classifier-free sequence-to-sequence diffusion frameworks based on encoder-only and encoder-decoder architectures, respectively.\nAlthough diffusion models have shown their generative capability on images and audio, its potential on discriminative tasks has not been explored thoroughly. Several pioneer works (Amit et al., 2021; Baranchuk et al., 2022; Chen et al., 2022) have made some attempts on diffusion models for object detection and semantic segmentation. Our proposed DIFFUSIONNER aims to solve an extractive task on discrete text sequences.\n\nPreliminary\nIn diffusion models, both the forward and reverse processes can be considered a Markov chain with progressive Gaussian transitions. Formally, given a data distribution x 0 \u223c q (x 0 ) and a predefined variance schedule {\u03b2 1 , . . . , \u03b2 T }, the forward process q gradually adds Gaussian noise with variance \u03b2 t \u2208 (0, 1) at timestep t to produce latent variables x 1 , x 2 , . . . , x T as follows:\nq (x 1 , . . . , x T | x 0 ) = T t=1 q (x t | x t\u22121 )\n(1)\nq (x t | x t\u22121 ) = N x t ; 1 \u2212 \u03b2 t x t\u22121 , \u03b2 t I (2)\nAn important property of the forward process is that we can sample the noisy latents at an arbitrary timestep conditioned on the data x 0 . With the notation \u03b1 t := 1 \u2212 \u03b2 t and \u1fb1t := t s=0 \u03b1 s , we have:\nEQUATION\nAs \u1fb1T approximates 0, x T follows the standard Gaussian distribution: p (x T ) \u2248 N (x T ; 0, I). Unlike the fixed forward process, the reverse process p \u03b8 (x 0:T ) is defined as a Markov chain with learnable Gaussian transitions starting at a prior p (x T ) = N (x T ; 0, I):\np \u03b8 (x 0:T ) = p (x T ) T t=1 p \u03b8 (x t\u22121 | x t ) p \u03b8 (x t\u22121 | x t ) = N (x t\u22121 ; \u00b5 \u03b8 (x t , t) , \u03a3 \u03b8 (x t , t))\nwhere \u03b8 denotes the parameters of the model and \u00b5 \u03b8 and \u03a3 \u03b8 are the predicted covariance and mean of q \n(x t\u22121 | x t ). We set \u03a3 \u03b8 (x t , t) = \u03c3 2 t I and build a neural network f \u03b8 to predict the data x 0 , denoted as x0 = f \u03b8 (x t , t). Then we have \u00b5 \u03b8 (x t , t) = \u03bct (x t , x0 ) = \u03bct (x t , f \u03b8 (x t , t)), where \u03bct denotes the mean of posterior q (x t\u22121 | x t , x0 ).\n\nMethod\nIn this section, we first present the formulation of diffusion model for NER (i.e., the boundary denoising diffusion process) in \u00a7 4.1. Then, we detail the architecture of the denoising network for boundary reverse process in \u00a7 4.2. Finally, we describe the inference procedure of DIFFUSIONNER in \u00a7 4.3.\n\nBoundary Denoising Diffusion Model\nGiven a sentence S with length M , the named entity recognition task is to extract the entities E = {(l i , r i , t i )} N i=0 contained in the sentence, where N is the number of entities and l i , r i , t i denote the left and right boundary indices and type of the i-th entity. We formulate NER as a boundary denoising diffusion process, as shown in Figure 2 . We regard entity boundaries as data samples, then the boundary forward diffusion is to add Gaussian noise to the entity boundaries while the reverse diffusion process is to progressively recover the original entity boundaries from the noisy spans. Boundary Forward Diffusion Boundary forward diffusion is the process of adding noise to the entity boundary in a stepwise manner. In order to align the number of entities in different instances, we first expand the entity set to a fixed number K (> N ). There are two ways to expand the entities, repetition strategy and random strategy, which add K \u2212 N entities by duplicating entities or sampling random spans from a Gaussian distribution 2 . For convenience, we use B \u2208 R K\u00d72 to denote the boundaries of the K expanded entities, with all of them normalized by the sentence length M and scaled to (\u2212\u03bb, \u03bb) interval. Formally, given the entity boundaries as data samples x 0 = B, we can obtain the noisy spans at timestep t using the forward diffusion process. According to Equation (3), we have:\nx t = \u221a \u1fb1t x 0 + \u221a 1 \u2212 \u1fb1t \u03f5 (4)\nwhere \u03f5 \u223c N (0, I) is the noise sampled from the standard Gaussian. At each timestep, the noisy spans have the same shape as x 0 , i.e.,\nx 1 , x 2 , . . . , x T \u2208 R K\u00d72 .\nBoundary Reverse Diffusion Starting from the noisy spans x T sampled from the Gaussian distribution, boundary reverse diffusion adopts a non-Markovian denoising practice used in DDIM (Song et al., 2021) to recover entities boundaries. Assuming \u03c4 is an arithmetic subsequence of the complete timestep sequence [1, . . . , T ] of length \u03b3 with \u03c4 \u03b3 = T . Then we refine the noisy spans x \u03c4 i to 2 We will discuss these two practices in \u00a7 6.3.\nx \u03c4 i\u22121 as follows:\nEQUATION\n)\n\u03b5\u03c4 i = x \u03c4 i \u2212 \u221a \u03b1 \u03c4 i x0 \u221a 1 \u2212 \u03b1 \u03c4 i (6) x \u03c4 i\u22121 = \u221a \u03b1 \u03c4 i\u22121 x0 + 1 \u2212 \u03b1 \u03c4 i\u22121 \u03b5\u03c4 i (7)\nwhere x0 and \u03b5\u03c4 i are the predicted entity boundary and noise at timestep \u03c4 i . f \u03b8 (x t , S, t) is a learnable denoising network and we will cover the network architecture in the next section ( \u00a7 4.2). After \u03b3 iterations of DDIM, the noisy spans are progressively refined to the entity boundaries.\n\nNetwork Architecture\nDenoising network f \u03b8 (x t , S, t) accepts the noisy spans x t and the sentence S as inputs and predicts the corresponding entity boundaries x0 . As shown in Figure 2 , we parameterize the denoising network with a sentence encoder and an entity decoder.\nSentence Encoder consists of a BERT (Devlin et al., 2019) plus a stacked bi-directional LSTM.\nThe whole span encoder takes the sentence S as input and outputs the sentence encoding H S \u2208 R M \u00d7h . The sentence encoding H S will be calculated only once and reused across all timesteps to save computations.\nEntity Decoder uses the sentence encoding H S to first compute the representations of K noisy spans x t and then predicts the corresponding entity boundaries. Specifically, we discretize the noisy spans into word indexes by rescaling, multiplying and rounding 3 , then perform mean pooling over the Take gradient descent step by optimize\n\u2212 K i=1 log P c i (\u03c0 c (i)) + \u03b4\u2208l,r log P \u03b4 i (\u03c0 \u03b4 (i)) 10 until converged;\ninner-span tokens. The extracted span representations can be denoted as H X \u2208 R K\u00d7h . To further encode the spans, we design a span encoder that consists of a self-attention and a cross-attention layer. The former enhances the interaction between spans with key, query, and value as H X . And the latter fuses the sentence encoding to the span representation with key, value as H S , and query as H X . We further add the sinusoidal embedding E t (Vaswani et al., 2017) of timestep t to the span representations. Thus the new representations HX of the noisy spans can be computed:\nHX = SpanEncoder(H S , H X ) + E t ,\nThen we use two boundary pointers to predict the entity boundaries. For boundary \u03b4 \u2208 {l, r}, we compute the fusion representation H \u03b4 SX \u2208 R K\u00d7M \u00d7h of the noisy spans and the words, and then the probability of the word as the left or right boundaries P \u03b4 \u2208 R K\u00d7M can be computed as:\nH \u03b4 SX = H S W \u03b4 S + HX W \u03b4 X P \u03b4 = sigmoid(MLP(H \u03b4 SX ))\nwhere W \u03b4 S , W \u03b4 X \u2208 R h\u00d7h are two learnable matrixes and MLP is a two-layer perceptron. Based on the boundary probabilities, we can predict the boundary indices of the K noisy spans. If the current step is not the last denoising step, we compute x0 by normalizing the indices with sentence length M and scaling to (\u2212\u03bb, \u03bb) intervals. Then we conduct the next iteration of the reverse diffusion process according to Equations ( 5) to (7).\nIt is worth noting that we should not only locate entities but also classify them in named entity recognition. Therefore, we use an entity classifier to classify the noisy spans. The classification probability P c \u2208 R K\u00d7C is calculated as follows:\nP c = Classifier( HX ) Algorithm 2: Inference 1 xT \u223c N (0, I) \u2208 R K eval \u00d72\n2 \u03c4 is an arithmetic sequence of length \u03b3 with \u03c4\u03b3 = T 3 for i = \u03b3, . . . , 1 do 4 Compute x0, P l , P r and P c via f \u03b8 (xt, S, t)\n5 x\u03c4 i\u22121 = \u221a \u03b1\u03c4 i\u22121 x0 + 1 \u2212 \u03b1\u03c4 i\u22121 \u2022 x\u03c4 i \u2212 \u221a \u03b1\u03c4 i x0 \u221a 1\u2212\u03b1\u03c4 i 6 end 7 Decode entities (li, ri, ci) K eval i=0\n, where \u03b4i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c} 8 Perform post-processing on (li, ri, ci) K eval i=0 9 return final entities where C is the number of entity types and Classifier is a two-layer perceptron with a softmax layer.\nTraining Objective With K entities predicted from the noisy spans and N ground-truth entities, we first use the Hungarian algorithm (Kuhn, 1955) to solve the optimal matching \u03c0 between the two sets 4 as in Carion et al. (2020) . \u03c0(i) denotes the ground-truth entity corresponding to the i-th noisy span. Then, we train the boundary reverse process by maximizing the likelihood of the prediction:\nL = \u2212 K i=1 \u03b4\u2208{l,r,c} log P \u03b4 i \u03c0\u03b4 (i)\nwhere \u03c0l (i), \u03c0r (i) and \u03c0c (i) denote the left and right boundary indexes and type of the \u03c0(i) entity. Overall, Algorithm 1 displays the whole training procedure of our model for an explanation.\n\nInference\nDuring inference, DIFFUSIONNER first samples K eval noisy spans from a Gaussian distribution, then performs iterative denoising with the learned boundary reverse diffusion process based on the denoising timestep sequence \u03c4 . Then with the predicted probabilities on the boundaries and type, we can decode K eval candidate entities (l i , r i , c i ) K eval i=0 , where \u03b4 i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c}. After that, we employ two simple post-processing operations on these candidates: de-duplication and filtering. For spans with identical boundaries, we keep the one with the maximum type probability. For spans with the sum of prediction probabilities less than the threshold \u03c6, we discard them. The inference procedure is shown in Algorithm 2. 5 Experimental Settings\n\nDatasets\nFor nested NER, we choose three widely used datasets for evaluation: ACE04 (Doddington et al., 2004) , ACE05 (Walker et al., 2006) , and GE-NIA (Ohta et al., 2002) . ACE04 and ACE05 belong to the news domain and GENIA is in the biological domain. For flat NER, we use three common datasets to validate: CoNLL03 (Tjong Kim Sang and De Meulder, 2003) , OntoNotes (Pradhan et al., 2013) , and MSRA (Levow, 2006) . More details about datasets can be found in Appendix B.\n\nBaselines\nWe choose a variety of recent advanced methods as our baseline, which include: 1) Tagging-based methods (Strakov\u00e1 et al., 2019; Ju et al., 2018; Wang et al., 2020) ; 2) Span-based methods (Yu et al., 2020; Li et al., 2020; Wan et al., 2022; Lou et al., 2022; Zhu and Li, 2022; Yuan et al., 2022b) ; 3) Generation-based methods (Tan et al., 2021; Yan et al., 2021b; Lu et al., 2022) . More details about baselines can be found in Appendix D.\n\nImplementation Details\nFor a fair comparison, we use bert-large (Devlin et al., 2019) ary refinement, and thus obtain better performance.\nThe results also validate that our DIFFUSIONNER can recover entity boundaries from noisy spans via boundary denoising diffusion.\n\nAnalysis\nInference Efficiency To further validate whether our DIFFUSIONNER requires more inference computations, we also conduct experiments to compare the inference efficiency between DIFFUSIONNER and other generation-based models (Lu et al., 2022; Yan et al., 2021a) . Just as shown in Table 3 , we find that DIFFUSIONNER could achieve better performance while maintaining a faster inference speed with minimal parameter scale. Even with a denoising timestep of \u03b3 = 10, DIFFUSIONNER is 18\u00d7 and 3\u00d7 faster than them. This is because DIFFU-SIONNER generates all entities in parallel within several denoising timesteps, which avoids generating the linearized entity sequence in an autoregressive manner. In addition, DIFFUSIONNER shares sentence encoder across timesteps, which further accelerates inference speed. speed of DIFFUSIONNER under various numbers of noisy spans. Just as shown in Figure 3 , we find that, with an increase of denoising steps, the model obtains incremental performance improvement while sacrificing inference speed. Considering the trade-off between performance and efficiency, we set \u03b3 = 5 as the default setting. In addition, when the noisy spans are smaller, the improvement brought by increasing the denoising timesteps is more obvious. This study indicates that our DiffusionNER can effectively counterbalance the negative impact of undersampling noise spans on performance by utilizing additional timesteps. \n\nSampling Number\nAs a generative latent model, DIFFUSIONNER can decouple training and eval-uation, and dynamically sample noisy spans during evaluation. To manifest this advantage, we train DIFFUSIONNER on ACE04 with K = 60 noisy spans and evaluate it with different sampling numbers K eval . The results are shown in Figure 4 . Overall, the model performance becomes better as the sampling number of noisy spans increases. Specifically, we find that DIFFUSIONNER performs worse when K eval < 30. We guess this is because fewer noisy spans may not cover all potential entities. When sampling number K eval > 60, we find it could also slightly improve model performance. Overall, the dynamic sampling of noisy spans in DIFFUSIONNER has the following advantages: 1) we can improve model performance by controlling it to sample more noisy spans; 2) dynamic sampling strategy also allows the model to predict an arbitrary number of entities in any realworld application, avoiding the limitations of the sampling number at the training stage.\n\nAblation Study\nNetwork Architecture As shown in Table 4 , we conduct experiments to investigate the network architecture of the boundary reverse diffusion process. We found that DIFFUSIONNER performs better with a stronger pre-trained language model (PLM), as evidenced by an improvement of +0.53% on ACE04 and +0.11% on CoNLL03 when using roberta-large. Additionally, for the span encoder, we find that directly removing self-attention between noisy spans or cross-attention of spans to the sentence can significantly impair performance. When both are ablated, model performance decreases by 1.37% and 1.15% on ACE04 and CoNLL03. These results indicate that the interaction between the spans or noisy spans and the sentence is necessary. the added noise at each timestep during boundary forward diffusion process. Therefore, we analyze the performance of DIFFUSIONNER on different variance schedulers with different noise timesteps T . The results on ACE04 and CoNLL03 are shown in Table 5 . We find that the cosine scheduler generally yields superior results on the ACE04, while the linear scheduler proves to be more effective on CoNLL03. In addition, the performance of DIFFU-SIONNER varies with the choice of noise timestep, with the best performance achieved at T = 1000 for ACE04 and T = 1500 for CoNLL03.\n\nExpansion Stratagy\nThe expansion stratagy of the entity set can make the number of K noisy spans consistent across instances during training.\nWe conduct experiments to analyze the performance of DIFFUSIONNER for different expansion strategies with various numbers of noisy spans. The experimental results are shown in Table 6 . Generally, we find that the random strategy could achieve similar or better performance than the repetitive strategy. In addition, Table 6 shows that DIFFU-SIONNER is insensitive to the number of noisy spans during training. Considering that using more noisy spans brings more computation and memory usage, we set K = 60 as the default setting.\n\nConclusion\nIn this paper, we present DIFFUSIONNER, a novel generative approach for NER that converts the task into a boundary denoising diffusion process. Our evaluations on six nested and flat NER datasets show that DIFFUSIONNER achieves comparable or better performance compared to previous stateof-the-art models. Additionally, our additional analyses reveal the advantages of DIFFUSIONNER in terms of inference speed, progressive boundary refinement, and dynamic entity sampling. Overall, this study is a pioneering effort of diffusion models for extractive tasks on discrete text sequences, and we hope it may serve as a catalyst for more research about the potential of diffusion models in natural language understanding tasks.\n"}
{"question": "What is the relationship between the three metrics? ", "evidence": " Relation between Metrics\nWe present pair-wise correlations between the three automatic metrics in Tab. 8 and also visualize them in Fig. 7 . Among the three metrics, creativity correlates with informativeness moderately, mainly because shorter vehicles tend to be less creative than longer ones. The correlations of all other pairwise metrics are relatively weak. Thus, it is evident that the three metrics are independent of each other and it is necessary to measure each one of them to obtain a holistic view of SG evaluation. \n ", "options": ["A. They are independent.", "B. They affect each other.", "C. Only two of them affect each other.", "D. Their relationship depends."], "answer": "A", "content": "\nIntroduction\nSimiles play a vital role in human expression, making literal sentences imaginative and graspable. For example, Robert Burns famously wrote \"My Luve is like a red, red rose\" to metaphorically depict the beloved as being beautiful. In this simile, \"Luve\" (a.k.a. topic) is compared with \"red rose\" (a.k.a. vehicle) via the implicit property \"beautiful\" and the event \"is\". Here, topic, vehicle, property, and event are four main simile components (Hanks, 2013) . As a figure of speech, similes have been widely used in literature and conversations (Zheng et al., 2019; Chakrabarty et al., 2022) .\nSimile generation (SG) is a crucial task in natural language processing (Chakrabarty et al., 2020; Zhang et al., 2021; Lai and Nissim, 2022) , with the aim of polishing literal sentences into similes. In Fig. 1 , the literal sentence \"He yelps and howls.\" is polished into a simile by inserting the phrase \"like a wolf \", resulting in \"He yelps and The commonly used automatic metric BLEU deems the second candidate as the most high-quality one among all the generated similes, while our proposed metrics HAUSER deem the first candidate as the best one regarding its quality, creativity and informativeness, which better correlates with human ratings and also provides more criteria for SG evaluation.\nhowls like a wolf \". The ability to generate similes can assist various downstream tasks, such as making the generations more imaginative in story or poet generation task (Tartakovsky and Shen, 2018; Chakrabarty et al., 2022) and the generated response more human-like in dialogue generation task (Zheng et al., 2019) .\nAutomatic evaluation is critical for the SG task since it enables efficient, systematic, and scalable comparisons between models in general (Celikyilmaz et al., 2020) . However, existing studies are inadequate for effective SG evaluation. Task-agnostic automatic metrics (Papineni et al., 2002; Zhang et al., 2019; Li et al., 2016) are widely adopted for SG evaluation (Zhang et al., 2021; Lai and Nissim, 2022) , which have several limitations: (1) The simile components should receive more attention than other words during SG evaluation (e.g. \"he\" and \"wolf \" in Fig. 1 ), while there are no automatic metrics that consider the key components. (2) The SG task is open-ended, allowing for multiple plausible generations for the same input (Chakrabarty et al., 2020 ) (e.g. the howling man can be compared to \"wolf \", \"buffalo\", or \"tiger\" in Fig. 1 ). Hence, the metrics based on word overlap with a few references are inadequate to accurately mea-\n\nCriterion Literal Sentence\nExample Simile Candidates Quality Relevance Some raindrops struck the roof, window and ran down its panes. Some raindrops struck the roof, window and ran down its panes (like tears | like arrows).\n\nLogical Consistency\nStefan moved, every movement easy and precisely controlled.\nStefan moved (like lightning | like a dancer), every movement easy and precisely controlled.\n\nSentiment Consistency\nThe idea resounded throughout the land. The idea resounded (like an earthquake | like a thunderous wave) throughout the land.\n\nCreativity\nHe possessed a power of sarcasm which could scorch.\nHe possessed a power of sarcasm which could scorch (like vitriol | like fire).\n\nInformativeness\nThey gleamed. They gleamed (like the eyes of a cat | like the eyes of an angry cat).\nTable 1 : Examples of our criteria for Simile Generation (SG) Evaluation. We design five criteria from three perspectives. The vehicles of the better simile candidates given by each criterion are highlighted in bold.\nsure the overall quality of generated similes. As shown in Fig. 1 , the commonly used metric BLEU deems the second candidate as the highest quality, as it has more overlapped words with the only referenced groundtruth, while human deems the first candidate as the most coherent one.\n(3) The existing metrics are inadequate to provide fine-grained and comprehensive SG evaluation, considering that the creative generation tasks have distinct criteria for desired generations (Celikyilmaz et al., 2020) , such as novelty and complexity for story generation (Chhun et al., 2022) and logical consistency for dialogue generation (Pang et al., 2020) . However, establishing a comprehensive, efficient, and reliable evaluation system for SG is nontrivial, which raises three main concerns: (1) What criteria should be adopted to evaluate the SG task in a comprehensive and non-redundant fashion? (2) How to quantify each criterion into a metric thus enabling efficient and objective SG evaluation, given that the human evaluation of creative generation task is not only time-consuming but also subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014; Celikyilmaz et al., 2020) ? (3) Whether the proposed metrics are effective in providing useful scores to guide actual improvements in the realworld application of the SG model?\nIn this paper, we establish HAUSER, a Holistic and AUtomatic evaluation system for Simile gEneRation task, consisting of five criteria (Tab. 1):\n(1) The relevance between topic and vehicle, as the foundation of a simile is to compare the two via their shared properties (Paul, 1970) . (2) The logical consistency between the literal sentence and generated simile, since the aim of SG task is to polish the original sentence without altering its semantics (Tversky, 1977) . (3) The sentiment consistency between the literal sentence and generated simile, since similes generally transmit certain sentiment polarity (Qadir et al., 2015) . (4,5) The creativity and informativeness of the simile, since novel similes or those with richer content can enhance the literary experience (Jones and Estes, 2006; Roncero and de Almeida, 2015; Addison, 2001) . Overall, these five criteria can be categorized into three perspectives: quality (which considers relevance, logical, and sentiment consistency jointly), creativity, and informativeness. We further quantify each criterion into automatic metrics (Fig. 2 ) and prove their effectiveness through extensive experiments.\nTo the best of our knowledge, we are the first to systematically investigate the automatic evaluation of the SG task. To summarize, our contributions are mainly three-fold: (1) We establish a holistic and automatic evaluation system for the SG task, consisting of five criteria based on linguistic theories, facilitating both human and automatic evaluation of this task. (2) We design automatic metrics for each criterion, facilitating efficient and objective comparisons between SG models. (3) We conduct extensive experiments to verify that our metrics are significantly more correlated with human ratings than prior metrics.\n\nSimile Generation Task\nThere are two primary forms of the simile generation (SG) task: simile triplet completion and literal sentence polishing. For simile triplet completion, a model receives simile components, topic and property, and is required to generate the vehicle (Roncero and de Almeida, 2015; Zheng et al., 2019; Chen et al., 2022; He et al., 2022) . For literal sentence polishing, a model receives a literal sentence and is expected to convert it into similes (Zhang et al., 2021; Stowe et al., 2020; Chakrabarty et al., 2020; Lai and Nissim, 2022) . We focus on the latter. However, prior works mainly adopt task-agnostic automatic metrics to evaluate the SG task, raising concern as to whether the claimed improvements are comprehensive and reliable.\n\nAutomatic Evaluation for NLG Systems\nExisting automatic metrics for Natural Language Generation (NLG) evaluation can be categorized into task-agnostic and task-specific metrics. Taskagnostic metrics can be applied to various NLG tasks, which generally focus on the coherence of generations (Papineni et al., 2002; Zhang et al., 2019) , including n-gram-based metrics (Papineni et al., 2002; Lin, 2004; Denkowski and Lavie, 2014) and embedding-based metrics (Zhang et al., 2019; Zhao et al., 2019) . There are also many metrics for evaluating the diversity of generations (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) . Task-specific metrics are proposed to evaluate NLG systems on specific tasks (Tao et al., 2018; Dhingra et al., 2019; Ren et al., 2020) . Specifically, various works systematically study the evaluation of the creative generation task (Pang et al., 2020; Tevet and Berant, 2021; Chhun et al., 2022) . Different from these works, we revisit SG evaluation, propose holistic criteria based on linguistic theories, and design effective automatic metrics for it.\n\nHAUSER for SG evaluation\nWe establish HAUSER, a holistic and automatic evaluation system for SG evaluation, containing five criteria from three perspectives, and further design automatic metrics for each criterion (Fig. 2 ).\n\nQuality\nWe measure the overall quality of generated similes using three criteria: relevance, logical consistency, sentiment consistency. The key simile components -topic and vehicle -should be relevant, as the foundation of a simile is to compare the two via their shared properties (relevance) (Paul, 1970) . In Tab. 1, comparing \"raindrops\" to \"tears\" is more coherent than to \"arrows\". Additionally, the generated simile should remain logically consistent with the original sentence (logical consistency), as the SG task aims to polish the plain text without changing its semantics (Tversky, 1977) . In Tab. 1, comparing \"Stefan\" to \"dancer\" better depicts his controlled and easy movement than to \"lightning\". Furthermore, as similes generally transmit certain sentiment polarity (Qadir et al., 2015) , the generated simile should enhance the sentiment polarity of the original sentence (sentiment consistency). In Tab. 1, the vehicle \"thunderous wave\" enhances the positive polarity of the original sentence, while the vehicle \"earthquake\" brings a negative sentiment polarity in opposition to the original sentence.\n\nRelevance\nFor the relevance score, if the components of one simile are relevant, they tend to co-occur in simile sentences (Xiao et al., 2016; He et al., 2022) and possess shared properties (Paul, 1970; Tversky, 1977) . Hence, obtaining the relevance score requires large-scale simile sentences as references, as well as knowledge about the properties (adjectives) of each simile component. For a simile s, the relevance score is defined as follows:\nEQUATION\nwhere there are m p topic-vehicle pairs extracted from simile s, each denoted as (t, v) 1 . \u0393(t, v) is the set of similes containing (t, v) as simile components, each denoted as e. P e (t, v) is the probability that the simile components (t, v) share properties in the context of the simile sentence e.\nAn effective way to obtain the frequency information \u0393(t, v) and property knowledge P e (t, v) is to utilize the large-scale probabilistic simile knowledge base MAPS-KB (He et al., 2022) , which contains millions of simile triplets in the form of (topic, property, vehicle), along with frequency and two probabilistic metrics to model each triplet 2 . Specifically, the probabilistic metric Plausibility is calculated based on the confidence score of the simile instance (topic, property, vehicle, simile sentence) supporting the triplet, indicating the probability that the topic and vehicle share the property. The relevance score r can be calculated as follow:\nEQUATION\nwhere G (t,v) is the set of triplets (t, p ,v) containing the (t, v) pair in MAPS-KB, with p referring to the property. n and P are the metrics provided by MAPS-KB, where n and P denote the frequency and the plausibility of the triplet respectively. It is noticed that the metric is not coupled with MAPS-KB, as the frequency information can be obtained by referencing a large set of simile sentences and the property knowledge can be contained via other knowledge bases. More methods are beyond the scope of this paper. However, we additionally provide a method to approximate the relevance score. If we assume the probability that the simile components (t, v) share properties in each sentence is 1, the relevance score can be approximated as:\nEQUATION\nwhere n(t, v) denotes the number of samples that contain the simile components (t, v) in large-scale simile sentences. We discuss the effects of the referenced dataset size in Sec. 4.2.1.\n\nLogical Consistency\nThe literal sentence and the generated simile that are logically inconsistent generally exhibit contra-1 All the simile components in our work are extracted and cleaned using rules from (He et al., 2022) which determines the optimal semantics a component should carry, e.g., \"a kid in a candy store\" instead of just \"a kid\".\n2 More details of MAPS-KB is provided in Appx. D dictory logic. Hence, for a generated simile, we input the <literal text(l), simile(s)> sentence pair into existing pre-trained Multi-Genre Natural Language Inference (MNLI) model 3 , which determines the relation between them is entailment, neutral, or contradiction. The logical consistency score c l of this simile is defined as follows (Pang et al., 2020) :\nEQUATION\nwhere P (h <l,s> = c) represents the probability that the model predicts the relation of the sentence pair < l, s > to be contradiction (denoted as c).\n\nSentiment Consistency\nBetter similes tend to enhance the sentiment polarity of the original sentence (Qadir et al., 2015) . Hence, we first apply the model fine-tuned on the GLUE SST-2 dataset 4 to classify each simile as being either positive or negative. Then, the sentiment consistency score c s is defined as follows:\nEQUATION\nwhere a is the sentiment polarity of the literal sentence (positive or negative) predicted by the model. P (h s = a) and P (h l = a) denote the probabilities that the model predicts the sentiment polarity of the simile s and the literal sentence l to be a, respectively. It is noticed that different <topic, vehicle> pairs within a sentence may have distinct sentiment polarities, such as <She, scared rabbit> and <I, bird> in the simile \"If she escapes like a scared rabbit, I will fly like a bird to catch her.\". Directly inputting text containing multiple topic-vehicle pairs into the sentiment classification model will result in inferior performance. Therefore, for each simile, only the text from the beginning up to the first vehicle is input into the model (i.e. \"If she escapes like a scared rabbit\" in the given example), and for each literal sentence, the text from the beginning up to the first event (i.e. \"If she escapes\" in the given example) is input into the model.\nSince the aim of the SG task is to polish the plain text, the quality of similes generated from different texts can not be compared. Therefore, the normalized score among the simile candidates for each original text is utilized. Suppose there are m simile candidates S = {s 1 , s 2 , ..., s m } for the literal text l, the original relevance scores of R is R = {r 1 , r 2 , ..., r m } respectively. The normalized relevance score r \u2032 i of s i is formulated as follows:\nEQUATION\nwhich ranges from 0 to 1. Then, the normalized logical and sentiment consistency score c \u2032 li , c \u2032 si for each simile s i are obtained in the same manner 5 .\nFinally, the quality for simile s i is defined as the weighted combination of three parts as follows:\nEQUATION\nwhere \u03b1, \u03b2, and \u03b3 are hyperparameters.\n\nCreativity\nCreative similes can provide a better literary experience (Jones and Estes, 2006). In Tab. 1, comparing \"sarcasm\" to \"vitriol\" is less common than to \"fire\", yet it better conveys the intensity of a person's sarcasm. Hence, we design creativity score. Previous studies mainly evaluate the creativity of text generation tasks via human evaluation (Sai et al., 2022) , since measuring the creativity of openended text is a relatively difficult task (Celikyilmaz et al., 2020) . Although there have been many works evaluating the diversity of open-ended text generation (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) , these metrics are not suitable for measuring the creativity of the text. Because the diversity metrics take a set of generated text as input and output one score, while a creativity metric is required to measure each text individually and output a set of corresponding scores.\nDifferent from other open-ended generation tasks, the components of the generated similes enable us to evaluate creativity automatically. According to linguists, the creativity of a simile is determined by vehicles (Pierce and Chiappe, 2008; Roncero and de Almeida, 2015) . Intuitively, the generated simile may be less creative if its extracted 5 If all the relevance scores ri in R are the same, the normalized relevance scores r \u2032 i in R \u2032 are set to 0.5 uniformly.\ntopic-vehicle pair co-occurs frequently, or if many topics are compared to its vehicle in the corpus. Therefore, we adopt large-scale corpora as references when designing our creativity metric. The creativity score of s is calculated as follows:\nEQUATION\nwhere there are m v vehicles extracted from the simile s, each denoted as v. N v denotes the frequency of the vehicles appearing in the similes in the corpora. The log transformation aims to reduce the influence of extreme values.\nAn effective way to obtain the adequate frequency information N v is to utilize the millionscale simile knowledge base MAPS-KB, where the N v can be defined as follows:\nEQUATION\nG v is the set of triplets containing the vehicle v in MAPS-KB, n denotes the frequency of the triplet.\nIt is noticed that the metric is not coupled with MAPS-KB, as N v can also be obtained by counting the samples containing the vehicle v in largescale simile sentences. The method of obtaining the simile sentences is beyond the scope of this paper. Nevertheless, we discuss the effects of the referenced dataset size in Sec. 4.2.2.\n\nInformativeness\nThe vehicle with richer content can create a more impact and vivid impression (Addison, 2001) . In the example from Tab. 1, the addition of the word \"angry\" makes the similes more expressive. Therefore, we design the metric informativeness to measure the content richness of the vehicles.\nIntuitively, the more words a vehicle contains, the richer its content will be. Hence, for a given simile s, we adopt the average length of the extracted vehicles to be the informativeness score 6 (Chakrabarty et al., 2020; Zhang et al., 2021) , defined as I i = 1 mv v\u2208s len(v), where there are m v vehicles extracted from simile s. Here, BLEU2, Rouge2, and BERTScorelarge are presented since they perform the best in their respective category. To avoid overlapping points, random jitters sampled from N (0, 0.05 2 ) were added to human ratings after fitting the regression.\n\nSimile Generation\nThe existing datasets for the SG task are either Chinese (Zhang et al., 2021) , limited to the simile triplet completion (Roncero and de Almeida, 2015; Chen et al., 2022) , or having all vehicles located at end of the sentence (Chakrabarty et al., 2022; Lai and Nissim, 2022) , which are not practical for English simile generation in a real-world application.\nTo bridge the gap, we construct a large-scale English dataset for SG task based on simile sentences from (He et al., 2022) , which contains 524k simile sentences labeled with topic and vehicle. The output decoder target is the simile sentence s and the input encoder source is s rewritten to drop the comparator \"like\" and the vehicle. For example, given s = \"The idea resounded like a thunderclap throughout the land.\", the encoder source would be \"The idea resounded throughout the land.\". In particular, we remove the simile sentences whose event is a linking verb (e.g. be, seem, turn) as they would be meaningless after the vehicle is removed. The final train, validation and test sets contain 139k, 2.5k, and 2.5k sentence pairs, respectively. Based on our constructed dataset, we finetune a pre-trained sequence-to-sequence model, BART (Lewis et al., 2020) , for the SG task, which has been demonstrated to be an effective framework for various figurative language generation (Zhang and Wan, 2021; Chakrabarty et al., 2022; He et al., 2022; Lai and Nissim, 2022) . The experiments are run on RTX3090 GPU and the implementation of BART is based on the HuggingFace Transformers 7 . The experiments are run with a batch size of 16, a max sequence length of 128, and a learning rate of 4e-5 for 10 epochs.\n\nEvaluation Dataset Construction\nFirstly, we randomly sample 50 literal sentences from the test set and adopt the trained SG model to generate five candidates for each one. Then, for each perspective, three raters are asked to rate each 7 https://github.com/huggingface/transformers/ simile from 1 to 5, where 1 denotes the worst and 5 denotes the best 8 . Since evaluating the quality of generated similes is subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014), we remove the simile-literal sentence pairs if (1) raters argue that the pairs lack context and are difficult to rate (e.g. \"Nobody can shoot.\") or (2) some raters rate them as low quality (quality score of 1-2), while others rate them as high quality (scores of 4-5) (Niculae and Danescu-Niculescu-Mizil, 2014) . Moreover, we measure the inter-rater agreement by holding out the ratings of one rater at a time, calculating the correlations with the average of the other rater's ratings, and finally calculating the average or maximum of all the held-out correlations (denoted as \"Mean\" and \"Max\", respectively). The inter-rater agreement before and after applying the filtering strategies is shown in Tab. 2. Overall, the final inter-rater agreement ensures the reliability of our evaluation of automatic metrics and the filtering strategies improve the inter-rater agreement generally. We finally get 150 simile candidates generated from 44 literal sentences.\n\nQuality\nWe compare our quality metric with the following automatic metrics 9 : (1) BLEU (Papineni et \n\n2002) calculates the precision of n-gram matches,\n(2) RougeL (Lin, 2004 ) is a recall-oriented metric, (3) METEOR (Denkowski and Lavie, 2014) proposes a set of linguistic rules to compare the hypothesis with the reference, ( 4) BERTScore (Zhang et al., 2019) calculates the cosine similarity between the BERT embeddings, ( 5) Perplexity (Pang et al., 2020) measures the proximity of a language model, the inverse of which is utilized.\nCorrelations with Human Ratings. Tab. 3 shows the correlation coefficients between automatic metrics and human ratings. Firstly, our metrics are significantly more correlated with human ratings than prior automatic metrics. Moreover, all the sentence-level metrics, which consider the semantics of the entire sentence, perform worse than almost all the n-gram-level metrics, which compare the n-grams between the hypothesis and the reference, which reveals that simile components need to be specifically considered during SG evaluation.\nAccording to the visualized correlation result in Fig. 3 , datapoints from prior automatic metrics tend to scatter at 0 or 1, while the datapoints from our metric are distributed closer to the fitter line, proving that our metric can better measure the quality.\nRecommendation Task. We compare the rankings given by automatic metrics with human rankings 10 . We adopt the following metrics: Hit Ratio at rank K (HR@K(K=1,3)), Norgenerated from different literal sentences can not be compared. Please refer to Appx. C for the implementation of them. 10 We remove the literal sentences with fewer than three valid simile candidates in this task, as they are too simple to rank. We finally get 134 sentences from 35 literal sentences.\n\nMetrics\nHR@1 HR@3 nDCG@1 nDCG@3 MRR malized Discounted Cumulative Gain at rank K (NDCG@K(K=1,3)) 11 , and Mean Reciprocal Rank (MRR). From Tab. 4, our metric achieves significant improvement compared to other metrics, indicating that our metric can yield more accurate rankings for quality. Also, the n-gram-level metrics generally outperform sentence-level metrics, which is consistent with the result in Tab. 3. Ablation Study. To investigate the importance of different sub-metrics in quality metric, we compare the correlation between quality metric and human ratings after removing each sub-metric individually. From Tab. 3, the removal of any sub-metric leads to a decline in performance, which proves the effectiveness of each sub-metric. Among three components, the removal of the relevance results in the largest performance drop, which reveals that relevance is the most important sub-metric.\nThe Effects of Hyperparameters. Since different sub-metrics have varying levels of importance, we study the correlation results when gradually increasing the weight of relevance component and decreasing the weight of sentiment consistency component (as in Tab. 5). From Fig. 4 (left), increasing the weight of the relevance component consistently results in improved performance, peaking at the combination [7](\u03b1, \u03b2, \u03b3 = 3/6, 2/6, 1/6), before eventually causing a decline in performance. This reveals that although relevance is the most important sub-metric, too much weight on it can be detrimental.\nThe Effects of Referenced Dataset Size. We sample different numbers of simile sentences from (He et al., 2022) as references for relevance 5/6, 1/12, 1/12\nTable 5 : The setting of each hyperparameters combination for the quality metric. The result is shown in Fig. 4 (left).\n[\n1] [2] [3] [4] [5] [6] [7] [8] [9]\nHyper-parameters Combination score and study the correlation between the quality metric and human ratings 12 . From Fig. 4 (right) 13 , correlations grow linearly with exponential growth in referenced dataset size, indicating that using datasets larger than 100k will improve the correlation coefficients. Moreover, the performance at the peak surpasses the prior automatic metrics, proving the effectiveness of our approximation method.\n\nCreativity\nWe compare our creativity metric with the following automatic metrics: (1) Perplexity which is often utilized to measure diversity as well (Tevet and Berant, 2021) , (2) Self-BLEU (Zhu et al., 2018) calculates the BLEU score of each generation against all other generations as references, (3) Distinct n-grams(Dist) (Tevet and Berant, 2021) , which is the fraction of distinct n-grams from all possible n-grams across all generations.\nCorrelations with Human Ratings. From Tab. 6, our metric creativity is significantly more correlated with human evaluation scores compared with prior diversity metrics. According to the visualized correlation result in Fig. 5 , the prior diversity metrics have either wide confidence intervals (Perplexity, Dist) or scattered datapoints (self-BLEU), whereas our creativity metrics exhibit stronger linear correlation and narrower confidence intervals (Creativty w/ Log), implying higher reliability.\nRecommendation Task. We compare the rankings given by automatic metrics with human rank- ings. According to Tab. 7, our creativity metric outperforms prior automatic metrics, which proves our metric can better measure the creativity of simile candidates given a literal sentence, which is consistent with the results in Tab. 6. Ablation Study. According to Tab. 6, removing the log transformation leads to significant performance drops. According to the visualized correlation result in Fig. 5 , the datapoints are distributed closer to the fitter line and exhibit narrower confidence intervals after applying the log transformation, which further proves that log transformation is essential for our creativity metric.\nThe Effects of Referenced Dataset Size. According to Fig. 6 (left), the correlation coefficients increase continuously and eventually converge as the number of referenced sentences increases. Moreover, the performance after convergence is comparable to that given by the creativity metric based on the simile KB. The trend reveals that our metric referencing 10k similes can achieve a promising correlation with human ratings.\n\nInformativeness\nThe Pearson and Spearman correlation coefficients between our informativeness metric and human ratings are 0.798 and 0.882, respectively. According to Fig. 6 (right), the strong linear correlation between the metric and human ratings proves that our informativeness metric is simple yet quite effective.\n\nRelation between Metrics\nWe present pair-wise correlations between the three automatic metrics in Tab. 8 and also visualize them in Fig. 7 . Among the three metrics, creativity correlates with informativeness moderately, mainly because shorter vehicles tend to be less creative than longer ones. The correlations of all other pairwise metrics are relatively weak. Thus, it is evident that the three metrics are independent of each other and it is necessary to measure each one of them to obtain a holistic view of SG evaluation. \n\nHAUSER Application\nWe perform a case study to prove that our designed automatic metrics are effective for various methods. Here, we apply our metrics to a retrieval method (Zhang et al., 2021) (denoted as BM25), which utilizes the 20 context words around the insertion position given by groundtruth to retrieve the 5 most similar samples based on the BM25 ranking score from the training set, and adopts the vehicles from these samples to be those of simile candidates. This method ensures the diversity of generated similes. The method introduced in Sec. 4.1 is denoted as Ours. Given the candidates generated by each method, we rerank them using a weighted combination of quality, creativity, and informativeness rankings obtained by HAUSER, with a ratio of 2:2:1. From Tab. 11 in Appendix, the candidates generated by various methods can be more correlated with human rankings after being ranked by our metrics, thus proving the generality of our metrics. It is noticed that the insertion position for BM25 is provided by the groundtruth, while the insertion position for Ours is predicted by the model, thus proving the effectiveness of our generation method.\n\nConclusion\nIn this work, we systematically investigate the evaluation of the Simile Generation (SG) task. We establish a holistic and automatic evaluation system for the SG task, containing five criteria from three perspectives, and propose holistic automatic metrics for each criterion. Extensive experiments verify the effectiveness of our metrics. more correlated with humans than prior referencebased metrics (e.g. BLEU, Rouge, BERTScore), our metrics are still reference-based and rely on the quality and scale of referenced data. We have discussed the effect of referenced dataset size in our paper and will design reference-free metrics to further complement our metrics in future work. Additionally, since our metrics utilize a million-scale simile knowledge base or large-scale simile sentences as references, the efficiency of our method is slightly lower than the automatic metrics based on a few references. Nevertheless, this limitation does not prevent our metrics from performing systematic and scalable comparisons between SG models.\n"}
{"question": "What do we mainly do in our work?", "evidence": "  In this work, we systematically investigate the evaluation of the Simile Generation (SG) task. We establish a holistic and automatic evaluation system for the SG task, containing five criteria from three perspectives, and propose holistic automatic metrics for each criterion. Extensive experiments verify the effectiveness of our metrics. more correlated with humans than prior referencebased metrics (e.g. BLEU, Rouge, BERTScore), our metrics are still reference-based and rely on the quality and scale of referenced data. We have discussed the effect of referenced dataset size in our paper and will design reference-free metrics to further complement our metrics in future work. Additionally, since our metrics utilize a million-scale simile knowledge base or large-scale simile sentences as references, the efficiency of our method is slightly lower than the automatic metrics based on a few references. Nevertheless, this limitation does not prevent our metrics from performing systematic and scalable comparisons between SG models.\n ", "options": ["A. Our metrics leverage a knowledge base consisting of millions of simile instances or a large collection of simile sentences as references.", "B. We develop a comprehensive and automated evaluation system for the task of SG.", "C. We do extensive experiments to verify the effectiveness of our metrics.", "D. We discovered the effect of referenced dataset size."], "answer": "B", "content": "\nIntroduction\nSimiles play a vital role in human expression, making literal sentences imaginative and graspable. For example, Robert Burns famously wrote \"My Luve is like a red, red rose\" to metaphorically depict the beloved as being beautiful. In this simile, \"Luve\" (a.k.a. topic) is compared with \"red rose\" (a.k.a. vehicle) via the implicit property \"beautiful\" and the event \"is\". Here, topic, vehicle, property, and event are four main simile components (Hanks, 2013) . As a figure of speech, similes have been widely used in literature and conversations (Zheng et al., 2019; Chakrabarty et al., 2022) .\nSimile generation (SG) is a crucial task in natural language processing (Chakrabarty et al., 2020; Zhang et al., 2021; Lai and Nissim, 2022) , with the aim of polishing literal sentences into similes. In Fig. 1 , the literal sentence \"He yelps and howls.\" is polished into a simile by inserting the phrase \"like a wolf \", resulting in \"He yelps and The commonly used automatic metric BLEU deems the second candidate as the most high-quality one among all the generated similes, while our proposed metrics HAUSER deem the first candidate as the best one regarding its quality, creativity and informativeness, which better correlates with human ratings and also provides more criteria for SG evaluation.\nhowls like a wolf \". The ability to generate similes can assist various downstream tasks, such as making the generations more imaginative in story or poet generation task (Tartakovsky and Shen, 2018; Chakrabarty et al., 2022) and the generated response more human-like in dialogue generation task (Zheng et al., 2019) .\nAutomatic evaluation is critical for the SG task since it enables efficient, systematic, and scalable comparisons between models in general (Celikyilmaz et al., 2020) . However, existing studies are inadequate for effective SG evaluation. Task-agnostic automatic metrics (Papineni et al., 2002; Zhang et al., 2019; Li et al., 2016) are widely adopted for SG evaluation (Zhang et al., 2021; Lai and Nissim, 2022) , which have several limitations: (1) The simile components should receive more attention than other words during SG evaluation (e.g. \"he\" and \"wolf \" in Fig. 1 ), while there are no automatic metrics that consider the key components. (2) The SG task is open-ended, allowing for multiple plausible generations for the same input (Chakrabarty et al., 2020 ) (e.g. the howling man can be compared to \"wolf \", \"buffalo\", or \"tiger\" in Fig. 1 ). Hence, the metrics based on word overlap with a few references are inadequate to accurately mea-\n\nCriterion Literal Sentence\nExample Simile Candidates Quality Relevance Some raindrops struck the roof, window and ran down its panes. Some raindrops struck the roof, window and ran down its panes (like tears | like arrows).\n\nLogical Consistency\nStefan moved, every movement easy and precisely controlled.\nStefan moved (like lightning | like a dancer), every movement easy and precisely controlled.\n\nSentiment Consistency\nThe idea resounded throughout the land. The idea resounded (like an earthquake | like a thunderous wave) throughout the land.\n\nCreativity\nHe possessed a power of sarcasm which could scorch.\nHe possessed a power of sarcasm which could scorch (like vitriol | like fire).\n\nInformativeness\nThey gleamed. They gleamed (like the eyes of a cat | like the eyes of an angry cat).\nTable 1 : Examples of our criteria for Simile Generation (SG) Evaluation. We design five criteria from three perspectives. The vehicles of the better simile candidates given by each criterion are highlighted in bold.\nsure the overall quality of generated similes. As shown in Fig. 1 , the commonly used metric BLEU deems the second candidate as the highest quality, as it has more overlapped words with the only referenced groundtruth, while human deems the first candidate as the most coherent one.\n(3) The existing metrics are inadequate to provide fine-grained and comprehensive SG evaluation, considering that the creative generation tasks have distinct criteria for desired generations (Celikyilmaz et al., 2020) , such as novelty and complexity for story generation (Chhun et al., 2022) and logical consistency for dialogue generation (Pang et al., 2020) . However, establishing a comprehensive, efficient, and reliable evaluation system for SG is nontrivial, which raises three main concerns: (1) What criteria should be adopted to evaluate the SG task in a comprehensive and non-redundant fashion? (2) How to quantify each criterion into a metric thus enabling efficient and objective SG evaluation, given that the human evaluation of creative generation task is not only time-consuming but also subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014; Celikyilmaz et al., 2020) ? (3) Whether the proposed metrics are effective in providing useful scores to guide actual improvements in the realworld application of the SG model?\nIn this paper, we establish HAUSER, a Holistic and AUtomatic evaluation system for Simile gEneRation task, consisting of five criteria (Tab. 1):\n(1) The relevance between topic and vehicle, as the foundation of a simile is to compare the two via their shared properties (Paul, 1970) . (2) The logical consistency between the literal sentence and generated simile, since the aim of SG task is to polish the original sentence without altering its semantics (Tversky, 1977) . (3) The sentiment consistency between the literal sentence and generated simile, since similes generally transmit certain sentiment polarity (Qadir et al., 2015) . (4,5) The creativity and informativeness of the simile, since novel similes or those with richer content can enhance the literary experience (Jones and Estes, 2006; Roncero and de Almeida, 2015; Addison, 2001) . Overall, these five criteria can be categorized into three perspectives: quality (which considers relevance, logical, and sentiment consistency jointly), creativity, and informativeness. We further quantify each criterion into automatic metrics (Fig. 2 ) and prove their effectiveness through extensive experiments.\nTo the best of our knowledge, we are the first to systematically investigate the automatic evaluation of the SG task. To summarize, our contributions are mainly three-fold: (1) We establish a holistic and automatic evaluation system for the SG task, consisting of five criteria based on linguistic theories, facilitating both human and automatic evaluation of this task. (2) We design automatic metrics for each criterion, facilitating efficient and objective comparisons between SG models. (3) We conduct extensive experiments to verify that our metrics are significantly more correlated with human ratings than prior metrics.\n\nSimile Generation Task\nThere are two primary forms of the simile generation (SG) task: simile triplet completion and literal sentence polishing. For simile triplet completion, a model receives simile components, topic and property, and is required to generate the vehicle (Roncero and de Almeida, 2015; Zheng et al., 2019; Chen et al., 2022; He et al., 2022) . For literal sentence polishing, a model receives a literal sentence and is expected to convert it into similes (Zhang et al., 2021; Stowe et al., 2020; Chakrabarty et al., 2020; Lai and Nissim, 2022) . We focus on the latter. However, prior works mainly adopt task-agnostic automatic metrics to evaluate the SG task, raising concern as to whether the claimed improvements are comprehensive and reliable.\n\nAutomatic Evaluation for NLG Systems\nExisting automatic metrics for Natural Language Generation (NLG) evaluation can be categorized into task-agnostic and task-specific metrics. Taskagnostic metrics can be applied to various NLG tasks, which generally focus on the coherence of generations (Papineni et al., 2002; Zhang et al., 2019) , including n-gram-based metrics (Papineni et al., 2002; Lin, 2004; Denkowski and Lavie, 2014) and embedding-based metrics (Zhang et al., 2019; Zhao et al., 2019) . There are also many metrics for evaluating the diversity of generations (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) . Task-specific metrics are proposed to evaluate NLG systems on specific tasks (Tao et al., 2018; Dhingra et al., 2019; Ren et al., 2020) . Specifically, various works systematically study the evaluation of the creative generation task (Pang et al., 2020; Tevet and Berant, 2021; Chhun et al., 2022) . Different from these works, we revisit SG evaluation, propose holistic criteria based on linguistic theories, and design effective automatic metrics for it.\n\nHAUSER for SG evaluation\nWe establish HAUSER, a holistic and automatic evaluation system for SG evaluation, containing five criteria from three perspectives, and further design automatic metrics for each criterion (Fig. 2 ).\n\nQuality\nWe measure the overall quality of generated similes using three criteria: relevance, logical consistency, sentiment consistency. The key simile components -topic and vehicle -should be relevant, as the foundation of a simile is to compare the two via their shared properties (relevance) (Paul, 1970) . In Tab. 1, comparing \"raindrops\" to \"tears\" is more coherent than to \"arrows\". Additionally, the generated simile should remain logically consistent with the original sentence (logical consistency), as the SG task aims to polish the plain text without changing its semantics (Tversky, 1977) . In Tab. 1, comparing \"Stefan\" to \"dancer\" better depicts his controlled and easy movement than to \"lightning\". Furthermore, as similes generally transmit certain sentiment polarity (Qadir et al., 2015) , the generated simile should enhance the sentiment polarity of the original sentence (sentiment consistency). In Tab. 1, the vehicle \"thunderous wave\" enhances the positive polarity of the original sentence, while the vehicle \"earthquake\" brings a negative sentiment polarity in opposition to the original sentence.\n\nRelevance\nFor the relevance score, if the components of one simile are relevant, they tend to co-occur in simile sentences (Xiao et al., 2016; He et al., 2022) and possess shared properties (Paul, 1970; Tversky, 1977) . Hence, obtaining the relevance score requires large-scale simile sentences as references, as well as knowledge about the properties (adjectives) of each simile component. For a simile s, the relevance score is defined as follows:\nEQUATION\nwhere there are m p topic-vehicle pairs extracted from simile s, each denoted as (t, v) 1 . \u0393(t, v) is the set of similes containing (t, v) as simile components, each denoted as e. P e (t, v) is the probability that the simile components (t, v) share properties in the context of the simile sentence e.\nAn effective way to obtain the frequency information \u0393(t, v) and property knowledge P e (t, v) is to utilize the large-scale probabilistic simile knowledge base MAPS-KB (He et al., 2022) , which contains millions of simile triplets in the form of (topic, property, vehicle), along with frequency and two probabilistic metrics to model each triplet 2 . Specifically, the probabilistic metric Plausibility is calculated based on the confidence score of the simile instance (topic, property, vehicle, simile sentence) supporting the triplet, indicating the probability that the topic and vehicle share the property. The relevance score r can be calculated as follow:\nEQUATION\nwhere G (t,v) is the set of triplets (t, p ,v) containing the (t, v) pair in MAPS-KB, with p referring to the property. n and P are the metrics provided by MAPS-KB, where n and P denote the frequency and the plausibility of the triplet respectively. It is noticed that the metric is not coupled with MAPS-KB, as the frequency information can be obtained by referencing a large set of simile sentences and the property knowledge can be contained via other knowledge bases. More methods are beyond the scope of this paper. However, we additionally provide a method to approximate the relevance score. If we assume the probability that the simile components (t, v) share properties in each sentence is 1, the relevance score can be approximated as:\nEQUATION\nwhere n(t, v) denotes the number of samples that contain the simile components (t, v) in large-scale simile sentences. We discuss the effects of the referenced dataset size in Sec. 4.2.1.\n\nLogical Consistency\nThe literal sentence and the generated simile that are logically inconsistent generally exhibit contra-1 All the simile components in our work are extracted and cleaned using rules from (He et al., 2022) which determines the optimal semantics a component should carry, e.g., \"a kid in a candy store\" instead of just \"a kid\".\n2 More details of MAPS-KB is provided in Appx. D dictory logic. Hence, for a generated simile, we input the <literal text(l), simile(s)> sentence pair into existing pre-trained Multi-Genre Natural Language Inference (MNLI) model 3 , which determines the relation between them is entailment, neutral, or contradiction. The logical consistency score c l of this simile is defined as follows (Pang et al., 2020) :\nEQUATION\nwhere P (h <l,s> = c) represents the probability that the model predicts the relation of the sentence pair < l, s > to be contradiction (denoted as c).\n\nSentiment Consistency\nBetter similes tend to enhance the sentiment polarity of the original sentence (Qadir et al., 2015) . Hence, we first apply the model fine-tuned on the GLUE SST-2 dataset 4 to classify each simile as being either positive or negative. Then, the sentiment consistency score c s is defined as follows:\nEQUATION\nwhere a is the sentiment polarity of the literal sentence (positive or negative) predicted by the model. P (h s = a) and P (h l = a) denote the probabilities that the model predicts the sentiment polarity of the simile s and the literal sentence l to be a, respectively. It is noticed that different <topic, vehicle> pairs within a sentence may have distinct sentiment polarities, such as <She, scared rabbit> and <I, bird> in the simile \"If she escapes like a scared rabbit, I will fly like a bird to catch her.\". Directly inputting text containing multiple topic-vehicle pairs into the sentiment classification model will result in inferior performance. Therefore, for each simile, only the text from the beginning up to the first vehicle is input into the model (i.e. \"If she escapes like a scared rabbit\" in the given example), and for each literal sentence, the text from the beginning up to the first event (i.e. \"If she escapes\" in the given example) is input into the model.\nSince the aim of the SG task is to polish the plain text, the quality of similes generated from different texts can not be compared. Therefore, the normalized score among the simile candidates for each original text is utilized. Suppose there are m simile candidates S = {s 1 , s 2 , ..., s m } for the literal text l, the original relevance scores of R is R = {r 1 , r 2 , ..., r m } respectively. The normalized relevance score r \u2032 i of s i is formulated as follows:\nEQUATION\nwhich ranges from 0 to 1. Then, the normalized logical and sentiment consistency score c \u2032 li , c \u2032 si for each simile s i are obtained in the same manner 5 .\nFinally, the quality for simile s i is defined as the weighted combination of three parts as follows:\nEQUATION\nwhere \u03b1, \u03b2, and \u03b3 are hyperparameters.\n\nCreativity\nCreative similes can provide a better literary experience (Jones and Estes, 2006). In Tab. 1, comparing \"sarcasm\" to \"vitriol\" is less common than to \"fire\", yet it better conveys the intensity of a person's sarcasm. Hence, we design creativity score. Previous studies mainly evaluate the creativity of text generation tasks via human evaluation (Sai et al., 2022) , since measuring the creativity of openended text is a relatively difficult task (Celikyilmaz et al., 2020) . Although there have been many works evaluating the diversity of open-ended text generation (Li et al., 2016; Zhu et al., 2018; Tevet and Berant, 2021) , these metrics are not suitable for measuring the creativity of the text. Because the diversity metrics take a set of generated text as input and output one score, while a creativity metric is required to measure each text individually and output a set of corresponding scores.\nDifferent from other open-ended generation tasks, the components of the generated similes enable us to evaluate creativity automatically. According to linguists, the creativity of a simile is determined by vehicles (Pierce and Chiappe, 2008; Roncero and de Almeida, 2015) . Intuitively, the generated simile may be less creative if its extracted 5 If all the relevance scores ri in R are the same, the normalized relevance scores r \u2032 i in R \u2032 are set to 0.5 uniformly.\ntopic-vehicle pair co-occurs frequently, or if many topics are compared to its vehicle in the corpus. Therefore, we adopt large-scale corpora as references when designing our creativity metric. The creativity score of s is calculated as follows:\nEQUATION\nwhere there are m v vehicles extracted from the simile s, each denoted as v. N v denotes the frequency of the vehicles appearing in the similes in the corpora. The log transformation aims to reduce the influence of extreme values.\nAn effective way to obtain the adequate frequency information N v is to utilize the millionscale simile knowledge base MAPS-KB, where the N v can be defined as follows:\nEQUATION\nG v is the set of triplets containing the vehicle v in MAPS-KB, n denotes the frequency of the triplet.\nIt is noticed that the metric is not coupled with MAPS-KB, as N v can also be obtained by counting the samples containing the vehicle v in largescale simile sentences. The method of obtaining the simile sentences is beyond the scope of this paper. Nevertheless, we discuss the effects of the referenced dataset size in Sec. 4.2.2.\n\nInformativeness\nThe vehicle with richer content can create a more impact and vivid impression (Addison, 2001) . In the example from Tab. 1, the addition of the word \"angry\" makes the similes more expressive. Therefore, we design the metric informativeness to measure the content richness of the vehicles.\nIntuitively, the more words a vehicle contains, the richer its content will be. Hence, for a given simile s, we adopt the average length of the extracted vehicles to be the informativeness score 6 (Chakrabarty et al., 2020; Zhang et al., 2021) , defined as I i = 1 mv v\u2208s len(v), where there are m v vehicles extracted from simile s. Here, BLEU2, Rouge2, and BERTScorelarge are presented since they perform the best in their respective category. To avoid overlapping points, random jitters sampled from N (0, 0.05 2 ) were added to human ratings after fitting the regression.\n\nSimile Generation\nThe existing datasets for the SG task are either Chinese (Zhang et al., 2021) , limited to the simile triplet completion (Roncero and de Almeida, 2015; Chen et al., 2022) , or having all vehicles located at end of the sentence (Chakrabarty et al., 2022; Lai and Nissim, 2022) , which are not practical for English simile generation in a real-world application.\nTo bridge the gap, we construct a large-scale English dataset for SG task based on simile sentences from (He et al., 2022) , which contains 524k simile sentences labeled with topic and vehicle. The output decoder target is the simile sentence s and the input encoder source is s rewritten to drop the comparator \"like\" and the vehicle. For example, given s = \"The idea resounded like a thunderclap throughout the land.\", the encoder source would be \"The idea resounded throughout the land.\". In particular, we remove the simile sentences whose event is a linking verb (e.g. be, seem, turn) as they would be meaningless after the vehicle is removed. The final train, validation and test sets contain 139k, 2.5k, and 2.5k sentence pairs, respectively. Based on our constructed dataset, we finetune a pre-trained sequence-to-sequence model, BART (Lewis et al., 2020) , for the SG task, which has been demonstrated to be an effective framework for various figurative language generation (Zhang and Wan, 2021; Chakrabarty et al., 2022; He et al., 2022; Lai and Nissim, 2022) . The experiments are run on RTX3090 GPU and the implementation of BART is based on the HuggingFace Transformers 7 . The experiments are run with a batch size of 16, a max sequence length of 128, and a learning rate of 4e-5 for 10 epochs.\n\nEvaluation Dataset Construction\nFirstly, we randomly sample 50 literal sentences from the test set and adopt the trained SG model to generate five candidates for each one. Then, for each perspective, three raters are asked to rate each 7 https://github.com/huggingface/transformers/ simile from 1 to 5, where 1 denotes the worst and 5 denotes the best 8 . Since evaluating the quality of generated similes is subjective and blurred (Niculae and Danescu-Niculescu-Mizil, 2014), we remove the simile-literal sentence pairs if (1) raters argue that the pairs lack context and are difficult to rate (e.g. \"Nobody can shoot.\") or (2) some raters rate them as low quality (quality score of 1-2), while others rate them as high quality (scores of 4-5) (Niculae and Danescu-Niculescu-Mizil, 2014) . Moreover, we measure the inter-rater agreement by holding out the ratings of one rater at a time, calculating the correlations with the average of the other rater's ratings, and finally calculating the average or maximum of all the held-out correlations (denoted as \"Mean\" and \"Max\", respectively). The inter-rater agreement before and after applying the filtering strategies is shown in Tab. 2. Overall, the final inter-rater agreement ensures the reliability of our evaluation of automatic metrics and the filtering strategies improve the inter-rater agreement generally. We finally get 150 simile candidates generated from 44 literal sentences.\n\nQuality\nWe compare our quality metric with the following automatic metrics 9 : (1) BLEU (Papineni et \n\n2002) calculates the precision of n-gram matches,\n(2) RougeL (Lin, 2004 ) is a recall-oriented metric, (3) METEOR (Denkowski and Lavie, 2014) proposes a set of linguistic rules to compare the hypothesis with the reference, ( 4) BERTScore (Zhang et al., 2019) calculates the cosine similarity between the BERT embeddings, ( 5) Perplexity (Pang et al., 2020) measures the proximity of a language model, the inverse of which is utilized.\nCorrelations with Human Ratings. Tab. 3 shows the correlation coefficients between automatic metrics and human ratings. Firstly, our metrics are significantly more correlated with human ratings than prior automatic metrics. Moreover, all the sentence-level metrics, which consider the semantics of the entire sentence, perform worse than almost all the n-gram-level metrics, which compare the n-grams between the hypothesis and the reference, which reveals that simile components need to be specifically considered during SG evaluation.\nAccording to the visualized correlation result in Fig. 3 , datapoints from prior automatic metrics tend to scatter at 0 or 1, while the datapoints from our metric are distributed closer to the fitter line, proving that our metric can better measure the quality.\nRecommendation Task. We compare the rankings given by automatic metrics with human rankings 10 . We adopt the following metrics: Hit Ratio at rank K (HR@K(K=1,3)), Norgenerated from different literal sentences can not be compared. Please refer to Appx. C for the implementation of them. 10 We remove the literal sentences with fewer than three valid simile candidates in this task, as they are too simple to rank. We finally get 134 sentences from 35 literal sentences.\n\nMetrics\nHR@1 HR@3 nDCG@1 nDCG@3 MRR malized Discounted Cumulative Gain at rank K (NDCG@K(K=1,3)) 11 , and Mean Reciprocal Rank (MRR). From Tab. 4, our metric achieves significant improvement compared to other metrics, indicating that our metric can yield more accurate rankings for quality. Also, the n-gram-level metrics generally outperform sentence-level metrics, which is consistent with the result in Tab. 3. Ablation Study. To investigate the importance of different sub-metrics in quality metric, we compare the correlation between quality metric and human ratings after removing each sub-metric individually. From Tab. 3, the removal of any sub-metric leads to a decline in performance, which proves the effectiveness of each sub-metric. Among three components, the removal of the relevance results in the largest performance drop, which reveals that relevance is the most important sub-metric.\nThe Effects of Hyperparameters. Since different sub-metrics have varying levels of importance, we study the correlation results when gradually increasing the weight of relevance component and decreasing the weight of sentiment consistency component (as in Tab. 5). From Fig. 4 (left), increasing the weight of the relevance component consistently results in improved performance, peaking at the combination [7](\u03b1, \u03b2, \u03b3 = 3/6, 2/6, 1/6), before eventually causing a decline in performance. This reveals that although relevance is the most important sub-metric, too much weight on it can be detrimental.\nThe Effects of Referenced Dataset Size. We sample different numbers of simile sentences from (He et al., 2022) as references for relevance 5/6, 1/12, 1/12\nTable 5 : The setting of each hyperparameters combination for the quality metric. The result is shown in Fig. 4 (left).\n[\n1] [2] [3] [4] [5] [6] [7] [8] [9]\nHyper-parameters Combination score and study the correlation between the quality metric and human ratings 12 . From Fig. 4 (right) 13 , correlations grow linearly with exponential growth in referenced dataset size, indicating that using datasets larger than 100k will improve the correlation coefficients. Moreover, the performance at the peak surpasses the prior automatic metrics, proving the effectiveness of our approximation method.\n\nCreativity\nWe compare our creativity metric with the following automatic metrics: (1) Perplexity which is often utilized to measure diversity as well (Tevet and Berant, 2021) , (2) Self-BLEU (Zhu et al., 2018) calculates the BLEU score of each generation against all other generations as references, (3) Distinct n-grams(Dist) (Tevet and Berant, 2021) , which is the fraction of distinct n-grams from all possible n-grams across all generations.\nCorrelations with Human Ratings. From Tab. 6, our metric creativity is significantly more correlated with human evaluation scores compared with prior diversity metrics. According to the visualized correlation result in Fig. 5 , the prior diversity metrics have either wide confidence intervals (Perplexity, Dist) or scattered datapoints (self-BLEU), whereas our creativity metrics exhibit stronger linear correlation and narrower confidence intervals (Creativty w/ Log), implying higher reliability.\nRecommendation Task. We compare the rankings given by automatic metrics with human rank- ings. According to Tab. 7, our creativity metric outperforms prior automatic metrics, which proves our metric can better measure the creativity of simile candidates given a literal sentence, which is consistent with the results in Tab. 6. Ablation Study. According to Tab. 6, removing the log transformation leads to significant performance drops. According to the visualized correlation result in Fig. 5 , the datapoints are distributed closer to the fitter line and exhibit narrower confidence intervals after applying the log transformation, which further proves that log transformation is essential for our creativity metric.\nThe Effects of Referenced Dataset Size. According to Fig. 6 (left), the correlation coefficients increase continuously and eventually converge as the number of referenced sentences increases. Moreover, the performance after convergence is comparable to that given by the creativity metric based on the simile KB. The trend reveals that our metric referencing 10k similes can achieve a promising correlation with human ratings.\n\nInformativeness\nThe Pearson and Spearman correlation coefficients between our informativeness metric and human ratings are 0.798 and 0.882, respectively. According to Fig. 6 (right), the strong linear correlation between the metric and human ratings proves that our informativeness metric is simple yet quite effective.\n\nRelation between Metrics\nWe present pair-wise correlations between the three automatic metrics in Tab. 8 and also visualize them in Fig. 7 . Among the three metrics, creativity correlates with informativeness moderately, mainly because shorter vehicles tend to be less creative than longer ones. The correlations of all other pairwise metrics are relatively weak. Thus, it is evident that the three metrics are independent of each other and it is necessary to measure each one of them to obtain a holistic view of SG evaluation. \n\nHAUSER Application\nWe perform a case study to prove that our designed automatic metrics are effective for various methods. Here, we apply our metrics to a retrieval method (Zhang et al., 2021) (denoted as BM25), which utilizes the 20 context words around the insertion position given by groundtruth to retrieve the 5 most similar samples based on the BM25 ranking score from the training set, and adopts the vehicles from these samples to be those of simile candidates. This method ensures the diversity of generated similes. The method introduced in Sec. 4.1 is denoted as Ours. Given the candidates generated by each method, we rerank them using a weighted combination of quality, creativity, and informativeness rankings obtained by HAUSER, with a ratio of 2:2:1. From Tab. 11 in Appendix, the candidates generated by various methods can be more correlated with human rankings after being ranked by our metrics, thus proving the generality of our metrics. It is noticed that the insertion position for BM25 is provided by the groundtruth, while the insertion position for Ours is predicted by the model, thus proving the effectiveness of our generation method.\n\nConclusion\nIn this work, we systematically investigate the evaluation of the Simile Generation (SG) task. We establish a holistic and automatic evaluation system for the SG task, containing five criteria from three perspectives, and propose holistic automatic metrics for each criterion. Extensive experiments verify the effectiveness of our metrics. more correlated with humans than prior referencebased metrics (e.g. BLEU, Rouge, BERTScore), our metrics are still reference-based and rely on the quality and scale of referenced data. We have discussed the effect of referenced dataset size in our paper and will design reference-free metrics to further complement our metrics in future work. Additionally, since our metrics utilize a million-scale simile knowledge base or large-scale simile sentences as references, the efficiency of our method is slightly lower than the automatic metrics based on a few references. Nevertheless, this limitation does not prevent our metrics from performing systematic and scalable comparisons between SG models.\n"}
{"question": "Which of the following is not our experiment steps?", "evidence": "  Experimental Implementation\nWe follow the standard MTC setting and adopt the same network architectures with the most recent baselines for fair comparisons (Mao et al., 2021) . We adopt the hard parameter sharing MTL framework shown in Figure 2 , where task-shared network is a TextCNN with kernel size of 3,5,7 and taskspecific network is a fully connected layer with a softmax function. Adam is utilized as the optimizer to train the model over 3000 epochs with a learning rate of 1e-3 for both sentiment analysis and topic classification. We set the batch size to 256. \n ", "options": ["A. In order to ensure fair comparisons, we adhere to the standard Multi-Task Classification settings and utilize the same network architectures as the latest baselines.", "B. We implement the hard parameter sharing Multi-Task Learning (MTL) framework ,where task-shared network is a TextCNN with kernel size of 3,5,7.", "C. Adam is used as the optimizer to train the model for less than 3000 epochs.", "D. The batch size we set is over 250."], "answer": "C", "content": "\nIntroduction\nMulti-task Learning (MTL), which aims to learn a single model that can tackle multiple correlated but different tasks simultaneously, makes multiple tasks benefit from each other and obtain superior performance over learning each task independently (Caruana, 1997; Ruder, 2017; Liu et al., 2015; Mao et al., 2020) . By discovering shared information/structure across the tasks, it has gained attention in many areas of research and industrial communities, such as computer vision (Misra et al., 2016; Gao et al., 2019; Yogamani et al., 2019; Sun et al., 2020 ) and text classification (Liu et al., 2017; Xiao et al., 2018; Mao et al., 2021 Mao et al., , 2022)) .\nHowever, it is observed in multi-task text classification (MTC) scenarios that some tasks could conflict with each other, which may be reflected via conflicting gradients or dominating gradients (Yu et al., 2020; Vandenhende et al., 2022) , leading to the degraded performance of MTL due to poor training. How to make a proper trade-off among jointing different tasks in MTC is a difficult problem. Recently, several methods have been proposed to mitigate gradient conflicts issue via both loss balance (linear weighted scalarization) such as homoscedastic uncertainty (Kendall et al., 2018) and task variance regularization (Mao et al., 2021) , and gradient balance like Pareto optimality (Sener and Koltun, 2018; Mao et al., 2020) . Existing methods devote to finding an arbitrary Pareto optimality solution in the Pareto set, which achieve a single arbitrary trade-off among all tasks. However, they can only satisfy the improved performance on part of tasks, not all tasks simultaneously. This means that these methods can not converge to a minimum average loss of all objectives.\nTo illustrate our idea, we give a two-task learning example shown in Figure 1 . As shown in Figure (1a) , it is observed that Pareto optimality-based methods can generate a set of Pareto solutions for a given two-task learning problem. However, some of Pareto solutions can increase the task 1 error while decreasing task 2 error, leading to unsatisfactory overall performance for MTL model. This im-plies that not all Pareto solutions always satisfy the goal of mitigating the tasks conflicts in MTL, and thus failing to achieve a better trade-off between tasks. Therefore, it is necessary to find a specific trade-off between tasks that is beyond what only using Pareto optimality can achieve.\nTo address this issue, inspired by multi-objective optimization (Sener and Koltun, 2018) , we argue that a more efficient way to mitigate task conflicts is to find a gradient trade-off between tasks in the neighborhood of the average loss rather than exhaustively searching for a proper solution from the set of Pareto solutions. As shown in Figure 1b , the Pareto solutions nearby the average loss can achieve a better trade-off between task 1 and task 2, leading to better performance on both tasks at the same time. Based on it, in this paper, we propose a novel gradient trade-off multi-task learning approach, named GetMTL, to mitigate task conflicts in multi-task text classification. Specifically, the gradients of each task are utilized to derive an update vector that can minimize the conflicts among task gradients in the neighborhood of the average gradient, so as to achieve a better trade-off performance among joint training tasks. In summary, the main contributions of our work are as follows:\n\u2022 A novel multi-task learning approach based on gradient trade-off between different tasks (GetMTL) is proposed to deal with task conflict in multi-task text classification problems, so as to improve the performance of all tasks simultaneously. \u2022 We give in-depth theoretical proofs and experimental analyses on establishing converge guarantees of our GetMTL. \u2022 We extensively verify the effectiveness of our GetMTL on two real-world text classification datasets, and the results show that our GetMTL performs competitively with a variety of state-of-the-art methods under a different number of task sets.\n\nRelated Works\nMulti-task Learning methods jointly minimize all task losses based on either loss balance methods (Kendall et al., 2018; Chen et al., 2018; Mao et al., 2021 Mao et al., , 2022) ) or gradient balance methods (Sener and Koltun, 2018; Mao et al., 2020) .\nThe loss balance methods adaptively adjust the tasks weights during training based on various heuristic approaches, such as task uncertainty quan-tification (Kendall et al., 2018) , gradient normalization (Chen et al., 2018) , task difficulty prioritization (Guo et al., 2018) , dynamic weight average (Liu et al., 2019) , random loss weighting (Lin et al., 2021) , task variance regularization (Mao et al., 2021) , and meta learning-based approach (Mao et al., 2022) . These methods are mostly heuristic and can have unstable performance while ignoring the task conflicts among all tasks, leading to the bad generalization performance of MTL models.\nRecently, some gradient balance based methods have been proposed to mitigate task conflicts for improving task performance. For example, D\u00e9sid\u00e9ri (2012) leverages multiple-gradient descent algorithm (MGDA) to optimize multiple objectives. Due to the guarantee of convergence to Pareto stationary point, this is an appealing approach. Sener and Koltun (2018) cast the multi-objective problem as a multi-task problem and devote to finding an arbitrary Pareto optimal solution. Mao et al. (2020) propose a novel MTL method based Tchebycheff procedure for achieving Pareto optimal without any convex assumption. However, these methods only consider achieving an arbitrary Pareto optimal solution while it is not the main objective. Unlike these methods, we propose an MTL approach based on multi-objective optimization and seek to find a set of solutions that are Pareto optimality and nearby the main MTC objective L 0 .\n\nPreliminaries\nConsider a multi-task learning problem with T 1 tasks over an input space X and a collection of task spaces {Y t } t\u2208 [T ] , where each task contains a set of i.i.d. training samples\nD t = {x i , y t i } i\u2208[nt] ,\nT is the number of tasks, and n t is the number of training samples of task t. The goal of MTL is to find parameters {\u03b8 sh , \u03b8 1 , ..., \u03b8 T } of a model F that can achieve high average performance across all training tasks over X , defined as F(X , \u03b8 sh , \u2022 \u2022 \u2022 , \u03b8 t ) : X \u2192 Y, where \u03b8 sh denotes the parameters shared between tasks and \u03b8 t denotes the task-specific parameters of task t. In particular, we further consider a parametric taskspecific map as f t (\u2022, \u03b8 sh , \u03b8 t ) : X \u2192 Y t . We also consider task-specific loss functions t (\u2022, \u2022) : Y t \u00d7 Y t \u2192 R + . We also denote the multi-task loss as L(\u03b8) = T i i (\u03b8), and the gradients of each task as g i = \u2207 i (\u03b8) for the particular \u03b8. In this paper, we choose the average loss as main objective of MTC problem, defined as L 0 (\u03b8) = 1 T T i i (\u03b8).\n\nMTL as Multi-objective Optimization\nMTL can be formulated as a specific case of multiple-objective optimization (MOO), which optimizes a set of potentially conflicting objectives (Sener and Koltun, 2018; Mao et al., 2020) . Given objective functions of T tasks, 1 , . . . , T , we formulate the optimization objective of MTL as the vectors of objective values :\nmin \u03b8 sh ,\u03b8 1 ,...,\u03b8 T (\u03b8 sh , \u03b8 1 ), . . . , (\u03b8 sh , \u03b8 T ) (1)\nSince there is no natural linear ordering on vectors, it is not possible to compare solutions and thus no single solution can optimize all objectives simultaneously. In other words, there is no clear optimal value. Alternatively, we can achieve Pareto optimality to obtain different optimal trade-offs among all objectives to solve MOO problem.\nDefinition 1 (Pareto dominance). Given two points {\u03b8, \u03b8} in \u2126, a point \u03b8 Pareto dominates \u03b8 (\u03b8 \u03b8) for MTL if two conditions are satisfied:\n(i) No one strictly prefers \u03b8 to \u03b8, that is, \u2200i \u2208 {1, . . . , T }, i (\u03b8 sh , \u03b8 i ) \u2264 i (\u03b8 sh , \u03b8 i ).\n(ii) At least one point strictly prefers \u03b8 to \u03b8, that is, \u2203j \u2208 {1, ..., T }, j (\u03b8 sh , \u03b8 j ) < j (\u03b8 sh , \u03b8 j ).\nDefinition 2 (Pareto optimality). \u03b8 * is a Pareto optimal point and (\u03b8 * ) is a Pareto optimal objective vector if it does not exist \u03b8 \u2208 \u2126 such that \u03b8 \u03b8 * . That is, a solution that is not dominated by any other is called Pareto optimal.\nThe set of all Pareto optimal solutions is called the Pareto set, and the image of Pareto set in the loss space is called Pareto front (Lin et al., 2019) . In this paper, we focus on gradient-based multiobjective optimization to achieve an appropriate Pareto trade-off among all tasks, which can approximate the Pareto front that minimizes the average loss.\n\nGradient-based Multi-Objective Optimization\nGradient-based MOO (Sener and Koltun, 2018) aims to find a direction d that we can iteratively find the next solution \u03b8 (t+1) that dominates the previous one \u03b8 (t) ( (\u03b8 (t+1) ) \u2264 (\u03b8 (t) )) by moving against d with step size \u03b7, i.e. \u03b8 (t+1) = \u03b8 (t) \u2212 \u03b7d. D\u00e9sid\u00e9ri (2012) ; Sener and Koltun (2018) propose to use multiple gradient descent algorithm (MGDA) that converges to a local Pareto optimal by iteratively using the descent direction d, which can be obtained as follows:\nd * = arg min d\u2208R m ,\u03b1\u2208R \u03b1 + 1 2 d 2 s.t. \u2207 i (\u03b8 (t) ) T d \u2264 \u03b1, i = 1, ..., T.\n(\nEQUATION\nwhere d * is the direction that can improve all tasks. Essentially, gradient-based MOO methods minimize the loss by combining gradients with adaptive weights, and obtaining an arbitrary Pareto optimality solution, ignoring the true objective (the average loss) (Liu et al., 2021) . In this paper, we generalize this method and propose a novel gradient-based approach to achieve a gradient trade-off among tasks for mitigating task conflicts, as well as constrain the solution that can minimize the average loss (L 0 (\u03b8)).\n\nGradient Trade-offs for Multi-task Text Classification\nFollowing most MTL methods, as shown in Figure 2 , we employ the hard parameter sharing MTL architecture, which includes f sh parameterized by heavy-weight task-shared parameters \u03b8 sh and f t parameterized by light-weight task-specific parameters \u03b8 t . All tasks take the same shared intermediate feature z = f sh (x; \u03b8 sh ) as input, and the t-th taskspecific network outputs the prediction as f t (z; \u03b8 t ).\nSince task-shared parameters \u03b8 sh are shared by all tasks, the different tasks may conflict with each other, leading to the degraded performance of MTL model. In this paper, we hypothesize that one of the main reasons for task conflicts arises from gradients from different tasks competing with each other in a way that is detrimental to making progress.\nWe propose a novel gradient-based MOO optimization to find a gradient trade-off among tasks in the neighborhood of the average loss, so as to mitigate task conflicts. Note that, we omit the subscript sh of task-shared parameters \u03b8 sh for the ease of notation.\n\nGetMTL\nGiven a task i, we define its gradient as g i = \u2207 i (\u03b8) via back-propagation from the raw loss i , and g i represents the optimal update direction for task i. However, due to the inconsistency of the MOO method and our GetMTL on three gradients (g 1 , g 2 and g 3 ) in R 3 , where g i denotes the gradient (black) of i-th task, g 0 is the average gradient, and blue arrows denote the projections of update direction to each task gradient.\noptimal update direction of task-shared parameters for each task, different task gradients may conflict with each other, leading to the training of networks being stuck in the over-training of some tasks and the under-training of other tasks. Intuitively, it is desirable to find a direction that can minimize the task conflicts among different tasks as well as achieve Pareto optimality to improve the performance of MTL model. We first achieve an arbitrary Pareto optimal via finding a descent direction d des by searching for a minimum-norm point in the Convex Hull CH of gradients, defined by,\nEQUATION\ns.t. S T = \u03b2 \u2208 R T + T j=1 \u03b2 j = 1 (4)\nwhere G \u2208 R T \u00d7m = {g 1 , ..., g T } is the matrix of task gradient, S T is the T -dimensional regular simplex. We use the multiple gradient descent algorithm (MGDA) (Sener and Koltun, 2018) to obtain an arbitrary Pareto optimal by iteratively using the descent direction, defined by,\nd des = arg min d\u2208CH d 2 2\n(5)\nIn addition, the d des can be reformulated as a linear combination of all task gradients, defined by,\nd des = T i=1 \u03b2 i g i (6)\nwhere g i = \u2207 i (\u03b8) is the i-th task gradient. It implies that, when converges to an arbitrary Pareto optimal, the optimal gradient value of each task via back-propagation is \u03b2 i g i , defined as\ng \u03b2 i = \u03b2 i g i .\nHowever, moving against d des does not guarantee that the solution meets the requirements of multi-task text classification task (MTC), that is, to alleviate the gradient conflict among tasks in MTC, so as to improve the performance of all tasks. To address this issue, we seek a direction that enables us to move from a solution \u03b8 (t) to \u03b8 (t+1) such that both \u03b8 (t+1) dominates \u03b8 (t) (L(\u03b8 (t+1) ) \u2264 L(\u03b8 (t) )) and alleviate the gradient conflict among all tasks. Based on it, as shown in Figure 2 (b), we propose to search for an update direction d in the Convex Hull CH \u03b2 of back-propagation gradients such that it can improve any worst objective and converge to an optimum of MTC objective L 0 (\u03b8). We first find the worst task gradient with respect to the update direction d, that is, it has a maximum angle with d, which can be formulated via the following optimization problem,\nEQUATION\nwhere g \u03b2 i is the i-task gradient after optimizing by MGDA algorithm.\nTo improve the worst gradient of any task and achieve a trade-off between all task gradients in a neighborhood of the average gradient (defined as g 0 = 1 T T i=1 g i ), we formulate this gradient trade-off optimization problem via the following Maximin Optimization Problem (dual problem).\nProblem 1.\nmax d\u2208R m min i\u2208[T ] g \u03b2 i , d s.t. d \u2212 g 0 \u2264 \u03b5g T 0 d, \u2212 g T 0 d \u2264 0 (8)\nwhere g \u03b2 i = \u03b2 i g i is the back-propagation gradient value of i-th task via solving Eq. ( 5), \u03b5 \u2208 (0, 1] is a hyper-parameter that controls the stability of MTC model.\n\nSolving Maximin Problem\nSince the optimal direction d can also be defined in the convex hull CH \u03b2 of g \u03b2 i , we can get\nEQUATION\nwhere\nG \u03b2 \u2208 R T \u00d7m = {g \u03b2 1 , ..., g \u03b2 T } is task gradi- ent matrix, W T = { w \u2208 R T + T j=1 w j = 1} is the T -dimensional\nprobability simplex, and w = (w 1 , ..., w T ). Therefore, we can get min i g \u03b2 i , d = min w\u2208W T i w i g \u03b2 i , d and Problem 1 can be transformed into the following form.\nAlgorithm 1: GetMTL Algorithm.\nInput: The number of task T , loss functions { i } T i=1 , network parameters \u03b8 (t) at t step, the pre-specified hyper-parameter \u03b5 \u2208 (0, 1] and step size \u00b5 \u2208 R + . 1: Task Gradients:\ng i = \u2207 i (\u03b8 (t) ), i \u2208 [T ] 2: Main Objective: g 0 = T i=1 g i 3:\nObtain {\u03b2 1 , ...\u03b2 T } by solving Eq.( 5). 4: Compute g w = i w i g \u03b2 i , where g \u03b2 i = \u03b2 i g i 5: Obtain {w 1 , ..., w T } by solving Eq.( 14) 6: Find direction d * by using Eq.( 13)\nEQUATION\nwhere g w = T i=1 w i g \u03b2 i is the convex combination in CH \u03b2 . For a given vector \u03bb \u2208 R + with non-negative components, the corresponding Lagrangian associated with the Eq.( 10) is defined as\nEQUATION\n11) Since the objective for d is concave with linear constraints and w \u2208 W T is a compact set 2 , according to the Sion's minimax theorem (Kindler, 2005) , we can switch the max and min without changing the solution of Problem 2. Formally,\nmin \u03bb,w\u2208W T max d\u2208R m g T w d \u2212 \u03bb d \u2212 g 0 2 /2 + \u03bb\u03b5 2 (g T 0 d) 2 /2 (12)\nWe get the optimal solution of primal problem (Problem 1) by solving the dual problem of Eq.( 12) (See the Appendix A for a detailed derivation procedure). Then we have\nd * = g w + \u03bb * g 0 (1 \u2212 \u03b5 2 g 2 0 )\u03bb * , where \u03bb * = g w \u03b5 g 0 2 (13)\nwhere \u03bb * is the optimal Lagrange multiplier, d * is the optimal update direction of MTC model. We can reformulate the problem of Eq.( 12) as following optimization problem w.r.t. w.\nEQUATION\n2 Compact set: a set that is bounded and closed. where g w is defined as\ng w = T i=1 w i g \u03b2 i .\nThe detailed derivation is provided in Appendix A. Algorithm 1 shows all the steps of GetMTL algorithm in each iteration.\n\nTheoretical Analysis\nIn this section, we analyze the equivalence of solutions to dual problem and then give a theoretical analysis about convergence of GetMTL algorithm. We define the Lagrangian of problem in Eq.( 10), Theorem 4.2 (Convergence of GetMTL). Assume loss functions i are convex and differential, and \u2207 i (\u03b8 (t) ) is L-lipschitz continuous with L > 0. The update rule is \u03b8 (t+1) = \u03b8 (t) \u2212 \u00b5 (t) d, where d is defined in Eq.( 13) and\nL(d, \u03bb, w) = g T w d \u2212 \u03bb 2 ( d \u2212 g 0 2 \u2212 \u03b5 2 (g T 0 d) 2 ) Theorem 4.1 (Equivalence of\n\u00b5 (t) = min i\u2208[k] d\u2212g 0 c\u2022L\u2022d 2 . All the loss functions 1 (\u03b8 (t) ) \u2022 \u2022 \u2022 T (\u03b8 (t) ) converges to ( 1 (\u03b8 * ) \u2022 \u2022 \u2022 T (\u03b8 * )).\nProof. The proof is provided in Appendix C.\n\nExperimental Datasets\nWe conduct experiments on two MTC benchmarks to evaluate the proposed GetMTL. 1) Amazon Review dataset (Blitzer et al., 2007) contains product reviews from 14 domains (See Details in Appendix D), including apparel, video, books, electronics, DVDs and so on. Each domain gives rise to a binary classification task and we follow Mao et al. (2021) to treat 14 domains in the dataset as distinct tasks, creating a dataset with 14 tasks, with 22180 training instances and 5600 test instances in total. 2) Topic classification dataset, 20 Newsgroup 3 , consists of approximately 20,000 newsgroup documents, partitioned evenly across 20 different newsgroups. We follow Mao et al. (2021) to select 16 newsgroups from 20 Newsgroup dataset shown in Table 1 and then divide them into four groups. Each group gives rise to a 4-way classification task, creating a dataset with four 4-way classification tasks, which is a more challenging dataset than amazon review dataset.\n\nExperimental Implementation\nWe follow the standard MTC setting and adopt the same network architectures with the most recent baselines for fair comparisons (Mao et al., 2021) . We adopt the hard parameter sharing MTL framework shown in Figure 2 , where task-shared network is a TextCNN with kernel size of 3,5,7 and taskspecific network is a fully connected layer with a softmax function. Adam is utilized as the optimizer to train the model over 3000 epochs with a learning rate of 1e-3 for both sentiment analysis and topic classification. We set the batch size to 256. \n\nComparison Models\nWe compare the proposed GetMTL with a series of MTC baselines, including Single-Task Learning (STL): learning each task independently.\nUniform Scaling: learning tasks simultaneously with uniform task weights.\nUncertainty: using the uncertainty weighting method (Kendall et al., 2018) .\nGradNorm: learning tasks simultaneously with gradient normalization method (Chen et al., 2018) .\nTchebycheffAdv: using adversarial Tchebycheff procedure (Mao et al., 2020) .\nMGDA: using gradient-based multi-objective optimization method (Sener and Koltun, 2018) .\nBanditMTL: learning tasks simultaneously with multi-armed bandit method (Mao et al., 2021) .\nMetaWeighting: using adaptive task weighting method (Mao et al., 2022) .\n\nMain Results\nThe main comparison results of GetMTL on two benchmark datasets are shown in Figure 3 and 4 . It is clear that (See detailed numerical comparison results in Appendix D), our proposed GetMTL model performs consistently better than the all comparison methods on all tasks of both amazon review and topic classification datasets, and its average performance is superior to that of all baselines. This verifies the effectiveness of our GetMTL method in MTC problem. More concretely, in comparison with the gradient-based MOO optimization model (MGDA), our GetMTL achieves significant improvement across all datasets. This indicates that achieving a gradient trade-off nearby average loss to mitigate task conflicts can better improve all task performance and generalization ability of MTC model. \n\nEmpirical Analysis on Convergence\nIn Section 4.3, we theoretically prove the convergence of our proposed GetMTL. Furthermore, we conduct extensive experiments about the convergence to better demonstrate the advantages of GetMTL shown in Figure 5 . It is clear that the learning curve of GetMTL is constantly decreasing as the number of iterations increases and converges to the lowest loss value compared with other baselines. It indicates that GetMTL can guarantee the convergence of the objective value and obtain better performance of all learning tasks.\nIn addition, we also conduct extensive experiments to investigate how GetMTL mitigates task conflict during training. We plot the task variance (variance between the task-specific losses) of all baselines on both amazon review and topic classification datasets shown in Figure 6 . It can be observed that all MTL baselines have lower task variance than STL method, which illustrates that MTL methods can indeed boost the learning of all tasks compared with STL method. Moreover, GetMTL has the lowest task variance and smoother evolution during training than other MTL baselines. This implies that our proposed GetMTL indeed mitigates task conflicts compared with other MTL methods.\n\nThe Evolution of Task Weight w\nIn this section, we visualize the task weights of our GetMTL and two weight adaptive MTL methods (MGDA and BanditMTL) throughout the training process using the topic classification dataset shown in Figure 7 . It can be observed from these four figures that the weight adaption process of our GetMTL is different from that of MGDA and Ban-ditMTL. GetMTL can automatically learn the task weights without pre-defined heuristic constraints. The weights adaption process of GetMTL is more stable and the search space is more compact compared with other MTL baselines.\n\nImpact of the Values of \u03b5\nTo investigate the impact of using different values of \u03b5 on the performance of our GetMTL, we conduct experiments on two datasets, and the results are shown in Figure 8 . Noting that model with \u03b5 = 0.0075 and \u03b5 = 0.025 perform overall better than other values on these two datasets, respectively. The model with larger value of \u03b5 performs unsatisfactorily overall all tasks on two datasets, one possible reason is that larger \u03b5 makes d pull far away from the average loss g 0 (see the conditions in Eq. ( 9)). That is, Pareto optimality found by GetMTL is getting further and further away from MTC objective L 0 , which can be quite detrimental to some tasks' performance, leading to degraded average performance.\n\nConclusion\nIn this paper, we propose a novel gradient tradeoff multi-task learning approach to mitigate the task conflict problem, which can achieve a specific trade-off among different tasks nearby the main objective of multi-task text classification problem. Moreover, we present a series of theoretical proofs to illustrate the effectiveness and superiority of our GetMTL. Experimental results on two benchmark datasets show that our GetMTL achieves state-ofthe-art performance in Multi-task Text Classification problem.\n"}
{"question": "In the network architecture described, what is the purpose of the BERT and LSTM?", "evidence": "  Sentence Encoder consists of a BERT (Devlin et al., 2019) plus a stacked bi-directional LSTM. Entity Decoder uses the sentence encoding H S to first compute the representations of K noisy spans x t and then predicts the corresponding entity boundaries.  ", "options": ["A. To calculate the probability of entity boundaries.", "B. To predict noisy spans.", "C. To compute the representations of noisy spans.", "D. To perform post-processing on candidate entities."], "answer": "C", "content": "\nIntroduction\nNamed Entity Recognition (NER) is a basic task of information extraction (Tjong Kim Sang and De Meulder, 2003) , which aims to locate entity mentions and label specific entity types such as person, location, and organization. It is fundamental to many structured information extraction tasks, such as relation extraction (Li and Ji, 2014; Miwa and Bansal, 2016) and event extraction (McClosky et al., 2011; Wadden et al., 2019) .\nMost traditional methods (Chiu and Nichols, 2016) formulate the NER task into a sequence labeling task by assigning a single label to each token. To accommodate the nested structure between entities, some methods (Ju et al., 2018; Wang et al., + + \u00b2 \u00bb N (0; 1) + \u00b2 \u00bb N (0; 1)\nFigure 1 : Boundary diffusion in named entity recognition. The fixed forward diffusion process adds Gaussian noise to the entity boundaries at each timestep, and the noisy boundaries recover their original state by denoising with the learnable reverse diffusion process. For inference, the reverse diffusion process generates entity boundaries and performs entity typing based on the noisy spans sampled from the Gaussian distribution. 2020) further devise cascaded or stacked tagging strategies. Another class of methods treat NER as a classification task on text spans (Sohrab and Miwa, 2018; Eberts and Ulges, 2020) , and assign labels to word pairs (Yu et al., 2020; Li et al., 2022a) or potential spans (Lin et al., 2019; Shen et al., 2021a) . In contrast to the above works, some pioneer works (Paolini et al., 2021; Yan et al., 2021b; Lu et al., 2022) propose generative NER methods that formulate NER as a sequence generation task by translating structured entities into a linearized text sequence. However, due to the autoregressive manner, the generation-based methods suffer from inefficient decoding. In addition, the discrepancy between training and evaluation leads to exposure bias that impairs the model performance.\nWe move to another powerful generative model for NER, namely the diffusion model. As a class of deep latent generative models, diffusion models have achieved impressive results on image, audio and text generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021; Li et al., 2022b; Gong et al., 2022) . The core idea of diffusion models is to systematically perturb the data through a forward diffusion process, and then recover the data by learning a reverse diffusion process.\nInspired by this, we present DIFFUSIONNER, a new generative framework for named entity recognition, which formulates NER as a denoising diffusion process (Sohl-Dickstein et al., 2015; Ho et al., 2020) on entity boundaries and generates entities from noisy spans. As shown in Figure 1 , during training, we add Gaussian noise to the entity boundaries step by step in the forward diffusion process, and the noisy spans are progressively denoised by a reverse diffusion process to recover the original entity boundaries. The forward process is fixed and determined by the variance schedule of the Gaussian Markov chains, while the reverse process requires learning a denoising network that progressively refines the entity boundaries. For inference, we first sample noisy spans from a prior Gaussian distribution and then generate entity boundaries using the learned reverse diffusion process.\nEmpowered by the diffusion model, DIFFUSION-NER presents three advantages. First, the iterative denoising process of the diffusion model gives DIFFUSIONNER the ability to progressively refine the entity boundaries, thus improve performance. Second, independent of the predefined number of noisy spans in the training stage, DIF-FUSIONNER can sample a different number of noisy spans to decode entities during evaluation. Such dynamic entity sampling makes more sense in real scenarios where the number of entities is arbitrary. Third, different from the autoregressive manner in generation-based methods, DIFFUSION-NER can generate all entities in parallel within several denoising timesteps. In addition, the shared encoder across timesteps can further speed up inference. We will further analyze these advantages of DIFFUSIONNER in \u00a7 6.2. In summary, our main contributions are as follows:\n\u2022 DIFFUSIONNER is the first to use the diffusion model for NER, an extractive task on discrete text sequences. Our exploration provides a new perspective on diffusion models in natural language understanding tasks.\n\u2022 DIFFUSIONNER formulates named entity recognition as a boundary denoising diffusion process from the noisy spans. DIFFUSION-NER is a novel generative NER method that generates entities by progressive boundary refinement over the noisy spans.\n\u2022 We conduct experiments on both nested and flat NER to show the generality of DIFFU-SIONNER. Experimental results show that our model achieves better or competitive performance against the previous SOTA models.\n2 Related Work\n\nNamed Entity Recognition\nNamed entity recognition is a long-standing study in natural language processing. Traditional methods can be divided into two folders: tagging-based and span-based. For tagging-based methods (Chiu and Nichols, 2016; Ju et al., 2018; Wang et al., 2020) , they usually perform sequence labeling at the token level and then translate into predictions at the span level. Meanwhile, the span-based methods (Sohrab and Miwa, 2018; Eberts and Ulges, 2020; Shen et al., 2021a,b; Li et al., 2022a) directly perform entity classification on potential spans for prediction. Besides, some methods attempt to formulate NER as sequence-to-set (Tan et al., 2021 (Tan et al., , 2022;; Wu et al., 2022) or reading comprehension (Li et al., 2020; Shen et al., 2022) tasks for prediction. In addition, autoregressive generative NER works (Athiwaratkun et al., 2020; De Cao et al., 2021; Yan et al., 2021b; Lu et al., 2022) linearize structured named entities into a sequence, relying on sequence-to-sequence language models (Lewis et al., 2020; Raffel et al., 2020) to decode entities. These works designed various translation schemas, including from word index sequence to entities (Yan et al., 2021b) and from label-enhanced sequence to entities (Paolini et al., 2021) , to unify NER to the text generation task and achieved promising performance and generalizability. Other works (Zhang et al., 2022) focus on the disorder of the entities and mitigate incorrect decoding bias from a causal inference perspective. Different from previous works, our proposed DIFFUSIONNER is the first one to explore the utilization of the generative diffusion model on NER, which enables progressive refinement and dynamic sampling of entities. Furthermore, compared with previous generation-based methods, our DIFFUSIONNER can also decode entities in a nonautoregressive manner, and thus result in a faster inference speed with better performance.\n\nDiffusion Model\nDiffusion model is a deep latent generative model proposed by (Sohl-Dickstein et al., 2015) . With the development of recent work (Ho et al., 2020) , diffusion model has achieved impressive results on image and audio generation (Rombach et al., 2022; Ramesh et al., 2022; Kong et al., 2021) . Diffusion model consists of the forward diffusion process and the reverse diffusion process. The former progressively disturbs the data distribution by adding noise with a fixed variance schedule (Ho et al., 2020) , and the latter learns to recover the data structure. Despite the success of the diffusion model in continuous state spaces (image or waveform), the application to natural language still remains some open challenges due to the discrete nature of text (Austin et al., 2021; Hoogeboom et al., 2022; Strudel et al., 2022; He et al., 2022) . Diffusion-LM (Li et al., 2022b) models discrete text in continuous space through embedding and rounding operations and proposes an extra classifier as a guidance to impose constraints on controllable text generation. DiffuSeq (Gong et al., 2022) and SeqDiffuSeq (Yuan et al., 2022a) extend diffusionbased text generation to a more generalized setting. They propose classifier-free sequence-to-sequence diffusion frameworks based on encoder-only and encoder-decoder architectures, respectively.\nAlthough diffusion models have shown their generative capability on images and audio, its potential on discriminative tasks has not been explored thoroughly. Several pioneer works (Amit et al., 2021; Baranchuk et al., 2022; Chen et al., 2022) have made some attempts on diffusion models for object detection and semantic segmentation. Our proposed DIFFUSIONNER aims to solve an extractive task on discrete text sequences.\n\nPreliminary\nIn diffusion models, both the forward and reverse processes can be considered a Markov chain with progressive Gaussian transitions. Formally, given a data distribution x 0 \u223c q (x 0 ) and a predefined variance schedule {\u03b2 1 , . . . , \u03b2 T }, the forward process q gradually adds Gaussian noise with variance \u03b2 t \u2208 (0, 1) at timestep t to produce latent variables x 1 , x 2 , . . . , x T as follows:\nq (x 1 , . . . , x T | x 0 ) = T t=1 q (x t | x t\u22121 )\n(1)\nq (x t | x t\u22121 ) = N x t ; 1 \u2212 \u03b2 t x t\u22121 , \u03b2 t I (2)\nAn important property of the forward process is that we can sample the noisy latents at an arbitrary timestep conditioned on the data x 0 . With the notation \u03b1 t := 1 \u2212 \u03b2 t and \u1fb1t := t s=0 \u03b1 s , we have:\nEQUATION\nAs \u1fb1T approximates 0, x T follows the standard Gaussian distribution: p (x T ) \u2248 N (x T ; 0, I). Unlike the fixed forward process, the reverse process p \u03b8 (x 0:T ) is defined as a Markov chain with learnable Gaussian transitions starting at a prior p (x T ) = N (x T ; 0, I):\np \u03b8 (x 0:T ) = p (x T ) T t=1 p \u03b8 (x t\u22121 | x t ) p \u03b8 (x t\u22121 | x t ) = N (x t\u22121 ; \u00b5 \u03b8 (x t , t) , \u03a3 \u03b8 (x t , t))\nwhere \u03b8 denotes the parameters of the model and \u00b5 \u03b8 and \u03a3 \u03b8 are the predicted covariance and mean of q \n(x t\u22121 | x t ). We set \u03a3 \u03b8 (x t , t) = \u03c3 2 t I and build a neural network f \u03b8 to predict the data x 0 , denoted as x0 = f \u03b8 (x t , t). Then we have \u00b5 \u03b8 (x t , t) = \u03bct (x t , x0 ) = \u03bct (x t , f \u03b8 (x t , t)), where \u03bct denotes the mean of posterior q (x t\u22121 | x t , x0 ).\n\nMethod\nIn this section, we first present the formulation of diffusion model for NER (i.e., the boundary denoising diffusion process) in \u00a7 4.1. Then, we detail the architecture of the denoising network for boundary reverse process in \u00a7 4.2. Finally, we describe the inference procedure of DIFFUSIONNER in \u00a7 4.3.\n\nBoundary Denoising Diffusion Model\nGiven a sentence S with length M , the named entity recognition task is to extract the entities E = {(l i , r i , t i )} N i=0 contained in the sentence, where N is the number of entities and l i , r i , t i denote the left and right boundary indices and type of the i-th entity. We formulate NER as a boundary denoising diffusion process, as shown in Figure 2 . We regard entity boundaries as data samples, then the boundary forward diffusion is to add Gaussian noise to the entity boundaries while the reverse diffusion process is to progressively recover the original entity boundaries from the noisy spans. Boundary Forward Diffusion Boundary forward diffusion is the process of adding noise to the entity boundary in a stepwise manner. In order to align the number of entities in different instances, we first expand the entity set to a fixed number K (> N ). There are two ways to expand the entities, repetition strategy and random strategy, which add K \u2212 N entities by duplicating entities or sampling random spans from a Gaussian distribution 2 . For convenience, we use B \u2208 R K\u00d72 to denote the boundaries of the K expanded entities, with all of them normalized by the sentence length M and scaled to (\u2212\u03bb, \u03bb) interval. Formally, given the entity boundaries as data samples x 0 = B, we can obtain the noisy spans at timestep t using the forward diffusion process. According to Equation (3), we have:\nx t = \u221a \u1fb1t x 0 + \u221a 1 \u2212 \u1fb1t \u03f5 (4)\nwhere \u03f5 \u223c N (0, I) is the noise sampled from the standard Gaussian. At each timestep, the noisy spans have the same shape as x 0 , i.e.,\nx 1 , x 2 , . . . , x T \u2208 R K\u00d72 .\nBoundary Reverse Diffusion Starting from the noisy spans x T sampled from the Gaussian distribution, boundary reverse diffusion adopts a non-Markovian denoising practice used in DDIM (Song et al., 2021) to recover entities boundaries. Assuming \u03c4 is an arithmetic subsequence of the complete timestep sequence [1, . . . , T ] of length \u03b3 with \u03c4 \u03b3 = T . Then we refine the noisy spans x \u03c4 i to 2 We will discuss these two practices in \u00a7 6.3.\nx \u03c4 i\u22121 as follows:\nEQUATION\n)\n\u03b5\u03c4 i = x \u03c4 i \u2212 \u221a \u03b1 \u03c4 i x0 \u221a 1 \u2212 \u03b1 \u03c4 i (6) x \u03c4 i\u22121 = \u221a \u03b1 \u03c4 i\u22121 x0 + 1 \u2212 \u03b1 \u03c4 i\u22121 \u03b5\u03c4 i (7)\nwhere x0 and \u03b5\u03c4 i are the predicted entity boundary and noise at timestep \u03c4 i . f \u03b8 (x t , S, t) is a learnable denoising network and we will cover the network architecture in the next section ( \u00a7 4.2). After \u03b3 iterations of DDIM, the noisy spans are progressively refined to the entity boundaries.\n\nNetwork Architecture\nDenoising network f \u03b8 (x t , S, t) accepts the noisy spans x t and the sentence S as inputs and predicts the corresponding entity boundaries x0 . As shown in Figure 2 , we parameterize the denoising network with a sentence encoder and an entity decoder.\nSentence Encoder consists of a BERT (Devlin et al., 2019) plus a stacked bi-directional LSTM.\nThe whole span encoder takes the sentence S as input and outputs the sentence encoding H S \u2208 R M \u00d7h . The sentence encoding H S will be calculated only once and reused across all timesteps to save computations.\nEntity Decoder uses the sentence encoding H S to first compute the representations of K noisy spans x t and then predicts the corresponding entity boundaries. Specifically, we discretize the noisy spans into word indexes by rescaling, multiplying and rounding 3 , then perform mean pooling over the Take gradient descent step by optimize\n\u2212 K i=1 log P c i (\u03c0 c (i)) + \u03b4\u2208l,r log P \u03b4 i (\u03c0 \u03b4 (i)) 10 until converged;\ninner-span tokens. The extracted span representations can be denoted as H X \u2208 R K\u00d7h . To further encode the spans, we design a span encoder that consists of a self-attention and a cross-attention layer. The former enhances the interaction between spans with key, query, and value as H X . And the latter fuses the sentence encoding to the span representation with key, value as H S , and query as H X . We further add the sinusoidal embedding E t (Vaswani et al., 2017) of timestep t to the span representations. Thus the new representations HX of the noisy spans can be computed:\nHX = SpanEncoder(H S , H X ) + E t ,\nThen we use two boundary pointers to predict the entity boundaries. For boundary \u03b4 \u2208 {l, r}, we compute the fusion representation H \u03b4 SX \u2208 R K\u00d7M \u00d7h of the noisy spans and the words, and then the probability of the word as the left or right boundaries P \u03b4 \u2208 R K\u00d7M can be computed as:\nH \u03b4 SX = H S W \u03b4 S + HX W \u03b4 X P \u03b4 = sigmoid(MLP(H \u03b4 SX ))\nwhere W \u03b4 S , W \u03b4 X \u2208 R h\u00d7h are two learnable matrixes and MLP is a two-layer perceptron. Based on the boundary probabilities, we can predict the boundary indices of the K noisy spans. If the current step is not the last denoising step, we compute x0 by normalizing the indices with sentence length M and scaling to (\u2212\u03bb, \u03bb) intervals. Then we conduct the next iteration of the reverse diffusion process according to Equations ( 5) to (7).\nIt is worth noting that we should not only locate entities but also classify them in named entity recognition. Therefore, we use an entity classifier to classify the noisy spans. The classification probability P c \u2208 R K\u00d7C is calculated as follows:\nP c = Classifier( HX ) Algorithm 2: Inference 1 xT \u223c N (0, I) \u2208 R K eval \u00d72\n2 \u03c4 is an arithmetic sequence of length \u03b3 with \u03c4\u03b3 = T 3 for i = \u03b3, . . . , 1 do 4 Compute x0, P l , P r and P c via f \u03b8 (xt, S, t)\n5 x\u03c4 i\u22121 = \u221a \u03b1\u03c4 i\u22121 x0 + 1 \u2212 \u03b1\u03c4 i\u22121 \u2022 x\u03c4 i \u2212 \u221a \u03b1\u03c4 i x0 \u221a 1\u2212\u03b1\u03c4 i 6 end 7 Decode entities (li, ri, ci) K eval i=0\n, where \u03b4i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c} 8 Perform post-processing on (li, ri, ci) K eval i=0 9 return final entities where C is the number of entity types and Classifier is a two-layer perceptron with a softmax layer.\nTraining Objective With K entities predicted from the noisy spans and N ground-truth entities, we first use the Hungarian algorithm (Kuhn, 1955) to solve the optimal matching \u03c0 between the two sets 4 as in Carion et al. (2020) . \u03c0(i) denotes the ground-truth entity corresponding to the i-th noisy span. Then, we train the boundary reverse process by maximizing the likelihood of the prediction:\nL = \u2212 K i=1 \u03b4\u2208{l,r,c} log P \u03b4 i \u03c0\u03b4 (i)\nwhere \u03c0l (i), \u03c0r (i) and \u03c0c (i) denote the left and right boundary indexes and type of the \u03c0(i) entity. Overall, Algorithm 1 displays the whole training procedure of our model for an explanation.\n\nInference\nDuring inference, DIFFUSIONNER first samples K eval noisy spans from a Gaussian distribution, then performs iterative denoising with the learned boundary reverse diffusion process based on the denoising timestep sequence \u03c4 . Then with the predicted probabilities on the boundaries and type, we can decode K eval candidate entities (l i , r i , c i ) K eval i=0 , where \u03b4 i = argmax P \u03b4 i , \u03b4 \u2208 {l, r, c}. After that, we employ two simple post-processing operations on these candidates: de-duplication and filtering. For spans with identical boundaries, we keep the one with the maximum type probability. For spans with the sum of prediction probabilities less than the threshold \u03c6, we discard them. The inference procedure is shown in Algorithm 2. 5 Experimental Settings\n\nDatasets\nFor nested NER, we choose three widely used datasets for evaluation: ACE04 (Doddington et al., 2004) , ACE05 (Walker et al., 2006) , and GE-NIA (Ohta et al., 2002) . ACE04 and ACE05 belong to the news domain and GENIA is in the biological domain. For flat NER, we use three common datasets to validate: CoNLL03 (Tjong Kim Sang and De Meulder, 2003) , OntoNotes (Pradhan et al., 2013) , and MSRA (Levow, 2006) . More details about datasets can be found in Appendix B.\n\nBaselines\nWe choose a variety of recent advanced methods as our baseline, which include: 1) Tagging-based methods (Strakov\u00e1 et al., 2019; Ju et al., 2018; Wang et al., 2020) ; 2) Span-based methods (Yu et al., 2020; Li et al., 2020; Wan et al., 2022; Lou et al., 2022; Zhu and Li, 2022; Yuan et al., 2022b) ; 3) Generation-based methods (Tan et al., 2021; Yan et al., 2021b; Lu et al., 2022) . More details about baselines can be found in Appendix D.\n\nImplementation Details\nFor a fair comparison, we use bert-large (Devlin et al., 2019) ary refinement, and thus obtain better performance.\nThe results also validate that our DIFFUSIONNER can recover entity boundaries from noisy spans via boundary denoising diffusion.\n\nAnalysis\nInference Efficiency To further validate whether our DIFFUSIONNER requires more inference computations, we also conduct experiments to compare the inference efficiency between DIFFUSIONNER and other generation-based models (Lu et al., 2022; Yan et al., 2021a) . Just as shown in Table 3 , we find that DIFFUSIONNER could achieve better performance while maintaining a faster inference speed with minimal parameter scale. Even with a denoising timestep of \u03b3 = 10, DIFFUSIONNER is 18\u00d7 and 3\u00d7 faster than them. This is because DIFFU-SIONNER generates all entities in parallel within several denoising timesteps, which avoids generating the linearized entity sequence in an autoregressive manner. In addition, DIFFUSIONNER shares sentence encoder across timesteps, which further accelerates inference speed. speed of DIFFUSIONNER under various numbers of noisy spans. Just as shown in Figure 3 , we find that, with an increase of denoising steps, the model obtains incremental performance improvement while sacrificing inference speed. Considering the trade-off between performance and efficiency, we set \u03b3 = 5 as the default setting. In addition, when the noisy spans are smaller, the improvement brought by increasing the denoising timesteps is more obvious. This study indicates that our DiffusionNER can effectively counterbalance the negative impact of undersampling noise spans on performance by utilizing additional timesteps. \n\nSampling Number\nAs a generative latent model, DIFFUSIONNER can decouple training and eval-uation, and dynamically sample noisy spans during evaluation. To manifest this advantage, we train DIFFUSIONNER on ACE04 with K = 60 noisy spans and evaluate it with different sampling numbers K eval . The results are shown in Figure 4 . Overall, the model performance becomes better as the sampling number of noisy spans increases. Specifically, we find that DIFFUSIONNER performs worse when K eval < 30. We guess this is because fewer noisy spans may not cover all potential entities. When sampling number K eval > 60, we find it could also slightly improve model performance. Overall, the dynamic sampling of noisy spans in DIFFUSIONNER has the following advantages: 1) we can improve model performance by controlling it to sample more noisy spans; 2) dynamic sampling strategy also allows the model to predict an arbitrary number of entities in any realworld application, avoiding the limitations of the sampling number at the training stage.\n\nAblation Study\nNetwork Architecture As shown in Table 4 , we conduct experiments to investigate the network architecture of the boundary reverse diffusion process. We found that DIFFUSIONNER performs better with a stronger pre-trained language model (PLM), as evidenced by an improvement of +0.53% on ACE04 and +0.11% on CoNLL03 when using roberta-large. Additionally, for the span encoder, we find that directly removing self-attention between noisy spans or cross-attention of spans to the sentence can significantly impair performance. When both are ablated, model performance decreases by 1.37% and 1.15% on ACE04 and CoNLL03. These results indicate that the interaction between the spans or noisy spans and the sentence is necessary. the added noise at each timestep during boundary forward diffusion process. Therefore, we analyze the performance of DIFFUSIONNER on different variance schedulers with different noise timesteps T . The results on ACE04 and CoNLL03 are shown in Table 5 . We find that the cosine scheduler generally yields superior results on the ACE04, while the linear scheduler proves to be more effective on CoNLL03. In addition, the performance of DIFFU-SIONNER varies with the choice of noise timestep, with the best performance achieved at T = 1000 for ACE04 and T = 1500 for CoNLL03.\n\nExpansion Stratagy\nThe expansion stratagy of the entity set can make the number of K noisy spans consistent across instances during training.\nWe conduct experiments to analyze the performance of DIFFUSIONNER for different expansion strategies with various numbers of noisy spans. The experimental results are shown in Table 6 . Generally, we find that the random strategy could achieve similar or better performance than the repetitive strategy. In addition, Table 6 shows that DIFFU-SIONNER is insensitive to the number of noisy spans during training. Considering that using more noisy spans brings more computation and memory usage, we set K = 60 as the default setting.\n\nConclusion\nIn this paper, we present DIFFUSIONNER, a novel generative approach for NER that converts the task into a boundary denoising diffusion process. Our evaluations on six nested and flat NER datasets show that DIFFUSIONNER achieves comparable or better performance compared to previous stateof-the-art models. Additionally, our additional analyses reveal the advantages of DIFFUSIONNER in terms of inference speed, progressive boundary refinement, and dynamic entity sampling. Overall, this study is a pioneering effort of diffusion models for extractive tasks on discrete text sequences, and we hope it may serve as a catalyst for more research about the potential of diffusion models in natural language understanding tasks.\n"}
{"question": "Which of the following is not a contribution of this paper?", "evidence": "  Our contributions are summarized below:A new pre-training approach for multidocument modeling, formulated as a crossdocument question answering task, further directing the LM to model cross-text relationships, focusing on both fine-and coarsegrained information.  We foresee that our method should be significant specifically for retrieval-augmented language modeling setups (Izacard et al., 2022)  Finally, the use of a single document in order to trigger cross-document relationships, as firstly introduced in this work, might be further investigated.\n ", "options": ["A. A novel pre-training scheme is proposed for multi-document tasks", "B. This paper facilitates the exploration of retrieval-enhanced language modeling settings", "C. The second proposes an approach that uses a single document to trigger cross-document relationships", "D. Directing the LM to model cross-text relationships"], "answer": "C", "content": "\nIntroduction\nAmong recent NLP research, multi-document processing is gaining increasing attention, due to the need to handle and process an increasing amount of textual data and available documents online. A * Work partly done as an intern at AI2. 1 Our code is available at https://github.com/ aviclu/peekacross. which we split into context documents (2) and a held-out document (3), we select the most salient sentence (4) that is used for generating a question-answer pair (5).\nThen, we pre-train a model by generating the proper answer and the salient sentence, given the question and the context documents (6).\nnumber of prominent applications that are concerned with aggregating information from multiple texts are multi-document summarization (Fabbri et al., 2019; Zhao et al., 2020) , query-focused multidocument summarization (Xu and Lapata, 2020; Pasunuru et al., 2021a) , and multi-hop question answering (Yang et al., 2018; Welbl et al., 2018) . These tasks remain challenging mostly since existing NLP models are designed to handle single texts, rather than processing multiple documents at once (Caciularu et al., 2021) .\nEarly solutions for multi-text processing were task-specific and used complex architectures that were difficult to generalize across different multidocument tasks (Liu and Lapata, 2019; Wang et al., 2020; Ginzburg et al., 2021) . Efficient LMs (Tay et al., 2021; Beltagy et al., 2020) recently demonstrated that by simply concatenating multiple documents into a single sequence, the transformer can offload the goal of identifying and connecting relevant information between the documents. Recently, it was suggested that these long-context LMs can be equipped with new pre-training objectives to enable them to process multiple documents more effectively (Caciularu et al., 2021; Xiao et al., 2022; Yasunaga et al., 2022) .\nThese pre-trained models demonstrated state-ofthe-art performance on a variety of multi-document downstream tasks, and outperformed underlying LMs and task-specific architectures. Such models are often pre-trained using a dataset where each instance is a set of related documents (e.g., news articles all discussing a specific event), which facilitates modeling of cross-text relationships. Existing multi-document pre-training objectives involve unmasking tokens in a document (Caciularu et al., 2021) , or generating a salient masked sentence (Zhang et al., 2020; Xiao et al., 2022) , encouraging the model to recover missing information using other documents. While successful, these models are either limited to classification tasks (Caciularu et al., 2021) or primarily designed for summarization (Zhang et al., 2020; Xiao et al., 2022) .\nIn this work, we propose a novel pre-training objective that supports both short and long text generation, resulting in a versatile and general multidocument language model. In particular, we hypothesize that using questions and answers involving multiple documents can encourage the model to better learn and incorporate both fine-grained information (by asking questions about core information units in a specific sentence) as well as coarsegrained cross-document relationships required to generate a long text such as a summary. We show that this approach holds not only for summarization, but for other multi-document downstream tasks as well.\nDuring the pre-training of existing multidocument language models, the goal is to unmask spans (for encoder-only models) or generate masked textual spans (for encoder-decoder models) under a multi-document context. To that end, multiple concatenated sequences of related documents are fed during pre-training, thus requiring a large number of sets of related documents for an effective pre-training phase (Hoffmann et al., 2022) . In a variety of existing multi-document benchmarks, such as multi-document summarization, only small to medium-scale document clusters are readily available. These are acquired either automatically with lexical similarity and retrieval (Fabbri et al., 2019) or semi-automatically (Gu et al., 2020) , but generally, this process requires a substantial amount of human effort for filtering instances and generating high quality corpora.\nBy employing a novel multi-document question-answer generation procedure, we propose an effective method for expanding the multi-document pre-training corpora. Our approach allows us to provide multiple views for every single cluster of documents, thereby artificially increasing the pretraining data size (in terms of number of instances) via augmentation. To expose the model to a variety of contexts and diversify the pre-training data, we propose to generate multiple pairs of questions and answers and condition them on a subset of the documents' cluster. We select a salient sentence in one held-out document and then employ a recent parser to generate a high-quality question-answer pair about one predicate in the selected sentence, using a systematic semantically-oriented approach (Klein et al., 2022) . This new multi-document pre-training objective challenges the model to generate both the answer to the question as well as the salient sentence, while discarding the held-out document or parts of it (see Figures 1, 2 for illustration). This procedure exposes the model to a variety of contexts -a question and a different subset of the documents in the cluster per instance, in contrast to prior methods that provide only a single view of the cluster. Our contributions are summarized below:\n\u2022 A new pre-training approach for multidocument modeling, formulated as a crossdocument question answering task, further directing the LM to model cross-text relationships, focusing on both fine-and coarsegrained information. \n\nRelated Work\nLong-context efficient text generation transformers (Tay et al., 2021 (Tay et al., , 2022) ) extend earlier transformer models (Vaswani et al., 2017) for processing long sequences, often using a sparse self-attention architecture. Examples include the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) , and LongT5 (Guo et al., 2022) . These models demonstrated that single-text approaches be can adapted to multi-document tasks by concatenat-ing multiple documents into a single sequence and processing them using their sparse attention patterns. They sparsify the full self-attention matrix of transformers by using a combination of a localized sliding window (called local attention), as well as a global attention pattern on a few specific input locations. LED is build upon the BART model (Lewis et al., 2020) by using additional positional embeddings and global attention weights, and introduces the global attention mode that operates over pre-selected tokens. LongT5 extends the T5 model (Raffel et al., 2020 ) by using a similar technique introduced in the ETC and BIGBIRD models (Ainslie et al., 2020; Zaheer et al., 2020) , relieving the requirement to manually select global tokens by automatically globalizing the aggregated representations of groups of tokens.\nFurther strategies have been proposed for increasing these models' abilities in multi-document tasks. The Cross-Document Language Model (CDLM) (Caciularu et al., 2021) suggested pretraining a Longformer-encoder (Beltagy et al., 2020) over sets of related documents, and showed superior performance results over several multidocument tasks. Following this methodology, the authors of LinkBERT (Yasunaga et al., 2022 ) used a similar approach, but utilized Wikipedia's hyperlinks in order to curate informative pairs of linked documents for LM pre-training.\nIn order to adopt the multi-document pretraining approach for sequence-to-sequence tasks, PRIMERA (Xiao et al., 2022) , which is built on top of the Longformer encoder-decoder model (LED), selected salient sentences within clusters of related documents using a pyramid estimation approach, resembling the method presented for pre-training the single-document PEGASUS model (Zhang et al., 2020) . While this work is the closest to ours, it was pre-trained to generate masked salient sentences without any control, which makes the model potentially hallucinate while generating text, while our model uses a controlled QA-based objective. Furthermore, unlike these works, our method generates significantly more data then used to pre-train PRIMERA, which is possible to obtain by the singledocument QA generation approach. Our QA pretraining formulation allows us to generate multiple contexts per document cluster.\nAnother related line of work includes methods that incorporate large-scale QA-generated data for pre-training LMs (He et al., 2020; Jia et al., 2022 ;\n\n(a) The held-out document is discarded from the context (c) The held-out document is included in the context, but the answer in the anchor sentence is masked (b) The held-out document is included in the context, but the anchor sentence is masked\nFigure 2 : A schematic of our pretraining data modes. The salient sentence which is used for QA generation is colored in yellow. (a) The context does not include the held-out document, therefore this mode is the most challenging. (b) The held-out document is present in the context, but the salient sentence used for the QA generation is masked (red). (c) The held-out document is present in the context, but the answer span within the salient sentence is masked (red). Huber et al., 2022) . These works hypothesize and show that pre-training by utilizing generated QA data can encourage contextual representations to encode useful semantic information for other non-QA downstream tasks. Inspired by that, we conjecture that LMs can strongly benefit from infusing QA during pre-training in the multi-document setup, for adding an additional signal for modelling cross-text relationships.\n\nAugmenting the Multi-Document Pre-training objective\nIn this section, we provide the required steps for compiling the pre-training dataset for QAMDEN.\nWe next elaborate on the details of the data creation and provide analysis of the resulted corpus.\nRecent works have shown that for text summarization, pre-training LMs to generate a \"summarylike\" sequence, termed pseudo summary, inherently provides gains over general-purpose pre-trained LMs (PEGASUS, PRIMERA; Zhang et al., 2020; Xiao et al., 2022) . The data in which the PEGASUS and PRIMERA models were pre-trained on was constructed using the Gap Sentence Generation (GSG) method, which suggests masking highly-ranked salient sentences, where salience is pre-determined by a sentence-scoring method of interest. Particularly, in PEGASUS, GSG has been adopted as its pre-training objective, where some sentences in a single document are masked in the input and the model is tasked to generate them.\nFormally, for each sentence s i in a given input document D, PEGASUS computes its salience score based on its ROUGE score (Lin, 2004) w.r.t the rest of the sentences within the document (D/{s i }), i.e. Score(s i ) = ROUGE(s i , D/{s i }). Intuitively, \u2026Pokemon Sword and Shield might have already been announced, but we now know there's another new Pokemon game on the way from DeNA\u2026 QASem QA generation (Klein et al., 2022) Q1: What might been announced? A1: Pokemon Sword and Shield. (Answer length: 4) Q3: Who knows something? A: We. (Answer length: 1) Q2: Where does someone know something? A: On the way from DeNA. (Answer length: 5) Contextualization (Pyatkin et al., 2022) Q: Where did we know there's another new Pokemon game? A: On the way from DeNA.\n\nSelected\nFigure 3 : A schematic of the process of QA generation using QASEM (Klein et al., 2022) and the contextualization model from Pyatkin et al. (2021) . This is an actual sample that was created and used for pre-training QAMDEN, where the document is taken from New-SHead (Gu et al., 2020) .\nthis metric assigns a high score to the sentences that have a high overlap and share more lexical information with the rest of the sentences in the document, thus assigning high scores to prominent sentences. PRIMERA has generalized this notion to support the multi-document setup, by applying a GSG variant over a cluster of related documents.\nCross-Document GSG. We propose augmenting the GSG technique to formulate a cross-document question answering pre-training objective for multidocument tasks, instead of the existing pseudo summary generation methods. Our approach supports identification of both fine-and coarse-grained information as we describe below, and results in a substantially larger amount of pre-training examples compared to the preceding methods.\nFormally, we are given a cluster of related documents S = D 1 , D 2 , . . . , D |S| in a corpus C. Our cross-document (CD) GSG salience score for the i th sentence within the k th document in the set (s i k ), is defined by its ROUGE score w.r.t the rest of the sentences within the document (D k /{s i k }) as well as the other documents (S/D k ), i.e. CD-GSG-Score(s i k ) = ROUGE(s i k , S/{s i k }). Then, for every document k, following Zhang et al. (2020) ; Xiao et al. (2022) we select the top-scored sentence s * k , and then we use this sentence to generate a pair of a question and an answer.\nGenerating Cross-Document QAs. For generating the cross-document questions and their answers, we employ QASEM, a recent semantic parsing framework for question generation (Klein et \nfor k \u2190 1 to |Sn| do 4 s * k \u2190 arg max i CD-GSG-Score(s i k ); 5 (q * k , a * k ) \u2190 QASEM(s * k ); 6 t * k = [a * k , s * k ] # target text; 7 D \u2190 D \u222a {([Sn/D k , q * k ] , t * k )} # (a); 8 D \u2190 D \u222a {([Sn/ {s * k } , q * k ] , t * k )} # (b); 9 D \u2190 D \u222a {([Sn/ {a * k } , q * k ] , t * k )} # (c); 10 Return D;\n2022). 2 QASEM intended soliciting a manageable, discrete account of information in a text for the sake of building natural language semantic representations. It automatically labels each verbal predicate-argument relation with a questionanswer pair, where a natural language question represents a semantic role, while the answers correspond to the arguments that appear in the input text. QASEM is thus an appealing approach since it is capable of generating multiple high-quality questions given a sentence. We apply QASEM over the sentences withing the pre-training data in order to generate question-answer pairs, and then apply the model from Pyatkin et al. (2021) which transforms the question into a more natural and clear form, with contextualized arguments (see example in Figure 3 ). In order to resemble a summarization task where the generated text is typically long, we select the question-answer pair with the longest argument produced by QASEM. Formally, QASEM(\u2022) receives a sentence s * k as an input, and produces question-answer pair (q * k , a * k ), where a * k is the longest among the generated answers. See a detailed example and full description in App. A.1.\nConsidering the question-answer pair, our goal is to encourage the LM to generate the correct answer as well as the salient sentence in a multi-document context in order to learn cross-text relationships.\nData Generation Process. In order to facilitate the construction of a multi-document context, we propose three different modes, each one is responsible for uncovering information by using different contexts. For all the modes, we first generate a QA pair out of the most salient sentence in the held-out document.\n(a) Excluding the source document. In this mode we disregard the held-out document D k from the context S n given to the model, i.e, S n /D k . Hence, the model is tasked to predict the answer without having access to the source document at all, and is restricted to observe only the other documents in the set. Thus, this mode is considered as the most challenging one.\n(b) Masking the salient sentence. In this mode, the source salient sentence is masked, i.e, S n / {s * k }. The model has access to the surrounding context of the masked sentence in the held-out document, as well as the other documents in the set.\n(c) Masking the answer. In this mode, only the answer span within the salient sentence is masked, i.e, S n / {a * k }. The model has access to the surrounding salient sentence, as well as all the documents in the set.\nAs part of the new pre-training process of our novel multi-document model, we append the question after the context and instruct the model to generate an answer followed by its salient sentence, i.e., output = \u27e8answer\u27e9, \u27e8sentence\u27e9, inspired by Bohnet et al. (2022) . Generating the salient sentence introduces a copying mechanism (allows the model to also learn to copy information from the source directly) as well as allowing longtext generation, which is crucial for summarization downstream tasks (Zhang et al., 2020) , as well as outperforming a model which was pre-trained for generating the answer solely -according to the ablations study, this setup yields the best performance results ( \u00a74.4). In the pre-training evaluation phase, the held-out set was split and the loss was measured separately for each mode of the data. As expected, we observed that the loss for (a) was significantly higher than those for the other modes, with (a)\u227b(b)\u227b(c) ranking highest. The procedure for generating the pre-training data is summarized in Algorithm 1 and Figure 2 .\nThe resulted pre-training corpus. We applied our procedure over the NewSHead corpus (Gu et al., 2020) , which consists of a set of related documents per instance. This is the exact same pre-training corpus used also by our main baseline PRIMERA (Xiao et al., 2022) \n\nExperimental Setup and Results\nThis section presents experiments conducted to evaluate QAMDEN, as well as the the ablations and baselines we used. For the intrinsic evaluation we evaluated the models over multi-document QA tasks. For extrinsic evaluations we considered the multi-document abstractive summarization task.\nModel Implementation Details Following Xiao et al. ( 2022), we use the large-sized Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) for our model initialization. The length limits of input and output are 4096 and 1024, respectively. 3 Following the Huggingface implementation (Wolf et al., 2020) , we set the sliding window size to 1024 for local attention in the encoder part.\nSimilar to the PRIMERA model (Xiao et al., 2022) , when concatenating the documents and the question, we add a special document separator token (<doc-sep>) between the documents to signal to the model to be aware of the document boundaries. We also assign the global attention mode to these tokens which enables the model to share information across documents (Caciularu et al., 2021) . For further hyperparameter and pre-training execution details, see App. B.\n\nMulti-Document Question Answering\nMulti-document QA is the task of generating the correct answer, given a set of related multiple documents. For several multi-document QA benchmarks, models are often tasked to implicitly solve multiple sub-tasks or follow intermediate steps, such as comprehending the question, filtering out distracting documents in the context, and stitching pieces of information across the relevant documents (Geva et al., 2021; Caciularu et al., 2022) . Recall that QAMDEN was pre-trained over a automatically generated multi-document QA dataset. Hence, as a preliminary assessment, we first investigate QAMDEN's performance over two multi-document QA benchmarks, HopotQAdistractor (Yang et al., 2018) and WikiHop (Welbl et al., 2018) (see more details of the datasets in App. C.1), and compare to other models that were pre-trained using underling un-masking objectives.\nFine-Tuning Format. To follow our pre-training scheme, we append the question to the context and fine-tune the model to generate the correct answer. We use the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) and PRIMERA (Xiao et al., 2022) as the baselines, for assesing the contribution of our pre-trainig format. Confirmed by Beltagy et al. (2020) , we found out that appending the question: and context: prefixes before the question and the context tokens, respectively, resulted in better performance.\nBaselines. We compare QAMDEN (447M parameters) against a set of strong long-context transformer baselines, including LED (447M parameters) (Beltagy et al., 2020) , PRIMERA (447M parameters) (Xiao et al., 2022) , 4 and LongT5-xl (3B parameters) 5 (Guo et al., 2022 ) (see \u00a72). 6 Results. The results on multi-document QA are shown in Table 2 . We adopted the F1 and Exact Match (EM) evaluation metrics corresponding to the original works. Our QAMDEN outperforms both PRIMERA, LED, and LongT5, confirming that our pre-training data and input format are beneficial for both capturing cross-document relationships (QAMDEN\u227bLED) as well as exploiting both context and question (QAMDEN\u227bPRIMERA).\n\nMulti-Document Summarization (MDS)\nThis task aims at generating a summary for a given set of topically-related documents. Inherently, end-Model F1 EM HotpotQA LED (Beltagy et al., 2020) 65.8 50.6 LongT5-xl (Guo et al., 2022) 66.1 50.9 PRIMERA (Xiao et al., 2022) 65 Results. Tables 3 and 4 present the evaluation results over the Multi-News and Multi-XScience datasets, respectively. Following previous MDS works, we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.2 for details). For a fair comparison, we include the results of PRIMERA as well as the results of the previous state-of-the-art methods (Pasunuru et al. (2021b) and Lu et al. (2020) , for Multi-News and for Multi-XScience, respectively), and LED (Beltagy et al., 2020) . As shown in the results tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, especially on the Multi-News dataset, clearly demonstrating its consistent advan- (Beltagy et al., 2020) 47.4 20.7 23.7 LongT5-xl (Guo et al., 2022) 47.4 20.7 23.7 PRIMERA (Xiao et al., 2022) 49.9 21.1 25.9 QAMDEN 50.9 23.1 27.2 tage. This excludes the results for Multi-XScience where QAMDEN slightly underperforms the prior work and LongT5. An explanation which Xiao et al. (2022) points refers to the fact that the clusters in Multi-XScience have less overlapping information compared to the corpus we used, attributed to the use of abstracts as the input documents in Multi-XScience. In addition, LongT5 advantage over QAMDEN is attributed to significantly larger number of parameters of LongT5-xl.\n\nQuery-Focused Multi-Document Abstractive Summarization\nThe task of Query-focused Multi-Document Summarization (QMDS) aims at generating a summary from a set of documents, that answers a specific given query. Unlike MDS, QMDS tries to solve more realistic query-based scenarios, since it suggests summarizing only predefined salient information of interest that best answers the query. Since we proposed pre-trainng under the multi-document question answering setup, we posit that QAMDEN might be effective for QMDS.\nWe consider the datasets constructed by Pasunuru et al. (2021a), QMDSCNN and QMDSIR (see more details of the datasets in App. C.3) as well as their strong baseline, and include also the results of PRIMERA and LED.\nBaselines. Similar to the previous experiments, we compare QAMDEN against LED, PRIMERA, LongT5-xl. In addition, we consider also the baseline from Pasunuru et al. (2021a) . 37.9 16.4 35.2 LED (Beltagy et al., 2020) 32.3 14.3 30.9 LongT5-xl (Guo et al., 2022) 35.5 15.9 34.3 PRIMERA (Xiao et al., 2022) 36 Results. Tables 5 and 6 present the evaluation results over the QMDSCNN and QMDSIR datasets, respectively. Following MDS tasks and Pasunuru et al. (2021a) , we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.3 for details). As shown in the tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, clearly demonstrating its consistent advantage over the baselines.\n\nAblation Study\nData Generation. We next turn to a broad ablation study, for assessing our configuration and design choices across our suggested pipeline. First, we show the advantage of combining the three proposed data modes, rather than using a subset of them. We evaluate all the resulted models by fine-tuning them over HopotQA-distractor ( \u00a74.1), Multi-XScience ( \u00a74.2), and QMDSIR ( \u00a74.3). For HopotQA-distractor we report the Exact Match (EM) score, and for the summarization tasks we report the ROUGE-1 (R-1) score.\nBaselines. We pre-train QAMDEN for 100k steps, for using every subset of the set of the set (superset) of modes {(a), (b), (c)} (all its possible combinations) of the generated pre-training data modes presented in \u00a73. Note that our QAMDEN model is referred to as using all the modes, i.e., For QA we used the EM score, and for MDS and QMDS we used the ROUGE-1 score.\nResults. Figure 4 shows the ablation results. In all tasks, pre-training using all modes yields the best results. Among all modes, mode (c) appears to be the most effective for QA, since this is an extractive QA task, and mode (c) provides data in this format. Mode (a) excels at the summarization tasks, attributed to their abstractive nature as well as the requirement of all the documents for generating appropriate summaries.\nInput Format We repeat the previous experiment and ablate the pre-training input format according to the multiple different formats, and compare to the model pre-training format described in \u00a73 (with the same pre-training data): without questions, with random question, with random context document, with prefixes, placing the question before the context, with question filtering, and without generating the salient sentence. Additionally, we assess the choice of QASEM as our questionanswer generation module by using the generators from Jia et al. ( 2022) and Khashabi et al. (2022) . Finally, we also include the results of PRIMERA, which was further pre-trained for additional 300k steps (fine-tuning LED for 400k steps in total), for a fair comparison to QAMDEN ablated models. See full details regarding all the ablations in App. D.\nResults. Overall, our QAMDEN model outperforms the ablation models on most of the tasks, which a significant margin.\nPre-training the model without any questions during or using random questions, negatively impacts the results of downstream tasks. An impor- tant function of the question is to facilitate the model's ability to generate the appropriate answer and the source sentence. This aligns with the findings from Caciularu et al. (2021) , who showed that pre-training with random documents rather than related ones is sub-optimal. The use of question and context prefixes for positioning input appears to be helpful for QA, but is inferior when applied to summarization tasks due to its unique format, which is well suited for QA but seems to generalize harder for other setups. When the question is placed before the context, performance slightly decreases over query-based tasks, while maintaining the same results for summarization (where the question location is irrelevant).\nUsing question filtering is found to harm the downstream results of QAMDEN, in accordance to other QA-based pre-training prior works (Jia et al., 2022) .\nPre-training without generating the attributed source sentence introduces a significant flow to the model, particularly for the summarization downstream tasks. As mentioned before, generating longer sequences, as well as teaching the model to copy text, is beneficial for summarization tasks.\nApplying a different question generator rather then QASEM yields inferior results overall, since the other generators produce open-ended questions and answers which are more prone to errors, while QASEM utilizes an existing span in the context as the answer. In addition, QASEM generated local questions, which allows QAMDEN to focus on the fine-grained details, and not only the coarsegrained information in the multi-document context.\nWhen PRIMERA is pre-trained with 400k steps (to match QAMDEN's number of further pretraining steps), it underperforms QAMDEN and even fails to add any significant improvements over its 100K checkpoint, possibly due to the small amount of pre-training data it contains. \n\nComparison with Large Language Models\nIn order to get insights into how QAMDEN compares with state-of-the-art Generalist Large Language Models (LLMs), we provide a small comparison with two capable models, GPT-3.5 turbo (Ouyang et al., 2022) and GPT-4 8 (OpenAI, 2023) (including the 8k input length version) evaluated on the zero-shot setting.\nFor a fair comparison, we used the same context window size of 4K tokens for all models (and up to 8k for GPT-4 8k). Due to the fact that multidocument tasks involve processing long sequences, the cost of API calls is significant for a comprehensive evaluation across all datasets. Therefore, we only evaluate on a sample of 200 instances from the multi-news dataset (see prompting details in App. E). Table 8 depicts the results. We observe that QAMDEN significantly outperforms both GPT-3.5 and GPT-4 models, though the performance of GPT-4 and GPT-3.5 is comparable. We leave more comprehensive comparisons with LLMs to future work.\nWe further assessed QAMDEN through manual comparison against PRIMERA, GPT-3.5, and GPT-4 8k. NLP graduate students were shown summaries for a given topic from the three systems and QAMDEN in arbitrary order, along with a corresponding reference summary. Following (Ernst et al., 2022) , participants were asked to rank the systems based on Content (overlap with the reference), Readability (the readability of a summary), Grammaticality (avoiding grammar errors), and Non-Redundancy (avoiding repetitions), and we extract the pairwise results out of the rankings (see (Ernst et al., 2022) for further details). In App. F, we provide several examples to system summaries and their corresponding reference summaries.\nThe results of this study are presented in Table 9 . Under each evaluation criterion, it indicates the percentage of cases where QAMDEN was preferred over both baselines. QAMDEN was favored in all cases except for grammatical errors and readability (which corresponds to the Reinforcement Learning from Human Feedback phase of the GPT models).\n\nConclusions\nIn this work, we present a novel pre-training scheme for multi-document tasks. First, our approach suggests to augment the existing multidocument pre-training objectives into a crossdocument question answering task. Second, we generate high-quality large-scale QA pre-training data using a controlled generation approach, in which each QA pair originates from a salient sentence in one of the documents in the set.\nDuring pre-training, we task the the Longformer Encoder-Decoder (LED) model to generate the answer and the salient sentence on the basis of the remaining context. This objective encourages the LED model to elicit cross-document relationships, and stitch pieces of information across the input documents, which are relevant for performing multi-document tasks. The resulted model QAMDEN shows significant performance improvements compared to prior models under extensive experimentation over multiple challenging multidocument summarization and QA datasets.\nFuture work can extend the ideas in this work for equipping decoder-only large LMs with crossdocument modeling using our proposed method, also in the setup of in-context learning and prompt tuning. We foresee that our method should be significant specifically for retrieval-augmented language modeling setups (Izacard et al., 2022) , where there is a use of related documents as an outsourced external non-parametric knowledge source. Finally, the use of a single document in order to trigger cross-document relationships, as firstly introduced in this work, might be further investigated.\n"}
{"question": "Which dataset was used for pre-training UPPAM?", "evidence": "   Thus, we start with legislators to construct our pre-training datasets.  ", "options": ["A. Data from journalists, news media, and anonymous users", "B. Data from congress legislators and other political actors", "C. Data from the 114th and 115th bills", "D. Data from self-promotion advertisements and notifications", "Compared to other political actors, congress legislators are more typical and they generate massive content every day. "], "answer": "B", "content": "\nIntroduction\nPolitical actors are shaping our attitudes, opinions, and decisions toward public issues. For instance, on social platforms, politicians can select and emphasize certain aspects of content to bias the discussion, through which they can derive an opinion climate from user engagement and acquire direct feedback from potential voters and opinion leaders (Bene, 2017; Heiss et al., 2019) . Political actor modeling is essential for quantitative political science and has applications in various downstream tasks such as roll call vote prediction (Yang et al., 2020) , frame detection (Johnson et al., 2017) and bias detection (Baly et al., 2020) .\nData-driven approaches utilize different kinds of information to profile political actors, including public statements, legislative behaviors and social Figure 1 : An illustration of political actors. They not only participate in legislative activities, but also form relationships with others, and convey opinions through tweets, speeches and etc. We propose to represent political actors based on their statements and learn the mapping from language to their representations using social networks and behaviors as self-constructed supervision.\nnetworks (Figure 1 ). Early research analyzes roll call data to estimate the ideology of political actors. Ideal point model (Clinton et al., 2004 ) is one of the most widely used approaches for votebased analysis that reveals how cleavages between legislators reflect partisan affiliation. Researchers further incorporate texts of bills to enhance the ideal point model (Gerrish and Blei, 2011, 2012; Kraft et al., 2016) and develop multidimensional vectors to replace one-dimension points. Recently, more abundant information has been considered to learn effective representations for political actors, such as co-sponsorship network (Yang et al., 2020) , relations of contributors (Davoodi et al., 2020) , stakeholders (Davoodi et al., 2022) , mention in documents (Pujari and Goldwasser, 2021) , and expert knowledge (Feng et al., 2021 (Feng et al., , 2022)) .\nGenerally speaking, previous research aims to learn representations for a certain group of political actors using supervision from specific downstream tasks as objectives. Although they report positive results on target tasks, their models lack generalization ability in two aspects. (1) Representations are learned on labeled data from specific tasks, e.g., state-level vote prediction, therefore they cannot be easily transferred to other tasks or scenarios. (2) The model is limited to the training setting and can not be adapted to dynamic social contexts. In other words, it's hard for the model to estimate new legislators, non-voting candidates and other political actors unseen.\nRecently, large-scale pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019b; Brown et al., 2020) have demonstrated a strong generalization ability and achieved excellent performance in many language modeling tasks. Motivated by PLMs, we explore representing political actors based on their statements and propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM) 1 . We employ a two-stage training procedure following the fashion of PLMs. Firstly, we pre-train our model to learn the mapping from statements to actor representation. We propose a multigranular method to represent political actors based on language, and information of political scenarios is further injected into our model via proposed structure-aware contrastive learning and behaviordriven contrastive learning tasks. Secondly, we fine-tune the model for downstream tasks using the corresponding supervised objectives.\nUPPAM is novel in three points. (1) We learn the mapping from statements to the representation of political actors, instead of directly learning actor representations. By doing so, the mapping parameters can be transferred to any downstream tasks easily, learning representations for unseen political actors based on their statements. (2) We propose several self-training tasks to inject general knowledge in the political scenarios into mapping parameters in the pre-training stage. (3) We propose a multigranular actor representation model, that can capture nuances of both general ideology and specific preferences between different political actors. We evaluate our approach on three types of tasks in quantitative political science, i.e., profile of actors, prediction of behaviors and analysis of languages. UPPAM outperforms general PLMs and other political domain-specific PLMs on these tasks. Our task-agnostic model also achieved competitive results compared to the task-specific models that employ architectures crafted for the 1 We have made our code publicly available at https:// github.com/xymou/UPPAM. vote prediction task. Further analysis shows the effectiveness and robustness of UPPAM in few-shot settings and different aggregation settings.\n\nMultigranular Actor Representation\nPolitical actors manifest themselves in political activities in multiple granularities. On the one hand, they hold a general ideology or bias, which is long-term and stable. On the other hand, when discussing or taking action on different issues, they hold specific positions (Gerrish and Blei, 2012) , which are the result of long-term bias and shorttime interests (Spell et al., 2020) . Based on this, we propose to represent political actors in two granularities to model both broad ideology and specific preferences for various downstream scenarios.\n\nGeneral and Specific Statements Collection\nIn practice, we use all statements a political actor has posted to get his general representation, characterizing the broad political leaning. Furthermore, issue-related content is adopted to help capture specific attitudes. Concretely, we use a handcrafted information retriever (see more details in Appendix A.2), to collect statements related to the queried policy area as input to encode the specific representation.\nStatements Aggregator Since a political actor can post thousands of statements, the first challenge is how to aggregate one's statements to get his representation. It is too expensive in time and computation cost to combine full sentences. Instead, we identify indicator words from statements for information aggregation. According to the framing theory (Entman, 1993) , entities and subjective content an author uses can implicitly reflect his political leaning. Following this, we identify entities, frame and sentiment words as indicators. We sort them by TFIDF (Jones, 1972) scores and keep indicators with the highest values to form an indicator sequence. In this case, for each political actor, we can get two kinds of indicator sequences, given a query about policy area j:\nS g i = w g 1 , w g 2 , ...w g N (1) S p j i = w p j 1 , w p j 2 , ...w p j M (2)\nwhere S g i is calculated from all the statements made by political actor i, S related to policy area j, and we reserve top N and M indicators with highest TFIDF value, where N and M are pre-defined hyper-parameters.\nIn subsequent pre-training and downstream tasks, we use general sequences as input when the goal is to profile the characters broadly, e.g., estimating ideology. And we input both sequences and average the representation when specific attitudes are required in tasks, as shown in Figure 2 . Note that even if the issue-related content can not be retrieved, we can use the general sequence as a substitute, to ensure input compatibility.\n\nMultidimensional Pre-training for Political Actor Modeling\nTo inject general knowledge of the political landscape into the mapping from statements to representation, we construct self-supervised tasks based on structural and behavioral information.\n\nStructure-aware Contrastive Learning (SCL)\nIn terms of structural information, we mainly focus on the relationship formed between political actors. Previous studies have revealed that homophily exists in political communities, where people with similar ideologies form a link with each other (Barber\u00e1, 2015) . We use two parts of links, namely party affiliation and co-sponsorship in voting. We treat party affiliation as a coarse relationship and cosponsorship as a fine relationship respectively. By doing this, the model can further capture nuances across parties as well as inside the same party.\nParty Affiliation Link We compare statements of legislators from different parties. We choose a legislator as the anchor, and then take another legislator with the same party affiliation as the positive sample, while those from the opposite party are regarded as negative samples. By comparing general statement sequences of legislators from different parties, the model can learn the differences in the languages of different ideologies.\nCo-sponsorship Link In the legislative process, a bill is initialized by a sponsor and several cosponsors. We assume that the more two legislators collaborate, the more they are alike since they reach agreements on many occasions (Yang et al., 2020; Mou et al., 2021) . Given an anchor legislator, other legislators are divided into three categories based on the number of times they co-sponsored with the anchor legislator: G 1 (the co-sponsorship times are above the average); G 2 (the co-sponsorship times are below the average); G 3 (they have never cosponsored). And we further sample positive and negative samples with the rule of\nG 1 < G 2 < G 3 .\nBased on the triplets constructed in the above two ways, the structure-aware contrastive objective is formulated as follows:\nLSCL = t\u2208T SCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4SCL + (3)\nwhere T SCL is the set of legislator triplets, t (a) , t (p) and t (n) are actor representation encoded by general sequences of anchor, positive and negative sample in triplet t, \u03b4 SCL is a hyperparameter and\n[\u2022] + is max(\u2022, 0).\nNotably, this task endows the model to capture general ideology of speakers from their languages.\n\nBehavior-driven Contrastive Learning (BCL)\nWhen it comes to behavioral information, we pay attention to the most common and important actions, i.e., voting. Specifically, we sample triplets consisting of an anchor bill and a pair of legislators, where the positive legislator p votes yea on the given bill and the negative one n votes nay.\nDifferent from the ideology cleavages modeled in Sec 2.2.1, the divergence of specific preferences is supposed to be reflected in the languages here. Thus, for each legislator, we extract statements about the policy area of the anchor bill as the specific sequence, input with the general sequence, as we mentioned in Sec 2.1. In this way, the behaviordriven contrastive objective is as follows:\nLBCL = t\u2208T BCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4BCL + (4)\nwhere T BCL contains all vote triplets, and \u03b4 BCL is a hyperparameter. t (a) is the bill representation, t (p) and t (n) are the average of representation of the general sequence and the specific sequence, for the positive and negative legislators respectively.\nIt's noticeable that this pattern is not limited to the roll-call vote scenarios, instead, it can be applied to model the preferences towards any bills, events, or targets with a text description.\n3 Pre-training Process\n\nLanguage Model Co-training\nAs mentioned in Sec 2.2.2, modeling political actors in political scenarios inevitably requires encoding textual information of the bills and issues they interact with, e.g., Equation 4. Meanwhile, it is important to understand their opinions in a single discourse without context. Thus, we incorporate additional modules to model political texts. Specifically, as shown in Figure 2 , we have two FFN layers in parallel in each transformer layer, to handle text and actor sequences separately. Given a sequence of input x = {x 1 , ..., x n }, the model first performs multi-head self-attention and then the corresponding module FNN k obtains the required representation:\nh k = FNN k ( Self-Attention ({x 1 , . . . , x n }))\n(5) where k \u2208 {0, 1} indicates the modules of actor and text respectively.\nWe adopt a masked language model objective to pre-train the language model. As mentioned before, political bias and framing effect are often reflected in the selection and mention of specific entities, subjective content, and emphasized frames. Thus, we take a masking strategy that upsamples entity tokens, sentiment words (Wilson et al., 2005) and frame indicators (Roy and Goldwasser, 2020) to be masked for the MLM objectives, with a 30% probability. More details can be found in Appendix B.\n\nOverall Pre-training\nSince the indicator sequence is not a normal sentence, we don't train the MLM task with contrastive learning together. Instead, the pre-training process is divided into two stages. In the first stage, we adopt the MLM task on the original statement sentences and activate text modules, to urge the model to understand the political text. Then, based on this checkpoint, we further conduct the multidimensional pre-training for political actor modeling by combining the objectives:\nEQUATION\n)\nwhere \u03b1 is hyperparameters.\n\nExperiment Setup\nWe fine-tune our model on different kinds of downstream tasks in quantitative political science. We then compare it with prior general PLMs and political domain-specific PLMs.\n\nPre-training Datasets\nCompared to other political actors, congress legislators are more typical and they generate massive content every day. Thus, we start with legislators to construct our pre-training datasets. Overall, we get 887 legislators and delete the meaningless tweets including self-promotion advertisements, notifications, etc., using regular expressions. Finally, the cleaned data contains 2,020,938 tweets, covering discussions of events in various areas. We keep 10K held-out tweets as the validation set.\n\nLegislative Context\nWe collect the party affiliation, sponsorship lists of bills, bills, and corresponding voting records from VoteView 2 and the website of U.S. Congress 3 . Each bill belongs to a specific policy area and has textual information of title and description. We get bills of 112th and 113th for pre-training and reserve those of 114th and 115th for the formulation of downstream tasks. In the pre-training stage, 1,045 bills and 375,440 voting records are involved.\nTo correlate legislators' votes to their statements in the related policy area, we filtered each legislator's tweets in each policy area by the handcrafted information retriever mentioned in Sec 2.1. We finally acquire 1,142,587 tweets, and the details can be found in Appendix A.2. The distribution of the policy agenda of bills and the percentage of legislators whose related tweets can be retrieved in each policy area are shown in Figure 3a and Figure 3b . Over 90% of legislators can be retrieved with relevant statements in most policy areas.\n\nImplementation Details\nUPPAM is produced via continued pre-training on RoBERTa-base model (Liu et al., 2019b) , where we add parallel FFN modules in each transformer layer with the same initialization as the original one. In the first stage, the model is trained on tweets, to minimize the MLM loss with AdamW (Loshchilov and Hutter, 2018) optimizer. In the second stage, the model is further trained on indicator sequences and bill texts, to minimize the L CL . We evaluate the model every 200 training steps on the validation set and keep the best checkpoint. The pre-training procedure takes around 96 hours on 4 Tesla V100-SXM2 GPUs. More details and hyperparameters can be found in Appendix B.\n\nDownstream Tasks and Datasets\nWe evaluate the models on three types of tasks, namely actor profiling, behavior prediction and language analysis. Notably, datasets include not only congress legislators but also other political actors such as journalists, news media, and even anonymous users, to validate the model's generalization capability.\n\nActor Profiling\nThis type of task can be formulated as a user-level classification task, where we aggregate multiple statements to predict the speaker's attribute.\nIdeology Detection is the main task to profile actors broadly, aiming to predict political leaning. Models are evaluated on the following datasets.\n\u2022 CongS (Gentzkow et al., 2018) collects speeches from US congressional records. \u2022 celeb (Wojcieszak et al., 2022) contains tweets of celebrities (journalists, politicians and media). We convert the ideology scores into labels according to the signs. \u2022 Reddit (Kitchener et al., 2022) collects comments of common users in non-political subreddits, and labels the users with ideology in the economic dimension. \u2022 PEM (Xiao et al., 2022) collects tweets of legislators, news outlets and cabinet of President Obama and President Trump. \u2022 TIMME (Xiao et al., 2020) includes Twitter accounts with location information and selfidentified political-polarity labels. These accounts are not run by politicians.\n\nBehavior Prediction\nThis type of task can be regarded as a relation prediction task, where we predict a political actor's attitude or action towards a given target with a piece of text description.\nVote Prediction tasks aim to predict votes of legislators towards bills with stances of yea or nay. We follow two configurations in (Mou et al., 2021) . \u2022 VoteIn refers to the in-session setup, where we randomly split the bills in the same congress session, i.e., the 114th session. \u2022 VoteOut refers to the more challenging outof-session setup, where we use data in the 114th session for training and validation while testing on the 115th session.\nGrade Prediction tasks are designed as classification tasks for ratings in a certain issue, given a politician's statements and background description of the given issue. We include datasets as follows:\n\u2022 NRA Grades (Pujari and Goldwasser, 2021) provides politicians' grades {A, B, C, D & F} assigned by National Rifle Association and their statements on guns, as well as background information of guns from ontheissues.org. \u2022 LCV Grades (Pujari and Goldwasser, 2021) is similar to NRA Grades, but it's about the scores in the environment area.\n\nLanguage Analysis\nIn addition to the overall characterization of political actors, we also test models' ability to understand individual discourses. We apply stance detection and frame detection as downstream tasks, which can be formulated as sentence-level classification tasks.\nStance detection tasks aim to predict one's stance towards a given target. The tasks take a 3-way label (favor, against, and neutral) or binary label (favor, against). We test on these datasets.\n\u2022 poldeb (Somasundaran and Wiebe, 2010) provides opinion-target pairs from several debating platforms covering different domains. \u2022 election (Kawintiranon and Singh, 2021) contains tweets related to the 2020 US presidential election, expressing stances towards President Trump and Biden.\n\u2022 SEval (Mohammad et al., 2016 ) is a shared task to detect stances in public tweets.\nFrame detection tasks aim to detect which frame dimensions are employed in a piece of text. It's a multi-label classification task with a pre-defined label set. We test on these datasets.\n\u2022 twitter (Johnson et al., 2017) annotates tweets of politicians with 17 general frames. \u2022 gvfc (Liu et al., 2019a) collects news headlines about gun violence, and annotates them with 9 issue-specific frame dimensions. \u2022 immi (Mendelsohn et al., 2021) collects immigration-related tweets posted by the public, annotated with 14 general frames.\n5 Experiment Results\n\nMain Results\nThe compared general PLMs include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) . We also compare our model with available PLMs for social science texts-SsciBERT (Shen et al., 2022) , and for the political domain: POLI-TICS (Liu et al., 2022) and PoliBERTweet (Kawintiranon and Singh, 2022). We fine-tune all the PLMs in the same settings, and we select the best fine-tuned model on validation sets using macro F1.\nThe implementation details and hyperparameters can be found in Appendix C.2. Table 1 presents macro F1 scores on the downstream tasks.\nActor Profiling Our model shows superior performance on various political actor modeling tasks.\nResults of ideology detection tasks indicate that our model can not only characterize the ideology of legislators but is also good at modeling other roles like journalists in the celeb dataset and cabinet in the PEM dataset, demonstrating the transferability of using languages to represent characters. The reason for not performing best on the Reddit dataset may be the gap between the expression habits of common users and that of politicians. Nevertheless, we still outperform the majority of baselines.\nBehavior Prediction All the models show excellent performance on vote prediction and grade prediction tasks, using languages to represent political actors. It indicates that it's a feasible scheme to infer political actors' behaviors from their languages. Among all the PLMs, our model is the best. We attribute the performance gain to our proposed behavior-driven pre-training task.\nLanguage Analysis Moreover, our model also achieves competitive performance on tasks of analyzing individual text including stance detection and frame detection, indicating that the ability to understand political languages is preserved while the model is learning to profile actors, benefiting from the co-training process in Sec 3.1.\n\nAblation Study\nTo explore the effects of different components, we conduct ablation studies and results are reported in Table 2 . Removing SCL or BCL mainly hurts the performance of actor profiling tasks. Removing the text modules results in the most loss in language analysis tasks, especially the frame detection task. This demonstrates the necessity of separate modules to guarantee the ability to model political text.\n\nFurther Analysis\nFew-shot Learning We fine-tune PLMs on different numbers of samples. Figure 4 tasks. Benefiting from the pre-training stages, our model can better capture ideology and preference differences, even when using only 16 samples.\nCompare with Task-specific Models Taking the vote prediction task as an example, we compare our model with previous task-specific models, where particular meta-data and structural information is crafted for the task. Table 3 shows that UPPAM achieves competitive results, indicating that we can deduce political actors' votes from languages. Additionally, our method can be used to analyze nonvoting actors, relieving the cold-start problem.\n\nMethods of Statements Aggregation\nWe show the impact of statements aggregation methods on ideology detection in fine-tuning. We mainly compare our method with concat (Table 4 ) and mean pooling (Table 5 ). concat means to concatenate each speaker's political statements into a flat sequence and then encode it. mean pooling encodes each sentence individually and uses the averaged representation as the final representation. We further discuss the impact of the number of aggregated sentences in Appendix C.2.2. Results illustrate that our model shows robustness in several settings and our aggregator is more effective and efficient.\n\nVisualization\nGeneral Ideology We perform Principle Component Analysis (PCA) on political actor representation generated by our model for the CongS dataset. As shown in Figure 5a , our method can well separate politicians of different ideologies.\nIndividual Specific Preferences We also visualize specific representation in different policy areas for individuals. Figure 5b shows the representation in several highly-discussed policy areas, learned by different models from the tweets of Rep. Rooney. We can observe that Rep. Rooney behaves conservatively in immigration, but expresses left-wing views on environment (Pujari and Goldwasser, 2021) . While most of our baselines fail to capture this nuance, UPPAM can well compare the relative polarity in each area.\n\nRelated Work\nPolitical Actor Modeling focuses on modeling attributes and behaviors of political actors, with special attention to estimating the ideology. Because of the publicity and typicality, politicians like legislators have been the research subject for most work.\nThe most widely used approach to estimate the ideology of legislators is ideal point model (Clinton et al., 2004 ) that represents legislators and bills as points in a one-dimension latent space from the rollcall data. After that, researchers further incorporate texts of bills (Gerrish and Blei, 2011; Gu et al., 2014) to enhance the model, solving the problem of prediction on new bills. Some embedding methods are also proposed to promote learning of legislators (Kraft et al., 2016; Kornilova et al., 2018) . More recently, external information including cosponsorship (Yang et al., 2020) , donors (Davoodi et al., 2020) , relevant stakeholders (Davoodi et al., 2022) and expert knowledge (Feng et al., 2021 (Feng et al., , 2022) ) is used to better learn legislator representation. They follow a mixed structure of textual encoder and graph encoder, to explicitly combine textual and structural information. Despite outstanding performance on target tasks, these methods are limited to certain settings or data, behaving inefficient in dynamic political scenarios. Thus they are hard to be transferred to all actors. By contrast, methods relying on texts (Vafa et al., 2020) provide more possibility for generalization.\n\nDomain-specific Pre-training\nBased on continued pre-training on domain-specific data, domainspecific Pre-trained Language Models have shown superiority on many NLP tasks. Domain-specific PLMs have been investigated in many areas including medical (Zhang et al., 2021) and financial (Araci, 2019) \n\nConclusion\nIn this paper, we propose to learn political actors from languages and inject multidimensional domain knowledge into the PLMs through structureaware contrastive learning and behavior-driven con-trastive learning. Experimental results validate the effectiveness and generalization capability of our approach.\n"}
{"question": "Which is the most impressive experiment result?", "evidence": "  Main Results\nTable 1 summarizes the main results on the FEVER and SCIFACT datasets. Both ZEROFEC and ZEROFEC-DA achieve significantly better performance than the distantly-supervised and zeroshot baselines. More impressively, they surpass the performance of the fully-supervised model on most metrics, even though the fully-supervised model is trained on 58K samples in the FEVER experiment.\n ", "options": ["A. Both ZEROFEC and ZEROFEC-DA outperform the fully-supervised model on the majority of metrics.", "B. Both ZEROFEC and ZEROFEC-DA demonstrate significantly superior performance compared to the distantly-supervised and zero-shot baselines.", "C. In the FEVER experiment, the fully-supervised model is trained using a dataset comprising 58,000 samples.", "D. The enhancements observed highlight the efficacy of our approach in generating accurate and faithful factual error corrections."], "answer": "A", "content": "\nIntroduction\nThe task of correcting factual errors is in high demand and requires a significant amount of human effort. The English Wikipedia serves as a notable case in point. It is continually updated by over 120K editors, with an average of around six factual edits made per minute 2 . Using machines to correct factual errors could allow the articles to be updated with the most current information automatically. This process, due to its high speed, can help retain the integrity of the content and prevent the spread of false or misleading information.\nIn addition, the hallucination issues have been shown to be a prime concern for neural models,\n\nEvidence\nThe novel COVID-19 is highly contagious and is transmitted mostly through respiratory droplets. But, whether its transmission can be forwarded by touching a surface (i.e., a fomite) is uncertain.... COVID-19 has a case fatality rate of below 2%.\n\nFinal Correction\nCOVID-19 is not infectious.\n\nInput Claim\nFigure 1 : An example of a factual but unfaithful correction leading to misleading information. While it is technically true that the majority of people infected with COVID-19 will recover, there is no information in the evidence that supports the final correction. Additionally, when this statement is taken out of context, it could mislead people to believe that COVID-19 is not dangerous and that there is no need for precautions, which is false. A factual and faithful correction is \"COVID-19 is highly contagious.\".\nwhere they are prone to generate content factually inconsistent with the input sources due to the unfaithful training samples (Maynez et al., 2020) and the implicit \"knowledge\" it learned during pre-training (Niven and Kao, 2019) . Factual error correction can be used in both pre-processing and post-processing steps to rectify the factual inconsistencies in training data and generated texts, respectively. This can help build trust and confidence in the reliability of language models.\nPrior work typically formulates factual error correction as a sequence-to-sequence task, either in a fully supervised or in a distantly supervised manner (Shah et al., 2020; Thorne and Vlachos, 2021) . While these approaches have made great strides in generating fluent and grammatically valid corrections, they only focus on the aspect of factuality: whether the outputs are aligned with facts. Little emphasis was placed on faithfulness: the factual consistency of the outputs with the evidence. Faithfulness is critical in this task as it indicates whether a generated correction reflects the information we intend to update. If faithfulness is not ensured, this could lead to the spread of misleading content, causing serious consequences. Figure 1 shows a concrete example. In the context of automatically updating textual knowledge bases, the topic of an unfaithful output would likely deviate much from that of the expected correction. Therefore, such an edit is not desirable, even if it is factual.\nIn this work, we present the first study on the faithfulness aspect of factual error correction. To address faithfulness, we propose a zero-shot factual error correction framework (ZEROFEC), inspired by how humans verify and correct factual errors. When humans find a piece of information suspicious, they tend to first identify potentially false information units, such as noun phrases, then ask questions about each information unit, and finally look for the correct answers in trustworthy evidence (Saeed et al., 2022; Chen et al., 2022) . Following a similar procedure, ZEROFEC breaks the factual error correction task into five sub-tasks:\n(1) claim answer generation: extracting all information units, such as noun phrases and verb phrases, from the input claim; (2) question generation: generating question given each claim answer and the original claim such that each claim answer is the answer to each generated question; (3) question answering: answering each generated question using the evidence as context; (4) QA-to-claim: converting each pair of generated question and answer to a declarative statement; (5) correction scoring: evaluating corrections based on their faithfulness to the evidence, where faithfulness is approximated by the entailment score between the evidence and each candidate correction. The highest-scoring correction is selected as the final output. An overview of our framework is shown in Figure 2 . Our method ensures the corrected information units are derived from the evidence, which helps improve the faithfulness of the generated corrections. In addition, our approach is naturally interpretable since the questions and answers generated directly reflect which information units are being compared with the evidence.\nOur contributions can be summarized as follows:\n\u2022 We propose ZEROFEC, a factual error correction framework that effectively addresses faithfulness by asking questions about the input claim, seeking answers in the evidence, and scoring the outputs by faithfulness. \u2022 Our approach outperforms all prior methods, including fully-supervised approaches trained on 58K instances, in ensuring faithfulness on two factual error correction datasets, FEVER (Thorne et al., 2018) and SCIFACT (Wadden et al., 2020) . \u2022 We analyze the correlation of human judgments with automatic metrics to provide intuition for future research on evaluating the faithfulness, factuality, and intelligibility of factual error corrections.\n\nTask\nIn Thorne and Vlachos (2021)'s setting, retrieved evidence is used, which means the model may be able to correct factual errors, even though there is no supporting information in the evidence. In this case, although the prediction is considered correct, the model is hallucinating, which is not a desired property. Additionally, due to the way data was collected, they require systems to alter the input claim even if the input claim is already faithful to the evidence. We argue that no edit is needed for claims that are faithful to the evidence.\nTo address these shortcomings, our setup aims to edit a claim using a given piece of grounded evidence that supports or refutes the original claim (see Figure 2 ). Using gold-standard evidence avoids the issue where a system outputs the correct answer by chance due to hallucinations. In our setting, a system must be faithful to the evidence to correct factual errors, allowing us to evaluate system performance more fairly. Furthermore, we require the model not to edit the original claim if it is already factually consistent with the provided evidence.\nConcretely, the input to our task is a claim C and a piece of gold-standard evidence E that supports or refutes C. The goal of factual error correction is to produce a corrected claim \u0108 that fixes factual errors in C while being faithful to E. If C is already supported by E, models should output the original claim (i.e. \u0108 = C).\n\nProposed Methods\nOur framework, ZEROFEC, faithfully corrects factual errors using question-answering and entailment.\nSpecifically, we represent the input claim C as question-answer pairs \n{(Q 1 , A C 1 ), ..., (Q n , A C n )} such that each question Q i reflects the corresponding information unit A C i ,\n\nCandidate Corrections\nNight of the Living Dead is a horror film.\n\nFinal Correction\nFigure 2 : An overview of our framework. First, given an input claim, we generate the claim answers by enumerating all information units in the input claim. Second, conditioned on each extracted answer and the input claim, a question is generated. Third, each question is then fed to a question answering model to produce an evidence answer using the given evidence as context. Fourth, using a sequence-to-sequence approach, each evidence answer and the corresponding question are transformed into a statement, which serves as a candidate correction. Finally, the final correction is produced by scoring candidate corrections based on faithfulness.\nanswer A E i in the given evidence E using a learned QA model ( \u00a73.3). Each candidate correction S i is obtained by converting the corresponding pair of Q i and A E i into a declarative statement ( \u00a73.4). This guarantees that the corrected information units we replace factual errors with are derived from the evidence and thus ensures high faithfulness. The final output of ZEROFEC is the S i with the highest faithfulness score computed by an entailment model ( \u00a73.5). An overview of our framework is shown in Figure 2 .\nOne major challenge that makes our task more difficult than prior studies on faithfulness (Wang et al., 2020; Fabbri et al., 2022a ) is that we need to handle more diverse factual errors, such as negation errors and errors that can only be abstractively corrected. For instance, in the second example of in Table 2 , the QA model should output \"Yes\" as the answer, which cannot be produced by extractive QA systems. To address this issue, we adopt abstractive QG and QA models that can handle diverse question types and train our QA-to-claim model on multiple datasets to cover cases that cannot be handled by extractive systems. The following subsections illustrate the details of each component in our framework.\n\nClaim Answer Generation\nThe goal of claim answer generation is to identify information units in the input claim that may be unfaithful to E. We aim to maximize the recall in this step since the missed candidates cannot be recovered in later steps. Therefore, we extract all noun chunks and named entities using Spacy 3 and extract nouns, verbs, adjectives, adverbs, noun phrases, verb phrases using Stanza 4 . Additionally, we also extract negation terms, such as \"not\" and \"never\", from the input claim. We name the extracted information units claim answers, denoted as\nA C = {A C 1 , A C 2 , ..., A C n }.\n\nQuestion Generation\nUpon claim answers are produced, we generate questions that will be later used to look for correct information units in the evidence. Questions are generated conditioned on the claim answers using the input claim as context. We denote the question generator as G. Each claim answer\nA C i is concatenated with the input claim C to generate a question Q i = G(A C i , C).\nWe utilize MixQG (Murakhovs 'ka et al., 2022) as our question generator G to cover the wide diversity of factual errors and candidates extracted. MixQG was trained on nine question generation datasets with various answer types, including boolean, multiple-choice, extractive, and abstractive answers.\n\nQuestion Answering\nThe question answering step identifies the correct information units A E i corresponding to each question Q i in the given evidence E. Our QA module answers questions from the question generation steps with the given evidence as context. Let F denote our QA model. We feed the concatenation of a generated question and the evidence to the QA model to produce an evidence answer (Khashabi et al., 2022) is used as our question answering model. UnifiedQA-v2 is a T5-based (Raffel et al., 2020b) abstractive QA model trained on twenty QA datasets that can handle diverse question types.\nA E i = F(Q i , E). UnifiedQA-v2\n\nQA-to-Claim\nAfter questions and answers are generated, we transform each pair of question and answer into a declarative statement, which serves as a candidate correction that will be scored in the next step. Previous studies on converting QAs to claims focus on extractive answer types only (Pan et al., 2021) . To accommodate diverse types of questions and answers, we train a sequence-to-sequence model that generates a claim given a question-answer pair on three datasets: QA2D (Demszky et al., 2018) for extractive answers, BoolQ (Clark et al., 2019) for boolean answers, and SciTail (Khot et al., 2018) for covering scientific domain QAs. Note that samples in BoolQ do not contain converted declarative statements. Using Stanza's constituency parser, we apply heuristics to transform all QAs to their declarative forms in BoolQ. Our QA-to-claim model is a T5-base fine-tuned on these three datasets. Concretely, let M denote our QA-to-claim model. M takes in a generated question Q i and an evidence answer A E i as inputs and outputs a statement\nS i = M(Q i , A E i ).\n\nCorrection Scoring\nThe final correction is produced by scoring the faithfulness of each candidate correction from the previous steps w.r.t. the evidence. We use entailment score to approximate faithfulness. Here, DocNLI (Yin et al., 2021) is used to compute such document-sentence entailment relations. Doc-NLI is more generalizable than other documentsentence entailment models, such as FactCC (Kryscinski et al., 2020) , since it was trained on five datasets of various tasks and domains. Conventional NLI models trained on sentence-level NLI datasets, such as MNLI (Williams et al., 2018) , are not applicable since previous work has found that these models are ill-suited for measuring entailment beyond the sentence level (Falke et al., 2019) . In addition, to prevent the final correction from deviating too much from the original claim, we also consider ROUGE-1 scores, motivated by Wan and Bansal (2022) . The final metric used for scoring is the sum of ROUGE-1 score 5 and DocNLI entailment score. Formally,\nEQUATION\nEQUATION\nwhere C \u2032 is the final correction produced by our framework. Furthermore, to handle cases where the input claim is already faithful to the evidence, we include the input claim in the candidate correction list to be scored.\n\nDomain Adaptation\nDuring the early stage of our experiments, we found that our proposed framework did not perform well in correcting factual errors in biomedical claims. This results from the fact that our QA and entailment models were not fine-tuned on datasets in the biomedical domain. To address this issue, we adapt UNIFIEDQA-V2 and DOCNLI on two biomedical QA datasets, PUBMEDQA (Jin et al., 2019) and BIOASQ (Tsatsaronis et al., 2015) , by further fine-tuning them for a few thousand steps. We later show that this simple domain adaptation technique successfully improves our overall factual error correction performance on a biomedical dataset without decreasing performance in the Wikipedia domain (see \u00a75.1).\n4 Experimental Setup\n\nDatasets\nWe conduct experiments on two English datasets, FEVER and SCIFACT. FEVER (Thorne and Vla-chos, 2021 ) is repurposed from the corresponding fact-checking dataset (Thorne et al., 2018 ) that consists of evidence collected from Wikipedia and claims written by humans that are supported or refuted by the evidence. Similarly, SCIFACT is another fact-checking dataset in the biomedical domain (Wadden et al., 2020) . We repurpose it for the factual error correction task using the following steps. First, we form faithful claims by taking all claims supported by evidence. Then, unfaithful claims are generated by applying Knowledge Base Informed Negations (Wright et al., 2022) , a semantic altering transformation technique guided by knowledge base, to a subset of the faithful claims. Appendix A shows detailed statistics.\n\nEvaluation Metrics\nOur evaluation focuses on faithfulness. Therefore, we adopt some recently developed metrics that have been shown to correlate well with human judgments in terms of faithfulness. BARTScore (Yuan et al., 2021) computes the semantic overlap between the input claim and the evidence by calculating the logarithmic probability of generating the evidence conditioned on the claim. FactCC (Kryscinski et al., 2020) is an entailment-based metric that predicts the faithfulness probability of a claim w.r.t. the evidence. We report the average of the COR-RECT probability across all samples. In addition, we consider QAFACTEVAL (Fabbri et al., 2022a) , a recently released QA-based metric that achieves the highest performance on the SUMMAC factual consistency evaluation benchmark (Laban et al., 2022) . Furthermore, we also report performance on SARI (Xu et al., 2016) , a lexical-based metric that has been widely used in the factual error correction task (Thorne and Vlachos, 2021; Shah et al., 2020) .\n\nBaselines\nWe compare our framework with the following baseline systems. T5-FULL (Thorne and Vlachos, 2021) is a fully-supervised model based on T5-base (Raffel et al., 2020a) that generates the correction conditioned on the input claim and the given evidence. MASKCORRECT (Shah et al., 2020) and T5-DISTANT (Thorne and Vlachos, 2021) are both distantly-supervised methods that are composed of a masker and a sequence-to-sequence (seq2seq) corrector. The masker learns to mask out information units that are possibly false based on a learned fact verifier or an explanation model (Ribeiro et al., 2016) and the seq2seq corrector learns to fill in the masks with factual information. The biggest difference is in the choice of seq2seq corrector. T5-DISTANT uses T5-base, while MASKCOR-RECT utilizes a two-encoder pointer generator. For zero-shot baselines, we selected two post-hoc editing frameworks that are trained to remove hallucinations from summaries, REVISEREF (Adams et al., 2022) and COMPEDIT (Fabbri et al., 2022b) .\nREVISEREF is trained on synthetic data where hallucinating samples are created by entity swaps.\nCOMPEDIT learns to remove factual errors with sentence compression, where training data are generated with a separate perturber that inserts entities into faithful sentences.\n\nImplementation Details\nNo training is needed for ZEROFEC. As for ZEROFEC-DA, we fine-tune UNIFIEDQA-V2 and DOCNLI on the BIOASQ and PUBMEDQA datasets for a maximum of 5,000 steps using AdamW (Loshchilov and Hutter, 2019) with a learning rate of 3e-6 and a weight decay of 1e-6.\nDuring inference time, all generative components use beam search with a beam width of 4.\n\nMain Results\nTable 1 summarizes the main results on the FEVER and SCIFACT datasets. Both ZEROFEC and ZEROFEC-DA achieve significantly better performance than the distantly-supervised and zeroshot baselines. More impressively, they surpass the performance of the fully-supervised model on most metrics, even though the fully-supervised model is trained on 58K samples in the FEVER experiment.\nThe improvements demonstrate the effectiveness of our approach in producing faithful factual error correction by combining question answering and entailment predictions. In addition, even though our domain adaptation technique is simple, it successfully boosts the performance on the SCIFACT dataset while pertaining great performance on the FEVER dataset. The first example in It is true that ZEROFEC-DA requires additional training, which is different from typical zero-shot methods. However, the key point remains that our framework does not require any task-specific training data. Hence, our approach still offers the benefits of zero-shot learning by not requiring any additional training data beyond what was already available for the question answering task, a field with much richer resources compared to the factchecking field.\n\nQualitative Analysis\nTo provide intuition for our framework's ability to produce faithful factual error corrections, we manually examined 50 correct and 50 incorrect outputs made by ZEROFEC on the FEVER dataset. The interpretability of ZEROFEC allows for insightful examinations of the outputs. Among the correct samples, our framework produces faithful corrections because all intermediate outputs are accurately produced rather than \"being correct by chance\". For the incorrect outputs, we analyze the source of mistakes, as shown in Figure 3 . The vast majority of failed cases result from DocNLI's failure to score candidate corrections faithfully. In addition to the mediocre performance of DocNLI, one primary reason is that erroneous outputs from other compo-nents would not be considered mistakes so long as the correction scoring module determines the resulting candidate corrections unfaithful to the evidence. A possible solution to improve DocNLI is to further fine-tune it on synthetic data generated by perturbing samples in FEVER and SCIFACT. Examples of correct and incorrect outputs are presented in Table 7 and Table 8 \n\nHuman Evaluation\nTo further validate the effectiveness of our proposed method, we recruited three graduate students who are not authors to conduct human evaluations on 100 and 40 claims from FEVER and SCIFACT, respectively. For each claim, human judges are presented with the ground-truth correction, the goldstandard evidence, and output produced by a factual error correction system and tasked to assess the quality of the correction with respect to three dimensions. Intelligibility evaluates the fluency of the correction. An intelligible output is free of grammatical mistakes, and its meaning must be T5-DISTANT's output: Fuller House ( TV series ) isn't airing on HBO.\nTable 2 : Example outputs from different approaches. The outputs from our framework are directly interpretable, as the generated questions and answers reflect which information units in the input claim are erroneous and which information in the evidence supports the final correction. We only show the generated answers and questions directly related to the gold correction. In the first example, ZEROFEC-DA corrects a mistake made by ZEROFEC thanks to domain adaptation. In the second example, ZEROFEC successfully produces a faithful factual error correction, whereas the output of T5-DISTANT, the distantly-supervised baseline, is factual yet unfaithful to the evidence.\nunderstandable by humans without further explanation. Factuality considers whether the input claim is aligned with facts. Systems' output can be factual and semantically different from the gold correction as long as it is consistent with the world's knowledge. Faithfulness examines whether the input is factually consistent with the given evidence. Note that a faithful output must be factual since we assume all evidence is free of factual error. To evaluate the annotation quality, we compute the inter-annotator agreement. Krippendorff's Alpha (Krippendorff, 2011 ) is 68.85%, which indicates a moderate level of agreement. Details of our human evaluation can be found in Appendix B.\nThe human evaluation results are demonstrated in Table 3 . We observe that: (1) ZEROFEC and ZEROFEC-DA achieve the best overall performance in Factuality and Faithfulness on both datasets, even when compared to the fully-supervised method, suggesting that our approach is the best in ensuring faithfulness for factual error correction.\n(2) Our domain adaptation for the biomedical domain surprisingly improves faithfulness and factuality in the Wikipedia domain (i.e. FEVER). This suggests that fine-tuning the components of our framework on more datasets helps improve robustness in terms of faithfulness.\n(3) Factual output produced by ZEROFEC and ZEROFEC-DA are always faithful to the evidence, preventing the potential spread of misleading information caused by factual but unfaithful corrections. The second example in Table 2 demonstrates an instance of factual but unfaithful correction made by baseline models. Here, the output of T5-DISTANT is unfaithful since the evidence does not mention whether Fuller House airs on HBO. In fact, although Fuller House was not on HBO when it premiered, it was later accessible on HBO Max. Therefore, the correction produced by T5-DISTANT is misleading.\n\nCorrelation with Human Judgments\nRecent efforts on faithfulness metrics have been mostly focusing on the summarization task. No prior work has studied the transferability of these metrics to the factual error correction task. We seek to bridge this gap by showing the correlation between the automatic metrics used in measure, the results are summarized in Table 4 .\nWe have the following observations. (1) SARI is the most consistent and reliable metric for evaluating Factuality and Faithfulness across two datasets. Although the other three metrics developed more recently demonstrate high correlations with human judgments of faithfulness in multiple summarization datasets, their transferability to the factual error correction task is limited due to their incompatible design for this particular task. For example, QA-based metrics like QAFACTEVAL are less reliable for evaluating faithfulness in this task due to their inability to extract a sufficient number of answers from a single-sentence input claim. In contrast, summaries in summarization datasets generally consist of multiple sentences, enabling the extraction of a greater number of answers. To validate this, we analyzed the intermediate outputs of QAFACTEVAL. Our analysis confirms that it extracts an average of only 1.95 answers on the FEVER dataset, significantly lower than the more than 10 answers typically extracted for summaries. (2) Across the two datasets, the correlations between all automatic metrics and Intelligibility are low. The extremely high proportion of intelligible outputs may explain the low correlation. (3) The correlation for learning-based metrics, including QAFACTEVAL and FACTCC, drop significantly when applied to SCIFACT. This is likely caused by the lack of fine-tuning or pre-training with biomedical data.\n6 Related Work\n\nFactual Error Correction\nAn increasing number of work began to explore factual error correction in recent years, following the rise of fact-checking (Thorne et al., 2018; Wadden et al., 2020; Gupta and Srikumar, 2021; Huang et al., 2022b) and fake news detection (Shu et al., 2020; Fung et al., 2021; Wu et al., 2022; Huang et al., 2022a) . Shah et al. (2020) propose a distant supervision learning method based on a masker-corrector architecture, which assumes access to a learned fact verifier. Thorne and Vlachos (2021) created the first factual error correction dataset by repurposing the FEVER (Thorne et al., 2018) dataset, which allows for fully-supervised training of factual error correctors. They also extended Shah et al. (2020) 's method with more advanced pre-trained sequence-to-sequence models. Most recently, Schick et al. (2022) proposed PEER, a collaborative language model that demonstrates superior text editing capabilities due to its multiple text-infilling pre-training objectives, such as planning and realizing edits as well as explaining the intention behind each edit 6 .\n\nFaithfulness\nPrevious studies addressing faithfulness are mostly in the summarization field and can be roughly divided into two categories, evaluation and enhancement. Within faithfulness evaluation, one line of work developed entailment-based metrics by training document-sentence entailment models on synthetic data (Kryscinski et al., 2020; Yin et al., 2021) or human-annotated data (Ribeiro et al., 2022; Chan et al., 2023) , or applying conventional NLI models at the sentence level (Laban et al., 2022) . Another line of work evaluates faithfulness by comparing information units extracted from summaries and input sources using QA (Wang et al., 2020; Deutsch et al., 2021) . There is a recent study that integrates QA into entailment by feeding QA outputs as features to an entailment model (Fabbri et al., 2022a) . We combine QA and entailment by using entailment to score the correction candidates produced by QA. Within faithfulness enhancement, some work improves factual consistency by incorporating auxiliary losses into the training process (Nan et al., 2021; Cao and Wang, 2021; Tang et al., 2022; Huang et al., 2023) . Some other work devises factuality-aware pre-training and fine-tuning objectives to reduce hallucinations (Wan and Bansal, 2022) . The most similar to our work are studies that utilize a separate rewriting model to fix hallucinations in summaries. For example, Cao et al. (2020) present a post-hoc corrector trained on synthetic data, where negative samples are created via perturbations. Adams et al. (2022) fix factually inconsistent information in the reference summaries to prevent the summarization from learning hallucinating examples. Fabbri et al. (2022b) propose a compression-based post-editor to correct extrinsic errors in the generated summaries. By contrast, we leverage the power of QA and entailment together to address faithfulness.\n\nConclusions and Future Work\nWe have presented ZEROFEC, a zero-shot framework that asks questions about an input claim and seeks answers from the given evidence to correct factual errors faithfully. The experimental results demonstrate the superiority of our approach over prior methods, including fully-supervised methods, as indicated by both automatic metrics and human evaluations. More importantly, the decomposability of ZEROFEC naturally offers interpretability, as the questions and answers generated directly reflect which information units in the input claim are incorrect and why. Furthermore, we reveal the most suitable metric for assessing faithfulness of factual error correction by analyzing the correlation between the reported automatic metrics and human judgments. For future work, we plan to extend our framework to faithfully correct misinformation in social media posts and news articles to inhibit the dissemination of false information. In addition, it may be meaningful to explore extending zero-shot factual error correction to multimedia task settings, such as identifying inconsistencies between chart and text (Zhou et al., 2023) .\n"}
{"question": "What is the main focus of CMIN in predicting stock movements, and how does it achieve this focus?", "evidence": "  In this paper, we propose the Causality-guided Multi-memory Interaction Network (CMIN) We employ CMIN to predict BAC's next movement direction. The most focused stock by CMIN is Berkshire Hathaway Inc. (BRK-A).  In this paper, we proposed CMIN, a causality-guided multi-memory interaction network that simultaneously models financial documents, causality-enhanced stock correlations.  ", "options": ["A. CMIN primarily relies on stock correlations.", "B. CMIN solely relies on financial documents.", "C. CMIN combines financial texts and stock correlations to make predictions.", "D. CMIN uses historical stock prices for predictions."], "answer": "C", "content": "\nIntroduction\nFinancial services, known for their competitiveness, have always been at the forefront of adopting data science techniques to drive investment decisions. Quantitative trading, a specific field within it, has drawn immense interest from both academia and industry over the last few decades. With the rapid advancements in deep learning recently, computer scientists and quantitative researchers have joined forces to apply AI techniques to tackle the challenges within this domain.\nAmong various tasks, one of the most prominent is stock price movement prediction (Bhardwaj, 2021) . The reason for its popularity is selfevident: once a model is able to predict future movement with considerable accuracy, numerous trading strategies can be easily built around it.\nRecent studies have shown that deep neural networks are ideal candidates for such prediction models (Yoo et al., 2021; Gunduz, 2021) . Supporters of the efficient-market hypothesis (EMH), which posits that asset prices reflect all available information, tackle the task with price information alone (Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . However, an alternative perspective suggests that additional insights can be gained from analyzing news articles and social media posts, which may hold valuable clues about the future (Hu et al., 2018; Xu and Cohen, 2018; Wang et al., 2019b; Tang et al., 2020) .\nAnother intriguing approach analyzes the relationships between different stocks. Clearly positive and negative correlations, or even non-correlations can be immensely useful in constructing a diversified stock portfolio (Borodin et al., 2003) . Several studies even empirically demonstrate that exploiting correlations can improve the accuracy of stock price movement prediction (Long et al., 2020; Yoo et al., 2021) . However, their correlations are often realized by acquiring industry sector and calculating correlation matrices or attention scores, which are bidirectional and symmetrical, leading to excessive attention on spurious correlations. Due to the lag problem widely existed between two time series, we are more concerned about the dominance of information flow between stocks, specifically, the direction of causality.\nAdditionally, we have observed that the situation can significantly change when incorporating text information. Let's consider two highly correlated companies (A and B) and there is promising news specifically about company A. In such a scenario, it's fairly easy to infer that the current news might still have a substantial impact on company B, despite there being no direct connection between the two companies on paper. However, it's impossible to reach this conclusion by just examining the news about company A or the correlation between A and B alone, which highlights the limitations of relying solely on individual pieces of textual information or traditional correlations between stocks.\nInspired by observations above, we propose the Causality-guided Multi-memory Interaction Network (CMIN), a novel end-to-end deep neural network which captures both financial news as well as the causality-enhanced correlations between stocks for better stock price movement prediction.\nTo achieve this goal, CMIN incorporates two key components: the Text Memory Network and the Stock Correlation Memory Network. Both networks utilize a recurrent neural network with nonlinear combination of memory attentions to generate a global memory abstraction. And we introduce a global causality matrix according to the transfer entropy between stock price time series to guide the abstraction process, forming a Causal Attention mechanism to capture the asymmetric correlations. By considering causality, CMIN goes beyond traditional symmetric correlations and captures the true inter-dependencies between stocks. Furthermore, we employ an attention-based fusion mechanism between the two networks, introducing multi-directional interactions through which CMIN learns not only the self-influence within each network but also the interactive influence between them. It captures the interrelationship between textual information and correlations, enhancing the overall predictive power of CMIN.\nWe further demonstrate the effectiveness of CMIN with experiments conducted on 3 real-world datasets collected from both the U.S. and Chinese markets, where CMIN achieves state-of-the-art prediction accuracy, surpassing existing models in terms of performance.\nTo summarize, our main contributions are:\n\u2022 Proposal of a causality-guided multi-memory interaction network for stock movement prediction which is to our best knowledge the first attempt to simultaneously consider causalityenhanced correlations and textual information to achieve higher prediction accuracy;\n\u2022 Introduction of the attention-based multidirectional interactions, so that CMIN captures not only the self-influence of temporal movements and textual information but also the interactions between these two types of information flows;\n\u2022 Collection and release of two new datasets: one for the U.S. market and another for the Chinese market.\nBoth datasets include comprehensive financial texts and stock price time series data, which are publicly available at https://github.com/ BigRoddy/CMIN-Dataset, facilitating further research and benchmarking in the field.\n\nStock Movement Prediction\nIn traditional trading practices, two main frameworks are commonly used to make predictions on future stock prices (Ferreira et al., 2021) . The first is fundamental analysis, which aims to assess the intrinsic value of a stock by considering various factors related to it as a whole, such as financial statements, industry trends and economic conditions. The other is technical analysis, which operates under the assumption that the market is efficient (i.e., the Efficient Market Hypothesis holds true) and focuses on analyzing only historical and current price patterns in order to predict future movements.\nAlthough both frameworks have been widely adopted by top hedge funds and investment firms, technical analysis has gained more popularity among AI practitioners, many of whom focus on employing long short-term memory networks and other innovative architectures to model stock price history alongside technical analysis indicators (Nelson et al., 2017; Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . This is primarily because processing a single stream of price data is relatively simpler than analyzing and synthesizing a range of diverse data sources with varying frequencies and characteristics.\n\nPredicting with the Help of Text Data\nThe recent advancement of natural language processing (NLP) techniques has opened up new possibilities for analyzing large volumes of text data in the context of stock movement prediction. Many researchers have recognized the potential value of incorporating news articles, analysis, commentaries and even social media posts (Xu and Cohen, 2018) , which are believed to provide valuable insights about the future. Some studies focus solely on textual information. For example, (Hu et al., 2018) leverages attention mechanism at multiple levels within a deep structure to identify the most important news articles and predict price trends. Others adopt a two-step approach. First, they extract features (e.g. investor sentiment) from financial texts. Then they fuse these features with price information to make predictions such as (Li et al., 2017) and (Jin et al., 2020) . This integration of text analysis with quantitative techniques holds promise for enhancing the accuracy and effectiveness of stock movement prediction models.\n\nExploiting the Relations between Stocks\nAnother important trading framework takes advantage of the correlations between different stocks. Portfolio selection, particularly pairs trading, is a well-known and successful trading strategy that exploits the correlated nature of stocks, whether positive or negative. In fact, as early as (Borodin et al., 2003) pointed out that stock correlations based portfolio selection could beat any strategy that relied on predicting trends or specific targets.\nThe incorporation of correlations in stock movement prediction has gained attention in recent years, drawing inspiration from several existing works. For example, (Yoo et al., 2021) utilizes transformer to learn dynamic correlations between stocks in an end-to-end manner. (Long et al., 2020) employs knowledge graphs and graph embedding techniques to model the relationships between stocks. These studies have achieved admirable results, potentially due to effective feature engineering however, because the direct benefit of stock correlations in predicting future prices lacks fundamental logic.\nIn this paper, we propose constructing a single model to handle both textual data and stock correlations simultaneously, aiming to shed light on the success of correlation-based approaches with the help of financial texts. We also introduce a novel causal attention mechanism to interpret the underlying logic behind stock correlations, leveraging transfer entropy to provide insights. We further model the multi-directional interactions between texts and correlations so that we could uncover not only relevant texts for prediction through correlations, but also the hidden stock correlations through texts. By integrating text data and stock correla-tions within a unified model, we aim to provide a comprehensive understanding of the relationship between the two and discover valuable insights for stock movement prediction.\n\nProblem Formulation\nThis paper is dedicated to predict the price movement of a target stock. To this end, we leverage both the correlations between stocks and textual information to make prediction.\nConsider a target stock with numerical features denoted as P target \u2208 R k\u00d7d , where k represents the number of time steps in the monitoring window and d represents the dimension of price features, such as the highest and the closing prices. The prices of n other relevant stocks are denoted as:\nP = {P 1 , P 2 , \u2022 \u2022 \u2022 , P n } \u2208 R n\u00d7k\u00d7d .\nBesides, we have financial documents associated with the target stock, which are represented as\nM = {M 1 , M 2 , \u2022 \u2022 \u2022 , M k } \u2208 R k\u00d7l\u00d7w ,\nwhere l denotes the number of documents in a time step and w is the maximum number of words in a document. In cases where a specific stock has fewer than l documents at a given time step, zero padding values are added to align the lengths. Similarly, if a document contains fewer than w words, zero padding is applied to ensure uniform length across all documents (Ang and Lim, 2022) .\nWe formulate the task as a binary classification problem whose goal is to predict the movement of the target stock at the next time step, denoted as \u0177target . Here, \u0177target = 1 indicates a rise in the price while \u0177target = 0 indicates a fall. (1) The feature embedding module includes two encoders, one for embedding the textual information and another for embedding the price time series. Additionally, a global causality matrix is introduced to capture the asymmetric correlations using transfer entropy, which then guides the calculation of attention weights in the multi-memory networks.\n(2) The multi-memory networks consist of the Text Memory Network and Stock Correlation Memory Network, which are designed to select and re- tain the most relevant and influential information (textual and correlational) for the target stock.\n(3) The multi-directional interaction module facilitates the interaction between the textual and correlational information. This interaction allows the two types of information to reinforce each other and leverage the advantages of different information flows for better prediction performance, enhancing the predictive capabilities of the CMIN.\n\nFeature Embedding\nSelf-attention mechanisms have proven to be effective in capturing long-term dependencies and modeling complex sequential patterns, particularly in the Transformer architecture (Vaswani et al., 2017) . Given the significance of historical information in financial documents and stock prices for stock price movement prediction, we employ attention mechanisms to summarize this information.\n\nText Encoder\nThe Text Encoder focuses on processing the financial documents M to extract meaningful representations for stock movement prediction. We firstly use a popular word representation tool Glove (Li et al., 2018) to generate the word embedding tensor M word \u2208 R k\u00d7l\u00d7w\u00d7dw , where d w is the size of word embeddings. Each word in the financial documents is represented as a d w -dimensional vector.\nThen the word embeddings are passed through a text embedding layer. Here we adopt the bidirectional Gated Recurrent Unit (Bi-GRU) (Li et al., 2022) to capture both preceding and succeeding contexts within each document. The average of the last hidden vectors is taken as the text embeddings M text \u2208 R k\u00d7l\u00d7dm , or equivalently M text \u2208 R s\u00d7dm , where s is the total number of documents in the monitoring window.\nAfter that, the text attention mechanism is applied to summarize all historical documents across time steps. The text embedding of the last time step M text,\u22121 \u2208 R l\u00d7dm , serves as the query matrix, while the entire text embeddings M text \u2208 R s\u00d7dm acts as both the key and value matrices. Soft scaled dot-product attention is used to compute the attention weights, which are then applied to the text embedding to obtain a representation E text \u2208 R l\u00d7dm enhanced by the history state attention:\nE text = softmax( M text,\u22121 M T text \u221a d m )M text . (1)\nThe resulting E text is the textual embedding that contains highly concentrated information from the stock's related texts. This embedding serves as a summary of the historical text data and is used for further processing in the multi-memory networks and multi-directional interaction module of CMIN.\n\nPrice Encoder\nThe Price Encoder is introduced to utilize multivariate features from historical prices and capture their temporal interrelationships. Firstly we employ a feature mapping layer to project them into a latent space of dimension d p , aiming to improve the learning capacity (Yoo et al., 2021) . For target stock price P target \u2208 R k\u00d7d , the historical price embeddings Ptarget \u2208 R k\u00d7dp can be formulated as:\nEQUATION\nwhere\nW t \u2208 R d\u00d7dp , b t \u2208 R dp are parameters.\nMoreover, recognizing that historical patterns can repeat themselves sometimes, we incorporate a multi-head price attention layer to capture each stock's distinctive changing patterns. The price embedding of the target stock at the last time step is donated as P\u22121 target \u2208 R dp . Then we employ the multi-head attention mechanism with the query P\u22121 target and the key/value Ptarget as follows:\nv target = MultiheadAtt( Ptarget , P\u22121 target ) (3)\nv target is a key vector that serves as the initial hidden state for the two memory networks, playing a crucial role in the final prediction. Similarly, we process the remaining stocks and obtain the correlational embedding E corr \u2208 R n\u00d7dp . Notably, the shared parameters across all stocks ensure the stability and generality of the extracted features (Wang et al., 2019a) .\n\nCausality Matrix\nWhen it comes to detecting causal relationships and conducting predictive analysis, transfer entropy, a non-linear generalization of Granger causality (Seth, 2007) , serves as a conceptually neat and mathematically rigorous method. It has been considered as an important tool for causality analysis and successfully applied in diverse domains including financial markets (Sandoval Junior et al., 2015) .\nTransfer entropy is derived from Shannon Entropy: H = \u2212 N i=1 p i log p i . In this context, considering the time series of a stock, we can partition the possible values into different bins and calculate the probabilities at each time step. Transfer entropy from series X to another series Y can be defined as the average amount of information contained in the source X but not contained in Y's past:\nEQUATION\nBased on this principle, for each monitoring window, we calculate the transfer entropy between all stocks using their historical closing prices and generate a transfer entropy matrix, referred to as the Causality Matrix C \u2208 R n\u00d7n , which illustrates the asymmetric flow of information from one stock to another. Specifically, C[i, j] represents the transfer entropy from stock i to stock j, and C[i, j] > C [j, i] indicates that stock i provides more predictive information about the movement of stock j than j to i. This Causality Matrix will next serve as a guide for the memory networks, enabling the identification of causal dependencies between multivariate stocks.\n\nMulti-memory Networks\nWe introduce a Text Memory Network and a Stock Correlation Memory Network (Sukhbaatar et al., 2015) to manage the textual and correlational information separately. They each maintain a continuous representation and update it iteratively using multiple computational steps (hops), ultimately producing a global memory abstraction.\nAs shown in Figure 1 , each layer of the memory network comprises an attention unit and a GRU unit, which receive textual or correlational embeddings as inputs and are supervised by the continuous representation generated in the previous layer. To initialize the continuous representations of each network, we use the target stock vector v target (generated from Eq.3):\nv (0) text = v (0) corr = v target .\n(5)\n\nText Memory Network\nIn each layer h \u2208 [1, H] of the Text Memory Network, we input the textual embeddings E text (Eq.1) and the continuous representation from the previous layer v\n(h\u22121)\ntext . We utilize an attention unit (Eq.3) to identify important information within the textual embeddings. Subsequently, a non-linear GRU cell unit (Xu et al., 2019) acts as an information aggregator, determining the amount of text information to retain:\nv Att(h) text = MultiheadAtt(E text , v (h\u22121) text ), (6)\nwhere v (h\u22121) text is the query matrix and E text represents the raw form of the key and value matrices.\nThen the GRU cell unit updates the current hidden state into the next hidden state and outputs it to the next layer as the new continuous representation:\nv (h) text = GRU (v Att(h) text , v (h\u22121) text ).\n(7)\n\nStock Correlation Memory Network\nThe Stock Correlation Memory Network is employed to dynamically identify stock relationships and update the continuous representation of stock correlations in an intuitive and asymmetric manner. However, the use of unsupervised attention weights in previous models can be problematic as they may be inevitably misled by the dataset bias, resulting in excessive attention on spurious stock correlations. To address this, we introduce extra knowledge in the form of Transfer Entropybased causality to guide the attention weights and mitigate potential confounding effects.\nFor each target stock, we extract a causal vector v causal = C[:, target] from the pre-calculated causality matrix, which quantifies the degree of information flow from other stocks to it. Then we modify the traditional attention mechanism into Causal Attention by incorporating causal guidance:\nS = softmax( QK T \u221a d ), S = f (S, v causal ). (8)\nHere, f is a function that aggregates the attention weight S and the causal vector v causal to produce a causality-guided attention weight S. We use the average aggregation method for simplicity (i.e., f (S, v causal ) = (S + v causal )/2). To better balance them, one can introduce a hyperparameter \u03bb \u2208 [0, 1]. Then f() updates to f (S, v causal ) = \u03bbS + (1 \u2212 \u03bb)v causal . We believe that different degrees of causal attention can impact the model's performance, and leave it for future exploration. The continuous representation is gradually updated through the Causal Attention, indicating the influence of causal relationships on movement prediction and the self-influence on the flow of correlation information:\nEQUATION\nIt is important to note that although we design multiple layers within each memory network to learn deep representations, different layers of the same memory network share the same unit. This enables the network to focus on crucial information that affects the movement of the target stock, thereby enhancing the continuous representation.\n\nMulti-directional Interactions\nIn reality, textual information and correlations have an impact on each other when it comes to stock price movement prediction. For instance, news about a technological breakthrough in the new energy sector may uplift the prices of most stocks in that industry, thereby affecting the correlations among those stocks.\nTo simulate this phenomenon and enhance the synergy between textual and correlational information, we introduce a multi-directional interaction module. This module allows textual and correlational information to reinforce each other and amplify the advantages of different information flows for better prediction performance.\nTake the Text Memory Network as an example, in each layer we firstly calculate the self-influence by using v (h\u22121) text as the query:\nv Att(h) text\u2212>text = MultiheadAtt(E text , v (h\u22121) text ) (11)\nNext we consider the interactive influences from correlations to texts using v (h\u22121) corr as the query:\nv Att(h) corr\u2212>text = MultiheadAtt(E text , v (h\u22121) corr ) (12)\nFinally, we produce a new attentional continuous representation by averaging these two influences:\nEQUATION\nwhich means that we replace Eqs. 6 with Eqs. 11-13 to obtain the new attention-aggregated vector.\nThe workings of Stock Correlation Memory Network are quite similar.\nConsequently, the fusion of different information flows is promoted due to the multi-directional interaction mechanism in which CMIN learns not only the influences from text/correlation to movement prediction within each information flow but also the interactive influences between different information flows, representing the interrelationship between text and correlations.\n\nLearning Objective\nWith the continuous representations v (H) text and v (H) corr from the last layer of each memory network, along with the target stock representation v target , we concatenate them and apply a softmax function to generate the final prediction vector \u0177:\n\u0177 = softmax(W y [v (H) text , v target , v (H) corr ] + b y ). (14)\nThe objective is to minimize the cross entropy loss:\nL(y, \u0177) = \u2212 n i=1 (yi log (\u0177i) + (1\u2212yi) log (1\u2212 \u0177i)) (15)\nwhere n is the size of the training set.\n\nExperiments\nIn this section, we empirically evaluate our CMIN model with three real-world datasets collected from the U.S. and Chinese stock markets.\n\nDatasets\nIn our experiments we have used three datasets, namely ACL18, CMIN-US and CMIN-CN, spanning different time periods to evaluate our proposed model CMIN against other baselines.\nACL18 (Xu and Cohen, 2018 ) is a classic dataset with tweets from Twitter as financial texts in the task of text-enhanced stock movement prediction. As there are few existing high-quality datasets containing both texts and price, we are also making available two new benchmark datasets along with this paper from 2018-01-01 to 2021-12-31 in the U.S. and Chinese market named CMIN-US and CMIN-CN. These two datasets are available at https://github.com/BigRoddy/ CMIN-Dataset to facilitate further research and enable reproducibility. More details and statistics of those three datasets are in Appendix A.\n\nBaselines\nWe compare CMIN against the following four baselines, all of which are high-performing stock movement prediction models proposed by recent studies:\n\u2022ALSTM (Qin et al., 2017 ) is a dual-stage attention-based recurrent neural network, which selects relevant time series across all time steps.\n\u2022Adv-LSTM (Feng et al., 2019) uses adversarial training to improve the generalization of ALSTM.\n\u2022Stocknet (Xu and Cohen, 2018) introduces recurrent continuous latent variables and uses variational inference to address the posterior inference.\n\u2022DTML (Yoo et al., 2021) is a newly published attention-based model that exploits the correlations between stocks to improve the prediction accuracy.\n\nEvaluation metrics\nAs we have formulated stock price movement prediction as a classification problem, we choose two classic metrics: Accuracy (Acc.) and Matthews Correlation Coefficient (MCC), similar to the previous work (Xu and Cohen, 2018; Yoo et al., 2021) . \nEQUATION\n\nImplementation details\nWe set our model for daily price prediction, with a history market window size k = 5 and the number of price features d p = d = 3, namely the highest, the lowest and the closing prices. We limit the maximum number of financial texts in one single day to be l = 20 , and the maximum length of a text document w = 30. Within the Text Encoder, we set the size of word embedding vector d w = 50 and the hidden state of Bi-GRU network d m = 50.\nWe implement the CMIN with Pytorch on a NVIDIA Tesla V100 and train it with an Adam optimizer (Kingma and Ba, 2015) . All parameters of our model are initialized with Xavier Initialization (Glorot and Bengio, 2010) . We search the hyperparameters of CMIN as follows: number of layers of each memory network H in {1, 2, 3, 4, 5}, dropout rate in {0.1, 0.2, 0.3}, number of epochs in {10, 20, 50}, and size of the price hidden state d p in {3, 10, 50}. For baselines, we use their default parameters and fine-tune them to fit our data.\n\nPerformance Analysis\nThe results are summarized in Table 1 .\nAmong all models, ALSTM and Adv-LSTM performed poorly with little improvement over random prediction. This could be attributed to the fact that these models rely solely on stock prices as the basis for decision-making. The Stocknet and DTML incorporate additional information beyond stock prices, demonstrated significant improvements over ALSTM and Adv-LSTM, which highlights the importance of utilizing financial texts and stock correlations for this challenging task. CMIN outperformed all baselines and achieved state-of-the-art performance on both two metrics across all datasets, showing its excellent capabilities to leverage both financial texts and stock correlations, as well as capture their interrelationship. \n\nAblation Studies\nTo evaluate the contribution of CMIN's different components, we compare against several variants:\n\u2022CMIN-TE: CMIN without the Text (TE), which makes decisions just based on stock prices.\n\u2022CMIN-PR: CMIN without the Price (PR), which makes decisions just based on related texts.\n\u2022CMIN-CM: CMIN without the guide of causality matrix (CM).\n\u2022CMIN-MI: CMIN without multi-directional interactions (MI) between memory networks.\nThe results are summarized in Table 2 . CMIN-TE only achieves a level of prediction accuracy on par with ALSTM and Adv-LSTM, and is worst among all the variants, again indicating the importance of text data. Similar to the performance of Stocknet, CMIN-PR has a relatively high Acc. but a low MCC, suggesting texts are particularly helpful to predict on one side of the binary classification. By modeling both text data and stock relationships, CMIN-CM reaches a good result. Finally, better performance achieved when causality matrix and multi-directional interactions are introduced into the network. Overall, the ablation studies show that every component makes an important contribution to CMIN, and as a result the full model with all components achieves the best performance.\n\nAnalysis of Memory Network Depth\nAs introduced before, we propose two memory networks to retain vital features of texts and correlations with multiple computational layers. And we want to understand what would be the ideal number of depths to achieve the best prediction results.\nWe change the number of layers H of each memory network to find out how the performance fluctuates with it. The results are summarized in Figure 2 . When we only have one memory layer, there is no multi-directional information flows between the two memory networks and as a result they only try to identify the vital information in the embeddings related to or having an impact on the movement of the target stock under the supervision of v target . As the number of memory layers increases, the interactions between two memory networks also intensifies. It is intuitive that the performance of CMIN reaches its peak when it has three memory layers. With further increase the number of memory layers, CMIN is prone to overfit.\n\nCase Study\nHere we present an example to illustrate how CMIN considers both financial texts and stock correlations to avoid random noises in time series.\nWe visualized the causality matrix of ACL18 using a heat map as shown in Figure 3 . Stocks are sequenced by their industry sector. The black box on the left shows weak causality, representing weak information flow from Utilities to Materials. On the other hand, the yellow box on the right indicates the relative strong information flow from Materials to Finance and within the Finance industry.\nThe target stock is Bank Of America (BAC) with a monitor window spanning from 13/11/2015 to 19/11/2015. We employ CMIN to predict BAC's next movement direction on the day of 20/11/2015 and then output the attention scores of texts and causality-guided correlation. The most focused stock by CMIN is Berkshire Hathaway Inc. (BRK-A). It's interesting to note that both are in the same industry sector: Finance, and they do appear to follow a very similar movement pattern in the trading days leading to 20/11/2015, which demonstrates the ability of CMIN to find the dynamic stock correlations with the guidance of Causality Matrix.\nThe financial text of BAC that obtains the highest attention score is \"Beer, Credit Card Debt And Other Positives For Bank Of America\", the title of an news article 1 which reports the rapidlyimproving banking landscape in the U.S.. This text is clearly highly relevant to BAC's subsequent stock performance, which demonstrates that CMIN is able to identify highly relevant texts having a impact on the target stock movement.\nFurthermore, it also illustrates the underlying interrelationship between financial texts and stock correlations. Except expressing an optimistic sentiment towards BAC, the news also shows a rapidly improving state of affairs for the wider financial industry. Therefore, through the Multi-directional Interactions mechanism, the text strengthens the model's attention stocks in the same sector. These two aspects mutually reinforce and complement each other to help the model make the best judgment that BAC's stock price will rise on the next day.\n\nConclusions\nIn this paper, we proposed CMIN, a causalityguided multi-memory interaction network that simultaneously models financial documents, causality-enhanced stock correlations and the interactions between the two, and recurrently learns a global memory representation for movement prediction. This multi-modality network was designed to enable the concurrent discovery of texts and stock correlations relevant to future price change and we demonstrated, through experiments on three datasets across two distinct markets, that each component of the proposed architecture made significant contributions to the model, leading CMIN to achieve state-of-the-art accuracy.\n"}
{"question": "According to the paper, what type of knowledge is stored in the feed-forward networks (FFNs) of pre-trained language models?", "evidence": "  Previous studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) suggest that factual associations are stored in the FFNs of some Transformer layers. To help models learn domain-specific knowledge, we propose a lightweight domain-adapter that works parallel to the FFNs... ", "options": ["A. Semantic knowledge", "B. Structured knowledge", "C. Domain-specific knowledge", "D. Common-sense knowledge "], "answer": "C", "content": "\nIntroduction\nPre-trained language models (PLMs) have achieved a multitude of successful applications in natural language understanding (Devlin et al., 2018; Liu et al., 2019; He et al., 2021b) and generation (Lewis et al., 2019; Zhang et al., 2020; Yang et al., 2020; Brown et al., 2020) . The predominant methodology for domain adaptation is fine-tuning on labeled domain-specific data or continued pre-training (Gururangan et al., 2020) on unlabeled domain-specific data. Although effective, both fine-tuning and continued pre-training methods require tuning all the parameters of a PLM, raising high costs beyond many institutions' reach. To mitigate this, multiple parameter-efficient fine-tuning (PEFT) methods are proposed, including prompt-based tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . However, they are more concerned about task adaptation and it is still unclear how to regularly, and inexpensively inject domain knowledge into PLMs for different domain-specific tasks. Moreover, directly tuning PLMs on a domain-specific corpus with PEFT methods will lead to the catastrophic forgetting problem (Yogatama et al., 2019; Gururangan et al., 2020) . These limitations highlight an important research question: how to adapt PLMs with the new domain knowledge while keeping the old-domain knowledge unmodified?\nInspired by the recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022 ) that found knowledge is stored in feed-forward networks (FFNs), we decouple the FFNs into two parts: the original pre-trained FFNs to maintain the olddomain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Specifically, we propose Mixture-of-Domain-Adapters (MixDA), a mixture of several domain adapters to inject domain-specific knowledge without affecting the old-domain knowledge. Our model has two stages: piq domain-specific tuning multiple knowledge adapters on unlabeled data and then piiq task-specific tuning adapters on labeled data. In the first stage, we train several domain adapters on both domain-specific corpus and pre-training corpus simultaneously while keeping the original feed-forward networks unchanged. In the second stage, we train a mixture-of-adapters gate to dynamically select the desired knowledge adapter and a task-specific adapter for task adaptation.\nWe conduct experiments on a broad range of tasks, including 4 out-of-domain datasets, 9 in-domain datasets, and 2 knowledge-intensive datasets. Our experimental results demonstrate the effectiveness of MixDA on 15 datasets, spanning biomedical, computer science publications, news, and reviews. Further analysis displays three key properties of our proposed approach: piq Reliability: it shows superior performance on both in-domain and out-of-domain tasks. piiq Scalability: it scales well to the increasing number of domains. piiiq Efficiency: it adds only a small number of parameters per domain. We claim that these properties are helpful for language models as a service, where a frozen PLM is served, and multiple adapters are inserted to support different customized services.\n\nRelated Work\nIn this section, we will review four research lines related to injecting domain knowledge into pretrained language models: knowledge injection, domain adaptation, parameter-efficient fine-tuning, and mixture-of-adapters.\n\nKnowledge Injection\nKnowledge can be injected into PLMs by pretraining or fine-tuning, each corresponding to a separate research direction. During pre-training, the knowledge carried by knowledge graphs (Zhang et al., 2019; He et al., 2020) , entities (Sun et al., 2019; Xiong et al., 2020) , n-grams (Diao et al., 2020) , knowledge embedding (Wang et al., 2021b) , synonym and hyponym-hypernym relations in WordNet (Lauscher et al., 2019) , word-supersense knowledge (Levine et al., 2020) , and knowledge bases (Peters et al., 2019) can be injected into PLMs by feeding knowledge inputs and designing new objectives. However, pre-training-based methods are costly, making the application to huge PLMs (e.g., models with 175 Billion parameters) impossible. Fine-tuning-based methods only require an additional fine-tuning process. Some studies inject extra information into the input sentences, like knowledge triples from knowledge graphs (Liu et al., 2020) and knowledge context (Faldu et al., 2021) , while other studies explored specific model and training designs, like knowledge adapter networks (Wang et al., 2021a) , graph convolutional networks and LSTMs (Lin et al., 2019) , and metalearning (Sinitsin et al., 2020) . Zhu et al. (2020) formulated knowledge injection as a constrained optimization problem by adding a constraint on the loss on the unmodified facts. Recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) reveal that knowledge is stored in the feed-forward networks in PLMs. Inspired by these studies, we propose a new efficient tuning method to inject domain knowledge into feed-forward networks with minimal costs.\n\nDomain Adaptation\nPrevious studies have observed that language models suffer from a significant performance drop during the domain shift (Beltagy et al., 2019; Alsentzer et al., 2019; Huang et al., 2019; Lee et al., 2020; Ke et al., 2022b) . Effective strategies that can bridge the domain gap are introduced. Pre-training language models from scratch is effective but costly, like SciBERT (Beltagy et al., 2019) , BioBERT (Lee et al., 2020) , and ClinicalBERT (Alsentzer et al., 2019) . Recent studies explored continued pretraining (Gururangan et al., 2020) and adapter networks (Diao et al., 2021) to save time by training on unlabeled downstream task data. In this paper, we introduce plug-in domain adaptors for domain adaptation, which are effective and mitigate catastrophic forgetting issues because of the explicit learning strategy and efficient model architecture.\n\nParameter-Efficient Fine-tuning\nAnother relevant research direction is parameterefficient fine-tuning (PEFT), which only fine-tunes a small number of parameters. Existing works solve this problem from two perspectives: promptbased tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . Several works in adapter-based tuning are closely related to ours. AdapterFusion (Pfeiffer et al., 2021) aims to combine multiple task adapters but does not offer specific architecture or training strategies to learn external knowledge. DEMix (Gururangan et al., 2022) and MixDA both train adapters that specialize in domains and use mechanisms to route different adapters, but differ in routing methods, base models, and training strategies. K-Adapter (Wang et al., 2021a ) is re-stricted by its training on T-REx triples and lacks the flexibility to train on unstructured knowledge. Similar to MixDA, CPT (Ke et al., 2022a) integrates domain knowledge into LMs, but it employs a different approach. While MixDA uses domain adapters to substitute FFN layers and task adapters to perform end tasks, CPT adds CL-Plugins that learn domain knowledge. Recent work by He et al. (2021a) presents a unified framework that establishes connections across different PEFT methods. Our work can leverage any PEFT method and complement them.\n\nMixture-of-Experts\nMixture-of-Experts (MoE) (Shazeer et al., 2017) is introduced with several expert networks, gating networks, and load-balancing techniques. The following studies improve MoE on initialization and training schemes (Fedus et al., 2022) , routing mechanisms (Zuo et al., 2021; Yang et al., 2021) , and load-balancing issues (Lewis et al., 2021; Roller et al., 2021) . AdaMix (Wang et al., 2022) proposed a mixture of adapters to improve the downstream task performance. Instead of mixing different designs of adapters, our domain adapter is a feedforward network specifically designed for domain knowledge.\n\nApproach\nGiven a pre-trained language model M, the input is a sentence X \" t 1 t 2 \u00a8\u00a8\u00a8t i \u00a8\u00a8\u00a8t T (t i indicates the i-th token) and the output is the representation of each token. The overall architecture of our model is shown in Figure 1 . The training process is divided into two-stage. In Stage 1 (Figure 1 (a)), we inject new feed-forward networks (FFNs) (namely domain-adapter) paralleled to the original pre-trained FFNs in some Transformer layers, acting as a key-value memory. The newly injected domain-adapter is trained on both domain-specific unlabeled data and original pre-training unlabeled data to store new factual associations while keeping old-domain ones. All modules are frozen except domain-adapter in this stage. In Stage 2 (Figure 1 (b)), we train a mixture-of-adapters (MoA) gate and a task-adapter on downstream tasks with labeled data, and only these two new modules are updated. The MoA gate receives outputs from the old-domain FFNs and domain-adapter, then outputs a weighted sum of them. An additional taskadapter is inserted in each Transformer block to facilitate downstream tasks. Figure 1 (c) shows the structures of the domain-adapter and the MoA gate.\nIn this section, we first introduce domainadapter, which learns and stores domain-specific knowledge, and then describe task-adapters that perform the downstream task. Finally, we discuss how the MoA gate integrates the outputs from the FFN and the domain-adapter.\n\nDomain-Adapter\nPrevious studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) suggest that factual associations are stored in the FFNs of some Transformer layers. To help models learn domain-specific knowledge, we propose a lightweight domain-adapter that works parallel to the FFNs, and a training method to learn domain-specific knowledge alongside keeping old-domain ones. Domain-adapter has a simple bottleneck architecture consisting of a down projection layer, a nonlinearity (such as ReLU (Agarap, 2018)), and an up projection layer. This helps keep the parameter size low (Houlsby et al., 2019) with competitive performance.\nIn Stage 1, the domain-adapter is trained with the domain-specific and old-domain datasets in one batch. Note that all other parameters are frozen except the domain-adapter at this stage. Let L K denote the knowledge loss related to domain-specific knowledge, and L S denote the sampling loss related to old-domain knowledge. The knowledge loss is a cross-entropy loss on predicting masked tokens, and the sampling loss is designed to align the latent spaces of the old-domain knowledge and new domain-specific knowledge. The total loss L is given by a weighted sum of the two, that is:\nEQUATION\nwhere \u03bb is a weight for the knowledge loss.\nThe knowledge loss is implemented by using cross-entropy loss. Given a sentence with M mask tokens whose answers are m 1 , m 2 , \u00a8\u00a8\u00a8, m M , respectively, the knowledge loss L K is given by\nEQUATION\nwhere ppm i q is the probability for token m i output by M. 2016)), we translate each relation into a sentence, and then mask out its object. For example, the relation \"the Eiffel tower-/r/LocatedAt-Paris\" is translated into \"The Eiffel Tower is located at Paris.\", then \"Paris\" is substituted with the mask token, and the model is trained to fill the mask. \u201a Unstructured knowledge For unstructured knowledge (e.g., downstream unlabeled texts), we use the masked language model (MLM) similar to RoBERTa pretraining. Some tokens are randomly sampled from the input sentence and replaced with the special token <mask>, and the model is trained to predict the masked token. The cross-entropy loss is calculated to optimize the model. For old-domain knowledge and sampling loss, we train the model on general corpora including Wikipedia and BookCorpus (Zhu et al., 2015) . Specifically, for each batch, sentences randomly sampled from the dataset are input into the model. Given L layers that have domain-adapters installed, for each such layer l, we collect token representations from the FFN F l , and representations from the domain-adapter K l . The goal is to keep them as similar as possible. Thus, we calculate the sampling loss L S with L2 loss:\nL S \" 1 L L \u00ff l\"1 ||F l \u00b4Kl || 2 2 .\n(3)\n\nTask-Adapter\nAfter training domain-adapters, the model is aware of the domain knowledge, which is not directly related to downstream tasks though. Therefore, we add task-adapters on top of the domain-adapter to adapt to downstream tasks. For example, a domainadapter trained in biomedical knowledge can sup- \n\nMixture-of-Adapters Gate\nOn downstream tasks, it is possible that the output from the FFN, or a weighted sum of the two, produces better results. Therefore, in Stage 2, we train an additional mixture-of-adapters (MoA) gate simultaneously. The MoA gate receives the outputs from the attention layer q, the domain-adapter K, and the FFN F . q is first sent into a multi-layer perceptron (MLP):\nEQUATION\n)\nThe MLP is composed of a down-projection layer W d and an up-projection layer W u , and h \" W u \u03c3pW d qq, where \u03c3 is the nonlinearity function.\nThen, h is input into a Sigmoid layer to generate the weights of the FFNs and other domain-adapters:\nw \" Sigmoidphq.\n(5)\nThe final output o is a weighted sum of the outputs of the FFNs and the domain-adapter:\nEQUATION\nwhere r; s denotes matrix concatenation.\n\nExperimental Settings\nIn this section, we first introduce the datasets, then the baseline models, the evaluation metrics, and implementation details in the following four subsections, respectively.\n\nDatasets\nWe conduct experiments on three types of datasets: in-domain (ID) tasks that require general-domain knowledge; out-of-domain (OOD) tasks that require domain-specific knowledge; knowledgeintensive (KI) tasks that require commonsense knowledge.\n\u201a ID: GLUE Benchmark (Wang et al., 2018) including MNLI (Williams et al., 2017) , CoLA (Warstadt et al., 2019) , MRPC (Dolan and Brockett, 2005) , SST-2 (Socher et al., 2013) , RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) , STS-B (Cer et al., 2017) , WNLI (Levesque et al., 2012) , QNLI (Rajpurkar et al., 2016) , and QQP (Iyer et al., 2017) . \u201a OOD: ChemProt (Kringelum et al., 2016) , RCT (Dernoncourt and Lee, 2017) , IMDB (Maas et al., 2011) , and Amazon (He and McAuley, 2016) . ChemProt is a manually annotated chemical-protein interaction dataset extracted from 5,031 abstractions. RCT is a dataset based on PubMed for sentence classification. IMDB provides 25,000 movie reviews for sentiment analysis. Amazon is a dataset containing product reviews from Amazon, annotated with user ratings. \u201a KI: FEVER (Thorne et al., 2018) and Common-senseQA (CSQA) (Talmor et al., 2019) . FEVER consists of 185,445 claims that correspond to Wikipedia articles and are classified as supported, refuted, and not enough information. Common-senseQA consists of 12,247 questions with 5 choices and requires commonsense knowledge to predict the correct answers.\nFor Stage 1, we train the domain-adapter with unstructured knowledge related to the dataset following Section 3.1. The unstructured knowledge used is listed in Table 1 . We also experiment with structured knowledge in Section 6.2. For Stage 2, we adopt the true few-shot setting following (Perez et al., 2021) to demonstrate the effectiveness of MixDA. For each dataset class, we randomly sample K \" 16 examples from the original training set as the new training set, and another different K \" 16 examples as the validation set. The original validation set will be used as the test set. The Pfeiffer adapter is used in Stage 2 unless stated otherwise.\n\nBaselines\nIn our experiments, we use the following models as the main baselines. For convenience, we refer to them with the abbreviations in the parentheses later. \u201a HOULSBY (HO): Houlsby adapter (Houlsby et al., 2019) Prefix-Tuning trains a number of prompt embeddings for each task and pre-pends it before tokens. \u201a FINE-TUNING (FT): Fine-tuning all of the parameters of the RoBERTa-large model on downstream tasks.\n\nEvaluation Metrics\nWe adopt the Pearson correlation for STS-B since it is a regression task. The remaining are text classification tasks. Following Wang et al. (2018) ; Gururangan et al. ( 2020); Diao et al. (2021) , we adopt macro-F1 for MRPC and QQP, and micro-F1 for others as evaluation metrics. Macro-F1 computes the F1 independently for each metric, while micro-F1 computes an average metric of all classes. To account for the instability of small datasets, we report the average performance and the standard deviation of 3 runs with different random seeds.\n\nImplementation\nWe implement our RoBERTa-large model based on the Transformers library from HuggingFace 2 . The Houlsby adapter, the Pfeiffer adapter, and Prefix-Tuning are implemented based on the adaptertransformers library (Pfeiffer et al., 2020a) . LoRA is implemented based on OpenDelta (Ding et al., 2022) . During Stage 1, we train the domain-adapter with learning rate 1e-4, batch size 20, and weight decay 0.05. The knowledge loss factor \u03bb is set to 0.5. We train the 7 and 11 layers of RoBERTa-large with domain-adapter in 10 epochs. In Stage 2, we use the Pfeiffer adapter as the default task-adapter and train 20 epochs. All the experiments are conducted on Nvidia 2080Ti GPUs. We find the best hyper-parameters through grid search and the best results are listed in Appendix A. The computation time can be found in Appendix B.\n\nExperimental Results\nWe compare the performance of MixDA with our baselines on 15 datasets. First, we train the domainadapter for each domain individually and then perform each task with its corresponding domainadapter, which shows significant improvement over our baselines. Next, we plug in several domainadapters trained on different domains parallelly to verify the scalability of our model. detect the chemical-protein interaction. For example, MixDA shows more familiarity with words associated with that field, such as \"gefitinib\" and \"tyrosine kinase inhibitor\". In contrast, MixDA falters on STS-B, falling behind Pfeiffer by 0.8%. This is because the knowledge in Stage 1 is not effectively utilized. STS-B consists of sentence pairs like \"The cat sat on the mat\" and \"The cat did not sit on the mat\", with little need for additional knowledge. Across the three task domains, MixDA has an average improvement of 4.8% over RoBERTa + Pfeiffer on out-of-domain tasks, 2.5% on indomain tasks, and 5.0% on knowledge-intensive tasks. It shows that MixDA is not only effective for out-of-domain tasks and knowledge-intensive tasks that require additional knowledge but is helpful for general-domain language tasks as well, demonstrating its ability to excel at both in-domain and out-of-domain tasks (reliability).\n\nParallel Domain Adapters\nIn the previous section, we explored using a single domain-adapter for each downstream task. Next, we show the scalability of MixDA by using parallel domain-adapters and only train the MoA layer and task-adapters in Stage 2. The training process in Stage 2 follows the previous experiments. Table 3 shows the comparison across single domainadapter, parallel domain-adapters, and RoBERTa + Pfeiffer on 7 datasets. On average, parallel domainadapters show an improvement of 0.6% over vanilla RoBERTa + Pfeiffer, even though they fall behind the single domain adapter by 1.9%. This could be attributed to the MoA gate choosing the suboptimal domain-adapter for some test data. Still, considering its improvement over Pfeiffer, the MoA gate chooses the correct domain-adapter in most cases. Therefore, MixDA demonstrates its scalability, allowing end users to train Stage 1 on different datasets and combine them later. Overall, in both single and parallel situations, MixDA significantly improves upon the vanilla RoBERTa + Pfeif- fer model with a small increase in model size. This is due to the ability of MixDA to capture knowledge and the MoA to select useful knowledge for downstream tasks.\n\nAnalysis\nIn this section, we analyze the respective contributions of each part of MixDA through detailed analysis, including the Stage 1 training, task-adapters in Stage 2, and the mixture-of-adapters gate.\n\nAblation Study\nIn this section, we conduct an ablation study to reveal the contributions of each part of the model. There are three variants: (1) We remove the MoA gate and choose the domain-adapter instead of the RoBERTa feed-forward layer (-MoA). ( 2 \n\nStructured and Unstructured Knowledge\nIn Section 5, the MixDA is only trained on unstructured knowledge. As a comparison, we also train the domain adapter on ConceptNet, a structured knowledge dataset, and then attach both the unstructured and structured to our model and train the MoA layer and the task-adapter during Stage 2.\nTable 5 shows the result of combining structured and unstructured knowledge in Stage 1. FEVER and CSQA, two knowledge-intensive tasks, have the greatest improvement: 10.3% for FEVER and 1.2% for CSQA. This is because ConceptNet stores commonsense knowledge that can help both tasks. Meanwhile, MRPC and STS-B also obtain improvement, showing that ConceptNet can benefit general language tasks as well. In conclusion, the experiment demonstrates the ability of MixDA to utilize structured knowledge, the extensibility of our model, and the possible benefits of structured knowledge.\n\nEffectiveness of Task-Adapters\nIn most experiments of this paper, we adopt Pfeiffer as the task-adapter unless otherwise specified. In this section, we test the performance of MixDA combined with other kinds of task-adapters, including Houlsby, Prefix-Tuning, LoRA, and Pfeiffer. parameters compared to Houlsby, making it the optimal choice of task-adapters in our experiment.\n\nConclusion\nIn this paper, we proposed MixDA, a mixture of adapters for domain adaptation. We first decouple the knowledge modules (i.e., FFNs) into the old-domain and domain-specific FFNs. Then we propose a two-stage adapter tuning strategy: first tuning the domain adapter on each domain and then tuning the task adapter on each task. Moreover, our model could be scaled to multiple domains easily with the introduction of the mixture-of-adapters gate. Empirically, MixDA achieved significant improvement over in-domain tasks, out-of-domain tasks, and knowledge-intensive tasks. Further analyses demonstrate the reliability, scalability, and efficiency of our method.\n"}
{"question": "What issue does our work aim to alleviate?", "evidence": "  Multi-task learning (MTL) has emerged as a promising approach for sharing inductive bias across multiple tasks to enable more efficient learning in text classification. However, training all tasks simultaneously often yields degraded performance of each task than learning them independently, since different tasks might conflict with each other. Existing MTL methods for alleviating this issue is to leverage heuristics or gradient-based algorithm to achieve an arbitrary Pareto optimal trade-off among different tasks. In this paper, we present a novel gradient trade-off approach to mitigate the task conflict problem, dubbed GetMTL, which can achieve a specific tradeoff among different tasks nearby the main objective of multi-task text classification (MTC), so as to improve the performance of each task simultaneously. The results of extensive experiments on two benchmark datasets back up our theoretical analysis and validate the superiority of our proposed GetMTL.  ", "options": ["A.Training all tasks concurrently often leads to lower performance for each task compared to training them independently.", "B. Sharing inductive bias across multiple tasks allows for more efficient learning in the domain of text classification.", "C. Improve the leverage heuristics or gradient-based algorithm.", "D. To enable more efficient learning in text classification. "], "answer": "A", "content": "\nIntroduction\nMulti-task Learning (MTL), which aims to learn a single model that can tackle multiple correlated but different tasks simultaneously, makes multiple tasks benefit from each other and obtain superior performance over learning each task independently (Caruana, 1997; Ruder, 2017; Liu et al., 2015; Mao et al., 2020) . By discovering shared information/structure across the tasks, it has gained attention in many areas of research and industrial communities, such as computer vision (Misra et al., 2016; Gao et al., 2019; Yogamani et al., 2019; Sun et al., 2020 ) and text classification (Liu et al., 2017; Xiao et al., 2018; Mao et al., 2021 Mao et al., , 2022)) .\nHowever, it is observed in multi-task text classification (MTC) scenarios that some tasks could conflict with each other, which may be reflected via conflicting gradients or dominating gradients (Yu et al., 2020; Vandenhende et al., 2022) , leading to the degraded performance of MTL due to poor training. How to make a proper trade-off among jointing different tasks in MTC is a difficult problem. Recently, several methods have been proposed to mitigate gradient conflicts issue via both loss balance (linear weighted scalarization) such as homoscedastic uncertainty (Kendall et al., 2018) and task variance regularization (Mao et al., 2021) , and gradient balance like Pareto optimality (Sener and Koltun, 2018; Mao et al., 2020) . Existing methods devote to finding an arbitrary Pareto optimality solution in the Pareto set, which achieve a single arbitrary trade-off among all tasks. However, they can only satisfy the improved performance on part of tasks, not all tasks simultaneously. This means that these methods can not converge to a minimum average loss of all objectives.\nTo illustrate our idea, we give a two-task learning example shown in Figure 1 . As shown in Figure (1a) , it is observed that Pareto optimality-based methods can generate a set of Pareto solutions for a given two-task learning problem. However, some of Pareto solutions can increase the task 1 error while decreasing task 2 error, leading to unsatisfactory overall performance for MTL model. This im-plies that not all Pareto solutions always satisfy the goal of mitigating the tasks conflicts in MTL, and thus failing to achieve a better trade-off between tasks. Therefore, it is necessary to find a specific trade-off between tasks that is beyond what only using Pareto optimality can achieve.\nTo address this issue, inspired by multi-objective optimization (Sener and Koltun, 2018) , we argue that a more efficient way to mitigate task conflicts is to find a gradient trade-off between tasks in the neighborhood of the average loss rather than exhaustively searching for a proper solution from the set of Pareto solutions. As shown in Figure 1b , the Pareto solutions nearby the average loss can achieve a better trade-off between task 1 and task 2, leading to better performance on both tasks at the same time. Based on it, in this paper, we propose a novel gradient trade-off multi-task learning approach, named GetMTL, to mitigate task conflicts in multi-task text classification. Specifically, the gradients of each task are utilized to derive an update vector that can minimize the conflicts among task gradients in the neighborhood of the average gradient, so as to achieve a better trade-off performance among joint training tasks. In summary, the main contributions of our work are as follows:\n\u2022 A novel multi-task learning approach based on gradient trade-off between different tasks (GetMTL) is proposed to deal with task conflict in multi-task text classification problems, so as to improve the performance of all tasks simultaneously. \u2022 We give in-depth theoretical proofs and experimental analyses on establishing converge guarantees of our GetMTL. \u2022 We extensively verify the effectiveness of our GetMTL on two real-world text classification datasets, and the results show that our GetMTL performs competitively with a variety of state-of-the-art methods under a different number of task sets.\n\nRelated Works\nMulti-task Learning methods jointly minimize all task losses based on either loss balance methods (Kendall et al., 2018; Chen et al., 2018; Mao et al., 2021 Mao et al., , 2022) ) or gradient balance methods (Sener and Koltun, 2018; Mao et al., 2020) .\nThe loss balance methods adaptively adjust the tasks weights during training based on various heuristic approaches, such as task uncertainty quan-tification (Kendall et al., 2018) , gradient normalization (Chen et al., 2018) , task difficulty prioritization (Guo et al., 2018) , dynamic weight average (Liu et al., 2019) , random loss weighting (Lin et al., 2021) , task variance regularization (Mao et al., 2021) , and meta learning-based approach (Mao et al., 2022) . These methods are mostly heuristic and can have unstable performance while ignoring the task conflicts among all tasks, leading to the bad generalization performance of MTL models.\nRecently, some gradient balance based methods have been proposed to mitigate task conflicts for improving task performance. For example, D\u00e9sid\u00e9ri (2012) leverages multiple-gradient descent algorithm (MGDA) to optimize multiple objectives. Due to the guarantee of convergence to Pareto stationary point, this is an appealing approach. Sener and Koltun (2018) cast the multi-objective problem as a multi-task problem and devote to finding an arbitrary Pareto optimal solution. Mao et al. (2020) propose a novel MTL method based Tchebycheff procedure for achieving Pareto optimal without any convex assumption. However, these methods only consider achieving an arbitrary Pareto optimal solution while it is not the main objective. Unlike these methods, we propose an MTL approach based on multi-objective optimization and seek to find a set of solutions that are Pareto optimality and nearby the main MTC objective L 0 .\n\nPreliminaries\nConsider a multi-task learning problem with T 1 tasks over an input space X and a collection of task spaces {Y t } t\u2208 [T ] , where each task contains a set of i.i.d. training samples\nD t = {x i , y t i } i\u2208[nt] ,\nT is the number of tasks, and n t is the number of training samples of task t. The goal of MTL is to find parameters {\u03b8 sh , \u03b8 1 , ..., \u03b8 T } of a model F that can achieve high average performance across all training tasks over X , defined as F(X , \u03b8 sh , \u2022 \u2022 \u2022 , \u03b8 t ) : X \u2192 Y, where \u03b8 sh denotes the parameters shared between tasks and \u03b8 t denotes the task-specific parameters of task t. In particular, we further consider a parametric taskspecific map as f t (\u2022, \u03b8 sh , \u03b8 t ) : X \u2192 Y t . We also consider task-specific loss functions t (\u2022, \u2022) : Y t \u00d7 Y t \u2192 R + . We also denote the multi-task loss as L(\u03b8) = T i i (\u03b8), and the gradients of each task as g i = \u2207 i (\u03b8) for the particular \u03b8. In this paper, we choose the average loss as main objective of MTC problem, defined as L 0 (\u03b8) = 1 T T i i (\u03b8).\n\nMTL as Multi-objective Optimization\nMTL can be formulated as a specific case of multiple-objective optimization (MOO), which optimizes a set of potentially conflicting objectives (Sener and Koltun, 2018; Mao et al., 2020) . Given objective functions of T tasks, 1 , . . . , T , we formulate the optimization objective of MTL as the vectors of objective values :\nmin \u03b8 sh ,\u03b8 1 ,...,\u03b8 T (\u03b8 sh , \u03b8 1 ), . . . , (\u03b8 sh , \u03b8 T ) (1)\nSince there is no natural linear ordering on vectors, it is not possible to compare solutions and thus no single solution can optimize all objectives simultaneously. In other words, there is no clear optimal value. Alternatively, we can achieve Pareto optimality to obtain different optimal trade-offs among all objectives to solve MOO problem.\nDefinition 1 (Pareto dominance). Given two points {\u03b8, \u03b8} in \u2126, a point \u03b8 Pareto dominates \u03b8 (\u03b8 \u03b8) for MTL if two conditions are satisfied:\n(i) No one strictly prefers \u03b8 to \u03b8, that is, \u2200i \u2208 {1, . . . , T }, i (\u03b8 sh , \u03b8 i ) \u2264 i (\u03b8 sh , \u03b8 i ).\n(ii) At least one point strictly prefers \u03b8 to \u03b8, that is, \u2203j \u2208 {1, ..., T }, j (\u03b8 sh , \u03b8 j ) < j (\u03b8 sh , \u03b8 j ).\nDefinition 2 (Pareto optimality). \u03b8 * is a Pareto optimal point and (\u03b8 * ) is a Pareto optimal objective vector if it does not exist \u03b8 \u2208 \u2126 such that \u03b8 \u03b8 * . That is, a solution that is not dominated by any other is called Pareto optimal.\nThe set of all Pareto optimal solutions is called the Pareto set, and the image of Pareto set in the loss space is called Pareto front (Lin et al., 2019) . In this paper, we focus on gradient-based multiobjective optimization to achieve an appropriate Pareto trade-off among all tasks, which can approximate the Pareto front that minimizes the average loss.\n\nGradient-based Multi-Objective Optimization\nGradient-based MOO (Sener and Koltun, 2018) aims to find a direction d that we can iteratively find the next solution \u03b8 (t+1) that dominates the previous one \u03b8 (t) ( (\u03b8 (t+1) ) \u2264 (\u03b8 (t) )) by moving against d with step size \u03b7, i.e. \u03b8 (t+1) = \u03b8 (t) \u2212 \u03b7d. D\u00e9sid\u00e9ri (2012) ; Sener and Koltun (2018) propose to use multiple gradient descent algorithm (MGDA) that converges to a local Pareto optimal by iteratively using the descent direction d, which can be obtained as follows:\nd * = arg min d\u2208R m ,\u03b1\u2208R \u03b1 + 1 2 d 2 s.t. \u2207 i (\u03b8 (t) ) T d \u2264 \u03b1, i = 1, ..., T.\n(\nEQUATION\nwhere d * is the direction that can improve all tasks. Essentially, gradient-based MOO methods minimize the loss by combining gradients with adaptive weights, and obtaining an arbitrary Pareto optimality solution, ignoring the true objective (the average loss) (Liu et al., 2021) . In this paper, we generalize this method and propose a novel gradient-based approach to achieve a gradient trade-off among tasks for mitigating task conflicts, as well as constrain the solution that can minimize the average loss (L 0 (\u03b8)).\n\nGradient Trade-offs for Multi-task Text Classification\nFollowing most MTL methods, as shown in Figure 2 , we employ the hard parameter sharing MTL architecture, which includes f sh parameterized by heavy-weight task-shared parameters \u03b8 sh and f t parameterized by light-weight task-specific parameters \u03b8 t . All tasks take the same shared intermediate feature z = f sh (x; \u03b8 sh ) as input, and the t-th taskspecific network outputs the prediction as f t (z; \u03b8 t ).\nSince task-shared parameters \u03b8 sh are shared by all tasks, the different tasks may conflict with each other, leading to the degraded performance of MTL model. In this paper, we hypothesize that one of the main reasons for task conflicts arises from gradients from different tasks competing with each other in a way that is detrimental to making progress.\nWe propose a novel gradient-based MOO optimization to find a gradient trade-off among tasks in the neighborhood of the average loss, so as to mitigate task conflicts. Note that, we omit the subscript sh of task-shared parameters \u03b8 sh for the ease of notation.\n\nGetMTL\nGiven a task i, we define its gradient as g i = \u2207 i (\u03b8) via back-propagation from the raw loss i , and g i represents the optimal update direction for task i. However, due to the inconsistency of the MOO method and our GetMTL on three gradients (g 1 , g 2 and g 3 ) in R 3 , where g i denotes the gradient (black) of i-th task, g 0 is the average gradient, and blue arrows denote the projections of update direction to each task gradient.\noptimal update direction of task-shared parameters for each task, different task gradients may conflict with each other, leading to the training of networks being stuck in the over-training of some tasks and the under-training of other tasks. Intuitively, it is desirable to find a direction that can minimize the task conflicts among different tasks as well as achieve Pareto optimality to improve the performance of MTL model. We first achieve an arbitrary Pareto optimal via finding a descent direction d des by searching for a minimum-norm point in the Convex Hull CH of gradients, defined by,\nEQUATION\ns.t. S T = \u03b2 \u2208 R T + T j=1 \u03b2 j = 1 (4)\nwhere G \u2208 R T \u00d7m = {g 1 , ..., g T } is the matrix of task gradient, S T is the T -dimensional regular simplex. We use the multiple gradient descent algorithm (MGDA) (Sener and Koltun, 2018) to obtain an arbitrary Pareto optimal by iteratively using the descent direction, defined by,\nd des = arg min d\u2208CH d 2 2\n(5)\nIn addition, the d des can be reformulated as a linear combination of all task gradients, defined by,\nd des = T i=1 \u03b2 i g i (6)\nwhere g i = \u2207 i (\u03b8) is the i-th task gradient. It implies that, when converges to an arbitrary Pareto optimal, the optimal gradient value of each task via back-propagation is \u03b2 i g i , defined as\ng \u03b2 i = \u03b2 i g i .\nHowever, moving against d des does not guarantee that the solution meets the requirements of multi-task text classification task (MTC), that is, to alleviate the gradient conflict among tasks in MTC, so as to improve the performance of all tasks. To address this issue, we seek a direction that enables us to move from a solution \u03b8 (t) to \u03b8 (t+1) such that both \u03b8 (t+1) dominates \u03b8 (t) (L(\u03b8 (t+1) ) \u2264 L(\u03b8 (t) )) and alleviate the gradient conflict among all tasks. Based on it, as shown in Figure 2 (b), we propose to search for an update direction d in the Convex Hull CH \u03b2 of back-propagation gradients such that it can improve any worst objective and converge to an optimum of MTC objective L 0 (\u03b8). We first find the worst task gradient with respect to the update direction d, that is, it has a maximum angle with d, which can be formulated via the following optimization problem,\nEQUATION\nwhere g \u03b2 i is the i-task gradient after optimizing by MGDA algorithm.\nTo improve the worst gradient of any task and achieve a trade-off between all task gradients in a neighborhood of the average gradient (defined as g 0 = 1 T T i=1 g i ), we formulate this gradient trade-off optimization problem via the following Maximin Optimization Problem (dual problem).\nProblem 1.\nmax d\u2208R m min i\u2208[T ] g \u03b2 i , d s.t. d \u2212 g 0 \u2264 \u03b5g T 0 d, \u2212 g T 0 d \u2264 0 (8)\nwhere g \u03b2 i = \u03b2 i g i is the back-propagation gradient value of i-th task via solving Eq. ( 5), \u03b5 \u2208 (0, 1] is a hyper-parameter that controls the stability of MTC model.\n\nSolving Maximin Problem\nSince the optimal direction d can also be defined in the convex hull CH \u03b2 of g \u03b2 i , we can get\nEQUATION\nwhere\nG \u03b2 \u2208 R T \u00d7m = {g \u03b2 1 , ..., g \u03b2 T } is task gradi- ent matrix, W T = { w \u2208 R T + T j=1 w j = 1} is the T -dimensional\nprobability simplex, and w = (w 1 , ..., w T ). Therefore, we can get min i g \u03b2 i , d = min w\u2208W T i w i g \u03b2 i , d and Problem 1 can be transformed into the following form.\nAlgorithm 1: GetMTL Algorithm.\nInput: The number of task T , loss functions { i } T i=1 , network parameters \u03b8 (t) at t step, the pre-specified hyper-parameter \u03b5 \u2208 (0, 1] and step size \u00b5 \u2208 R + . 1: Task Gradients:\ng i = \u2207 i (\u03b8 (t) ), i \u2208 [T ] 2: Main Objective: g 0 = T i=1 g i 3:\nObtain {\u03b2 1 , ...\u03b2 T } by solving Eq.( 5). 4: Compute g w = i w i g \u03b2 i , where g \u03b2 i = \u03b2 i g i 5: Obtain {w 1 , ..., w T } by solving Eq.( 14) 6: Find direction d * by using Eq.( 13)\nEQUATION\nwhere g w = T i=1 w i g \u03b2 i is the convex combination in CH \u03b2 . For a given vector \u03bb \u2208 R + with non-negative components, the corresponding Lagrangian associated with the Eq.( 10) is defined as\nEQUATION\n11) Since the objective for d is concave with linear constraints and w \u2208 W T is a compact set 2 , according to the Sion's minimax theorem (Kindler, 2005) , we can switch the max and min without changing the solution of Problem 2. Formally,\nmin \u03bb,w\u2208W T max d\u2208R m g T w d \u2212 \u03bb d \u2212 g 0 2 /2 + \u03bb\u03b5 2 (g T 0 d) 2 /2 (12)\nWe get the optimal solution of primal problem (Problem 1) by solving the dual problem of Eq.( 12) (See the Appendix A for a detailed derivation procedure). Then we have\nd * = g w + \u03bb * g 0 (1 \u2212 \u03b5 2 g 2 0 )\u03bb * , where \u03bb * = g w \u03b5 g 0 2 (13)\nwhere \u03bb * is the optimal Lagrange multiplier, d * is the optimal update direction of MTC model. We can reformulate the problem of Eq.( 12) as following optimization problem w.r.t. w.\nEQUATION\n2 Compact set: a set that is bounded and closed. where g w is defined as\ng w = T i=1 w i g \u03b2 i .\nThe detailed derivation is provided in Appendix A. Algorithm 1 shows all the steps of GetMTL algorithm in each iteration.\n\nTheoretical Analysis\nIn this section, we analyze the equivalence of solutions to dual problem and then give a theoretical analysis about convergence of GetMTL algorithm. We define the Lagrangian of problem in Eq.( 10), Theorem 4.2 (Convergence of GetMTL). Assume loss functions i are convex and differential, and \u2207 i (\u03b8 (t) ) is L-lipschitz continuous with L > 0. The update rule is \u03b8 (t+1) = \u03b8 (t) \u2212 \u00b5 (t) d, where d is defined in Eq.( 13) and\nL(d, \u03bb, w) = g T w d \u2212 \u03bb 2 ( d \u2212 g 0 2 \u2212 \u03b5 2 (g T 0 d) 2 ) Theorem 4.1 (Equivalence of\n\u00b5 (t) = min i\u2208[k] d\u2212g 0 c\u2022L\u2022d 2 . All the loss functions 1 (\u03b8 (t) ) \u2022 \u2022 \u2022 T (\u03b8 (t) ) converges to ( 1 (\u03b8 * ) \u2022 \u2022 \u2022 T (\u03b8 * )).\nProof. The proof is provided in Appendix C.\n\nExperimental Datasets\nWe conduct experiments on two MTC benchmarks to evaluate the proposed GetMTL. 1) Amazon Review dataset (Blitzer et al., 2007) contains product reviews from 14 domains (See Details in Appendix D), including apparel, video, books, electronics, DVDs and so on. Each domain gives rise to a binary classification task and we follow Mao et al. (2021) to treat 14 domains in the dataset as distinct tasks, creating a dataset with 14 tasks, with 22180 training instances and 5600 test instances in total. 2) Topic classification dataset, 20 Newsgroup 3 , consists of approximately 20,000 newsgroup documents, partitioned evenly across 20 different newsgroups. We follow Mao et al. (2021) to select 16 newsgroups from 20 Newsgroup dataset shown in Table 1 and then divide them into four groups. Each group gives rise to a 4-way classification task, creating a dataset with four 4-way classification tasks, which is a more challenging dataset than amazon review dataset.\n\nExperimental Implementation\nWe follow the standard MTC setting and adopt the same network architectures with the most recent baselines for fair comparisons (Mao et al., 2021) . We adopt the hard parameter sharing MTL framework shown in Figure 2 , where task-shared network is a TextCNN with kernel size of 3,5,7 and taskspecific network is a fully connected layer with a softmax function. Adam is utilized as the optimizer to train the model over 3000 epochs with a learning rate of 1e-3 for both sentiment analysis and topic classification. We set the batch size to 256. \n\nComparison Models\nWe compare the proposed GetMTL with a series of MTC baselines, including Single-Task Learning (STL): learning each task independently.\nUniform Scaling: learning tasks simultaneously with uniform task weights.\nUncertainty: using the uncertainty weighting method (Kendall et al., 2018) .\nGradNorm: learning tasks simultaneously with gradient normalization method (Chen et al., 2018) .\nTchebycheffAdv: using adversarial Tchebycheff procedure (Mao et al., 2020) .\nMGDA: using gradient-based multi-objective optimization method (Sener and Koltun, 2018) .\nBanditMTL: learning tasks simultaneously with multi-armed bandit method (Mao et al., 2021) .\nMetaWeighting: using adaptive task weighting method (Mao et al., 2022) .\n\nMain Results\nThe main comparison results of GetMTL on two benchmark datasets are shown in Figure 3 and 4 . It is clear that (See detailed numerical comparison results in Appendix D), our proposed GetMTL model performs consistently better than the all comparison methods on all tasks of both amazon review and topic classification datasets, and its average performance is superior to that of all baselines. This verifies the effectiveness of our GetMTL method in MTC problem. More concretely, in comparison with the gradient-based MOO optimization model (MGDA), our GetMTL achieves significant improvement across all datasets. This indicates that achieving a gradient trade-off nearby average loss to mitigate task conflicts can better improve all task performance and generalization ability of MTC model. \n\nEmpirical Analysis on Convergence\nIn Section 4.3, we theoretically prove the convergence of our proposed GetMTL. Furthermore, we conduct extensive experiments about the convergence to better demonstrate the advantages of GetMTL shown in Figure 5 . It is clear that the learning curve of GetMTL is constantly decreasing as the number of iterations increases and converges to the lowest loss value compared with other baselines. It indicates that GetMTL can guarantee the convergence of the objective value and obtain better performance of all learning tasks.\nIn addition, we also conduct extensive experiments to investigate how GetMTL mitigates task conflict during training. We plot the task variance (variance between the task-specific losses) of all baselines on both amazon review and topic classification datasets shown in Figure 6 . It can be observed that all MTL baselines have lower task variance than STL method, which illustrates that MTL methods can indeed boost the learning of all tasks compared with STL method. Moreover, GetMTL has the lowest task variance and smoother evolution during training than other MTL baselines. This implies that our proposed GetMTL indeed mitigates task conflicts compared with other MTL methods.\n\nThe Evolution of Task Weight w\nIn this section, we visualize the task weights of our GetMTL and two weight adaptive MTL methods (MGDA and BanditMTL) throughout the training process using the topic classification dataset shown in Figure 7 . It can be observed from these four figures that the weight adaption process of our GetMTL is different from that of MGDA and Ban-ditMTL. GetMTL can automatically learn the task weights without pre-defined heuristic constraints. The weights adaption process of GetMTL is more stable and the search space is more compact compared with other MTL baselines.\n\nImpact of the Values of \u03b5\nTo investigate the impact of using different values of \u03b5 on the performance of our GetMTL, we conduct experiments on two datasets, and the results are shown in Figure 8 . Noting that model with \u03b5 = 0.0075 and \u03b5 = 0.025 perform overall better than other values on these two datasets, respectively. The model with larger value of \u03b5 performs unsatisfactorily overall all tasks on two datasets, one possible reason is that larger \u03b5 makes d pull far away from the average loss g 0 (see the conditions in Eq. ( 9)). That is, Pareto optimality found by GetMTL is getting further and further away from MTC objective L 0 , which can be quite detrimental to some tasks' performance, leading to degraded average performance.\n\nConclusion\nIn this paper, we propose a novel gradient tradeoff multi-task learning approach to mitigate the task conflict problem, which can achieve a specific trade-off among different tasks nearby the main objective of multi-task text classification problem. Moreover, we present a series of theoretical proofs to illustrate the effectiveness and superiority of our GetMTL. Experimental results on two benchmark datasets show that our GetMTL achieves state-ofthe-art performance in Multi-task Text Classification problem.\n"}
{"question": "Which is not the limitation of the existing conceptualization methods?", "evidence": "  Though abstract commonsense knowledge can be derived by using existing conceptualization methods to abstract a certain instance from factual commonsense knowledge, several limitations still exist.\nFirst, the plausibility of abstract commonsense knowledge banks on both the correctness of conceptualization and proper contextualization under specific assertions. The latter one, which is an essential step for the deduction of abstract knowledge, is missing from current methodologies. Second, instantiating abstract commonsense knowledge can yield much more and diverse concrete commonsense knowledge that can serve as an augmentation of the training dataset, while current methods undervalue such a process and only focus on conceptualization. Finally, the complex contextualization and conceptualization of commonsense knowledge can easily bring more than two orders of magnitude of data on top of the original dataset. This makes current labeled data scarce and infeasible for practitioners to annotate all of them, leaving a large amount of unlabeled data. However, when encountering situations beyond the data given, more abstract background knowledge must be acquired and generalized to assist the reasoning, and language models trained with an autoregressive language modeling objective do not explicitly leverage such abstract knowledge during inference.\n ", "options": ["A. Proper contextualization under specific assertions, which is an essential step for the deduction of abstract knowledge, is missing from current methodologies.", "B. Current methods undervalue instantiating abstract commonsense knowledge and only focus on conceptualization.", "C. Models trained with an autoregressive language modeling objective do not explicitly leverage such abstract knowledge during inference.", "D.  Current labeled data scarce and infeasible for practitioners to annotate all of them, leaving a large amount of unlabeled data."], "answer": "C", "content": "\nIntroduction\n\"Concepts are the glue that holds our mental world together.\"- Murphy (2004) Commonsense reasoning is a crucial ability for machines to make situational presumptions and draw inferences from the knowledge that reflects our humans' understanding of situations and common facts (Davis, 1990; Davis and Marcus, 2015) . It has gained increasing popularity in the Natural Language Processing (NLP) community with the emergence of CommonSense Knowledge Bases (CSKB) (Sap et al., 2019a; Speer et al., 2017;  Figure 1 : A demonstration of commonsense reasoning on an unknown situation, PersonX plays with his dog, with the aid of abstract commonsense knowledge. Decontextualized conceptualization, such as observe, may yield wrong abstract commonsense knowledge that cannot be instantiated within the corresponding context. Hwang et al., 2021) and large language models (Bosselut et al., 2019; Rajani et al., 2019; Liu et al., 2022b; Su et al., 2022; Yu et al., 2022b) . However, when encountering situations beyond the data given, more abstract background knowledge must be acquired and generalized to assist the reasoning (Tenenbaum et al., 2011) , and language models trained with an autoregressive language modeling objective do not explicitly leverage such abstract knowledge during inference.\nInstead, humans rely on conceptual induction and deduction (Murphy, 2004) to make inferences on novel situations without the need to memorize all special cases. As shown in Figure 1 , humans can derive conceptualizations based on the assertion that \"PersonX watches a football game, as a result, he feels relaxed\" to infer that \"relaxing events can make someone feel relaxed,\" where the acquired abstract commonsense knowledge can be further used as general knowledge to perform reasoning on similar or associated situations. A new commonsense knowledge \"PersonX plays with his dog, as a result, he feels happy and relaxed\" can be deduced by instantiating relaxing events to playing with his dog.\nAs the cornerstone of generalizable commonsense reasoning, such a process is extremely challenging for machines to replicate due to the absence of contextualized conceptualizations and abstract commonsense knowledge in CSKBs and a lack of relevant methodologies.\nYet, existing works address the process of induction and deduction separately via conceptualization and instantiation. Several methods performing conceptualization are proposed with a specific focus on entity-level (Durme et al., 2009; Song et al., 2011; Gong et al., 2016; He et al., 2020; Peng et al., 2022; Song et al., 2015) and event-level (Chen et al., 2020; He et al., 2022) semantics. Instantiation (Allaway et al., 2023) , as the process that simulates conceptual deduction, is tackled separately and not leveraged by these methods. Though abstract commonsense knowledge can be derived by using existing conceptualization methods to abstract a certain instance from factual commonsense knowledge, several limitations still exist.\nFirst, the plausibility of abstract commonsense knowledge banks on both the correctness of conceptualization and proper contextualization under specific assertions. The latter one, which is an essential step for the deduction of abstract knowledge, is missing from current methodologies. Take Figure 1 as an example, the concept observe will not necessarily lead to the result of \"feeling relaxed\", as observe omits the entertaining property of the original instance as a cost of abstraction. Second, instantiating abstract commonsense knowledge can yield much more and diverse concrete commonsense knowledge that can serve as an augmentation of the training dataset, while current methods undervalue such a process and only focus on conceptualization. Finally, the complex contextualization and conceptualization of commonsense knowledge can easily bring more than two orders of magnitude of data on top of the original dataset. This makes current labeled data scarce and infeasible for practitioners to annotate all of them, leaving a large amount of unlabeled data.\nTo fill in these research gaps, we propose CAT (Contextualized ConceptuAlization and InsTantiation), a semi-supervised learning framework that unites event conceptualization and instantiation in cascade to conceptualize CSKBs and acquire abstract commonsense knowledge to aid commonsense reasoning. Inspired by how humans learn with concepts (Carey, 2004) , we design a novel bootstrapping 1 method to enhance conceptualizations and abstract commonsense knowledge verification with the help of similar conceptualizations and instantiations as a reference. We demonstrate the effectiveness of CAT by using the acquired abstract commonsense knowledge to train COMET (Bosselut et al., 2019) , a commonsense inference language model that generates if-then commonsense knowledge, and showing that our derived abstract commonsense knowledge can significantly improve commonsense inference modeling.\nOur contributions are three-fold: (1) We introduce a semi-supervised learning framework, CAT, to conceptualize CSKBs with the assistance of progressively bootstrapping similar abstract concepts or instantiations in the conceptualization process.\n(2) We use CAT to acquire abstract commonsense knowledge at scale with high quality, which can be used for commonsense inference modeling. (3) We demonstrate the effectiveness of our framework by achieving state-of-the-art performance on two CSKB conceptualization tasks and remarkably improving commonsense inference modeling with our derived abstract commonsense knowledge.\n\nRelated Works\nConceptualization and Instantiation. Many existing works have studied conceptualization and instantiation separately. Durme et al. (2009) first attempted to derive more general knowledge by abstracting over large sets of factoids obtained from WordNet (Miller, 1995) synsets. Song et al. (2011 Song et al. ( , 2015) ) and Gong et al. (2016) proposed to turn instances in a sentence into concepts via weight matching from Probase (Wu et al., 2012) . Recently, Liu et al. (2022c) proposed a taxonomy-guided induction method to mine verb-oriented commonsense knowledge from verb phrases. Peng et al. (2022) constructed a conceptual knowledge benchmark to evaluate language models with three zeroshot probing tasks. While these works focus on the conceptualization of entities, He et al. (2022) constructed an event conceptualization benchmark based on ATOMIC (Sap et al., 2019a ) by combining syntactic parsing, semantically heuristic matching, and human annotation. Besides, the line of works focusing on ultra-fine entity typing (Choi et al., 2018; Dai et al., 2021; Li et al., 2022) similar objectives of typing named entities, nominal nouns, and pronouns into a set of free-form phrases. Instantiation was attempted by Allaway et al. (2023) , who proposed a controllable generative framework to probe valid instantiations for abstract knowledge automatically. Though Porada et al. (2021) and Peng et al. (2022) both proved that existing pretrained language models lack conceptual knowledge, none of existing works explicitly combine both techniques to derive abstract knowledge that is context-sensitive and generalizable.\nCommonsense Reasoning. Endowing NLP systems with the ability to perform commonsense reasoning is an elusive goal of artificial intelligence (Sap et al., 2020) . A diverse collection of commonsense reasoning tasks have been proposed as evaluation benchmarks (Talmor et al., 2019; Omura et al., 2020; Ponti et al., 2020; Fang et al., 2021a) . Among them, Bosselut et al. (2019) proposed a generative model, COMET, to learn to produce if-then commonsense knowledge as an effective approach toward modeling commonsense inference that can be applied in various commonsense reasoning tasks (Talmor et al., 2019) .\nSemi-Supervised Learning. Semi-supervised learning (SSL) aims at taking advantage of unlabeled data to equip models with stronger generalization ability (van Engelen and Hoos, 2020) . The most common approach is using pseudo labels (Iscen et al., 2019; Wang et al., 2022) to expose more unseen data to the student model. It has been applied in various machine learning tasks such as image classification (Liu et al., 2022a; Hu et al., 2021 ), text classification (Li et al., 2021; Meng et al., 2019; Xiao et al., 2019) , commonsense knowledge base population (Fang et al., 2022) , and named entity recognition (Liu et al., 2021; Chen et al., 2021) .\n\nProblem Definition\nDefinition. Conceptualizing an event-centric CSKB to derive abstract commonsense knowledge comprises two steps (He et al., 2022) : event conceptualization and triple conceptualization.\n\nDenote the triples in the original CSKB as\nD o = {(h o , r, t)|h o \u2208 H o , r \u2208 R, t \u2208 T }, where H o , R,\nand T are the set of heads, relations, and tails in the original CSKB. The first step only operates on head events without considering the context in r and t. The goal of event conceptualization is to produce conceptualized head event h a from the original head h o to represent an abstraction of h o . In the second step, the task is to verify whether the conceptualized head h a still makes sense in the context of r and t, as r and t will further restrict the level of abstractness in h a . As shown in Figure 1 , conceptualizing watch football game to observe is wrong within the context of having feel relaxed as a result. Plausible (h a , r, t) triples will be considered as valid abstract commonsense knowledge. Specifically, in the first step, there are two ways of conceptualizing head events alone: a retrievalbased discriminative way and a generative way. The retrieval-based discriminative paradigm identifies and links a component i in h o to a concept c in a concept taxonomy C to form a conceptualization h a by replacing i with c. The model needs to verify whether h a is a valid conceptualization of h o . The generative paradigm aims to generate a h a directly given h o and the designated component i in h o .\nFormally, denote the annotated dataset in the first step, event conceptualization, as\nD l h = {(h o , h a , y)|h o \u2208 H o , h a \u2208 H a , y \u2208 {0, 1}},\nwhere h o is an original head event without conceptualization, h a is a corresponding conceptualization of h o , and y is the human-annotated label indicating whether such a conceptualization is plausible or not. The labeled dataset in the second step, triple conceptualization, is denoted as\nD l t = {(h, r, t, y)|h \u2208 H a , r \u2208 R, t \u2208 T, y \u2208 {0, 1}},\nwhere h is a conceptualized head event from the first step, r and t are a relation and a tail from the original CSKB accompanied with the corresponding original head h o , and y is the human-annotated label indicating whether such abstract commonsense knowledge, in the form of a conceptualized triple, is plausible or not. Besides labeled datasets, unlabeled datasets are defined similarly as D u h and D u t only with the difference that labels y are missing. Thus, the task objective for discriminative event conceptualization is to determine whether a h o can be properly abstracted using h a , where h a is derived by replacing a component i \u2282 h o with its linked concept c from a concept taxonomy C. The task objective for generative event conceptualization is to generate h a directly from h o with text generation models. For the triple conceptualization task, the objective is to distinguish whether a conceptualized triple (h a , r, t), representing abstract commonsense knowledge, is plausible or not.\nDataset. To study conceptualization over CSKBs, we use the AbstractATOMIC dataset provided by He et al. (2022) as the benchmark. In Ab-stractATOMIC, ATOMIC is used as the original CSKB. And the event conceptualization adopts a discriminative way, where a syntactic parsing schema is defined to identify the components i in h o to be heuristically linked to concept taxonomies Probase (Wu et al., 2012) and WordNet (Miller, 1995) to form conceptualized h a . Such a heuristic can produce over 32 times more candidate conceptualized head events and over 10 times more conceptualized triples compared with the original ATOMIC, as the number of retrieved concepts from the concept taxonomy C can be manually controlled to acquire a large number of conceptualizations. Triple conceptualization is defined as predicting the plausibility of the triples whose head is conceptualized. Only 131K (26%) conceptualizations of 7K (45%) ATOMIC head events and 81K (1.3%) conceptualized triples are manually anno-tated as D l h and D l t , while others remain unlabeled D u h and D u t . The trn/dev/tst partition follows the same split as in the original ATOMIC. Statistics and more detailed explanations of AbstractATOMIC are shown in Table 1 and Appendix A.\n\nCAT Framework\nThis section introduces our proposed Contextualized ConceptualizAtion and InsTantiation (CAT) framework for conceptualizing commonsense knowledge bases and acquiring abstract commonsense knowledge. An overview is presented in Figure 2 . Our motivation is two-fold: first, adding instantiation after conceptualization to form a cycle can strongly benefit two conceptualization tasks simultaneously. On the one hand, instantiating conceptualized triple relies on the correctness of event conceptualization. On the other hand, properly conceptualized triples can benefit event conceptualization via instantiation by providing more context brought by (r, t). Second, to address the lack of annotations, we resort to pseudo labeling, a typical semi-supervised learning approach to automatically assign pseudo labels to the vast majority of unlabeled data using a teacher model.\nFollowing He et al. (2022) , we study the retrieval-based discriminative paradigm of event conceptualization and leave the generative paradigm as an intrinsic evaluation. In CAT, we unify event conceptualization and triple conceptualization into one cycle and make them mutually benefit each other through instantiation and conceptualization. Our framework can be summarized into four steps:\n(1) Train teacher models for both event conceptualization and triple conceptualization on the labeled dataset D l h and D l t , respectively. Use the two teachers to assign pseudo labels to unlabeled datasets.\n(2) Conduct alternative conceptualization or instantiation on labeled and pseudo-labeled data.\n(3) Bootstrap (aggregate) the alternative concepts and instances in the second step using natural language prompt templates and train student models on both labeled and pseudo-labeled data. (4) Use the student models to refine the pseudo labels and then re-train the student models.\n\nTeacher Model Training\nTwo teacher models on both event and triple conceptualization tasks are trained separately on the labeled dataset D l h and D l t . As both tasks are inherently text/triple classification, we adopt KG-BERT (Yao et al., 2019) as the skeleton of our models. The event conceptualization model determines whether h a is a valid conceptualization of h o , and the triple conceptualization model determines whether a conceptualized triple (h a , r, t) is plausible or not. The two models \u03b8 are trained on annotated examples x i with a cross-entropy loss (Eq. 1) and used to provide pseudo labels to instances from the unlabeled datasets D u h and D u t . Two thresholds, T + and T \u2212 , are set to determine the pseudo labels of unlabeled examples with high confidence. Examples with a pseudo-labeled score higher than T + will be labeled y i = 1, and those lower than T \u2212 will be labeled y i = 0. The rest will be discarded.\nL(x i , \u03b8) = \u2212 |x| i=1 y i log(\u03b8(x i ))\n(1)\n\nAlternative Conceptualization and Instantiation\nAccording to Murphy (2004) , when humans learn a new concept, we pre-extract similar known concepts in our minds and infer possibly equivalent unknown concepts on the fly. Inspired by this theory, we retrieve additional abstract concepts or instantiated events to help discriminate conceptualizations and abstract commonsense knowledge. For event conceptualization, we retrieve some alternative possible conceptualizations of h o to accompany the learning of h a . Additional conceptualizations of h o from both labeled and pseudo-labeled examples are predicted again by the teacher model and ranked according to their plausibility score prediction. And top m conceptualizations are retrieved with m being a hyperparameter to control the number of retrievals. For triple conceptualization, we perform instantiation in cascade to instantiate c to some concrete instances to assist the learning process.\nPossible instantiations of c are extracted from annotated and pseudo-labeled event conceptualizations by searching for conceptualized events h \u2032 a \u2208 H a other than h a with c as the concept and extracting their corresponding instances i \u2282 h \u2032 a . Similarly, the instances are then scored by the teacher model, and the top n of them are retrieved. Intuitively, alternative event conceptualizations can serve as hints for discriminating the correctness of the target conceptualization, and instantiations can carry additional contextualized information to help verify the plausibility of a conceptualized triple, which meets the objective of deriving abstract commonsense knowledge that is context-sensitive.\n\nPrompt Aggregation\nWe then bootstrap the retrieved alternative conceptualizations/instantiations via natural language prompts. Here bootstrap (Carey, 2004 ) can be understood as binding the alternative retrievals and the target concept/triple together to strengthen the discrimination of the target concept/triple. As shown in Figure 2 step (3), the initially given input and retrieved concepts/instances are concatenated via human-defined prompts for both conceptualization tasks. Alternative concepts/instances are sorted in the order of their plausibility score ranking. Two student models S h and S t for both tasks are trained using the modified text with such prompts as inputs. They are expected to learn the bootstrapping connectionism between the target and the additional retrievals we provided. More detail about the prompt design is in Appendix B.\n\nPseudo-Label Refinement\nAll pseudo labels, initially derived by a teacher model trained on the original labeled dataset, are relabeled according to the plausibility score predicted by our newly enhanced student models S h and S t . Similar to the teacher model, two thresholds, T + and T \u2212 , are applied to distinguish positive and negative examples for both tasks. In addition, negative Table 2 : Performance (%) by our CAT framework on the discriminative event conceptualization and triple conceptualization tasks. We report the average AUC score and standard deviation across experiments with three random seeds. The best performances within each framework are underlined, and the best among all models are bold-faced. labels are assigned to triples whose conceptualized head events are predicted as wrong conceptualizations by S h , as wrong conceptualizations will not yield plausible abstract commonsense knowledge.\n\nApplication and Evaluation of CAT\nThe resulting models of CAT include an event conceptualization model and a triple conceptualization model, both fine-tuned on the refined pseudo labels and the labeled data. These two models can be used to conceptualize ATOMIC to a larger commonsense knowledge base on a more abstract level. We further conduct intrinsic evaluations on the acquired event conceptualization model under a generative event conceptualization paradigm and extrinsic evaluations on the resulting conceptualized CSKB with commonsense inference modeling task (COMET; Bosselut et al. (2019) ) in Section 5. Here we select COMET as the representative because it is a general commonsense model that can be applied to various downstream commonsense reason-ing tasks such as SocialIQA (Sap et al., 2019b) , self-talk (Shwartz et al., 2020) , and CSKB completion (Malaviya et al., 2020) . Meanwhile, generative event conceptualization enables performing automatic conceptualization scalably. Both are important applications and evaluations of CAT.\n\nExperiments\nWe conduct conceptualization experiments using CAT in Section 5.1 and generative experiments as evaluations in Section 5.2. These experiments demonstrate that CAT has a strong capability in conceptualizing CSKBs, and better conceptualization modeling can help populate more novel and diverse commonsense knowledge and thus help commonsense modeling (COMET).\n\nCSKB Conceptualization\nBaselines. We collectively introduce the baselines for both event and triple conceptualization tasks, as they are inherently classification tasks. Table 3 : Performance (%) of GPT2 (XL) on the generative event conceptualization task. D l h stands for annotated labeled data, and D u stands for the data acquired by CAT. The underfoot value indicates the threshold for selecting plausible pseudo labels. The best performances are bold-faced, and the second-best ones are underlined.\nAUC is used as the evaluation metric. Under a supervised learning setting, we apply KG-BERT (Yao et al., 2019) model with BERT (Devlin et al., 2019) , BART (Lewis et al., 2020) , RoBERTa (Liu et al., 2019 ), DeBERTa (He et al., 2021 , 2023 ), and ELECTRA (Clark et al., 2020) as the backbone language models. We also attempt to leverage supervised generative language models as baselines. GPT2 (Radford et al., 2019) models are trained with a text generation objective only on positive examples, and we use perplexity as the prediction scores to calculate AUC. For the semi-supervised learning baselines, we leverage UDA (Xie et al., 2020a) , NoisyStudent (Xie et al., 2020b), and Pseu-doReasoner (Fang et al., 2022) with RoBERTalarge being the backbone model. Additional explanations can be found in Appendix C.1.1.\nDiscriminative Results. The results for both tasks are presented in Table 2 . Under a supervised learning setting, KG-BERT family mostly performs better on both tasks than GPT2 due to the fact that GPT2 is only fine-tuned on positive examples and thus cannot learn from negative examples that contain wrong conceptualizations and implausible abstract commonsense knowledge. As for the semi-supervised learning setting, previous SSL baselines are rather limited in improving the performance against supervised learning. The best Pseu-doReasoner only improves by 0.5% and 0.3% on the test set for both tasks compared with supervised RoBERTa-large models. Instead, models trained with CAT can outperform all other training methodologies. Comparing the test set performance with PseudoReasoner, small backbone models (BERTbase) can improve by 3.4% and 2.2%, and large models (RoBERTa-large) can be improved by 2.1% and 2.2%. This shows pipelining two-step concep-tualizations as a loop and leveraging our proposed bootstrapping-based method can yield a larger performance gain compared with simply applying a semi-supervised learning strategy. Due to limited space, ablation studies on framework components and the semi-supervised learning paradigm of CAT are conducted in Appendix C.1.4. For example, the results indicate that bootstrapping alternative conceptualization and instantiation plays the most important role in assisting learning conceptualization among all components of CAT. Additional results and a computational cost study can be found in Appendix C.1.3 and Appendix D.\n\nApplication and Evaluation of CAT\nAs CAT is a framework for acquiring conceptualized commonsense knowledge, including both conceptualized head events (from h o to h a ) and abstract commonsense triples (h a , r, t), we assess these pseudo-labeled outcomes via two generative tasks with various threshold tuning as evaluations.\nGenerative Event Conceptualization. To intrinsically evaluate the effectiveness of CAT's event conceptualization, we use the acquired conceptualized head events as training data to learn a generative event conceptualizer. Specifically, the models are trained with instance-conceptualizations pairs in the format of \"<instance> is an instance of <concept>\". At the evaluation phase, the model is prompted with \"<instance> is an instance of [GEN]\" where <instance> is the instance to be conceptualized and [GEN] is the generation token. We then retrieve the top-1 generation and compare it against the target set from the evaluation dataset to compute four NLG metrics, as listed in Appendix C.2.1. These scores can be regarded as an approximation of the top-1 generations' recall. 40.0 40.3 27.1 27.8 20.0 20.8 16.5 17.5 16.1 16.3 35.3 35.7 31.6 31 Additionally, we uniformly sample 500 generations from each evaluation split and conduct expert annotations on the plausibility of each conceptualization to ensure that out-of-domain concepts can be properly evaluated. The experts are asked to determine whether each top-1 generation is indeed a plausible conceptualization or not, such that the top-1 generations' precision is reflected. Thus, current evaluation measures jointly evaluate the top-1 generations' precision and recall, which makes it robust and non-easy to be impacted by repetition problems (Li et al., 2020) . Zero-shot GPT2 and GPT2 fine-tuned on the originally labeled event conceptualizations in D l h are used as baselines. We also study the effect of the threshold T + that selects plausible conceptualized heads, where higher thresholds indicate higher plausibility regarded by CAT. The results are presented in Table 3 . With a relatively high threshold, generators trained on a mixture of pseudo-labeled data by CAT and annotated concepts significantly outperform the baselines in every automated metric. A plausible rate of 93.3% is maximally achieved on the test set, which is 11.8% higher than the baseline. Gradually reducing the threshold also decreases the performance, indicating abstract heads with lower plausibility scores can be of poorer quality. Such results indicate that CAT can produce high-quality event conceptualizations for generative models to learn better conceptualizers without the need to annotate a large number of data.\n\nCommonsense Inference Modeling (COMET).\nThe second component of CAT produces triple-level abstract commonsense knowledge. We evaluate these abstract commonsense triples with a commonsense inference task that generates commonsense tails given heads and relations as inputs, as in COMET (Bosselut et al., 2019) . Following He et al. ( 2022), we apply the same training and evaluation process to the models. The base training data we use are a subset of ATOMIC triples corresponding to those annotated abstract triples in D l t , which contains 17K (3.7%) among the original ATOMIC. We derive abstract commonsense knowledge using CAT from a subset of D u t where the heads correspond to those in the ATOMIC subset to ensure no data leakage, denoted as D u CAT . GPT2 is fine-tuned on the ATOMIC subset, the annotated abstract triples D l t , the abstract knowledge verified by CAT, or their combinations. The commonsense generation results are presented in Table 4 . Similar to COMET (Bosselut et al., 2019) , all models are evaluated on the original ATOMIC's full validation and testing sets. The best result is achieved using a mixture of the ATOMIC subset and abstract triples pseudo-labeled by our framework, with 0.95 as the threshold for selecting plausible triples. This indicates high-quality abstract commonsense triples can indeed provide a more general view of the original commonsense knowledge, thus helping commonsense inference. Additionally, training with our pseudo-labeled examples outperforms training with those annotated triples in AbstractATOMIC, which also validates the effectiveness of our model that leverages a large amount of unlabeled data. To further investigate how conceptual knowledge 5HWULHYDO1XPEHU (YHQW&RQFHSWXDOL]DWLRQ$8& improves commonsense inference modeling, we conduct more empirical analysis in Section 5.4. Additional experiment results with other thresholds and case studies can be found in Appendix C.2.3 and Appendix E, respectively.\n\nNumber of Retrieved Alternative\nConceptualizations and Instantiations.\nWe then study the ablation of bootstrapping different numbers of alternative conceptualizations/instantiations (denoted as #retrieval) in our CAT framework. For simplicity, when tuning the #retrieval for one task, the #retrieval of the other task is fixed at the best value we acquired. We plot the test AUC score with #retrieval from 0 to 11 using BERT-base as the backbone model in Figure 3 . #retrieval=0 refers to training with a simple student-teacher framework without bootstrapping alternative conceptualizations and instantiations. For event conceptualization, the performance generally positively correlates with the number of retrievals, while it starts dropping after 9. A reversed trend is observed for triple conceptualization, where using only two instances achieves the best performance. One possible reason is that in triple conceptualization, the retrieved instances are events and much longer than the retrieved concepts in event conceptualization, and aggregating various alternative events for a triple will cause language models to be less sensitive to the semantics of the original triple (Holtzman et al., 2020) .\n\nThe Effect of Abstract Knowledge\nWe finally study the effect of abstract commonsense knowledge acquired by CAT by studying the semantic overlaps between training and testing data. We sort the test set by the BERTScore (Zhang \n\nConclusion\nIn conclusion, this paper proposes CAT, a semisupervised learning framework for commonsense reasoning, by leveraging the power of abstract commonsense knowledge. By achieving state-of-theart performances in CSKB conceptualization tasks, we remarkably improve modeling commonsense inference, as an important cornerstone of many commonsense reasoning tasks. Our analysis also demonstrates that high-quality abstract commonsense knowledge can benefit commonsense inference modeling by providing more generalizability on hard commonsense knowledge. We hope this work can draw insights toward commonsense reasoning from a conceptualization perspective.\n"}
{"question": "What does the self-consistency approach involve?", "evidence": "  There may be different valid chainof-thoughts for a given question and as a result, large language models distribute probability mass for a certain label across many diverse chain-of-thoughts (Wang et al., 2022b) . ", "options": ["A. Predicting the most likely chain-of-thought and label.", "B. Decoding a single chain-of-thought per example.", "C. Sampling multiple reasoning paths and taking a majority vote.", "D. Training the teacher model on labeled data."], "answer": "C", "content": "\nIntroduction\nEmpirical scaling laws suggest that the accuracy of Large Language Models (LLMs) on benchmark tasks can be improved by increasing model size and pre-training data volume (Hoffmann et al., 2022) . Beyond these training-time improvements, however, an inference-time strategy dubbed \"chain-ofthought\" (CoT) prompting, 1 i.e., eliciting verbalizations of predictive processes via key-phrases like \"Let's think step-by-step\" (Kojima et al., 2022) , can *Work done during an internship at AI2. Figure 1 : Symbolic Chain-of-thought Distillation (SCoTD) applied to a student model, ranging in size from 125M-1.3B parameters. We show that fine-tuning on a (diverse and potentially filtered) corpus of expert chain-of-thought demonstrations from a teacher model is an effective strategy to make smaller models capable of chain-of-thought reasoning.\nsimilarly improve performance, e.g., Suzgun et al. (2022) demonstrate additional performance gains on a hard subset of the BigBench tasks (BIG-bench collaboration, 2022) using chain-of-thought.\nHowever, chain-of-thought prompting has only been shown to be beneficial for models of sufficient scale (e.g., with more than 60B parameters (Wei et al., 2022b) ). In this work, we study whether small language models can be \"taught\" the capacity for chain-of-thought reasoning by larger language models. We adopt a simple strategy, which we call Symbolic Chain-of-thought Distillation (SCoTD): first, we sample chain-of-thought rationales from large language model given (unlabeled) input instances from a dataset; then, we train a smaller language model to predict the sampled rationale CommonsenseQA: Q: The man working in the attic swatted away a bee, but soon the single bee was an entire what? (a) swarm; (b) pack; (c) countryside; (d) soft drink; (e) field of flowers GPT-3: The answer must be related to bees, but also connected with being numerous. Of the above answers, only swarm fits the scenario. So the answer is: (a) OPT-1.3B: The answer must be a swarm of bees. Of the above choices, only soft drink is used to describe a swarm of bees. So the answer is: (c) OPT-1.3B + SCoTD: The answer must be the name of something that has bees. Of the above choices, only swarms have bees. So the answer is: (a) QuaRel: Q: Steve is waiting at the gym ... He starts hitting around a hockey puck with his stick. The gym has a carpeted floor and more friction than the ice rink he goes to. Which surface is rougher? (A) carpet (B) ice rink GPT-3: When something is smoother, it is easier to slide on and easier to pass through. So the carpet is rougher. So the answer is: (A) OPT-1.3B: _________ is rougher than carpet. Thus, the gym floor is rougher than the ice rink. So the answer is: (A) OPT-1.3B + SCoTD: When something is rougher, it has more friction. Thus, the gym has more friction than the ice rink he goes to. So the answer is: (A)\nOpenBookQA: Q: Magnets may be attracted to (a) clay pots (b) roofing nails (c) paper plates (d) plastic cutlery GPT-3: Magnets are attracted to metal objects. These objects include roofing nails. So the answer is: (b) OPT-1.3B: Magnets are attracted to clay pots, roofing nails, paper plates, plastic cutlery. So the answer is: (d) OPT-1.3B + SCoTD: Magnets may be attracted to some metals, but not to clay pots, roofing nails, paper plates or plastic cutlery. So the answer is: (b) Table 1 : Few-shot chain-of-thoughts produced by GPT-3 (code-davinci-002, the teacher model), OPT-1.3B (the un-distilled student model), and OPT-1.3B + SCoTD (ours), the student model trained using Symbolic Chainof-thought Distillation. \u00a73 shows this process significantly improves the task-accuracy of the student model in a variety of settings, and in \u00a73.1.1, human evaluations show that, even when the un-distilled student model happens to get the multiple choice question correct (see QuaRel example), humans tend to prefer OPT-1.3B + SCoTD. and sampled label. This process follows the \"symbolic knowledge distillation\" paradigm as in West et al. (2022) , wherein corpora are sampled from a larger language model to serve as training data for a smaller one.\nWe find that through SCoTD, smaller language models learn to self-rationalize and perform significantly better on 3 commonsense QA tasks compared to learning without rationalizations. This result holds for both supervised and few-shot settings, and across student models of varying scales (125M-1.3B parameters). Performance gains are especially pronounced when applying distilled chain-ofthought models to difficult scenarios like: contrast sets (Gardner et al., 2020) ( \u00a73.4 ; SCoTD significantly outperforms supervised learning on labels) and fully held-out tasks ( \u00a73.5; few-shot SCoTD significantly outperforms in-context learning).\nKey to the success of this process is sampling a relatively large number of rationales per example from the teacher model (e.g., 30 rationales/example) (Figure 2 ). This is different from many prior practices that train with one rationale per example (Camburu et al., 2018; Li et al., 2022a) . In ablation studies, we investigate several competing hypotheses for what are the most important factors within the corpus: we filter the corpus to CoTs that are assigned high probability by GPT-3 vs. filtering to CoTs that are diverse vs. filtering to CoTs that explain more open-ended input instances.\nWhile diversity and high probability are reasonable filters that on average perform well, the \"null hypothesis\" of random downsampling performs well, suggesting that the sheer volume of the rationales is also a key contributing factor.\nWe will release code and the corpus of sampled chain-of-thoughts at https://github.com/ allenai/cot_distillation.\n\nSymbolic Chain-of-Thought Distillation\nOur primary goal is to improve the accuracy of a (relatively small) student language model S on a target classification 2 task D Test = {(x i , y i )}. 3 We assume access to 1) (an unlabeled) training set D Train = {(x i )}; and 2) a large teacher language model T (e.g., GPT-3 (Brown et al., 2020) ), capable of generating chain-of-thoughts in a few-shot fashion.\nOur first step is to curate a set of labeled chainof-thoughts to serve as few-shot Prompts for T . For each target task, we sample a small number (e.g., 10) of examples x i from D Train , provide a gold classification label y i , and manually author a chain-of-thought z i for each to form the prompt set P = {(x i , y i , z i )} 4 . Then, for each x i in D Train , we sample N chainof-thoughts zi along with the resulting prediction \u1ef9i from the teacher model, i.e.,\n(\u1ef9 k i , zk i ) \u223c N T (y i , z i |x i , P).\nThe result of this sampling is a corpus\nC = {(x i , {(\u1ef9 k i , zk i )} N k=1\n)}, which contain teacherpredicted chain-of-thoughts/labels. Depending on the experimental setting (details in \u00a7 3), we sometimes filter the entries of C, e.g., in the fully supervised case where D Train instances have associated labels, we discard samples for which the sample the teacher model predicted an incorrect label. Next, we train the student model using the standard language modeling loss, i.e., we maximize\nE (x,\u1ef9,z) \u223c C [S(\u1ef9, z|x)].\nAfter fine-tuning the student model on the corpus sampled from the teacher, to evaluate the model on a test instance (x test , y test ) from the target task, we decode both a chain-of-thought ztest and a predicted label \u1ef9test from the student and evaluate \u1ef9test versus the true label y test . We consider two strategies for decoding. (1) Predict the most likely chain-of-thought and the label ztest , \u1ef9test = argmax z,y S(z, y|x test ). This can be approximated by greedy decoding or beam search. (2) There may be different valid chainof-thoughts for a given question and as a result, large language models distribute probability mass for a certain label across many diverse chain-of-thoughts (Wang et al., 2022b) . Thus, it is beneficial to marginalize out the reasoning paths to find the most consistent answer: \u1ef9test = argmax y E z\u223cS(z|xtest) S(y|z, x test ). This can be approximated by sampling multiple reasoning paths and take a majority vote among the predicted answers, dubbed \"self-consistency\" (Wang et al., 2022b) . We experiment with both approaches and conduct a discussion in \u00a73.2.\n\nExperiments\nWe evaluate primarily on 3 target tasks: 1) Com-monsenseQA (CSQA) (Talmor et al., 2019) , a 5way multi-choice dataset; 2) OpenBookQA (Mihaylov et al., 2018) , and 3) QuaRel (Tafjord et al., 2019) . While any model capable of few-shot chain-of-thought could be substituted, we use the thought prompts from prior work (Wei et al., 2022b; Wang et al., 2022b) code-davinci-002 version of GPT-3 5 (Brown et al., 2020) as our teacher model T . We use OPT (Zhang et al., 2022) as our student model S. Our standard student model is OPT-1.3B (though we explore a range of student model sizes in \u00a73.3).\nWe sample from GPT-3 with a temperature of T = 1.0. For each training example, we sample N = 30 rationales. OPT is fine-tuned with a batch size of 32 and a learning rate of 2 \u00d7 10 \u22125 . We use HuggingFace transformers (Wolf et al., 2019) , Pytorch (Paszke et al., 2019) , and Accelerate 6 for the implementation. Main experiments can be reproduced on one GPU with 48GB of memory.\n\nResults in Default SCoTD Setting\nWe first consider both a few-shot learning setting and a supervised setting. For the few-shot setting, the only labeled examples available to our teacher/student models are contained in the prompt set P (but we use the unlabeled examples and teacher-generated chain-of-thoughts/labels for training). 7 We also consider the supervised setting, where we assume access to labels in D Train . Supervised SCoTD involves simply discarding the samples within C that do not have the correct label prior to fine-tuning the student: for Common-\nCommonsenseQA QuaRel OpenBookQA\nFigure 2 : For three commonsense QA tasks, accuracy (y-axis) improves significantly as the student is trained on more chain-of-thoughts sampled from the teacher (x-axis). Oversampling chain-of-thoughts is sometimes required to improve student performance beyond the supervised label-only baseline, e.g., as in OpenbookQA.\nsenseQA, OpenBookQA, and QuaRel, this results in discarding 40.4%, 45.0%, 34.2% of chain-ofthoughts. For the few-shot setting, we decode with the self-consistency approach; for the supervised setting, we decode with greedy decoding (introduced in \u00a7 2; see an discussion in \u00a7 3.2). We compare SCoTD to 2 baselines: 1) Label-Only, the student is fine-tuned on just the label (in the few-shot setting, the label comes from the teacher and could be wrong; in the supervised setting, we use the gold label), instead of also with CoT; 2) Greedy-CoT, we decode a single-CoT per example (instead of N = 30 samples) from T for each training example instead of sampling. For additional reference, Table 2 (a) reports the performance of the student (and teacher) in a variety of few-shot settings prior to applying any distillation: No CoT = few shot prompting with labeled instances from P but no z i , Greedy and Self-Consistency are prompting with CoT but with different decoding strategies ( \u00a7 2).\nTable 2 (b) gives the performance of the student model after distillation in the supervised and fewshot settings. In all cases, distillation significantly improves the student model, and in all-but-one case, learning with CoT outperforms the label-only distillation baseline. While the student model initially fails to perform CoT through prompting (Table 2 (a)) it learns to do so through distillation.\nThe number of samples. In our default setting, to serve as our distillation corpus C, we sample N = 30 rationales from the teacher T for each (unlabelled) training instance. Figure 2 shows the performance of the student model when it is trained on corpora with fewer sampled CoT per instance: results suggest that learning with multiple sampled (albeit nosier) rationales/chain-of-thoughts per example is more beneficial than learning with one (most likely) rationale. Will more rationales bring more performance improvement? We sampled more rationales from GPT-3 to train the student model; however, this does not bring more performance gains. When N = 50, the performance is similar to N = 30: the model achieves 67.0 in accuracy on OpenBookQA (v.s. 67.0), 67.2 on CommonsenseQA (v.s. 67.0), 84.9 on QuaRel (v.s. 83.8).\n\nHuman Evaluations\nWhile SCoTD improves task accuracy significantly, we additionally conduct human evaluations to assess the generated chain-of-thoughts themselves (see Table 1 for samples). We sample instances from the CommonsenseQA, OpenBookQA, and QuaRel validation sets (300 instances per dataset), and conduct head-to-head human evaluations 8 to assess:\nQ1: Does SCoTD result in higher-quality chainof-thoughts? Test: OPT-1.3B versus OPT-1.3B + SCoTD. Result: Yes. We assess this hypothesis on two subsets of instances: 1) a pure random sample (N=900); and 2) a set of instances for which both models eventually predicted the correct label (N=654). The second setting focuses more closely on the chain-of-thoughts themselves rather than the predictive accuracy of the model. SCoTD is superior in both settings: for the random sample setting, SCoTD won in 59% of cases (p<.001), whereas in the correctness controlled setting, SCoTD won in 61% of cases (p<.001). Results hold with p < .05 for each QA dataset individually.\nQ2: Does a SCoTD student surpass the much larger teacher? Test: OPT-1.3B + SCoTD versus text-davinci-002. While the task accuracy of the teacher is still higher in most cases, the studentgenerated CoT are comparable. 9 We again evaluate on: 1) a pure random sample (N=900); and 2) a correctness-controlled setting (N=659). The 100x smaller SCoTD's generations are competitive in both cases; we can't reject the null hypothesis of the crowd having equal preferences (OPT-1.3B + SCoTD wins in 47% and 51% of cases respectively, p > .01). Results hold for each dataset individually, as well.\n\nSelf-Consistency for the Student\nWang et al. (2022b) find that, for chain-of-thought prompted models, taking a majority vote over a large set of sample of predicted labels (resulting from a diverse range of CoTs) can improve performance. Our results regarding the effectiveness of sampling N = 30 rationales from the teacher during SCoTD are similar-in-spirit: i.e., we also show performance gains from sampling multiple rationalization chains per instance.\n9 See \u00a76 for more discussion about the disparity between CoT-quality and task accuracy. A natural question is, does the student model S exhibit the same phenomenon, i.e., can we sample multiple chain-of-thoughts from it and take a majority vote? We find that the student model can benefit from \"self-consistency,\" but not in all cases. In Table 3 , we report the performance with/without self-consistency (majority vote among 30 sampled reasoning paths with a temperature of 0.7). When training with filtered CoTs (Table 3 (a) bottom rows) or training with few CoTs per example (Table 3 (b), when #CoTs/Example is small), the student model does not benefit from self-consistency. Only when we train with multiple rationales per example without filtering (the few-shot setting), self-consistency is beneficial on CSQA and Open-BookQA. Overall, the results show that student models benefit from being shown a diverse/noisy set of rationales, and that self-consistency can be effectively applied after distillation.\n\nSCoTD across Model and Dataset Sizes\nWe also verify the effectiveness of SCoTD across model and dataset sizes; in these experiments, we consider the supervised setting. Data scaling. Figure 3 shows the effect of varying the size of D Train (for simplicity, we show only performance on CSQA as an example). Learning with CoTs is beneficial under all data scales. Interestingly, SCoTD, trained with access to only 40% of the labelled data, can surpass the direct supervised label-only model with 100% of the labelled corpus; this result aligns with the argument in Zaidan et al. (2007) -providing more explanations from the teacher model could be more beneficial than providing more labels.\nStudent model size scaling. Figure 4 presents results when varying the size of the student model from 125M to 1.3B parameters for CSQA. For all model three model sizes, SCoTD outperforms the standard supervised fine-tuning baseline (Label Only). Sampling multiple rationales per input instance is an effective strategy for all model sizes.\n\nSCoTD on Challenging Contrast Sets\nCan learning with explanations help generalization, as hypothesized by (Zaidan et al., 2007) ? As a preliminary study, we show that SCoTD enables better generalization to contrast sets. Contrast sets (Gardner et al., 2020) are proposed to evaluate a model's robustness to perturbations around the decision boundary, by asking annotators to modify the original test instances in small but meaningful ways that (typically) change the gold label.\nWe experiment on the IMDB (Maas et al., 2011 ) sentiment analysis task in the supervised setting; we consider the corresponding contrast set of IMDB proposed by Gardner et al. (2020) . We train two models on the training set of IMDB: Label-Only and SCoTD. For efficiency, we sub-sample 100K examples from the training set of IMDB and truncate input sequences to 700 tokens. As shown in Figure 5 , while both models with/without SCoTD achieve high performance on the original IMDB test set (96.1% v.s. 95.5%, with the Label-Only model performing slightly better), the model with SCoTD achieves significantly higher performance on the contrast set: 92.0% vs. 81.6%. This result supports the hypothesis of (Zaidan et al., 2007) ; that explanations can support more robust generalization.\n\nSCoTD on Unseen, Out-of-domain Tasks\nLarge language models can perform few-shot, incontext learning with chain-of-thought prompting, i.e., generating reasonable chain-of-thoughts on unseen tasks with a few demonstrations (Suzgun et al., 2022) . We conduct a preliminary experiment, inspired by Min et al. (2021) 's MetaICL, to test whether student models trained with SCoTD acquire the same ability. We train a supervised SCoTD model on ANLI, CommonsenseQA, and OpenBookQA, and evaluate it on SST-2 (Socher et al., 2013) , a sentiment analysis task.\nThe SCoTD model achieves a few-shot accuracy of 79.6% on the validation set (an example prediction is shown in Figure 6 ). 10 Compared to a baseline model that learns with no CoT(i.e., a re-implementation of MetaICL trained on 3 source tasks); the baseline fails to recognize the input/output format of the new task and predicts answers out of the desired label set. It achieves (an effective) 0% accuracy on SST-2. This suggests the potential of including CoTs during instruction/incontext tuning (Wei et al., 2022a; Min et al., 2021) .\n\nWhat Factors are Important for\nDistillation?\nAn important factor underlying the performance gains highlighted in \u00a73 was the number of chain-ofthoughts we sampled from the teacher model perinstance (more samples = better; Figure 2 ). Here we ask: is data volume the key contributing factor to the performance improvement? Or, are specific aspects of chain-of-thought samples key for the performance improvements?\nWe design several filters to identify potentially important examples/CoTs among the correct rationales. We apply designed filters (to be introduced) to C \u2032 , the corpus sampled from the teacher (with wrong CoTs dropped), that operationalize different hypotheses about what factors are important to distill. We control for dataset size when filtering, i.e., \n\nLabel Only SCoTD\nThe author said that they love this movie and they are never tired of watching it.\nThey say that the movie is wonderful and they are grateful to see such an outstanding picture. So the answer is: positive\nThis was a wonderfully clever and entertaining movie that I shall never tire of watching many, many times\u2026 I can only be grateful when I see such an outstanding picture for most of the motion pictures made more\nThis was a wonderfully thick as two short planks and soul-destroying movie that I shall never watch any number of times\u2026 I can only be sorry when I see such an abysmal picture just as most of the motion pictures \u2026\n\nIMDB Dataset\nThe author said that the movie was 'thick as two short planks and souldestroying', implying that the movie is bad. So the answer is: negative all filtered corpora have the same number of training CoTs. We downsample with a budget of 5 CoT per instance on average 11 . Then, we train the same student model on each of the filtered corpora, and compare on downstream tasks. If a student model trained on filtered corpus A tends to outperform the student model trained on filtered corpus B, then we argue that the property that produced corpus A is more important. The hypotheses we consider are:\nNull hypothesis: data volume. As a null hypothesis, we randomly sub-sample 5 CoT per instance; this filter operationalizes the assumption that an arbitrary set of samples is sufficient.\n\nDiversity.\nFor each instance, we compute S-BERT (Reimers and Gurevych, 2019) embed-11 In rare cases, we may end up with less as there are less than 5 correct CoTs for the instance. dings 12 of each of the chain-of-thoughts, and cluster the resulting embeddings using hierarchical clustering into k = 5 clusters. Then, we randomly sample a single instance from each cluster: the resulting sample covers all clusters, and thus represents a diverse+representative sample.\nTeacher likelihood. For each instance, we keep the 5 CoT samples with the highest per-token loglikelihood according to the teacher model.\nOpen-endedness. Some instances in each dataset lead to a broader range of chain-of-thought samples than others. For example, on CommonsenseQA, the question \"What form of alcohol is made from grapes?\" leads to a narrower range of rationalizations vs. \"Why might someone purposefully be going into trance?\"\nWe hypothesize that openended instances could benefit from relatively more sampled rationales. We sort instances into quintiles based on the unique bi-grams in their corresponding 30 CoTs; for high-ranking instances (more unique CoT bi-grams, like the \"trance\" example above), we keep more rationales and for low-ranking instances, we keep less rationales. We keep 1, 3, 5, 7, 9 rationales for instances of different bins (thus controlling for the total number of CoT).\nResults Figure 7 reports the accuracy of the student model when fine-tuned on the different subsampled corpora for the three tasks we consider. Overall, random subsampling is a strong baseline, but, we see some evidence that diversity among the rationales is important. None of the models trained on the sub-sampled data could approach the model trained on the full 30x/instance CoT set. This suggests that the sheer volume of the CoTs is a key driving force for the performance improvement.\n\nRelated Work\nChain-of-thought prompting. As an extension of few-shot prompting (Brown et al., 2020) Learning with explanations. Hase and Bansal (2022) discuss how explanations can serve as inputs (Talmor et al., 2020) , targets (Hendricks et al., 2016; Fidler et al., 2017; Camburu et al., 2018; Zhou et al., 2020; Narang et al., 2020; Kayser et al., 2021; Wiegreffe et al., 2022) , and priors (Zhang et al., 2016; Srivastava et al., 2018) for machine learning models. Chain-of-thought extends earlier efforts which treat explanations as intermediate structures, generated at inference time (Rajani et al., 2019) . Most related to our work is Li et al. (2022a) , who do also learn with GPT-3 generated explanations; we show multiple samples improve significantly over their single-sample method, and also use chain-of-thought prompting at inference time vs. predicting explanations+labels via independent multitasking.\nKnowledge distillation. Recent work, inspired by Knowledge Distillation (Hinton et al., 2015) , has considered symbolic knowledge distillation, (West et al., 2022) , i.e., instead of distilling from soft representations like logits, large language model serve as training data generators (Xiong et al., 2019; Petroni et al., 2019; Schick and Sch\u00fctze, 2021; West et al., 2022; Liu et al., 2022; Meng et al., 2022; Bhagavatula et al., 2022) ; this paper continues this line of work.\nContemporaneous work. There are several contemporaneous papers: Huang et al. (2022) , Magister et al. (2022), and Ho et al. (2022) all show that smaller models can benefit from large models' chains of thought. We contributes beyond these by: 1) showing that sampling a large number of chain-of-thoughts is paramount; 2) exploring transfer performance to challenge sets/unseen tasks; and 3) analysis that address what factors are important in the teacher corpus.\n\nConclusion\nWe demonstrate the effectiveness of Symbolic Chain-of-thought Distillation (SCoTD): a method that enables smaller language models to effectively use chain-of-thought-style reasoning. We demonstrate the method's effectiveness across several downstream tasks, different student model sizes, different levels of supervision, and in difficult settings (challenge sets, unseen tasks). Our ablations shed light on what factors are particularly important to distill in these chain-of-thoughts. Our concrete recommendations are: 1) sampling multiple and diverse CoTs for each input instance, and 2) performing self-consistency when the teacher CoTs are noisy. Several promising av-enues for future work include:\n1. Exploring SCoTD for generation tasks in addition to classification tasks;\n2. Scaling up the number of source tasks in \u00a7 3.5 to generalize to more tasks;\n3. Using the down-sampling setup introduced in \u00a74 to explore additional hypotheses about what other factors may be of importance in CoTs.\n"}
{"question": "Which method is used to aggregate statements made by political actors in this study?", "evidence": "  Instead, we identify indicator words from statements for information aggregation.  We sort them by TFIDF scores and keep indicators with the highest values to form an indicator sequence.  ", "options": ["A. Full sentence combination", "B. Random selection of sentences", "C. Indicator words based on TFIDF scores", "D. Summarizing the statements"], "answer": "C", "content": "\nIntroduction\nPolitical actors are shaping our attitudes, opinions, and decisions toward public issues. For instance, on social platforms, politicians can select and emphasize certain aspects of content to bias the discussion, through which they can derive an opinion climate from user engagement and acquire direct feedback from potential voters and opinion leaders (Bene, 2017; Heiss et al., 2019) . Political actor modeling is essential for quantitative political science and has applications in various downstream tasks such as roll call vote prediction (Yang et al., 2020) , frame detection (Johnson et al., 2017) and bias detection (Baly et al., 2020) .\nData-driven approaches utilize different kinds of information to profile political actors, including public statements, legislative behaviors and social Figure 1 : An illustration of political actors. They not only participate in legislative activities, but also form relationships with others, and convey opinions through tweets, speeches and etc. We propose to represent political actors based on their statements and learn the mapping from language to their representations using social networks and behaviors as self-constructed supervision.\nnetworks (Figure 1 ). Early research analyzes roll call data to estimate the ideology of political actors. Ideal point model (Clinton et al., 2004 ) is one of the most widely used approaches for votebased analysis that reveals how cleavages between legislators reflect partisan affiliation. Researchers further incorporate texts of bills to enhance the ideal point model (Gerrish and Blei, 2011, 2012; Kraft et al., 2016) and develop multidimensional vectors to replace one-dimension points. Recently, more abundant information has been considered to learn effective representations for political actors, such as co-sponsorship network (Yang et al., 2020) , relations of contributors (Davoodi et al., 2020) , stakeholders (Davoodi et al., 2022) , mention in documents (Pujari and Goldwasser, 2021) , and expert knowledge (Feng et al., 2021 (Feng et al., , 2022)) .\nGenerally speaking, previous research aims to learn representations for a certain group of political actors using supervision from specific downstream tasks as objectives. Although they report positive results on target tasks, their models lack generalization ability in two aspects. (1) Representations are learned on labeled data from specific tasks, e.g., state-level vote prediction, therefore they cannot be easily transferred to other tasks or scenarios. (2) The model is limited to the training setting and can not be adapted to dynamic social contexts. In other words, it's hard for the model to estimate new legislators, non-voting candidates and other political actors unseen.\nRecently, large-scale pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019b; Brown et al., 2020) have demonstrated a strong generalization ability and achieved excellent performance in many language modeling tasks. Motivated by PLMs, we explore representing political actors based on their statements and propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM) 1 . We employ a two-stage training procedure following the fashion of PLMs. Firstly, we pre-train our model to learn the mapping from statements to actor representation. We propose a multigranular method to represent political actors based on language, and information of political scenarios is further injected into our model via proposed structure-aware contrastive learning and behaviordriven contrastive learning tasks. Secondly, we fine-tune the model for downstream tasks using the corresponding supervised objectives.\nUPPAM is novel in three points. (1) We learn the mapping from statements to the representation of political actors, instead of directly learning actor representations. By doing so, the mapping parameters can be transferred to any downstream tasks easily, learning representations for unseen political actors based on their statements. (2) We propose several self-training tasks to inject general knowledge in the political scenarios into mapping parameters in the pre-training stage. (3) We propose a multigranular actor representation model, that can capture nuances of both general ideology and specific preferences between different political actors. We evaluate our approach on three types of tasks in quantitative political science, i.e., profile of actors, prediction of behaviors and analysis of languages. UPPAM outperforms general PLMs and other political domain-specific PLMs on these tasks. Our task-agnostic model also achieved competitive results compared to the task-specific models that employ architectures crafted for the 1 We have made our code publicly available at https:// github.com/xymou/UPPAM. vote prediction task. Further analysis shows the effectiveness and robustness of UPPAM in few-shot settings and different aggregation settings.\n\nMultigranular Actor Representation\nPolitical actors manifest themselves in political activities in multiple granularities. On the one hand, they hold a general ideology or bias, which is long-term and stable. On the other hand, when discussing or taking action on different issues, they hold specific positions (Gerrish and Blei, 2012) , which are the result of long-term bias and shorttime interests (Spell et al., 2020) . Based on this, we propose to represent political actors in two granularities to model both broad ideology and specific preferences for various downstream scenarios.\n\nGeneral and Specific Statements Collection\nIn practice, we use all statements a political actor has posted to get his general representation, characterizing the broad political leaning. Furthermore, issue-related content is adopted to help capture specific attitudes. Concretely, we use a handcrafted information retriever (see more details in Appendix A.2), to collect statements related to the queried policy area as input to encode the specific representation.\nStatements Aggregator Since a political actor can post thousands of statements, the first challenge is how to aggregate one's statements to get his representation. It is too expensive in time and computation cost to combine full sentences. Instead, we identify indicator words from statements for information aggregation. According to the framing theory (Entman, 1993) , entities and subjective content an author uses can implicitly reflect his political leaning. Following this, we identify entities, frame and sentiment words as indicators. We sort them by TFIDF (Jones, 1972) scores and keep indicators with the highest values to form an indicator sequence. In this case, for each political actor, we can get two kinds of indicator sequences, given a query about policy area j:\nS g i = w g 1 , w g 2 , ...w g N (1) S p j i = w p j 1 , w p j 2 , ...w p j M (2)\nwhere S g i is calculated from all the statements made by political actor i, S related to policy area j, and we reserve top N and M indicators with highest TFIDF value, where N and M are pre-defined hyper-parameters.\nIn subsequent pre-training and downstream tasks, we use general sequences as input when the goal is to profile the characters broadly, e.g., estimating ideology. And we input both sequences and average the representation when specific attitudes are required in tasks, as shown in Figure 2 . Note that even if the issue-related content can not be retrieved, we can use the general sequence as a substitute, to ensure input compatibility.\n\nMultidimensional Pre-training for Political Actor Modeling\nTo inject general knowledge of the political landscape into the mapping from statements to representation, we construct self-supervised tasks based on structural and behavioral information.\n\nStructure-aware Contrastive Learning (SCL)\nIn terms of structural information, we mainly focus on the relationship formed between political actors. Previous studies have revealed that homophily exists in political communities, where people with similar ideologies form a link with each other (Barber\u00e1, 2015) . We use two parts of links, namely party affiliation and co-sponsorship in voting. We treat party affiliation as a coarse relationship and cosponsorship as a fine relationship respectively. By doing this, the model can further capture nuances across parties as well as inside the same party.\nParty Affiliation Link We compare statements of legislators from different parties. We choose a legislator as the anchor, and then take another legislator with the same party affiliation as the positive sample, while those from the opposite party are regarded as negative samples. By comparing general statement sequences of legislators from different parties, the model can learn the differences in the languages of different ideologies.\nCo-sponsorship Link In the legislative process, a bill is initialized by a sponsor and several cosponsors. We assume that the more two legislators collaborate, the more they are alike since they reach agreements on many occasions (Yang et al., 2020; Mou et al., 2021) . Given an anchor legislator, other legislators are divided into three categories based on the number of times they co-sponsored with the anchor legislator: G 1 (the co-sponsorship times are above the average); G 2 (the co-sponsorship times are below the average); G 3 (they have never cosponsored). And we further sample positive and negative samples with the rule of\nG 1 < G 2 < G 3 .\nBased on the triplets constructed in the above two ways, the structure-aware contrastive objective is formulated as follows:\nLSCL = t\u2208T SCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4SCL + (3)\nwhere T SCL is the set of legislator triplets, t (a) , t (p) and t (n) are actor representation encoded by general sequences of anchor, positive and negative sample in triplet t, \u03b4 SCL is a hyperparameter and\n[\u2022] + is max(\u2022, 0).\nNotably, this task endows the model to capture general ideology of speakers from their languages.\n\nBehavior-driven Contrastive Learning (BCL)\nWhen it comes to behavioral information, we pay attention to the most common and important actions, i.e., voting. Specifically, we sample triplets consisting of an anchor bill and a pair of legislators, where the positive legislator p votes yea on the given bill and the negative one n votes nay.\nDifferent from the ideology cleavages modeled in Sec 2.2.1, the divergence of specific preferences is supposed to be reflected in the languages here. Thus, for each legislator, we extract statements about the policy area of the anchor bill as the specific sequence, input with the general sequence, as we mentioned in Sec 2.1. In this way, the behaviordriven contrastive objective is as follows:\nLBCL = t\u2208T BCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4BCL + (4)\nwhere T BCL contains all vote triplets, and \u03b4 BCL is a hyperparameter. t (a) is the bill representation, t (p) and t (n) are the average of representation of the general sequence and the specific sequence, for the positive and negative legislators respectively.\nIt's noticeable that this pattern is not limited to the roll-call vote scenarios, instead, it can be applied to model the preferences towards any bills, events, or targets with a text description.\n3 Pre-training Process\n\nLanguage Model Co-training\nAs mentioned in Sec 2.2.2, modeling political actors in political scenarios inevitably requires encoding textual information of the bills and issues they interact with, e.g., Equation 4. Meanwhile, it is important to understand their opinions in a single discourse without context. Thus, we incorporate additional modules to model political texts. Specifically, as shown in Figure 2 , we have two FFN layers in parallel in each transformer layer, to handle text and actor sequences separately. Given a sequence of input x = {x 1 , ..., x n }, the model first performs multi-head self-attention and then the corresponding module FNN k obtains the required representation:\nh k = FNN k ( Self-Attention ({x 1 , . . . , x n }))\n(5) where k \u2208 {0, 1} indicates the modules of actor and text respectively.\nWe adopt a masked language model objective to pre-train the language model. As mentioned before, political bias and framing effect are often reflected in the selection and mention of specific entities, subjective content, and emphasized frames. Thus, we take a masking strategy that upsamples entity tokens, sentiment words (Wilson et al., 2005) and frame indicators (Roy and Goldwasser, 2020) to be masked for the MLM objectives, with a 30% probability. More details can be found in Appendix B.\n\nOverall Pre-training\nSince the indicator sequence is not a normal sentence, we don't train the MLM task with contrastive learning together. Instead, the pre-training process is divided into two stages. In the first stage, we adopt the MLM task on the original statement sentences and activate text modules, to urge the model to understand the political text. Then, based on this checkpoint, we further conduct the multidimensional pre-training for political actor modeling by combining the objectives:\nEQUATION\n)\nwhere \u03b1 is hyperparameters.\n\nExperiment Setup\nWe fine-tune our model on different kinds of downstream tasks in quantitative political science. We then compare it with prior general PLMs and political domain-specific PLMs.\n\nPre-training Datasets\nCompared to other political actors, congress legislators are more typical and they generate massive content every day. Thus, we start with legislators to construct our pre-training datasets. Overall, we get 887 legislators and delete the meaningless tweets including self-promotion advertisements, notifications, etc., using regular expressions. Finally, the cleaned data contains 2,020,938 tweets, covering discussions of events in various areas. We keep 10K held-out tweets as the validation set.\n\nLegislative Context\nWe collect the party affiliation, sponsorship lists of bills, bills, and corresponding voting records from VoteView 2 and the website of U.S. Congress 3 . Each bill belongs to a specific policy area and has textual information of title and description. We get bills of 112th and 113th for pre-training and reserve those of 114th and 115th for the formulation of downstream tasks. In the pre-training stage, 1,045 bills and 375,440 voting records are involved.\nTo correlate legislators' votes to their statements in the related policy area, we filtered each legislator's tweets in each policy area by the handcrafted information retriever mentioned in Sec 2.1. We finally acquire 1,142,587 tweets, and the details can be found in Appendix A.2. The distribution of the policy agenda of bills and the percentage of legislators whose related tweets can be retrieved in each policy area are shown in Figure 3a and Figure 3b . Over 90% of legislators can be retrieved with relevant statements in most policy areas.\n\nImplementation Details\nUPPAM is produced via continued pre-training on RoBERTa-base model (Liu et al., 2019b) , where we add parallel FFN modules in each transformer layer with the same initialization as the original one. In the first stage, the model is trained on tweets, to minimize the MLM loss with AdamW (Loshchilov and Hutter, 2018) optimizer. In the second stage, the model is further trained on indicator sequences and bill texts, to minimize the L CL . We evaluate the model every 200 training steps on the validation set and keep the best checkpoint. The pre-training procedure takes around 96 hours on 4 Tesla V100-SXM2 GPUs. More details and hyperparameters can be found in Appendix B.\n\nDownstream Tasks and Datasets\nWe evaluate the models on three types of tasks, namely actor profiling, behavior prediction and language analysis. Notably, datasets include not only congress legislators but also other political actors such as journalists, news media, and even anonymous users, to validate the model's generalization capability.\n\nActor Profiling\nThis type of task can be formulated as a user-level classification task, where we aggregate multiple statements to predict the speaker's attribute.\nIdeology Detection is the main task to profile actors broadly, aiming to predict political leaning. Models are evaluated on the following datasets.\n\u2022 CongS (Gentzkow et al., 2018) collects speeches from US congressional records. \u2022 celeb (Wojcieszak et al., 2022) contains tweets of celebrities (journalists, politicians and media). We convert the ideology scores into labels according to the signs. \u2022 Reddit (Kitchener et al., 2022) collects comments of common users in non-political subreddits, and labels the users with ideology in the economic dimension. \u2022 PEM (Xiao et al., 2022) collects tweets of legislators, news outlets and cabinet of President Obama and President Trump. \u2022 TIMME (Xiao et al., 2020) includes Twitter accounts with location information and selfidentified political-polarity labels. These accounts are not run by politicians.\n\nBehavior Prediction\nThis type of task can be regarded as a relation prediction task, where we predict a political actor's attitude or action towards a given target with a piece of text description.\nVote Prediction tasks aim to predict votes of legislators towards bills with stances of yea or nay. We follow two configurations in (Mou et al., 2021) . \u2022 VoteIn refers to the in-session setup, where we randomly split the bills in the same congress session, i.e., the 114th session. \u2022 VoteOut refers to the more challenging outof-session setup, where we use data in the 114th session for training and validation while testing on the 115th session.\nGrade Prediction tasks are designed as classification tasks for ratings in a certain issue, given a politician's statements and background description of the given issue. We include datasets as follows:\n\u2022 NRA Grades (Pujari and Goldwasser, 2021) provides politicians' grades {A, B, C, D & F} assigned by National Rifle Association and their statements on guns, as well as background information of guns from ontheissues.org. \u2022 LCV Grades (Pujari and Goldwasser, 2021) is similar to NRA Grades, but it's about the scores in the environment area.\n\nLanguage Analysis\nIn addition to the overall characterization of political actors, we also test models' ability to understand individual discourses. We apply stance detection and frame detection as downstream tasks, which can be formulated as sentence-level classification tasks.\nStance detection tasks aim to predict one's stance towards a given target. The tasks take a 3-way label (favor, against, and neutral) or binary label (favor, against). We test on these datasets.\n\u2022 poldeb (Somasundaran and Wiebe, 2010) provides opinion-target pairs from several debating platforms covering different domains. \u2022 election (Kawintiranon and Singh, 2021) contains tweets related to the 2020 US presidential election, expressing stances towards President Trump and Biden.\n\u2022 SEval (Mohammad et al., 2016 ) is a shared task to detect stances in public tweets.\nFrame detection tasks aim to detect which frame dimensions are employed in a piece of text. It's a multi-label classification task with a pre-defined label set. We test on these datasets.\n\u2022 twitter (Johnson et al., 2017) annotates tweets of politicians with 17 general frames. \u2022 gvfc (Liu et al., 2019a) collects news headlines about gun violence, and annotates them with 9 issue-specific frame dimensions. \u2022 immi (Mendelsohn et al., 2021) collects immigration-related tweets posted by the public, annotated with 14 general frames.\n5 Experiment Results\n\nMain Results\nThe compared general PLMs include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) . We also compare our model with available PLMs for social science texts-SsciBERT (Shen et al., 2022) , and for the political domain: POLI-TICS (Liu et al., 2022) and PoliBERTweet (Kawintiranon and Singh, 2022). We fine-tune all the PLMs in the same settings, and we select the best fine-tuned model on validation sets using macro F1.\nThe implementation details and hyperparameters can be found in Appendix C.2. Table 1 presents macro F1 scores on the downstream tasks.\nActor Profiling Our model shows superior performance on various political actor modeling tasks.\nResults of ideology detection tasks indicate that our model can not only characterize the ideology of legislators but is also good at modeling other roles like journalists in the celeb dataset and cabinet in the PEM dataset, demonstrating the transferability of using languages to represent characters. The reason for not performing best on the Reddit dataset may be the gap between the expression habits of common users and that of politicians. Nevertheless, we still outperform the majority of baselines.\nBehavior Prediction All the models show excellent performance on vote prediction and grade prediction tasks, using languages to represent political actors. It indicates that it's a feasible scheme to infer political actors' behaviors from their languages. Among all the PLMs, our model is the best. We attribute the performance gain to our proposed behavior-driven pre-training task.\nLanguage Analysis Moreover, our model also achieves competitive performance on tasks of analyzing individual text including stance detection and frame detection, indicating that the ability to understand political languages is preserved while the model is learning to profile actors, benefiting from the co-training process in Sec 3.1.\n\nAblation Study\nTo explore the effects of different components, we conduct ablation studies and results are reported in Table 2 . Removing SCL or BCL mainly hurts the performance of actor profiling tasks. Removing the text modules results in the most loss in language analysis tasks, especially the frame detection task. This demonstrates the necessity of separate modules to guarantee the ability to model political text.\n\nFurther Analysis\nFew-shot Learning We fine-tune PLMs on different numbers of samples. Figure 4 tasks. Benefiting from the pre-training stages, our model can better capture ideology and preference differences, even when using only 16 samples.\nCompare with Task-specific Models Taking the vote prediction task as an example, we compare our model with previous task-specific models, where particular meta-data and structural information is crafted for the task. Table 3 shows that UPPAM achieves competitive results, indicating that we can deduce political actors' votes from languages. Additionally, our method can be used to analyze nonvoting actors, relieving the cold-start problem.\n\nMethods of Statements Aggregation\nWe show the impact of statements aggregation methods on ideology detection in fine-tuning. We mainly compare our method with concat (Table 4 ) and mean pooling (Table 5 ). concat means to concatenate each speaker's political statements into a flat sequence and then encode it. mean pooling encodes each sentence individually and uses the averaged representation as the final representation. We further discuss the impact of the number of aggregated sentences in Appendix C.2.2. Results illustrate that our model shows robustness in several settings and our aggregator is more effective and efficient.\n\nVisualization\nGeneral Ideology We perform Principle Component Analysis (PCA) on political actor representation generated by our model for the CongS dataset. As shown in Figure 5a , our method can well separate politicians of different ideologies.\nIndividual Specific Preferences We also visualize specific representation in different policy areas for individuals. Figure 5b shows the representation in several highly-discussed policy areas, learned by different models from the tweets of Rep. Rooney. We can observe that Rep. Rooney behaves conservatively in immigration, but expresses left-wing views on environment (Pujari and Goldwasser, 2021) . While most of our baselines fail to capture this nuance, UPPAM can well compare the relative polarity in each area.\n\nRelated Work\nPolitical Actor Modeling focuses on modeling attributes and behaviors of political actors, with special attention to estimating the ideology. Because of the publicity and typicality, politicians like legislators have been the research subject for most work.\nThe most widely used approach to estimate the ideology of legislators is ideal point model (Clinton et al., 2004 ) that represents legislators and bills as points in a one-dimension latent space from the rollcall data. After that, researchers further incorporate texts of bills (Gerrish and Blei, 2011; Gu et al., 2014) to enhance the model, solving the problem of prediction on new bills. Some embedding methods are also proposed to promote learning of legislators (Kraft et al., 2016; Kornilova et al., 2018) . More recently, external information including cosponsorship (Yang et al., 2020) , donors (Davoodi et al., 2020) , relevant stakeholders (Davoodi et al., 2022) and expert knowledge (Feng et al., 2021 (Feng et al., , 2022) ) is used to better learn legislator representation. They follow a mixed structure of textual encoder and graph encoder, to explicitly combine textual and structural information. Despite outstanding performance on target tasks, these methods are limited to certain settings or data, behaving inefficient in dynamic political scenarios. Thus they are hard to be transferred to all actors. By contrast, methods relying on texts (Vafa et al., 2020) provide more possibility for generalization.\n\nDomain-specific Pre-training\nBased on continued pre-training on domain-specific data, domainspecific Pre-trained Language Models have shown superiority on many NLP tasks. Domain-specific PLMs have been investigated in many areas including medical (Zhang et al., 2021) and financial (Araci, 2019) \n\nConclusion\nIn this paper, we propose to learn political actors from languages and inject multidimensional domain knowledge into the PLMs through structureaware contrastive learning and behavior-driven con-trastive learning. Experimental results validate the effectiveness and generalization capability of our approach.\n"}
{"question": "Why is TOME proposed?", "evidence": "  Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which enables the complete capture of the relevance between queries and documents and simplifies the classic indexretrieval-rerank pipeline. Despite its attractive qualities, there remain several major challenges in model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. To deal with the above challenges, we propose a novel two-stage model-based retrieval approach called TOME ", "options": ["A. There are still several significant challenges associated with model-based retrieval that need to be addressed.", "B. Model-based retrieval is becoming increasingly recognized as a novel paradigm in the field of text retrieval.", "C. Model-based retrieval adapts a sequence-to-sequence paradigm to generate document identifiers.", "D.  Model-based retrieval simplifies the classic indexretrieval-rerank pipeline. "], "answer": "A", "content": "\nIntroduction\nInformation retrieval systems have undergone continuous development over the past few decades, with the aim of obtaining relevant resources, such as documents, in response to a user query from a vast collection. With the recent success of Pretrained Language Models (PLMs) (Devlin et al., 2019; Raffel et al., 2020; Zhao et al., 2023) , researchers have developed PLM-based dense retrievers (Lin et al., 2021; Zhao et al., 2022) , which utilize dual-encoders and nearest neighbor search index for retrieval and achieve significant improvements over sparse retrievers.\nMore recently, a new retrieval paradigm, known as model-based retrieval (Tay et al., 2022; Zhou et al., 2022c) , has been introduced by developing an alternative architecture for retrieval. In contrast to traditional retrieval methods, it does not explicitly maintain a corpus index, thereby simplifying the classic index-retrieve-rerank process. Typically, a model-based retrieval system is built based on a sequence-to-sequence generation model with an encoder-decoder architecture, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) . It accepts a query as input and directly generates the corresponding document identifier via the generation model.\nDespite its attractive benefits in simplifying the retrieval pipeline, model-based retrieval still faces following major challenges.\n\u2022 Firstly, since the retrieval task is framed as a prediction task of document identifiers, making it crucial to design document identifiers that are well-suited to the underlying generative PLM. However, this issue is rarely discussed in prior research, and most existing approaches employ manually or randomly constructed identifiers (i.e., docids) as generation targets. Such docids are not adequately captured in the pretraining stage of the generative PLM, thus limiting PLM's capabilities for generative prediction (e.g., unseen docids during pre-training). This creates a discrepancy between the pre-training and fine-tuning phases.\n\u2022 Secondly, there is a discrepancy between training and inference in the single-model generative architecture. While most existing studies incorporate multi-task learning (Tay et al., 2022) and auxiliary pre-training tasks (Zhou et al., 2022b) to model both documents and queries during training, the model only processes queries dur- ing inference, resulting in a gap between the training and inference stages.\nTo this end, in this paper, we propose a novel TwO-stage Model-based rEtrieval approach, TOME (as illustrated in Figure 1 ), which makes two major technical contributions.\n\u2022 Firstly, we suggest using tokenized URLs (or URIs) as text identifiers, which are widely available for web pages or Wikipedia pages 1 . By using URL-based identifiers, the tokenized symbols are well aligned with the vocabulary of the generative PLM, thereby enhancing the generative capacity of the PLM. URLs are typically comprised of normal text, as opposed to manually or randomly constructed identifiers. As a result, such an identifier design can be used to help alleviate the gap between pre-training and fine-tuning.\n\u2022 Secondly, our approach decomposes the prediction task into two consecutive stages, namely passage generation and URL generation, which are fulfilled by two separate T5-based generation models, respectively. The first stage aims to generate a relevant passage in the corpus based on the query, while the second stage aims to generate the corresponding URL of the generated passage from the first stage. This two-stage architecture can reduce the discrepancy between training and inference. In addition, the entire generation process is progressive. Consequently, the second stage is capable of tolerating errors that may be introduced by the preceding stage and generates correct URLs.\nMoreover, we discover that optimizing modelbased retrieval becomes a challenging task when dealing with a vast corpus. As a result, we propose a number of improved training strategies to optimize the generation models, including query augmentation, passage length reduction, and model scaling.\nTo verify the effectiveness of TOME, we conduct extensive experiments on the publicly available MS MARCO and NQ datasets. Experimental results demonstrate the effectiveness of the proposed method, including the URL identifier design and the two-stage generation process. Additionally, case studies indicate that the second stage can tolerate errors induced by the first stage. Furthermore, we investigate the scaling laws of TOME by examining different model sizes, corpus sizes, and text lengths. We anticipate that these experimental results will facilitate further research on model-based retrieval.\n\nRelated Works\nText Retrieval. Text retrieval endeavors to find textual information related to a query from a large candidate corpus. Early studies on sparse retrieval focused on term matching by utilizing sparse representations and inverted indices, such as BM25 (Robertson et al., 2009) . In recent years, with the resurgence of neural networks and the emergence of pre-trained language models (PLMs) (Devlin et al., 2019; Raffel et al., 2020) , dense retrieval achieves better performance beyond traditional sparse retrieval on multiple tasks (Khattab and Zaharia, 2020; Karpukhin et al., 2020; Xiong et al., 2021; Qu et al., 2021) . The dense retrieval and the technique of approximate nearest neighbor search have been widely adopted in various applications (Oguz et al., 2020; Ren et al., 2021a,b; Asai et al., 2021; Ren et al., 2022; Zhou et al., 2022a) . Recently, Zhao et al. (2022) have made a very comprehensive survey about the recent progress of dense retrieval based on PLMs, and we refer the readers to this survey paper for more details.\nModel-based Retrieval. Both sparse retrieval and dense retrieval rely on explicit indices. Recently, researchers have proposed model-based retrieval (a.k.a., generative retrieval) models (Metzler et al., 2021; Tay et al., 2022) . These methods consider model parameters as retrieval indices and directly generate the identifiers of related documents. Such an idea is initially proposed for entity retrieval (Cao et al., 2021) , which autoregressively generates unique entity identifiers. Following this approach, researchers have introduced sequenceto-sequence encoder-decoder architecture for document retrieval (Zhou et al., 2022c; Bevilacqua et al., 2022; Zhuang et al., 2022; Wang et al., 2022; Lee et al., 2022; Chen et al., 2022; Zhou et al., 2022b) . As discussed in the previous section, there still remain issues with model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. Our work tries to deal with these issues with a two-stage generation architecture with URL identifiers.\n\nApproach\nIn this section, we first introduce the task formulation, followed by the description of the proposed two-stage generation approach TOME.\n\nTask Formulation\nIn this work, we consider the task of text retrieval, which aims to find relevant text resources (e.g., documents) related to a query from a large corpus. We further assume that these texts can be accessed by an associated URL 2 (or URI).\nTo develop our approach, we adopt the recently proposed model-based paradigm for text retrieval (Tay et al., 2022; Zhuang et al., 2022) . For retrieval, a model-based retrieval model takes a query q as input and uses the text-to-text model to generate the identifier y (length n) of the relevant document in an autoregressive manner, with the conditional probability:\nEQUATION\nwhere y i denotes the i-th output token in the identifier y, y <i denotes the previous tokens y 1 , . . . , y i\u22121 , and M represents the PLM. The identifier can be an atomic token or a string (Tay et al., 2022) . In our setting, it is assigned to an associated URL of a text (refer to Section 3.2.1). Typically, a generative pre-trained language model (PLM) with an encoder-decoder architecture is employed to implement the text-to-text model (e.g., T5), which is typically optimized by a cross-entropy loss as follows:\nL(M) = \u2212 log Pr M (y|q) = \u2212 n i=1 log Pr M (y i |y <i , q) . (2)\nThe key to model-based retrieval is to design a generative architecture that employs suitable document identifiers, and to develop effective training methods that can effectively associate queries to the identifiers of documents. Next, we expound our approach in detail.\n\nModel Architecture\nIn this section, we first introduce the design of document identifiers, and then present the two-stage generation architecture.\n\nIdentifier Design\nExisting studies typically use docids to represent a document (Tay et al., 2022; Zhuang et al., 2022) . These docids are often randomly generated or manually constructed, which may not exist in realworld text corpora. However, the generative PLM is pre-trained based on large-scale text corpora, leading to a discrepancy between pre-training and fine-tuning.\nDifferent from previous approaches, we consider a tokenized form of URLs as the docids. We directly treat the URL as a text string and tokenize it into a sequence of tokens using a T5 tokenizer. For instance, a sample URL 'https://en.wikipedia.org/wiki/Nevada' can be tokenized to {'https', '://', 'en', '.', 'wikipedia', '.', 'org', '/', 'wiki', '/', 'N', 'e', 'vada'}. We use the token sequence as the prediction target of the generative PLM, following the generation formula of Equation (1). It is worth noting that Ultron (Zhou et al., 2022b ) also uses URLs as identifiers, where a URL is reversed and only used as part of an identifier (also involving titles and domains). As a comparison, we solely utilize tokenized URLs as the identifier, without any additional processing.\nCompared to non-linguistic docids, URLs typically contain more meaningful tokens in the form normal text and widely exist in real-world text corpora, making them more suitable to modeling and prediction using generative PLMs. During decoding, we can directly adopt the general text decoding method to generate the URL, without resorting to limited search strategies such as constrained beam search (Tay et al., 2022; Bevilacqua et al., 2022) . Since these tokenized symbols often overlap among different URLs (e.g., web pages from the same domains), they naturally derives semantic strings as the clustering method in DSI (Tay et al., 2022) .\n\nTwo-stage Generation Architecture\nThe objective of the generative model for retrieval is to establish a correlation between a query and its corresponding docid (i.e., URL). However, owing to the scarcity of annotated data, various improved strategies such as multi-task learning (Tay et al., 2022) or pre-training (Zhou et al., 2022b) have been proposed. Typically, a model processes both documents and queries during training, while it processes only queries during inference, resulting in the discrepancy between training and inference. To tackle this issue, we propose a two-stage generation approach with two different generation models: one for passage generation and the other for URL generation, as shown in Figure 1 .\nPassage Generation. In the first stage, we employ a T5-based passage generation model to map an input query to the passage content according to Equation (1). The generated passage is anticipated as a relevant passage in the corpus that can provide an answer to the query. The objective of the passage generation model is to memorize the passages in the corpus, so as to generate the passages with utmost precision. It is trained with query-passage pairs, where each pair comprises a query and a passage from the document, along with the corresponding labeled URL. Different from existing methods (Tay et al., 2022; Bevilacqua et al., 2022) , we do not utilize any data structure to restrict the decoding process and simply use greedy search to generate an individual result for a query in an autoregressive manner, which has a high decoding efficiency. By incorporating the intermediate passage generation, our approach can mitigate the training-inference discrepancy that the query encoder also needs to process documents (Tay et al., 2022) . URL Generation. In the second stage, another T5-based PLM is employed to predict the corresponding URL as the retrieval result, utilizing the passage generated by the passage generation model as input. The URL is generated by means of greedy search decoding in a similar manner as in Equa-tion (1). The URL generation model is trained with passage-URL pairs, where each pair comprises a passage and its corresponding URL. The objective of the URL generation model is to memorize all the URLs in the corpus, so as to map a generated passage related to a query to a corresponding URL. Meanwhile, even if the generated passages contain some irrelevant content or noise, this stage can still make reliable predictions since it can employ long passages as the context, rather than short queries.\nOverall, such a two-stage generation approach can more effectively capture the semantic relatedness between queries and identifiers by both reducing the training-inference discrepancy and enriching the generation context, which is specifically tailored for model-based retrieval.\n\nTraining\nFor both the passage generation model and the URL generation model, we optimize them independently by utilizing the cross-entropy loss for optimizing standard T5 models, as shown in Equation (2). Nevertheless, optimizing model-based retrieval approaches (Zhuang et al., 2022; Wang et al., 2022 ) is a challenging task as they essentially require memorizing the corpus information, and generating long text also poses challenges in model convergence. In this part, we further propose several strategies for improving the training of our approach.\nQuery Augmentation. Generating pseudo queries is proven to be effective in improving the performance of model-based retrieval (Wang et al., 2022; Zhuang et al., 2022) . Here, we utilize query generation for constructing the training data for passage generation. Specifically, we take the passage collection as the corpus, and use an existing query generation model (i.e., DocT5query (Nogueira et al., 2019) ) trained on the labeled dataset to generate multiple pseudo queries for each passage in the corpus. Following DSI-QG (Zhuang et al., 2022) , we use the top-k sampling strategy for query generation, and set k up to 20. The generated pseudo queries and their corresponding passages are then used to construct query-passage pairs as the training data for the passage generation model. Such a query augmentation method can significantly increase the availability of training data, and also enhance the generalization capability of the model for different queries.\nReducing the Passage Length. Since passages are much longer than URLs, passage generation is more complicated than URL generation. In the generation task, a more extensive generation target results in larger search space, which typically leads to a decrease in efficiency and effectiveness. While, in our approach, passage generation serves as an indirect step for predicting the URL, so that we consider reducing the passage length for improving the training efficiency. For this purpose, we shorten the maximum truncation length of the passage, from 128 to 32. However, reducing the passage length will probably results in a information loss, thus hurting the generation performance. As the solution, we concatenate the title (a short text) and the shortened passage for enhancing the contained semantics. We also add prompts before titles and passage contents like \"title:\" or \"passage:\" for better generation performance.\nIncreasing Model Scale. Model-based retrieval requires a strong memorization capacity from the generative PLM, especially for our approach that involves a passage generation stage. Besides, scaling up the text corpus will significantly increase the difficulty of corpus memorization, and the PLM with a small parameter scale will have a limited memorization capacity when the data scale reaches a certain level. Considering the two aspects, we scale the model size accordingly and employ a larger PLM when necessary. Specifically, we use T5-large (the first stage is more difficult) and T5base for the two stages of our approach on a small corpus (e.g., subsets of MS MARCO), respectively. Further, we increase them to T5-3B and T5-large accordingly on a large corpus (e.g., the full set of MS MARCO). Besides the improved capacity, we find that using a larger model size is also useful in improving the convergence rate (as detailed in Section 5.4).\n\nExperimental Settings\nThis section describes the major experimental settings, including datasets, evaluation metrics, baselines and implementation details.\n\nDatasets and Evaluation Metrics\nDatasets. We conduct experiments on two public available datasets, namely MS MARCO (Nguyen et al., 2016) Passage Ranking and Natural Questions (NQ) (Kwiatkowski et al., 2019) . (1) MS MARCO contains Bing search queries as well as passages from web documents, making it one of the largest web search datasets to date, with a full corpus of over 8.8 million passages. In addition, we also consider two subsets, each containing 100K and 1M passages, by following (Tay et al., 2022; Zhuang et al., 2022) . Based on the MS MARCO Question Answering dataset, we extract the URLs associated with the passages, selecting a random URL if a passage contains multiple URLs (2) The NQ dataset is a question answering dataset where the query data is collected from Google search logs, and the document data is from Wikipedia. We use the NQ320K version by following NCI (Wang et al., 2022) , which contains 320K labeled querydocument pairs and 100K documents. We collect abstracts of documents as intermediate-generated passages.\nEvaluation Metric. Following previous works, we adopt Hits@1 as the evaluation metric. This metric is calculated as the percentage of queries to which the top-1 generation result is positive. Since the outputs of models at different stages are either passage texts or URL texts, unlike the conventional MS MARCO evaluation by determining whether the retrieved identifiers are in the identifier label list, we evaluate the results by determining whether it is an exact match to the label text.\n\nBaselines\nFor comparison, we chose the following baselines including sparse retrieval, dense retrieval, and model-based retrieval.\nBM25 (Robertson et al., 2009 ) is a classical sparse retriever that uses the inverted index to find relevant passages by term overlap. DPR (Karpukhin et al., 2020) and ANCE (Xiong et al., 2021) are two representative dense retrievers that adopts dual-encoder architecture. For modelbased retrievers, DSI (Tay et al., 2022 ) is a pioneer work for model-based retrieval that uses a sequence-to-sequence model to map the input query to the relevant docid. We use the open-source code released by DSI-QG for reproducing DSI baseline on MS MARCO. SEAL (Bevilacqua et al., 2022) is proposed to generate multiple ngrams for a query with an auxiliary Ferragina Manzini index. DSI-QG (Zhuang et al., 2022) proposes to improve DSI with augmented data constructed by query generation. NCI (Wang et al., 2022 ) also utilizes pseudo queries for improving model-based retrieval with tailored architecture. Due to the different experimental settings of different methods, we copy the performance values for some baselines on NQ in NCI and reproduce all of the baselines on MS MARCO under the same evaluation strategy. All the model-based retrieval baselines adopt the \"large\" version of PLMs.\n\nImplementation Details\nWe conduct our experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) and natural language processing toolkit PaddleNLP (Contributors, 2021) on up to 32 NVIDIA Tesla A100 GPUs (with up to 80G RAM).\nPLM. The generation models adopted in our work are initialized with different parameter scales of T5 (Raffel et al., 2020) . In the passage generation model, we use T5-3B for initialization on MS MARCO Full, and other models are initialized with T5-large. In the URL generation model, we use T5large for initialization on MS MARCO Full, and other models are initialized with T5-base.\nHyper-parameters. We adopt Adam optimizer with a learning rate of 5e-5, and train the models for a maximum of 3M steps with bf16 mixed precision strategy. The batchsize is set up to 128, 384 and 80 for T5-base, T5-large and T5-3B, respectively. The maximal length of queries, passages and URLs are set as 32, 32 and 80, respectively. The warm-up step is set as 100K and 10K for passage and URL generation task, respectively. Query Augmentation. We adopt the existing docT5query-large (Nogueira et al., 2019) model that trained on MS MARCO training set, and generate 20 and 15 queries per passage for MS MARCO and NQ, respectively. For training data, we only use pseudo-labeled data constructed by query generation on MS MARCO, and use both pseudolabeled data and labeled data on NQ.\n\nExperimental Results and Analysis\nIn this section, we report the experimental results of our proposed approach and conduct comprehensive empirical analysis. (Karpukhin et al., 2020) 71.84 52.52 29.54 DSI (Tay et al., 2022) 11.75 --DSI-QG (Zhuang et al., 2022) Table 1 : The Hits@1 results of different methods on variant corpus scales of MSMARCO.\n\nMethods\nHits@1 BM25 (Yang et al., 2017) 15.11 ANCE (Xiong et al., 2021) 52.63 DSI (Tay et al., 2022) 35.60 SEAL (Bevilacqua et al., 2022) 59.93 NCI (Wang et al., 2022) 66.23 DSI-QG (Zhuang et al., 2022) 61.34 Comparison with Model-based Retrievers. We observe that TOME consistently outperforms model-based retrievers on three subsets of MS MARCO and NQ320K datasets, thereby demonstrating the effectiveness of the proposed method. Moreover, NCI is a competitive baseline on NQ320K, which uses tailored decoder architecture, preprocessed semantic docid, and regularization on top of DSI-QG, while our method is simply trained with the standard T5 configuration without any additional processing. We also discover that DSI-QG is unable to effectively converge when trained on the MS MARCO Full. We speculate that random non-linguistic docids become a bottleneck as the corpus scales up, while the loss can normally converge when using normal text (e.g., URL) as a generation target.\nEffect of Two-stage Generation Architecture. By simply substituting the generation target of DSI-QG from random string docids to URLs (singlestage of our method), the performance has been improved (refer to DSI-QG and TOME single-stage in Table 1 and 2 ), indicating that natural language identifiers are more suitable for model-based retrieval tasks than non-linguistic docids. Furthermore, if we employ the two-stage generation that includes an intermediate step to generate passages before generating URLs, the performance will be further improved (refer to TOME single-stage and TOME two-stage in Table 1 and 2 tion demonstrates that integrating passage generation in the process of model-based retrieval leads to better performance.\nComparison with Dense Retrievers. By adopting a series of training strategies, we successfully train TOME on large-scale corpora. However, although TOME outperforms dense retrieval methods on MS MARCO 100K and NQ320K, there still remains a performance gap when compared to DPR on larger corpora such as MS MARCO 1M and Full. This indicates that our method still has gaps compared to advanced dense retrieval methods when the corpus scales up. Since the model-based method necessitates complete memorization of the entire corpus, it inherently possesses a disadvantage in larger-scale corpora when compared to dense retrievers, which needs to be further explored.\n\nAblation Study\nIn this section, we conduct an ablation study to examine the effectiveness of strategies in TOME.\nWe report the results on MS MARCO 100K and NQ320K. Here, we consider three variants based on TOME for comparison: (a) w/o prompt removes the prompts before titles and passages; (b) w/ increased maxlen increases the maximum truncated length of passage from 32 to 128; (c) w/ reduced pseudo query reduces the amount of pseudo query to 10 per passage. Table 3 presents the results for variants of TOME. We can observe the following findings: (a) The performance drops in w/o prompt, demonstrating that adding prompts for identifying the title and passage is helpful for generating better results. (b) The performance drops in w/ increased maxlen, demonstrating that due to various training strategies, shortening the maximum truncated passage length does not bring performance loss but reduces the difficulty of training. (c) The performance drops in w/ reduced pseudo query, demonstrating the effectiveness of generating a large number of pseudo queries for data augmentation.\n\nAnalysis on Two-stage Generation\nIn this section, we investigate the generation results of the passage generation model quantitatively and qualitatively to showcase the superiority of the proposed two-stage generation approach.\n\nQuantitative Analysis\nWe quantitatively analyze the generation results on MSMARCO dev set with the passage generation models trained on MS MARCO 100K.\nFirst, we are surprised to find that on the entire dev set, the proportion of generated passages are the passages exist in the corpus is about 95%. In cases where the model failed to generate labels correctly, about 85% of the generated passages still exist in the corpus. This result indicates that the model is capable of memorizing the corpus precisely and is able to generate a retrieval-like result. Moreover, previous studies of dense retrieval reveal that there are a lot of false negatives in MS-MARCO (Qu et al., 2021) . We also observe that approximately 80% of the generation results that are not labeled as positives but appear in the corpus are false negatives, showing that model-based retrieval suffers from the same issue of false negatives as dense retrieval. Despite this, the passage generation model actually has strong generation capability.\n\nQualitative Analysis\nTo explore the generative capabilities of TOME, we conduct a case study on MSMARCO 100K, utilizing a maximum truncation length of 128 for better illustration.\nTable 4 gives two sampled queries, along with their corresponding label passages, evidence passages (if available) and generated passages. With respect to the first query, the generated passage is not exactly the same as the labeled passage. In comparison with the labeled positive passage, the second half of the generated passage is altered. Despite the alteration in the generation passage, the URL generation model is still able to accurately map it to the correct URL, indicating that the URL generation model can tolerate changes introduced by the passage generation model. In the second example, the model extracts relevant content from both the label passage and the evidence passage, and then combines the contents to create the generated passage. It is interesting Ginger is an analgesic (a pain-killer) that may alleviate the pain associated with a sore throat. It is also a good antibacterial and antifungal and can help fight the infection causing your sore throat. . . . Table 4 : The comparison of the labeled passages and generated passages. The evidence passages are not manually labeled but contain relevant content. The italic words with underline represents the different parts of two passages, the ::::::::::::::::::::::::::\nitalic words with wavy underline and bold words with underline in different passages represent the reference parts. to observe that the passage generation model is capable of summarizing multiple passages.\n\nAnalysis on Scaling\nWe observe that long text generation poses a challenge to the convergence of loss, so we investigate the training efficiency and capability of the model under varying conditions. In particular, we use the same computing resource and conduct training on the passage generation stage (i.e., the first stage) of TOME. Considering that the trend is similar in the second stage, it has been omitted here due to limited space.\nEffect on Data Scale. We investigate the impact of expanding the corpus on model training and examine whether the model capacity is insufficient when dealing with a large corpus. We fix the T5-large model and conduct training on MSMARCO 100K, 1M and Full datasets, respectively, without shortening the length of passages. We use perplexity (PPL) to estimate the model capacity and monitor how perplexity changes as training steps increase. The results are shown in Figure 2 (a). It can be observed that the perplexity of the T5-large model fails to converge to a lower level after corpus scale expansion, which illustrates that under this task, a certain amount of data will lead to the capacity bottleneck of the model. In addition, the decline rate of perplexity slows down on larger corpora, indicating that models with the same parameter size have low learning efficiency on a large-scale corpus.. Effect on Passage Length. In order to investi-gate the effect of reducing the length of generated passages, we fixed the model as T5-large, and conducted experiments on passages with different maximum truncated lengths as generation targets on MSMARCO 1M. Figure 2 shows that after reducing the maximum truncated length of the generated passage, the perplexity significantly decreases, indicating that such a strategy is beneficial to mitigate the difficulty of the passage generation task. Moreover, the model exhibited enhanced efficiency when generating shorter passages.\n\nConclusion\nIn this paper, we introduce TOME, a innovative two-stage model-based retrieval approach. To implement our approach, we make two major technical contributions in the design of the identifier and the architecture of two-stage generation. Moreover, we also employ a number of training strategies to better optimize our proposed architecture, especially on large-scale corpora. Extensive results demonstrate the effectiveness of TOME. Furthermore, we perform a thorough analysis and summarize the scaling law for the proposed method. We believe such an idea itself is worthwhile for exploring in designing new model-based retrieval architecture.\n"}
{"question": "How does the performance of byte-level approach compare to subword-level approach in Icelandic error correction task?", "evidence": "  We conclude that adopting a byte-level approach rather than a subword approach leads to best results for the task of GEC, at the very least in the case of a morphologically rich language such as Icelandic ", "options": ["A. Byte-level approach is better", "B. Subword-level approach is better", "C. The performance is the same", "D. Insufficient information provided to answer the question ", "Comparing models with different architectures calls for defining which factors are compared. In particular, byte sequences are longer than subword sequences when counting the number of tokens, roughly 4 times longer on average in the original multilingual mT5/ByT5 training data"], "answer": "A", "content": "\nIntroduction\nSpelling mistakes due to typos and rushed writing, nonstandard punctuation and spelling, and grammatical and stylistic issues are common to almost everyone who writes any kind of text. This applies in any language and can distract the reader or make the communication miss its mark. This can hinder people who have difficulties writing text conforming to a particular language standard, be it due to disability, dyslexia, linguistic background, limited access to education or any other reason. Prejudice against people whose writing deviates from the standard can make some shy away from communicating with others, leaving their voices out of important discussions and restricting their opportunities (Alexander-Passe, 2015) .\nGrammatical error correction (GEC) is the task of adjusting a text's spelling, grammar, and linguistic style to conform to an approved language standard or convention (Rauf et al., 2017) . While the latest work on GEC is based on Transformer models (Vaswani et al., 2017) , the subword tokenization methods commonly used in these models are a source of problems when it comes to typos and other variants (Schmaltz et al., 2017) . Subword tokenization (Sennrich et al., 2016; Kudo, 2018) was presented as a solution to the open vocabulary problem, as it is a compromise between character encoding and whole word encoding, enabling unknown words to be represented using known subwords.\nHowever, a significant downside of subword tokenization is how it is affected by noisy input; if a word contains a typo or other spelling variants, this can completely shift its representation. In addition, in languages with rich morphology, a word can have many different surface forms, some rarer than others, that all carry the meaning of the base word, but appear in different syntactic contexts. A subword-tokenized model may struggle to capture the nuances of such a language effectively since it may need several different subwords to represent a single word, depending on spelling and context. When an unfamiliar variant of the word appears in unseen text, the model is challenged to decode it correctly, even when it results in uncommon subword units.\nOur motivation is that a byte or character-level approach should intuitively be more robust to spelling or morphology variations, as it is not constrained by the subword vocabulary. We explore using a byte-level architecture, ByT5 (Xue et al., 2022) , for correcting everything from typos to com-Figure 1 : Overview of training data and comparison of output. The Icelandic-English mBART-ENIS model and the multilingual ByT5 and T5 models are first trained on generated parallel error corpora before being adapted on curated (collected) true error corpora in Icelandic. The final models are compared on an error correction (EC) task in Icelandic. The example demonstrates how the byte-level model performs well while the subword model cannot see the individual characters in every word, leading to degraded performance. plex grammatical issues in text. The language studied is Icelandic, a highly-inflected North Germanic language. For instance, the morphological complexity in Icelandic means that nouns can have up to 16 different surface forms, and adjectives over 50. GEC for a morphologically complex language needs to go beyond correcting only single words or limited phrases; it needs to consider the syntax of the whole sentence. This is the case for Icelandic but also for other languages with rich morphology, such as Arabic, Hebrew, Polish, Basque, Lithuanian and Hungarian, to name a few.\nWe compare the performance of the byte-level architecture to two subword-based architectures; ByT5's predecessor, mT5 (Xue et al., 2021) , and an mBART (Liu et al., 2020) model that has been pretrained further on both Icelandic and English. We employ real and synthetic error data for training, and present models and a framework for error generation methods that can be adapted to other languages. For under-resourced languages such as Icelandic (R\u00f6gnvaldsson, 2022) , using synthetic training data makes neural training for GEC a viable option.\nOur main contributions include a comparison between subword tokenization and byte-level tokenization for GEC when training over a combination of curated and synthesized data. We demonstrate how byte-level models not only bypass subword-related issues, but can also correct long-range errors in text. We release our error generation framework as well as models for GEC using byte-level and subword tokens in Icelandic. While our work focuses on the Icelandic language, we have no reason to believe that similar results do not hold for other languages, particularly those similar to Icelandic in terms of morphological complexity.\n\nRelated work\nThe bulk of research on grammatical error detection and correction has been focused on English and English learner texts, due to existing training data and benchmarks, and the large market of English learners worldwide who benefit from an automatic language correction tool (N\u00e1plava and Straka, 2019) . While spelling and grammar errors appear in every language, each language has its own set of error types that are more common than others, due to different phonetic, morphological and syntactic characteristics.\n\nSynthetic data generation for GEC\nThe problem of data scarcity in GEC, when approached as a sequence-to-sequence task, is typically addressed with synthetic data generation (Stahlberg and Kumar, 2021) . One approach to cre-ating ungrammatical sentences uses random character noise and simple rules to manipulate the text. Another approach is using a spell checker in reverse to noise text (Grundkiewicz and Junczys-Dowmunt, 2019) . In contrast, others have used probabilities derived from an annotated corpus of naturally occurring errors to corrupt text (Felice and Yuan, 2014) . Recent efforts widely employ neural networks to create synthetic errors (Stahlberg and Kumar, 2021) ; many use methods derived from machine translation (Junczys-Dowmunt et al., 2018) , for example, by creating worse text using deliberately bad translation models (Xie et al., 2018; Zhou et al., 2020) or roundtrip translations between languages (Lichtarge et al., 2019) . Yet another option is to leverage available resources with edits, such as Wikipedia edit histories, to generate corrupted corpora (Grundkiewicz and Junczys-Dowmunt, 2014) .\n\nSequence segmentation for GEC\nGEC can essentially be considered the task of generating grammatical target text from an ungrammatical source, similar to machine translation. The idea of approaching GEC as a machine translation problem dates back to 2006 (Brockett et al., 2006) , and this approach has since become the most prevalent method of GEC, with the focus shifting from statistical machine translation (SMT) to neural methods as they developed (Yuan and Briscoe, 2016; Ji et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018) . However, phrase-based SMT continued to be the state-of-the-art for GEC for longer than in the field of interlingual neural machine translation (NMT) (Junczys-Dowmunt et al., 2018) . This is partly because of the data scarcity problem and partly because of the tokenization methods typically used in transformer models. Breaking words up into subword units decreases the vocabulary size while addressing the out-of-vocabulary problem (Sennrich et al., 2016) . A prominent drawback of this approach is that the fixed subword vocabulary makes the models sensitive to noise in the text (Tay et al., 2021; Eger and Benz, 2020) .\nIn a subword-based GEC model, when a word contains a typo or is spelled unconventionally, it may look like an unknown word, for which no known representation exists. The model may then segment the word differently from what was seen during training, causing mispredictions. If the subword representation for \"different\" is [_diff, er, ent] , but the word is misspelled as \"diffirent\", its subwords might be [_diffi, ren, t], and a subword-based GEC model might correct the typo by outputting a different word, [_diffi, cul, t] . This issue is highlighted in Figure 1 , also showing how a byte-based approach is not limited by this issue.\nThis is also true for unseen words that are correctly spelled, such as foreign-named entities, which can lead to the subword-based GEC model \"correcting\" a perfectly spelled word it has not seen before, by replacing it with the most likely candidate. In the sentence \"The tournament was held in Espoo, Finland.\", the place name \"Espoo\" may be represented by a single subword token Espoo. Since this token is unfamiliar, the model finds the most likely subword token for this particular sentence, Helsinki. 1 This changes the semantics of the sentence and can introduce serious errors. The result is a grammatically correct and meaningful sentence, but the semantics have drifted away from the original text.\nDue to this known shortcoming of subword tokenization (Schmaltz et al., 2017) , efforts have been made to design architectures where the characters or the underlying bytes are used directly as input tokens. Byte and character-level models inevitably result in much longer sequences than subword models, making them more costly to train, and slower in inference. Some truly token-free general-purpose architectures that are increasingly competitive to token-based models have emerged recently, including CANINE (Clark et al., 2022 ) (character-level), PIXEL (Rust et al., 2023 ) (text-to-image), and ByT5 (Xue et al., 2022) (byte-level) . The training of ByT5 is based on the subword-based multilingual mT5 (Xue et al., 2021) approach, but in comparison, the model is equipped with a heavier encoder (three times the depth of the decoder). Compared to mT5, ByT5 is more robust to noisy input, but inference is slower (1.5 to 2.6 times slower on average on a transliteration task, and up to 9 times longer on tasks with longer input sequences) (Xue et al., 2022) . Another approach to making subword models more robust is using subword regularization to produce multiple segmentations of the same word (Kudo, 2018; Provilkov et al., 2020) . This is commonly used for addressing the open vocabulary problem and noisy data, such as ungram-matical text.\nDespite the reported advantage of character or byte-based Transformer models on noisy text (Libovick\u00fd et al., 2022) , work using this approach to GEC is not very common. A notable exception is work for GEC in the Lithuanian language, where ByT5 has been used for diacritics restoration (Stankevi\u010dius et al., 2022) and limited GEC (Stankevi\u010dius and Luko\u0161evi\u010dius, 2022) . They generate synthetic data by applying noise (common typographical errors, swapping letters for similar sounding ones, other non-grammatical word-level noise) to a crawled and filtered corpus and compare results when training with T5 and ByT5. Their findings agree with ours that the byte-level model outperformed the subword model.\nOur work deviates from that of Stankevi\u010dius et al. (2022) in the following key ways: We (a) generate more sophisticated and realistic errors using grammatical information from part-of-speech (PoS) tags and using custom rules based on empirical findings; (b) combine generated error data with true error corpora from a wide range of demographic sources; and (c) explore in detail which error and text types benefit the most from finetuning on true error corpora, as opposed to training on synthetic data. As far as we are aware, no attempt at bytelevel Transformer-based GEC, trained on synthetic and real error corpora, has been published. Concurrent work using ByT5 for Icelandic is (Jasonarson et al., 2023) , where errors in the OCR output of historical Icelandic texts are corrected using a generated corpus of errors extracted from real data.\nCurrent state-of-the-art in GEC is based on sequence-tagging methods (Omelianchuk et al., 2020) , which instead of generating whole sequences, tag the erroneous sentence with its corrections, and on sequence-to-sequence methods, as has been described. Further work has explored automatic character transformations for GEC tagging (Straka et al., 2021) to better handle character-level errors. One of the current highest-scoring models on English GEC benchmarks is gT5 (Rothe et al., 2021) , which is based on the mT5 model.\n\nPrior work for Icelandic\nApart from some rule-based spell checkers that don't make use of the full context, one rule-based correction system exists for Icelandic, based around parse trees, GreynirCorrect (\u00d3lad\u00f3ttir et al., 2022) . This system is contingent on the sentence parsing according to a pre-defined context-free grammar, and can only handle issues that fit pre-defined rules. This setup is both a strength and a weakness as the system is highly configurable and capable of many things, such as detecting syntactic inconsistencies and errors, and can give the user useful information on the errors found. Still, when a text has many errors, complexity builds up and rules can start interfering. Sentences containing many issues, such as from users with dyslexia, generally have lower accuracy using this method.\nThe work presented here is the first where neural networks are used in GEC for Icelandic. Snaebjarnarson et al. (2022) use neural methods for detecting such errors, but not for correcting them.\n\nCurated dataset\nA single collection of parallel error corpora exists for Icelandic, the Icelandic Error Corpus (IceEC). The corpora are annotated and corrected by language experts (Arnard\u00f3ttir et al., 2021) . The dataset is highly granular in its categories, containing hundreds of labels, but with a limited number of highlevel groups (coherence, grammar, orthography, style and vocabulary).\nThe IceEC is split into a larger general corpus and three specialized corpora (Arnard\u00f3ttir et al., 2022) . The general one contains 58,239 sentences of student essays, online news texts and Wikipedia articles. This corpus is annotated with around 50k errors of different categories. The three specialized corpora are much smaller and contain texts from Icelandic language learners (6270 sentences), dyslexic native speakers (1362 sentences), and children (2070 sentences), volunteered by the users themselves. These smaller corpora contain more errors per sentence than the general one, and add diversity to the training data.\nThis curated error data was used for finetuning our models, and combined into one training dataset for a total of 64k input sequences (single sentences), after setting aside validation and test data. The general IceEC also includes a 5.3k sentence test set used for evaluation.\n\nSynthetic dataset\nWe applied a diverse set of methods for error generation, both using linguistic knowledge and random noising methods. This rule-based approach to synthetic data generation gave us control over the types of noise applied, and allowed us to generate evaluation data for each error type.\nAs our basis of correct text to be noised, we used the Icelandic Gigaword Corpus (IGC) (Steingr\u00edmsson et al., 2018) , a collection of Icelandic editorial texts. These are mostly news articles, published literature and legal texts. We selected from this corpus those text sources that are the most likely to have been reviewed as part of the editorial process of each publication/source (literature, journals, news, laws, adjudications and transcribed parliamentary speeches). Some of these texts still have their share of typos and other errors and inconsistencies, especially the news articles, which were deemed important training data because of their general vocabulary, not found in the more formal text sources. As a preprocessing step, we filtered out lower-quality and irrelevant sentences, by removing sentences containing mostly foreign texts, illegal characters and words with known misspellings, sourced from lists of common misspellings. The corpus was tokenized using the Greynir Tokenizer (\u00deorsteinsson et al., 2019) and PoS tagged using the GreynirSeq tagger (Snaebjarnarson et al., 2022) .\nWe generated three categories of errors: 1) noise within words; 2) noise at the sequence level; and 3) grammatical and morphological modifications. The first two resemble those used when noising backtranslation data (Edunov et al., 2018) . The third type is based on using available tools and linguistic knowledge to create errors that are unlikely to be formed randomly, but resemble those of human writers.\nIn order to explore to what extent subword and byte-level models can learn and generalize grammatically complex issues in a morphologically rich language, we go beyond naive language-agnostic noising of text. A more detailed explanation of the Icelandic-specific noise is given in Appendix C. The noise methods are shown in Table 1 . This is by no means a finite list of linguistic variants or errors found in Icelandic texts, but constitutes examples chosen for studying the model performance on these more grammatically complex challenges.\nThe error generator allows for noising levels to be configured via hyperparameters. Experiments with different noise ratios in the synthetic data showed that highly noised text provided the best training examples, without the models learning to \"overcorrect\", i.e., to introduce false positives. Instead of producing even more synthetic data, we geared up the noise to produce highly error-dense examples, setting the random and more naive error noise to appear in 80% of cases, and the rule-based error noise to be used wherever possible. 2 Examples of parallel sentences with and without synthetic errors are shown in Table 2 . A total of 35M synthetic parallel sentences were generated using these methods. 2000/4000 lines were set aside for validation/testing, respectively, and special evaluation sets were generated for each error type (see Section 4).\n\nModels\nWe compared three model architectures to evaluate the differences between using subword tokenization and a byte-level method. Comparing models with different architectures calls for defining which factors are compared. In particular, byte sequences are longer than subword sequences when counting the number of tokens, roughly 4 times longer on average in the original multilingual mT5/ByT5 training data (Xue et al., 2021) . And as Xue et al. (2022) note, the mT5 architecture tends to need less training time than ByT5. We compared the models after an equal amount of training samples (100k updates). We also continued the training of the ByT5 model, using more than five times the number of updates.\n\nmBART\nWe continued training of the pretrained multilingual BART25 model (mBART) (Liu et al., 2020) , using the original pretraining objective on texts in Icelandic and English. The training of the model is detailed in Appendix A. This model, mBART-ENIS, was then finetuned on the synthetic error data, to teach it to correct errors in Icelandic texts.\nTraining on the synthetic error data was performed with an effective batch size of 3000 input tokens (roughly 60 sequences), a learning rate of 3e-5 with an inverse square root scheduler, 0.1 dropout, 0.3 attention dropout, 1000 warmup steps, 0.1 label smoothing, no weight decay, and using the Adam optimizer, for 100k updates on an A100 card for a day. 3 In addition to the above experiments, we conducted separate experiments using segmentation regularization (Kudo, 2018) to introduce more noise to the training examples and explore alternative measures to mitigate the subword tokenization problem. The BART architecture uses unigram subword units; we applied subword regularization with \u03b1 = 0.7 and keep all other parameters unchanged.\n\nByT5\nFor the byte-level approach we employed the ByT5base model (Xue et al., 2022) , which is based on the multilingual T5 model (Raffel et al., 2020) , but operates on bytes instead of subwords. The ByT5 model is pretrained on over 100 languages, but has only seen a limited amount of Icelandic. The mC4 dataset which is used to train ByT5 and mT5 is also lacking in quality for low-resource languages, in particular for Icelandic, as shown by Snaebjarnarson et al. (2022) .\nThe pretraining task in ByT5 has been adapted\nto a byte-level model, with span infilling based on bytes, not subwords. Apart from this, the main difference between the mT5 and ByT5 model architecture is the heavier encoder of ByT5.\nSequences in byte-level architectures are long and correspond more or less to the number of characters in Icelandic, resulting in increased training time. We trained the ByT5-base model using a maximum sequence length of 512 bytes, which was found to be a reasonable compromise, as most sentences in Icelandic texts are shorter than this.\nThe ByT5-base model was finetuned on the synthetic data with an effective batch size of 32 sequences (sentences). The learning rate was set to 2e-5 using the Adam optimizer with 1000 warmup steps and no weight decay. This model was further trained for a total of 550k updates, or 13 A100 card days. 4\n\nmT5\nFor a more direct comparison of byte-level and subword-level models, we also finetuned the mT5base (Xue et al., 2021) model on the same data, with the notable difference to the mBART model that it was not further trained on Icelandic. The mT5 models were pretrained on the same data as ByT5 and have a similar architecture, as described above, and are thus as comparable as subword and byte-level models can be. As previously mentioned, mT5-base is the base for the state-of-the-art gT5 (Rothe et al., 2021) model for multilingual GEC.\nWe finetuned the mT5-base model on the synthetic data using the same parameters as in our ByT5-base finetuning and evaluated it at 100k updates. 5 \n\nFinetuning on curated corpora\nUsing the curated error corpora (IceEC), we finetuned the byte-level and subword-level models to convergence. For the mBART model, this meant training with a learning rate of 2e-6 for 53k updates (67 epochs), with attention dropout set to 0.1, weight decay to 0.001 and other parameters being the same as during the synthetic finetuning.\nThe ByT5 and mT5 models were finetuned with a learning rate of 2e-6, other parameters were the same as during finetuning on the synthetic data. The ByT5 model had converged at 120k updates (60 epochs), while the mT5 was still improving on the validation data at 200k updates (100 epochs), but with time we found it forgot too much of the synthetic error correction task. We report evaluation scores at 130k.\nFor comparison, we also finetuned the different models (mBART-ENIS, mT5 and ByT5-base) on the IceEC data only, without the synthetic finetuning phase. This was done to examine how much the models learn from the added synthetic examples, and how far we can get using a small amount of hand-corrected examples. The mT5 and ByT5 models were trained for 100k updates and the mBART-ENIS model for 10k updates.\n\nResults\nDifferent metrics exist for evaluating GEC performance, but most are language-specific, and have not been adapted to Icelandic. Here we employ a language agnostic metric for scoring our models, the GLEU score (Napoles et al., 2015 (Napoles et al., , 2016)) . GLEU 6 is a variant of the BLEU (Papineni et al., 2002) score used to evaluate machine-translation output. It has been modified to account for both the source and the reference, by rewarding overlap between the source and the target sentence, and penalizing n-grams that should have been changed, but were not.\nWhen evaluating GEC for English, ERRANT (Bryant et al., 2017 ) is commonly used. It is a spanbased correction method that uses the F 0.5 metric, where precision weighs twice as much as recall. Though this metric has not been implemented for Icelandic, we also report ERRANT scores using a language-agnostic approach, disregarding error types and only reporting the span-based F 0.5 scores for each test set. These results are shown in Table 5 in Appendix E; they align well with the GLEU results in Table 3 which are described below.\nWe consider a variety of curated and synthetic test sets to get a good overview of the differences between the byte-level and subword-level approach for GEC. For the real errors, we report scores over the IceEC.test set, the test set from the IceEC, which contains around 5000 sentences. In con-trast, the dyslexic, L2 and children test sets contain 500 held-out sentences each from the respective specialized error corpora described in section 3.1 (only 100 examples were collected for the dativitis error type, a rarer occurrence in the data). We also annotated a small dataset (163 sentences) of data from an Icelandic news outlet (news), where each sentence contains at least one error; this is further described in Appendix B.\nFor the synthetic errors, we report GLEU scores over the test.synth set, which contains around 4000 held-out sentences from the synthetic data. Furthermore, we generated test sets of synthetic examples, each containing a particular error type in each sentence (dativitis, spaces, commas, dupl-words, mood, rand-noise, noun-case). This last group of test sets was generated using source texts that, while editorial, may include other errors, just like the synthetic training examples. The models, as they get better, learn to correct these errors as well. This may paradoxically lower the GLEU score as the corrected output deviates from the erroneous reference. These generated test sets still provide valuable information about what the models learn about each error type in isolation.\nTo understand what approach is best suited for GEC we trained the models on different data combinations and using different pre-trained models. The Synth-100k models are all trained for 100k updates on the same synthetic data, and the Synth-100k-EC models are additionally finetuned on the curated IceEC error corpus. To provide a baseline for the GLEU scores, we also report no_corr scores, where the source text is not corrected. This gives some idea of the noise level of the test sets, with test.synth being the noisiest and IceEC.test containing the least noise.\nThe GreynirCorrect (\u00d3lad\u00f3ttir et al., 2022) (GC) results were produced by applying GreynirCorrect to the test sets in its two configurations; correcting single-word errors on the one hand (spell.) and all errors found on the other (all). The GC system was developed in part to focus on the error categories and error frequencies defined in the IceEC training data, and this may be reflected in the scores for the IceEC test sets (asterisked in Table 3 ). The news test set ( \u2020) was created using only sentences flagged by GC (see Appendix B) and is therefore heavily biased towards that system. Models trained only on the synthetic data (Synth 100k) generally perform best on the synthetic er- ror corpora and thus solve the more simple and systematic spelling errors such as mistakes in punctuation, missing white space and word duplication. Their performance on the curated error corpora is somewhat lacking. In contrast, models trained only on the curated error corpora (EC) generally produce somewhat better GLEU scores on the curated error corpora than the Synth 100k models, but do not generalize to the error categories presented in the synthetic test sets. They are also unable to correct multiple errors in a single sentence (test.synth) .\nTraining on the synthetic data and then finetuning on the curated error corpora (Synth100k/ Synth550k+EC) performs best on the curated errors and retains much of the performance on the synthetic test sets. In all of these experiments, we can see that the ByT5 models generally perform better than the subword counterparts. This is also reflected in the ERRANT scores in Appendix E, Table 5 , where the ByT5 models score highest overall.\n\nDiscussion & Conclusion\nOur results show that the ByT5 models are the overall high-scorers on the real-world test sets, and on most of the synthetic ones. We include finetuning results on the ByT5 model that has been trained for longer on the synthetic data (550k updates) to compare how performance improves with time. We see the GLEU scores keep going up with time, and more importantly, when taking a close look at the actual generated output, this is the model that best corrects real-world errors. This makes it the most feasible option for use in real-world scenarios. A comparison of the output of the models trained on both data sources is shown in Appendix D.\nAn example from the test data is when the subword-tokenized model mBART-ENIS-Synth100k+EC incorrectly changes the name of a person from a rare name (\"L\u00e1retta\") to a more common one (\"L\u00e1ra\"). This kind of error is not seen in the byte-level model, which is quite conservative in its corrections of unknown entities. While this means ByT5 occasionally misses actual errors, we find that it is much better suited for production than a subword-level model that makes serious semantic errors. These more nuanced error correction examples may not be fully captured by the automated metrics, but are crucial for real-world use.\nThe subword regularization experiments are included as an alternative approach for mitigating the subword tokenization problem. The results are marginally better than the model without subword regularization when trained on the synthetic data, and the model performs better than the ByT5-Synth100k model in the case of dupli-cate words, which linguistically is a quite trivial task, and in more intricate mood errors. It however doesn't do any better than the mB-ISEN-Synth100k trained without subword regularization on the curated datasets, and this also holds when the model is finetuned additionally on curated data. The model finetuned on only the curated data with subword regularization (mB-ISEN-reg-EC) however performs consistently much better than its counterpart without subword regularization, often on par with or surpassing ByT5. This model has not seen any of the highly noised synthetic data, and thus has the most to gain from the subword noise. We speculate that this is one of the reasons we don't see more gains from adding subword regularization; the training examples are already so highly noised that there is not much to be learned from the added subword noise.\nThe IceEC finetuning data contain real-world errors which have been hand-corrected. These texts are somewhat different from the highly noised training examples with synthetic errors, have fewer errors on average and are more varied as they are naturally occurring. They also include stylistic edits from the reviewers, which improve the text's fluency, but in those cases the original is not necessarily incorrect as per the language standard. With these differences in mind, we expect the models to have to forget some of the synthetic error correction task in order to adapt to this \"new\" denoising task. We see this happen in the mBART-ENIS finetuning on the curated data, and to a lesser extent in the ByT5 finetuning. The denoising task performance on the synthetic errors from the previous step has in part been lost, which is expected, since some of these errors are not particularly common in real texts.\nFor the more grammatically complex error-types in the synthetic data (dativitis and changes to noun cases and verb moods), we find that the mBART-ENIS trained on synthetic data generally does well; for some subsets even surpassing the ByT5 counterpart that was finetuned on curated corpora. We suspect that this has to do with the linguistic knowledge the model has already gained during its pretraining on Icelandic texts, as explained in Appendix A. The ByT5 model that was trained for longer however manages to surpass it on the mood error type, indicating that it is still adapting to the Icelandic language, alongside its primary denoising task.\nThe models trained on only the finetuning data perform the worst throughout. The results show that they do not manage to correct the synthetic categories much beyond the baseline, except for mBART-ENIS in some cases. We expect that this has to do with their extra knowledge of Icelandic and the denoising objective used in the synthetic error correction finetuning. The results for these models on the curated in-domain test sets are in fact mostly on par with the models finetuned on the synthetic data only. Looking at the generated output, we see that the error types these models correct are not the same as those that the syntheticonly models are able to correct, which is expected, as they are trained on different data.\nWe conclude that adopting a byte-level approach rather than a subword approach leads to best results for the task of GEC, at the very least in the case of a morphologically rich language such as Icelandic. Finally, we find that the optimal way of capturing a wide range of errors is to train on a combination of synthetic and curated data, particularly when the curated data is limited.\n"}
{"question": "what is the main proposed solution by the authors to enhance multimodal translation quality?", "evidence": "  In this article, we aim to overcome these problems by proposing (i) a new MMT approach that is able to exploit (text-only) monolingual and parallel data as well as (multimodal) captioning data. we propose to adapt a strong MT model to multimodal inputs with lightweight modules...to exploit the large amount of textual data it was trained on. ", "options": ["A. The authors propose using more image data for training.", "B. The authors propose using more complex neural network structures.", "C. The authors propose using lightweight modules to adapt powerful text translation models.", "D. The authors propose using more language pairs for training."], "answer": "C", "content": "\nIntroduction\nMultimodal machine translation (MMT) typically refers to the use of additional non-textual data in text-based machine translation (MT). Here, we focus on the case where source texts are accompanied by images, the idea being to exploit visual data to improve the translation of ambiguous sentences. For example, in Figure 1 , the English word glasses can either be translated as French verres 'drinking vessels' or lunettes 'spectacles', an ambiguity which is resolved using the image.\nA main research direction of MMT has been how to best exploit image representations and combine the image and text modalities (Yin et al., 2020; Caglayan et al., 2021; Calixto et al., 2017; Li et al., 2022) . It has typically been difficult to surpass strong text-only baselines, the image modality often being ignored (Wu et al., 2021) . A major issue holding back progress is that most current stateof-the-art MMT models (Yin et al., 2020; Elliott and K\u00e1d\u00e1r, 2017; Wu et al., 2021; Li et al., 2022) are trained solely on the \u223c30k examples of the Multi30k dataset (Elliott et al., 2016) , comprising image captions and their translations. This causes two issues: (i) the models do not exploit the large amount of text-only data available and therefore perform poorly in comparison to state-of-the-art text-only MT systems, and (ii) we show that very few examples require images to be correctly translated, which means that the datasets are ill-adapted to evaluating the use of the image modality.\nIn this article, we aim to overcome these problems by proposing (i) a new MMT approach that is able to exploit (text-only) monolingual and parallel data as well as (multimodal) captioning data, and that reaches a good balance between maintaining high MT quality and effectively exploiting images, and (ii) a test set, CoMMuTE, containing contrastive evaluation pairs, where images provide the necessary context to disambiguate between multiple meanings of the same source sentence.\nOur suggested model is inspired by work on adapting frozen language models (LMs) to multimodal inputs (Sung et al., 2022; Yang et al., 2022; Eichenberg et al., 2021; Pfeiffer et al., 2022) ; we propose to adapt a strong MT model to multimodal inputs with lightweight modules (Houlsby et al., 2019) to exploit the large amount of textual data it was trained on. We also propose to better exploit the image by introducing guided self-attention and by combining the standard MMT objective with a visually-conditioned masked language modelling (VMLM) objective (Li et al., 2019; Lu et al., 2019; Su et al., 2020) . Our model obtains competitive results compared to strong text-only baselines on standard En\u2192{Fr,De,Cs} MMT benchmarks (Elliott et al., 2016 (Elliott et al., , 2017;; Barrault et al., 2018) and outperforms them and state-of-the-art MMT models on our lexically ambiguous contrastive test set. 3 2 Related Work Multimodal MT data. The reference dataset to train and evaluate MMT models is Multi30k (Elliott et al., 2016) . However, recent work has shown that most MMT systems trained and evaluated on it do not effectively exploit the image information; Elliott (2018) showed that replacing the ground truth image with a random one does not lead to the drop in performance that would be expected, while Wu et al. (2021) argued that the observed gain in performance was due to a regularisation effect. It is also notoriously difficult to beat text-only baselines on this benchmark (Barrault et al., 2018) . This may be due to (i) some subsets of Multi30k having been translated independently from the images (Elliott et al., 2016) and (ii) most of the time, the source text being sufficient in theory to produce a perfect translation (i.e. the image is not necessary; see Section 5.2 for our own analysis).\nBased on this, alternative test sets and evaluation methods have been proposed. Caglayan et al. (2019) proposed to probe the use of images in MMT models, while Li et al. (2021) proposed another training corpus and evaluation benchmark to evaluate MMT systems, but their work is only based on gender ambiguity and requires specific training data to train MMT models. Lala and Specia (2018) released a lexically ambiguous MMT evaluation dataset to evaluate models ability to disambiguate source sentences, but we found that text context is generally sufficient to translate the evaluation dataset correctly.\nContrastive MT datasets. Another means of evaluating (and the one we adopt here) is to target specific phenomena through the use of contrastive test sets. They involve evaluating models based on their ability to rank pairs of translations, where one is correct and the other incorrect. They have been used for the evaluation of different linguistic phenomena, including grammaticality (Sennrich, 2017) , multi-sense word disambiguation (Rios Gonzales et al., 2017; Raganato et al., 2019) , pronoun translation (M\u00fcller et al., 2018; Bawden et al., 2018; Voita et al., 2019) and lexical coherence/consistency (Bawden et al., 2018; Voita et al., 2019) . Bawden et al. (2018) introduced the idea of conditioning which of the translations is correct depending on linguistic context, and we adopt the same strategy here with our CoMMuTE dataset, composed of lexically ambiguous sentences whose translations are determined by the visual context.\n\nAdapting pretrained LMs to multimodal inputs.\nA lot of progress has been made through the use of pretrained LMs (Devlin et al., 2019; Conneau and Lample, 2019; Liu et al., 2020) , often trained on raw text for text-only models or image captioning data for multimodal ones (Radford et al., 2021; Alayrac et al., 2022; Chen et al., 2022) . One of the most efficient ways to learn multimodal LMs is the visually-conditioned masked language modelling (VMLM) objective (Chen et al., 2020; Lu et al., 2019; Su et al., 2020; Li et al., 2020; Zhou et al., 2021; Huang et al., 2021a; Li et al., 2019) . Inspired by the masked language modelling (MLM) objective (Devlin et al., 2019) , it consists in randomly masking input text tokens and predicting them conditionally based on the visual features. A lot of interest has also been shown in lightweight modules such as adapters (Houlsby et al., 2019) to adapt large frozen LMs to multimodal tasks (Eichenberg et al., 2021; Yang et al., 2022; Pfeiffer et al., 2022; Tsimpoukelli et al., 2021; Sung et al., 2022) in order to avoid catastrophic forgetting (De Lange et al., 2021) . Based on these approaches, we propose to adapt a strong text-only MT model with lightweight modules in order to exploit the large amount of data it previously learned.\n\nWhich type of visual features in MMT systems?\nIn terms of how images are represented in multimodal models, different strategies exist. Many works first proposed to incorporate global visual features from object recognition models pretrained Frozen with trainable adapters\nv n t 1 t n <DE> CLIP emb.\nTwo ___ wearing ___ .\n\nVisual projection Embedding men hats\nVisually Conditioned MLM\nv 1 v n t 1 t n\n\n<EN>\nFigure 2 : Overview of our approach, multimodal MT (MMT) (left) and visually-conditioned masked language modeling (VMLM) (right) objectives. We train VGAMT on both objectives jointly.\non ImageNet (Deng et al., 2009) , such as ResNet50 (He et al., 2016) , either in the form of a single vector or a set of features (Calixto et al., 2017; Elliott and K\u00e1d\u00e1r, 2017; Calixto and Liu, 2017; Yao and Wan, 2020; Helcl et al., 2018) . More recent global features extractor such as CLIP (Radford et al., 2021) exist, but to our knowledge have not been used in MMT models. Extending this idea, other works focused on entities in the image and extracted bounding boxes using a pretrained Faster R-CNN (Ren et al., 2015) in order to introduce more semantic visual information into MT (Gr\u00f6nroos et al., 2018; Ive et al., 2019; Caglayan et al., 2021) . Recent efforts have been made to only select parts of the image that are relevant to the translation of the sentence. Some proposed to use a more selective attention mechanism between modalities (Liu et al., 2021; Ye et al., 2022) , while others suggested extracting other types of visual features (Huang et al., 2021b; Fang and Feng, 2022) . Based on this, Yin et al. (2020) decided to exploit local image-text correspondences in their model Graph-MMT. Similar to their approach, we use a simpler method to extract relevant visual features, using the output queries from a state-of-the-art free-form text object detector MDETR (Kamath et al., 2021) as our local visual features (in addition to global features from CLIP). \n\nOur approach: VGAMT\nThe two main aims of our approach are to (i) exploit a maximum available data (not just multimodal parallel text data) and to (ii) provide an effective way to combine image and text modalities. Our approach, shown in Figure 2 , consists in taking a strong text-only MT model 4 and adapting it to multimodal MT. To adapt this strong text-only model to multimodal inputs, we add several lightweight modules-bottleneck adapters (Houlsby et al., 2019) and linear visual projection layers-to the otherwise frozen initial model. The bottleneck adapters are lightweight linear layers introduced after each attention block and each feedforward layer to project embeddings down before projecting them up.\nIn terms of representing visual information, we choose to use two types of representation. We concatenate local (MDETR) features and global (CLIP) features to the text inputs. We choose to use global features too, since the source sentence can describe more general aspects of the image than mere objects (such as scenes). We jointly train the non-frozen parts of our model on two distinct objectives: multimodal MT (MMT) and visuallyconditioned masked language modelling (VMLM), as described in Section 3.1. We also introduce a guided self-attention to exploit image information in a straightforward manner (see Section 3.2) in the encoder (while the decoder uses regular self-and cross-attentions and can only attend to embeddings related to text positions). We call our approach Visually Guided and Adapted Machine Translation (VGAMT).\n\nCombining training objectives\nAs shown in Figure 2 , we jointly train VGAMT on two objectives: visual masked language modelling (VMLM) and multimodal MT (MMT). VMLM (resp. MMT) consists in predicting masked tokens (resp. translating the sentence) conditioned on the image. 5 The use of the VMLM objective in addition to MMT ensures that the model does not learn to ignore the visual inputs when translating (since Multi30k is mainly composed of very standard and unambiguous parallel sentences). We make sure to mask a high percentage (25%) of the text inputs so that the model is forced to attend to the image when producing translations.\n\nGuided self-attention\nThe backbone of VGAMT is an encoder-decoder MT model, in which image features are concatenated to textual input embeddings and shared selfattention is used over the two input modalities (see Figure 2 ). Instead of using full self-attention (Caglayan et al., 2021) (connections between all image parts and all text tokens), we introduce guided self-attention. Guided self-attention consists in masking irrelevant connections between text and image representations; each text (resp. image) embedding can attend to itself and all other text (resp. image) positions, but can only attend to image (resp. text) positions conditioned on pre-extracted textimage alignments. We obtain these alignments (in the form of a cross-modal correspondence matrix) using MDETR (Kamath et al., 2021) , which detects image regions and corresponding text spans based on a free-form text (see Figure 3 and Appendix B for more details).\nConcretely, let Q, K and V denote the learnable query, key and value parameters of a standard self-attention mechanism. Attention can be defined as Attention(Q, K, V ) = A \u2022 V , where the attention matrix A = (a ij ) is defined as\nA = softmax QK T / \u221a d k\n, where d k is the dimension of the key vector, i.e.:\na ij = e Q i K T j / \u221a d k l e Q i K T l / \u221a d k (1)\nThe idea behind our guided self-attention mechanism is that we want to allow subwords to attend to all subwords, all bounding boxes to attend to all bounding boxes, but to only allow cross-modal attention between a subword and bounding boxes that are linked by MDETR (see Figure 3 ). We therefore define a binary masking matrix C = (c ij ) where (i) c ij = 1 if indices i and j correspond to embeddings coming from the same modality, and (ii) c ij is provided by the MDETR matrix otherwise: it is 1 if MDETR has created a link between subword (resp. bounding box) i and bounding box (resp. subword) j. Once this guiding matrix C is defined, we can replace the standard attention (1) with our guided attention:\nEQUATION\n)\nThe main advantage of guided self-attention over full self-attention is that the model does not have to learn to ignore irrelevant text-image correspondences since alignments are introduced as a prior.\n\nContrastive Multilingual Multimodal Translation Evaluation (CoMMuTE)\nTo overcome the flaws of existing benchmarks (see Section 5.2), we introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation dataset 6 . It is composed of 155 lexically ambiguous sentences in English, each associated with two translations corresponding to two of the possible meanings of each sentence and two images that determine which of the translations is correct. It covers English\u2192French, English\u2192German and English\u2192Czech. An example is given in Figure 4 . 2018), and we created the remaining ones. 7 We collected two images for each sentence under Creative Commons license (either Google Images or our own photos), so that the image illustrates without ambiguity one of the two meanings of the sentence. We do not restrict the image-text relation to be strictly descriptive (as for image captions) in order to have a more general evaluation dataset. Each sentence was translated into two possible translations (each corresponding to one of the images) by a native speaker of the target language.\nAppendix A provides some basic statistics.\nThe idea of CoMMuTE is to use MMT models to rank each of the two translations based on image information. The perplexity of a sentence for a given model is defined as: P P L q (y) = N i=1 q(y i ) \u2212 1 N , where q is the probability distribution output by the model, N is the sequence length and y 1 , . . . , y N is the sequence of tokens. Now, let y 1 , . . . , y N 1 be the sequence of tokens of the correct translation and y \u2032 1 , . . . , y \u2032 N 2 the sequence of tokens of the incorrect translation, a model makes a correct prediction if P P L q (y) \u2264 P P L q (y \u2032 ). i.e. the model considers the correct translation more likely than the incorrect one. For each example, we rank each of the translations based on each of the images (2 comparisons per example), and report the accuracy over all the examples. As CoMMuTE is perfectly balanced, a text-only model will get exactly 50% accuracy on this task. 5 Experiments\n\nText-only data\nAll our experiments are based on the strong MT model mBART 8 (Liu et al., 2020) , which we finetune on parallel text (see Table 1 ). We use Open-\n1 2\nWe'll have to get rid of that mole.\nIl va falloir enlever ce grain de beaut\u00e9. Subtitles2018 9 (Lison et al., 2018) , Wikipedia (Wo\u0142k and Marasek, 2014) , Ted Talks (Reimers and Gurevych, 2020) and the Books datasets (Tiedemann, 2012). We preprocess the data using Moses scripts (Koehn et al., 2007 ). 10\n\nMultimodal data\nTest2016 Test2017 MSCOCO Ambiguous (%) 21 (2.1%) 20 (2%) 6 (1.3%) (2017) and Barrault et al. (2018) released two additional related test sets (Test2017 and Ambiguous Coco). However, on analysis of these sets and as shown in Table 2 , we found that very few examples are image-dependent (i.e. the source sentence is ambiguous and the image is required to solve the ambiguity in the target language), 11 meaning that an MMT system is unlikely to perform better than a text-only system. Moreover, most of these ambiguities are semantically similar and they only cover a few multi-sense words. Although Ambiguous Coco (Elliott et al., 2017) is designed to be an ambiguous test set as it is built around multi-sense verbs, it was automatically created from sentences from MSCOCO (Lin et al., 2014) for which the textual context is often sufficient for disambiguation. These benchmarks remain useful to make sure MMT systems do not perform worse than text-only MT models on examples where images are not necessary to translate correctly. However, we consider them insufficient to assess how well MMT systems exploit images to improve translation.\nMonolingual multimodal data. For the VMLM objective, we train our model on the Conceptual Captions (CC) dataset (Sharma et al., 2018) composed of 3.3M 12 images aligned with English text.\n\nImplementation details\nFor all our experiments, we use the mBART implementation from Hugging Face (Wolf et al., 2020) . Experiments with adapters used bottleneck adapters (Houlsby et al., 2019) with a reduction factor of 8 and ReLU activation (Agarap, 2018).\nWe use the implementation provided by adaptertransformers (Pfeiffer et al., 2020) . We use a batch size of 512, the Adam optimiser (Kingma and Ba, 2014) with \u03b2 1 = 0.9, \u03b2 2 = 0.99 and a learning rate of 10 \u22124 for En\u2192Fr and 10 \u22125 for En\u2192{De,Cs}.\nWe also applied 0.1 label smoothing (Szegedy et al., 2016) during training. We selected our final model according to the best BLEU score (Papineni et al., 2002) on the Multi30k dev set after at least one full pass over the Multi30k and Conceptual Captions training sets. We ran each experiment 3 times with different seeds and report the average BLEU 13 (Papineni et al., 2002) and COMET (Rei et al., 2020 ) scores 14 and the standard errors. We also report METEOR scores (Banerjee and Lavie, 2005) in Appendix E. All experiments were carried out on 8 NVIDIA V100 GPUs for \u223c15h.\n\nBaselines\nWe consider several text-only and multimodal baselines. All baselines except the MT models finetuned from mBART were trained from scratch with the original codebases and features released by the papers' authors. Models trained on the (multimodal) MT objective only where trained on Multi30k, while models jointly trained on the (multimodal) MT and (V)MLM objectives were trained on Multi30k and Conceptual Captions.\nText-only. We trained a text-only Seq2seq Transformer (Vaswani et al., 2017) from scratch and a text-only Seq2Seq Transformer initialised from TLM weights (Conneau and Lample, 2019) . We refer to these models as Vanilla MT and TLM + MT respectively. We also trained several MT models initialised from pretrained mBART (Liu et al., 2020 ) and which we fine-tuned on parallel data (Lison et al., 2018; Wo\u0142k and Marasek, 2014) . We refer to these models as mBART + MT. 'w/ adapters' specifies that the model's weights are frozen except bottleneck adapters (Houlsby et al., 2019) .\nMultimodal. We trained several state-of-the-art multimodal MT models: Graph-MMT (Yin et al., 2020) , Gated Fusion (Wu et al., 2021) and a Seq2Seq Transformer trained from VTLM weights (Caglayan et al., 2021) (hereafter VTLM + MMT).\n\nResults and Analysis\nQuatre cyclistes font une course sur un parcours avec une foule en arri\u00e8re-plan.\nQuatre motards font une course sur un parcours avec une foule en arri\u00e8re-plan. Tables 3 and 4 show BLEU, COMET and accuracy scores for all models compared on several En\u2192{Fr,De,Cs} test sets including CoMMuTE. An initial observation is that the text-only model is a strong baseline on the three standard benchmarks (Test2016, Test2017 and MSCOCO). As mentioned in Section 5.2, most of these evaluation datasets do not need visual context to be correctly translated. Our model VGAMT is on average on par with its counterpart text-only mBART+MT w/ adapters baseline for all Multi30k En\u2192Fr test sets, while being on average just below this baseline on En\u2192{De,Cs} Multi30k benchmarks. It outperforms other MMT models with a large margin due to both the effective use of textual knowledge from the frozen MT model but also guided self-attention. Note that the scores reported for the baselines are lower than the ones reported in the original papers of the models for several reasons. First, we computed the scores on fully detokenised data to have a uniform evaluation between all models. We also report the average score from three different runs using different seeds and not the best score obtained over a single run. More importantly, our VGAMT obtains strong improvements over both text-only baselines and state-of-the-art MMT systems on CoMMuTE; our model can use visual context to disambiguate sentences. This can be seen in Figure 5 (one of the \n\nAblation Study\nTo better understand the role of VGAMT's components, we carry out several ablations for En\u2192Fr and report all results in Table 5 .\nAdapters versus Fine-tuning. We compare the results of fine-tuning an unfrozen VGAMT model (w/o adapters) in comparison to our frozen model with adapters (VGAMT), all other things remaining equal. The unfrozen version faces a drop in scores on all test sets except Test2017. Notably, the unfrozen model's accuracy score of 60.5 on CoM-MuTE is 6.6 points lower than our final VGAMT model. As well as providing a more lightweight solution that does not involve fine-tuning all parameters, using neural adapters and freezing other weights is useful in terms of performance.\nImpact of the VMLM objective. VMLM sampling probability and degree of masking. We ran experiments to vary the VMLM sampling probability (see Section 3.1) and the percentage of masked text inputs (see Figure 7 for results on CoMMuTE). For the sampling between VMLM and MMT objectives, the maximum value is reached for p =50%, i.e. equal sampling between VMLM and MMT objectives (Figure 7a ). Similar results are obtained for p = 75%, i.e. 3 VMLM batches for 1 MMT batch, but the translation quality is lower. For the percentage of masking, there is a peak at 25% masked text inputs and a constant decrease for higher values (Figure 7b ).\n\nConclusion\nWe propose a new MMT approach (VGAMT) based on (i) adapting a strong text-only MT model with lightweight adapters and (ii) introducing better use of the text and image modalities through a novel guided self-attention mechanism and joint MMT and VMLM training. We also introduce \n"}
{"question": "Which of the following models is used by LongT5 to extend the T5 model?", "evidence": "  LongT5 extends the T5 model (Raffel et al., 2020 ) by using a similar technique introduced in the ETC and BIGBIRD models (Ainslie et al., 2020; Zaheer et al., 2020) .  ", "options": ["A. BIGBIRD", "B. LED", "C. PRIMERA", "D. CDLM"], "answer": "A", "content": "\nIntroduction\nAmong recent NLP research, multi-document processing is gaining increasing attention, due to the need to handle and process an increasing amount of textual data and available documents online. A * Work partly done as an intern at AI2. 1 Our code is available at https://github.com/ aviclu/peekacross. which we split into context documents (2) and a held-out document (3), we select the most salient sentence (4) that is used for generating a question-answer pair (5).\nThen, we pre-train a model by generating the proper answer and the salient sentence, given the question and the context documents (6).\nnumber of prominent applications that are concerned with aggregating information from multiple texts are multi-document summarization (Fabbri et al., 2019; Zhao et al., 2020) , query-focused multidocument summarization (Xu and Lapata, 2020; Pasunuru et al., 2021a) , and multi-hop question answering (Yang et al., 2018; Welbl et al., 2018) . These tasks remain challenging mostly since existing NLP models are designed to handle single texts, rather than processing multiple documents at once (Caciularu et al., 2021) .\nEarly solutions for multi-text processing were task-specific and used complex architectures that were difficult to generalize across different multidocument tasks (Liu and Lapata, 2019; Wang et al., 2020; Ginzburg et al., 2021) . Efficient LMs (Tay et al., 2021; Beltagy et al., 2020) recently demonstrated that by simply concatenating multiple documents into a single sequence, the transformer can offload the goal of identifying and connecting relevant information between the documents. Recently, it was suggested that these long-context LMs can be equipped with new pre-training objectives to enable them to process multiple documents more effectively (Caciularu et al., 2021; Xiao et al., 2022; Yasunaga et al., 2022) .\nThese pre-trained models demonstrated state-ofthe-art performance on a variety of multi-document downstream tasks, and outperformed underlying LMs and task-specific architectures. Such models are often pre-trained using a dataset where each instance is a set of related documents (e.g., news articles all discussing a specific event), which facilitates modeling of cross-text relationships. Existing multi-document pre-training objectives involve unmasking tokens in a document (Caciularu et al., 2021) , or generating a salient masked sentence (Zhang et al., 2020; Xiao et al., 2022) , encouraging the model to recover missing information using other documents. While successful, these models are either limited to classification tasks (Caciularu et al., 2021) or primarily designed for summarization (Zhang et al., 2020; Xiao et al., 2022) .\nIn this work, we propose a novel pre-training objective that supports both short and long text generation, resulting in a versatile and general multidocument language model. In particular, we hypothesize that using questions and answers involving multiple documents can encourage the model to better learn and incorporate both fine-grained information (by asking questions about core information units in a specific sentence) as well as coarsegrained cross-document relationships required to generate a long text such as a summary. We show that this approach holds not only for summarization, but for other multi-document downstream tasks as well.\nDuring the pre-training of existing multidocument language models, the goal is to unmask spans (for encoder-only models) or generate masked textual spans (for encoder-decoder models) under a multi-document context. To that end, multiple concatenated sequences of related documents are fed during pre-training, thus requiring a large number of sets of related documents for an effective pre-training phase (Hoffmann et al., 2022) . In a variety of existing multi-document benchmarks, such as multi-document summarization, only small to medium-scale document clusters are readily available. These are acquired either automatically with lexical similarity and retrieval (Fabbri et al., 2019) or semi-automatically (Gu et al., 2020) , but generally, this process requires a substantial amount of human effort for filtering instances and generating high quality corpora.\nBy employing a novel multi-document question-answer generation procedure, we propose an effective method for expanding the multi-document pre-training corpora. Our approach allows us to provide multiple views for every single cluster of documents, thereby artificially increasing the pretraining data size (in terms of number of instances) via augmentation. To expose the model to a variety of contexts and diversify the pre-training data, we propose to generate multiple pairs of questions and answers and condition them on a subset of the documents' cluster. We select a salient sentence in one held-out document and then employ a recent parser to generate a high-quality question-answer pair about one predicate in the selected sentence, using a systematic semantically-oriented approach (Klein et al., 2022) . This new multi-document pre-training objective challenges the model to generate both the answer to the question as well as the salient sentence, while discarding the held-out document or parts of it (see Figures 1, 2 for illustration). This procedure exposes the model to a variety of contexts -a question and a different subset of the documents in the cluster per instance, in contrast to prior methods that provide only a single view of the cluster. Our contributions are summarized below:\n\u2022 A new pre-training approach for multidocument modeling, formulated as a crossdocument question answering task, further directing the LM to model cross-text relationships, focusing on both fine-and coarsegrained information. \n\nRelated Work\nLong-context efficient text generation transformers (Tay et al., 2021 (Tay et al., , 2022) ) extend earlier transformer models (Vaswani et al., 2017) for processing long sequences, often using a sparse self-attention architecture. Examples include the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) , and LongT5 (Guo et al., 2022) . These models demonstrated that single-text approaches be can adapted to multi-document tasks by concatenat-ing multiple documents into a single sequence and processing them using their sparse attention patterns. They sparsify the full self-attention matrix of transformers by using a combination of a localized sliding window (called local attention), as well as a global attention pattern on a few specific input locations. LED is build upon the BART model (Lewis et al., 2020) by using additional positional embeddings and global attention weights, and introduces the global attention mode that operates over pre-selected tokens. LongT5 extends the T5 model (Raffel et al., 2020 ) by using a similar technique introduced in the ETC and BIGBIRD models (Ainslie et al., 2020; Zaheer et al., 2020) , relieving the requirement to manually select global tokens by automatically globalizing the aggregated representations of groups of tokens.\nFurther strategies have been proposed for increasing these models' abilities in multi-document tasks. The Cross-Document Language Model (CDLM) (Caciularu et al., 2021) suggested pretraining a Longformer-encoder (Beltagy et al., 2020) over sets of related documents, and showed superior performance results over several multidocument tasks. Following this methodology, the authors of LinkBERT (Yasunaga et al., 2022 ) used a similar approach, but utilized Wikipedia's hyperlinks in order to curate informative pairs of linked documents for LM pre-training.\nIn order to adopt the multi-document pretraining approach for sequence-to-sequence tasks, PRIMERA (Xiao et al., 2022) , which is built on top of the Longformer encoder-decoder model (LED), selected salient sentences within clusters of related documents using a pyramid estimation approach, resembling the method presented for pre-training the single-document PEGASUS model (Zhang et al., 2020) . While this work is the closest to ours, it was pre-trained to generate masked salient sentences without any control, which makes the model potentially hallucinate while generating text, while our model uses a controlled QA-based objective. Furthermore, unlike these works, our method generates significantly more data then used to pre-train PRIMERA, which is possible to obtain by the singledocument QA generation approach. Our QA pretraining formulation allows us to generate multiple contexts per document cluster.\nAnother related line of work includes methods that incorporate large-scale QA-generated data for pre-training LMs (He et al., 2020; Jia et al., 2022 ;\n\n(a) The held-out document is discarded from the context (c) The held-out document is included in the context, but the answer in the anchor sentence is masked (b) The held-out document is included in the context, but the anchor sentence is masked\nFigure 2 : A schematic of our pretraining data modes. The salient sentence which is used for QA generation is colored in yellow. (a) The context does not include the held-out document, therefore this mode is the most challenging. (b) The held-out document is present in the context, but the salient sentence used for the QA generation is masked (red). (c) The held-out document is present in the context, but the answer span within the salient sentence is masked (red). Huber et al., 2022) . These works hypothesize and show that pre-training by utilizing generated QA data can encourage contextual representations to encode useful semantic information for other non-QA downstream tasks. Inspired by that, we conjecture that LMs can strongly benefit from infusing QA during pre-training in the multi-document setup, for adding an additional signal for modelling cross-text relationships.\n\nAugmenting the Multi-Document Pre-training objective\nIn this section, we provide the required steps for compiling the pre-training dataset for QAMDEN.\nWe next elaborate on the details of the data creation and provide analysis of the resulted corpus.\nRecent works have shown that for text summarization, pre-training LMs to generate a \"summarylike\" sequence, termed pseudo summary, inherently provides gains over general-purpose pre-trained LMs (PEGASUS, PRIMERA; Zhang et al., 2020; Xiao et al., 2022) . The data in which the PEGASUS and PRIMERA models were pre-trained on was constructed using the Gap Sentence Generation (GSG) method, which suggests masking highly-ranked salient sentences, where salience is pre-determined by a sentence-scoring method of interest. Particularly, in PEGASUS, GSG has been adopted as its pre-training objective, where some sentences in a single document are masked in the input and the model is tasked to generate them.\nFormally, for each sentence s i in a given input document D, PEGASUS computes its salience score based on its ROUGE score (Lin, 2004) w.r.t the rest of the sentences within the document (D/{s i }), i.e. Score(s i ) = ROUGE(s i , D/{s i }). Intuitively, \u2026Pokemon Sword and Shield might have already been announced, but we now know there's another new Pokemon game on the way from DeNA\u2026 QASem QA generation (Klein et al., 2022) Q1: What might been announced? A1: Pokemon Sword and Shield. (Answer length: 4) Q3: Who knows something? A: We. (Answer length: 1) Q2: Where does someone know something? A: On the way from DeNA. (Answer length: 5) Contextualization (Pyatkin et al., 2022) Q: Where did we know there's another new Pokemon game? A: On the way from DeNA.\n\nSelected\nFigure 3 : A schematic of the process of QA generation using QASEM (Klein et al., 2022) and the contextualization model from Pyatkin et al. (2021) . This is an actual sample that was created and used for pre-training QAMDEN, where the document is taken from New-SHead (Gu et al., 2020) .\nthis metric assigns a high score to the sentences that have a high overlap and share more lexical information with the rest of the sentences in the document, thus assigning high scores to prominent sentences. PRIMERA has generalized this notion to support the multi-document setup, by applying a GSG variant over a cluster of related documents.\nCross-Document GSG. We propose augmenting the GSG technique to formulate a cross-document question answering pre-training objective for multidocument tasks, instead of the existing pseudo summary generation methods. Our approach supports identification of both fine-and coarse-grained information as we describe below, and results in a substantially larger amount of pre-training examples compared to the preceding methods.\nFormally, we are given a cluster of related documents S = D 1 , D 2 , . . . , D |S| in a corpus C. Our cross-document (CD) GSG salience score for the i th sentence within the k th document in the set (s i k ), is defined by its ROUGE score w.r.t the rest of the sentences within the document (D k /{s i k }) as well as the other documents (S/D k ), i.e. CD-GSG-Score(s i k ) = ROUGE(s i k , S/{s i k }). Then, for every document k, following Zhang et al. (2020) ; Xiao et al. (2022) we select the top-scored sentence s * k , and then we use this sentence to generate a pair of a question and an answer.\nGenerating Cross-Document QAs. For generating the cross-document questions and their answers, we employ QASEM, a recent semantic parsing framework for question generation (Klein et \nfor k \u2190 1 to |Sn| do 4 s * k \u2190 arg max i CD-GSG-Score(s i k ); 5 (q * k , a * k ) \u2190 QASEM(s * k ); 6 t * k = [a * k , s * k ] # target text; 7 D \u2190 D \u222a {([Sn/D k , q * k ] , t * k )} # (a); 8 D \u2190 D \u222a {([Sn/ {s * k } , q * k ] , t * k )} # (b); 9 D \u2190 D \u222a {([Sn/ {a * k } , q * k ] , t * k )} # (c); 10 Return D;\n2022). 2 QASEM intended soliciting a manageable, discrete account of information in a text for the sake of building natural language semantic representations. It automatically labels each verbal predicate-argument relation with a questionanswer pair, where a natural language question represents a semantic role, while the answers correspond to the arguments that appear in the input text. QASEM is thus an appealing approach since it is capable of generating multiple high-quality questions given a sentence. We apply QASEM over the sentences withing the pre-training data in order to generate question-answer pairs, and then apply the model from Pyatkin et al. (2021) which transforms the question into a more natural and clear form, with contextualized arguments (see example in Figure 3 ). In order to resemble a summarization task where the generated text is typically long, we select the question-answer pair with the longest argument produced by QASEM. Formally, QASEM(\u2022) receives a sentence s * k as an input, and produces question-answer pair (q * k , a * k ), where a * k is the longest among the generated answers. See a detailed example and full description in App. A.1.\nConsidering the question-answer pair, our goal is to encourage the LM to generate the correct answer as well as the salient sentence in a multi-document context in order to learn cross-text relationships.\nData Generation Process. In order to facilitate the construction of a multi-document context, we propose three different modes, each one is responsible for uncovering information by using different contexts. For all the modes, we first generate a QA pair out of the most salient sentence in the held-out document.\n(a) Excluding the source document. In this mode we disregard the held-out document D k from the context S n given to the model, i.e, S n /D k . Hence, the model is tasked to predict the answer without having access to the source document at all, and is restricted to observe only the other documents in the set. Thus, this mode is considered as the most challenging one.\n(b) Masking the salient sentence. In this mode, the source salient sentence is masked, i.e, S n / {s * k }. The model has access to the surrounding context of the masked sentence in the held-out document, as well as the other documents in the set.\n(c) Masking the answer. In this mode, only the answer span within the salient sentence is masked, i.e, S n / {a * k }. The model has access to the surrounding salient sentence, as well as all the documents in the set.\nAs part of the new pre-training process of our novel multi-document model, we append the question after the context and instruct the model to generate an answer followed by its salient sentence, i.e., output = \u27e8answer\u27e9, \u27e8sentence\u27e9, inspired by Bohnet et al. (2022) . Generating the salient sentence introduces a copying mechanism (allows the model to also learn to copy information from the source directly) as well as allowing longtext generation, which is crucial for summarization downstream tasks (Zhang et al., 2020) , as well as outperforming a model which was pre-trained for generating the answer solely -according to the ablations study, this setup yields the best performance results ( \u00a74.4). In the pre-training evaluation phase, the held-out set was split and the loss was measured separately for each mode of the data. As expected, we observed that the loss for (a) was significantly higher than those for the other modes, with (a)\u227b(b)\u227b(c) ranking highest. The procedure for generating the pre-training data is summarized in Algorithm 1 and Figure 2 .\nThe resulted pre-training corpus. We applied our procedure over the NewSHead corpus (Gu et al., 2020) , which consists of a set of related documents per instance. This is the exact same pre-training corpus used also by our main baseline PRIMERA (Xiao et al., 2022) \n\nExperimental Setup and Results\nThis section presents experiments conducted to evaluate QAMDEN, as well as the the ablations and baselines we used. For the intrinsic evaluation we evaluated the models over multi-document QA tasks. For extrinsic evaluations we considered the multi-document abstractive summarization task.\nModel Implementation Details Following Xiao et al. ( 2022), we use the large-sized Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) for our model initialization. The length limits of input and output are 4096 and 1024, respectively. 3 Following the Huggingface implementation (Wolf et al., 2020) , we set the sliding window size to 1024 for local attention in the encoder part.\nSimilar to the PRIMERA model (Xiao et al., 2022) , when concatenating the documents and the question, we add a special document separator token (<doc-sep>) between the documents to signal to the model to be aware of the document boundaries. We also assign the global attention mode to these tokens which enables the model to share information across documents (Caciularu et al., 2021) . For further hyperparameter and pre-training execution details, see App. B.\n\nMulti-Document Question Answering\nMulti-document QA is the task of generating the correct answer, given a set of related multiple documents. For several multi-document QA benchmarks, models are often tasked to implicitly solve multiple sub-tasks or follow intermediate steps, such as comprehending the question, filtering out distracting documents in the context, and stitching pieces of information across the relevant documents (Geva et al., 2021; Caciularu et al., 2022) . Recall that QAMDEN was pre-trained over a automatically generated multi-document QA dataset. Hence, as a preliminary assessment, we first investigate QAMDEN's performance over two multi-document QA benchmarks, HopotQAdistractor (Yang et al., 2018) and WikiHop (Welbl et al., 2018) (see more details of the datasets in App. C.1), and compare to other models that were pre-trained using underling un-masking objectives.\nFine-Tuning Format. To follow our pre-training scheme, we append the question to the context and fine-tune the model to generate the correct answer. We use the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) and PRIMERA (Xiao et al., 2022) as the baselines, for assesing the contribution of our pre-trainig format. Confirmed by Beltagy et al. (2020) , we found out that appending the question: and context: prefixes before the question and the context tokens, respectively, resulted in better performance.\nBaselines. We compare QAMDEN (447M parameters) against a set of strong long-context transformer baselines, including LED (447M parameters) (Beltagy et al., 2020) , PRIMERA (447M parameters) (Xiao et al., 2022) , 4 and LongT5-xl (3B parameters) 5 (Guo et al., 2022 ) (see \u00a72). 6 Results. The results on multi-document QA are shown in Table 2 . We adopted the F1 and Exact Match (EM) evaluation metrics corresponding to the original works. Our QAMDEN outperforms both PRIMERA, LED, and LongT5, confirming that our pre-training data and input format are beneficial for both capturing cross-document relationships (QAMDEN\u227bLED) as well as exploiting both context and question (QAMDEN\u227bPRIMERA).\n\nMulti-Document Summarization (MDS)\nThis task aims at generating a summary for a given set of topically-related documents. Inherently, end-Model F1 EM HotpotQA LED (Beltagy et al., 2020) 65.8 50.6 LongT5-xl (Guo et al., 2022) 66.1 50.9 PRIMERA (Xiao et al., 2022) 65 Results. Tables 3 and 4 present the evaluation results over the Multi-News and Multi-XScience datasets, respectively. Following previous MDS works, we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.2 for details). For a fair comparison, we include the results of PRIMERA as well as the results of the previous state-of-the-art methods (Pasunuru et al. (2021b) and Lu et al. (2020) , for Multi-News and for Multi-XScience, respectively), and LED (Beltagy et al., 2020) . As shown in the results tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, especially on the Multi-News dataset, clearly demonstrating its consistent advan- (Beltagy et al., 2020) 47.4 20.7 23.7 LongT5-xl (Guo et al., 2022) 47.4 20.7 23.7 PRIMERA (Xiao et al., 2022) 49.9 21.1 25.9 QAMDEN 50.9 23.1 27.2 tage. This excludes the results for Multi-XScience where QAMDEN slightly underperforms the prior work and LongT5. An explanation which Xiao et al. (2022) points refers to the fact that the clusters in Multi-XScience have less overlapping information compared to the corpus we used, attributed to the use of abstracts as the input documents in Multi-XScience. In addition, LongT5 advantage over QAMDEN is attributed to significantly larger number of parameters of LongT5-xl.\n\nQuery-Focused Multi-Document Abstractive Summarization\nThe task of Query-focused Multi-Document Summarization (QMDS) aims at generating a summary from a set of documents, that answers a specific given query. Unlike MDS, QMDS tries to solve more realistic query-based scenarios, since it suggests summarizing only predefined salient information of interest that best answers the query. Since we proposed pre-trainng under the multi-document question answering setup, we posit that QAMDEN might be effective for QMDS.\nWe consider the datasets constructed by Pasunuru et al. (2021a), QMDSCNN and QMDSIR (see more details of the datasets in App. C.3) as well as their strong baseline, and include also the results of PRIMERA and LED.\nBaselines. Similar to the previous experiments, we compare QAMDEN against LED, PRIMERA, LongT5-xl. In addition, we consider also the baseline from Pasunuru et al. (2021a) . 37.9 16.4 35.2 LED (Beltagy et al., 2020) 32.3 14.3 30.9 LongT5-xl (Guo et al., 2022) 35.5 15.9 34.3 PRIMERA (Xiao et al., 2022) 36 Results. Tables 5 and 6 present the evaluation results over the QMDSCNN and QMDSIR datasets, respectively. Following MDS tasks and Pasunuru et al. (2021a) , we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.3 for details). As shown in the tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, clearly demonstrating its consistent advantage over the baselines.\n\nAblation Study\nData Generation. We next turn to a broad ablation study, for assessing our configuration and design choices across our suggested pipeline. First, we show the advantage of combining the three proposed data modes, rather than using a subset of them. We evaluate all the resulted models by fine-tuning them over HopotQA-distractor ( \u00a74.1), Multi-XScience ( \u00a74.2), and QMDSIR ( \u00a74.3). For HopotQA-distractor we report the Exact Match (EM) score, and for the summarization tasks we report the ROUGE-1 (R-1) score.\nBaselines. We pre-train QAMDEN for 100k steps, for using every subset of the set of the set (superset) of modes {(a), (b), (c)} (all its possible combinations) of the generated pre-training data modes presented in \u00a73. Note that our QAMDEN model is referred to as using all the modes, i.e., For QA we used the EM score, and for MDS and QMDS we used the ROUGE-1 score.\nResults. Figure 4 shows the ablation results. In all tasks, pre-training using all modes yields the best results. Among all modes, mode (c) appears to be the most effective for QA, since this is an extractive QA task, and mode (c) provides data in this format. Mode (a) excels at the summarization tasks, attributed to their abstractive nature as well as the requirement of all the documents for generating appropriate summaries.\nInput Format We repeat the previous experiment and ablate the pre-training input format according to the multiple different formats, and compare to the model pre-training format described in \u00a73 (with the same pre-training data): without questions, with random question, with random context document, with prefixes, placing the question before the context, with question filtering, and without generating the salient sentence. Additionally, we assess the choice of QASEM as our questionanswer generation module by using the generators from Jia et al. ( 2022) and Khashabi et al. (2022) . Finally, we also include the results of PRIMERA, which was further pre-trained for additional 300k steps (fine-tuning LED for 400k steps in total), for a fair comparison to QAMDEN ablated models. See full details regarding all the ablations in App. D.\nResults. Overall, our QAMDEN model outperforms the ablation models on most of the tasks, which a significant margin.\nPre-training the model without any questions during or using random questions, negatively impacts the results of downstream tasks. An impor- tant function of the question is to facilitate the model's ability to generate the appropriate answer and the source sentence. This aligns with the findings from Caciularu et al. (2021) , who showed that pre-training with random documents rather than related ones is sub-optimal. The use of question and context prefixes for positioning input appears to be helpful for QA, but is inferior when applied to summarization tasks due to its unique format, which is well suited for QA but seems to generalize harder for other setups. When the question is placed before the context, performance slightly decreases over query-based tasks, while maintaining the same results for summarization (where the question location is irrelevant).\nUsing question filtering is found to harm the downstream results of QAMDEN, in accordance to other QA-based pre-training prior works (Jia et al., 2022) .\nPre-training without generating the attributed source sentence introduces a significant flow to the model, particularly for the summarization downstream tasks. As mentioned before, generating longer sequences, as well as teaching the model to copy text, is beneficial for summarization tasks.\nApplying a different question generator rather then QASEM yields inferior results overall, since the other generators produce open-ended questions and answers which are more prone to errors, while QASEM utilizes an existing span in the context as the answer. In addition, QASEM generated local questions, which allows QAMDEN to focus on the fine-grained details, and not only the coarsegrained information in the multi-document context.\nWhen PRIMERA is pre-trained with 400k steps (to match QAMDEN's number of further pretraining steps), it underperforms QAMDEN and even fails to add any significant improvements over its 100K checkpoint, possibly due to the small amount of pre-training data it contains. \n\nComparison with Large Language Models\nIn order to get insights into how QAMDEN compares with state-of-the-art Generalist Large Language Models (LLMs), we provide a small comparison with two capable models, GPT-3.5 turbo (Ouyang et al., 2022) and GPT-4 8 (OpenAI, 2023) (including the 8k input length version) evaluated on the zero-shot setting.\nFor a fair comparison, we used the same context window size of 4K tokens for all models (and up to 8k for GPT-4 8k). Due to the fact that multidocument tasks involve processing long sequences, the cost of API calls is significant for a comprehensive evaluation across all datasets. Therefore, we only evaluate on a sample of 200 instances from the multi-news dataset (see prompting details in App. E). Table 8 depicts the results. We observe that QAMDEN significantly outperforms both GPT-3.5 and GPT-4 models, though the performance of GPT-4 and GPT-3.5 is comparable. We leave more comprehensive comparisons with LLMs to future work.\nWe further assessed QAMDEN through manual comparison against PRIMERA, GPT-3.5, and GPT-4 8k. NLP graduate students were shown summaries for a given topic from the three systems and QAMDEN in arbitrary order, along with a corresponding reference summary. Following (Ernst et al., 2022) , participants were asked to rank the systems based on Content (overlap with the reference), Readability (the readability of a summary), Grammaticality (avoiding grammar errors), and Non-Redundancy (avoiding repetitions), and we extract the pairwise results out of the rankings (see (Ernst et al., 2022) for further details). In App. F, we provide several examples to system summaries and their corresponding reference summaries.\nThe results of this study are presented in Table 9 . Under each evaluation criterion, it indicates the percentage of cases where QAMDEN was preferred over both baselines. QAMDEN was favored in all cases except for grammatical errors and readability (which corresponds to the Reinforcement Learning from Human Feedback phase of the GPT models).\n\nConclusions\nIn this work, we present a novel pre-training scheme for multi-document tasks. First, our approach suggests to augment the existing multidocument pre-training objectives into a crossdocument question answering task. Second, we generate high-quality large-scale QA pre-training data using a controlled generation approach, in which each QA pair originates from a salient sentence in one of the documents in the set.\nDuring pre-training, we task the the Longformer Encoder-Decoder (LED) model to generate the answer and the salient sentence on the basis of the remaining context. This objective encourages the LED model to elicit cross-document relationships, and stitch pieces of information across the input documents, which are relevant for performing multi-document tasks. The resulted model QAMDEN shows significant performance improvements compared to prior models under extensive experimentation over multiple challenging multidocument summarization and QA datasets.\nFuture work can extend the ideas in this work for equipping decoder-only large LMs with crossdocument modeling using our proposed method, also in the setup of in-context learning and prompt tuning. We foresee that our method should be significant specifically for retrieval-augmented language modeling setups (Izacard et al., 2022) , where there is a use of related documents as an outsourced external non-parametric knowledge source. Finally, the use of a single document in order to trigger cross-document relationships, as firstly introduced in this work, might be further investigated.\n"}
{"question": "What types of errors can models trained on synthetic data correct?", "evidence": "  This rule-based approach to synthetic data generation gave us control over the types of noise applied, and allowed us to generate evaluation data for each error type The results show that they do not manage to correct the synthetic categories much beyond the baseline, except for mBART-ENIS in some cases ", "options": ["A. Grammatical errors", "B. Spelling errors", "C. Semantic errors", "D. All types of errors "], "answer": "D", "content": "\nIntroduction\nSpelling mistakes due to typos and rushed writing, nonstandard punctuation and spelling, and grammatical and stylistic issues are common to almost everyone who writes any kind of text. This applies in any language and can distract the reader or make the communication miss its mark. This can hinder people who have difficulties writing text conforming to a particular language standard, be it due to disability, dyslexia, linguistic background, limited access to education or any other reason. Prejudice against people whose writing deviates from the standard can make some shy away from communicating with others, leaving their voices out of important discussions and restricting their opportunities (Alexander-Passe, 2015) .\nGrammatical error correction (GEC) is the task of adjusting a text's spelling, grammar, and linguistic style to conform to an approved language standard or convention (Rauf et al., 2017) . While the latest work on GEC is based on Transformer models (Vaswani et al., 2017) , the subword tokenization methods commonly used in these models are a source of problems when it comes to typos and other variants (Schmaltz et al., 2017) . Subword tokenization (Sennrich et al., 2016; Kudo, 2018) was presented as a solution to the open vocabulary problem, as it is a compromise between character encoding and whole word encoding, enabling unknown words to be represented using known subwords.\nHowever, a significant downside of subword tokenization is how it is affected by noisy input; if a word contains a typo or other spelling variants, this can completely shift its representation. In addition, in languages with rich morphology, a word can have many different surface forms, some rarer than others, that all carry the meaning of the base word, but appear in different syntactic contexts. A subword-tokenized model may struggle to capture the nuances of such a language effectively since it may need several different subwords to represent a single word, depending on spelling and context. When an unfamiliar variant of the word appears in unseen text, the model is challenged to decode it correctly, even when it results in uncommon subword units.\nOur motivation is that a byte or character-level approach should intuitively be more robust to spelling or morphology variations, as it is not constrained by the subword vocabulary. We explore using a byte-level architecture, ByT5 (Xue et al., 2022) , for correcting everything from typos to com-Figure 1 : Overview of training data and comparison of output. The Icelandic-English mBART-ENIS model and the multilingual ByT5 and T5 models are first trained on generated parallel error corpora before being adapted on curated (collected) true error corpora in Icelandic. The final models are compared on an error correction (EC) task in Icelandic. The example demonstrates how the byte-level model performs well while the subword model cannot see the individual characters in every word, leading to degraded performance. plex grammatical issues in text. The language studied is Icelandic, a highly-inflected North Germanic language. For instance, the morphological complexity in Icelandic means that nouns can have up to 16 different surface forms, and adjectives over 50. GEC for a morphologically complex language needs to go beyond correcting only single words or limited phrases; it needs to consider the syntax of the whole sentence. This is the case for Icelandic but also for other languages with rich morphology, such as Arabic, Hebrew, Polish, Basque, Lithuanian and Hungarian, to name a few.\nWe compare the performance of the byte-level architecture to two subword-based architectures; ByT5's predecessor, mT5 (Xue et al., 2021) , and an mBART (Liu et al., 2020) model that has been pretrained further on both Icelandic and English. We employ real and synthetic error data for training, and present models and a framework for error generation methods that can be adapted to other languages. For under-resourced languages such as Icelandic (R\u00f6gnvaldsson, 2022) , using synthetic training data makes neural training for GEC a viable option.\nOur main contributions include a comparison between subword tokenization and byte-level tokenization for GEC when training over a combination of curated and synthesized data. We demonstrate how byte-level models not only bypass subword-related issues, but can also correct long-range errors in text. We release our error generation framework as well as models for GEC using byte-level and subword tokens in Icelandic. While our work focuses on the Icelandic language, we have no reason to believe that similar results do not hold for other languages, particularly those similar to Icelandic in terms of morphological complexity.\n\nRelated work\nThe bulk of research on grammatical error detection and correction has been focused on English and English learner texts, due to existing training data and benchmarks, and the large market of English learners worldwide who benefit from an automatic language correction tool (N\u00e1plava and Straka, 2019) . While spelling and grammar errors appear in every language, each language has its own set of error types that are more common than others, due to different phonetic, morphological and syntactic characteristics.\n\nSynthetic data generation for GEC\nThe problem of data scarcity in GEC, when approached as a sequence-to-sequence task, is typically addressed with synthetic data generation (Stahlberg and Kumar, 2021) . One approach to cre-ating ungrammatical sentences uses random character noise and simple rules to manipulate the text. Another approach is using a spell checker in reverse to noise text (Grundkiewicz and Junczys-Dowmunt, 2019) . In contrast, others have used probabilities derived from an annotated corpus of naturally occurring errors to corrupt text (Felice and Yuan, 2014) . Recent efforts widely employ neural networks to create synthetic errors (Stahlberg and Kumar, 2021) ; many use methods derived from machine translation (Junczys-Dowmunt et al., 2018) , for example, by creating worse text using deliberately bad translation models (Xie et al., 2018; Zhou et al., 2020) or roundtrip translations between languages (Lichtarge et al., 2019) . Yet another option is to leverage available resources with edits, such as Wikipedia edit histories, to generate corrupted corpora (Grundkiewicz and Junczys-Dowmunt, 2014) .\n\nSequence segmentation for GEC\nGEC can essentially be considered the task of generating grammatical target text from an ungrammatical source, similar to machine translation. The idea of approaching GEC as a machine translation problem dates back to 2006 (Brockett et al., 2006) , and this approach has since become the most prevalent method of GEC, with the focus shifting from statistical machine translation (SMT) to neural methods as they developed (Yuan and Briscoe, 2016; Ji et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018) . However, phrase-based SMT continued to be the state-of-the-art for GEC for longer than in the field of interlingual neural machine translation (NMT) (Junczys-Dowmunt et al., 2018) . This is partly because of the data scarcity problem and partly because of the tokenization methods typically used in transformer models. Breaking words up into subword units decreases the vocabulary size while addressing the out-of-vocabulary problem (Sennrich et al., 2016) . A prominent drawback of this approach is that the fixed subword vocabulary makes the models sensitive to noise in the text (Tay et al., 2021; Eger and Benz, 2020) .\nIn a subword-based GEC model, when a word contains a typo or is spelled unconventionally, it may look like an unknown word, for which no known representation exists. The model may then segment the word differently from what was seen during training, causing mispredictions. If the subword representation for \"different\" is [_diff, er, ent] , but the word is misspelled as \"diffirent\", its subwords might be [_diffi, ren, t], and a subword-based GEC model might correct the typo by outputting a different word, [_diffi, cul, t] . This issue is highlighted in Figure 1 , also showing how a byte-based approach is not limited by this issue.\nThis is also true for unseen words that are correctly spelled, such as foreign-named entities, which can lead to the subword-based GEC model \"correcting\" a perfectly spelled word it has not seen before, by replacing it with the most likely candidate. In the sentence \"The tournament was held in Espoo, Finland.\", the place name \"Espoo\" may be represented by a single subword token Espoo. Since this token is unfamiliar, the model finds the most likely subword token for this particular sentence, Helsinki. 1 This changes the semantics of the sentence and can introduce serious errors. The result is a grammatically correct and meaningful sentence, but the semantics have drifted away from the original text.\nDue to this known shortcoming of subword tokenization (Schmaltz et al., 2017) , efforts have been made to design architectures where the characters or the underlying bytes are used directly as input tokens. Byte and character-level models inevitably result in much longer sequences than subword models, making them more costly to train, and slower in inference. Some truly token-free general-purpose architectures that are increasingly competitive to token-based models have emerged recently, including CANINE (Clark et al., 2022 ) (character-level), PIXEL (Rust et al., 2023 ) (text-to-image), and ByT5 (Xue et al., 2022) (byte-level) . The training of ByT5 is based on the subword-based multilingual mT5 (Xue et al., 2021) approach, but in comparison, the model is equipped with a heavier encoder (three times the depth of the decoder). Compared to mT5, ByT5 is more robust to noisy input, but inference is slower (1.5 to 2.6 times slower on average on a transliteration task, and up to 9 times longer on tasks with longer input sequences) (Xue et al., 2022) . Another approach to making subword models more robust is using subword regularization to produce multiple segmentations of the same word (Kudo, 2018; Provilkov et al., 2020) . This is commonly used for addressing the open vocabulary problem and noisy data, such as ungram-matical text.\nDespite the reported advantage of character or byte-based Transformer models on noisy text (Libovick\u00fd et al., 2022) , work using this approach to GEC is not very common. A notable exception is work for GEC in the Lithuanian language, where ByT5 has been used for diacritics restoration (Stankevi\u010dius et al., 2022) and limited GEC (Stankevi\u010dius and Luko\u0161evi\u010dius, 2022) . They generate synthetic data by applying noise (common typographical errors, swapping letters for similar sounding ones, other non-grammatical word-level noise) to a crawled and filtered corpus and compare results when training with T5 and ByT5. Their findings agree with ours that the byte-level model outperformed the subword model.\nOur work deviates from that of Stankevi\u010dius et al. (2022) in the following key ways: We (a) generate more sophisticated and realistic errors using grammatical information from part-of-speech (PoS) tags and using custom rules based on empirical findings; (b) combine generated error data with true error corpora from a wide range of demographic sources; and (c) explore in detail which error and text types benefit the most from finetuning on true error corpora, as opposed to training on synthetic data. As far as we are aware, no attempt at bytelevel Transformer-based GEC, trained on synthetic and real error corpora, has been published. Concurrent work using ByT5 for Icelandic is (Jasonarson et al., 2023) , where errors in the OCR output of historical Icelandic texts are corrected using a generated corpus of errors extracted from real data.\nCurrent state-of-the-art in GEC is based on sequence-tagging methods (Omelianchuk et al., 2020) , which instead of generating whole sequences, tag the erroneous sentence with its corrections, and on sequence-to-sequence methods, as has been described. Further work has explored automatic character transformations for GEC tagging (Straka et al., 2021) to better handle character-level errors. One of the current highest-scoring models on English GEC benchmarks is gT5 (Rothe et al., 2021) , which is based on the mT5 model.\n\nPrior work for Icelandic\nApart from some rule-based spell checkers that don't make use of the full context, one rule-based correction system exists for Icelandic, based around parse trees, GreynirCorrect (\u00d3lad\u00f3ttir et al., 2022) . This system is contingent on the sentence parsing according to a pre-defined context-free grammar, and can only handle issues that fit pre-defined rules. This setup is both a strength and a weakness as the system is highly configurable and capable of many things, such as detecting syntactic inconsistencies and errors, and can give the user useful information on the errors found. Still, when a text has many errors, complexity builds up and rules can start interfering. Sentences containing many issues, such as from users with dyslexia, generally have lower accuracy using this method.\nThe work presented here is the first where neural networks are used in GEC for Icelandic. Snaebjarnarson et al. (2022) use neural methods for detecting such errors, but not for correcting them.\n\nCurated dataset\nA single collection of parallel error corpora exists for Icelandic, the Icelandic Error Corpus (IceEC). The corpora are annotated and corrected by language experts (Arnard\u00f3ttir et al., 2021) . The dataset is highly granular in its categories, containing hundreds of labels, but with a limited number of highlevel groups (coherence, grammar, orthography, style and vocabulary).\nThe IceEC is split into a larger general corpus and three specialized corpora (Arnard\u00f3ttir et al., 2022) . The general one contains 58,239 sentences of student essays, online news texts and Wikipedia articles. This corpus is annotated with around 50k errors of different categories. The three specialized corpora are much smaller and contain texts from Icelandic language learners (6270 sentences), dyslexic native speakers (1362 sentences), and children (2070 sentences), volunteered by the users themselves. These smaller corpora contain more errors per sentence than the general one, and add diversity to the training data.\nThis curated error data was used for finetuning our models, and combined into one training dataset for a total of 64k input sequences (single sentences), after setting aside validation and test data. The general IceEC also includes a 5.3k sentence test set used for evaluation.\n\nSynthetic dataset\nWe applied a diverse set of methods for error generation, both using linguistic knowledge and random noising methods. This rule-based approach to synthetic data generation gave us control over the types of noise applied, and allowed us to generate evaluation data for each error type.\nAs our basis of correct text to be noised, we used the Icelandic Gigaword Corpus (IGC) (Steingr\u00edmsson et al., 2018) , a collection of Icelandic editorial texts. These are mostly news articles, published literature and legal texts. We selected from this corpus those text sources that are the most likely to have been reviewed as part of the editorial process of each publication/source (literature, journals, news, laws, adjudications and transcribed parliamentary speeches). Some of these texts still have their share of typos and other errors and inconsistencies, especially the news articles, which were deemed important training data because of their general vocabulary, not found in the more formal text sources. As a preprocessing step, we filtered out lower-quality and irrelevant sentences, by removing sentences containing mostly foreign texts, illegal characters and words with known misspellings, sourced from lists of common misspellings. The corpus was tokenized using the Greynir Tokenizer (\u00deorsteinsson et al., 2019) and PoS tagged using the GreynirSeq tagger (Snaebjarnarson et al., 2022) .\nWe generated three categories of errors: 1) noise within words; 2) noise at the sequence level; and 3) grammatical and morphological modifications. The first two resemble those used when noising backtranslation data (Edunov et al., 2018) . The third type is based on using available tools and linguistic knowledge to create errors that are unlikely to be formed randomly, but resemble those of human writers.\nIn order to explore to what extent subword and byte-level models can learn and generalize grammatically complex issues in a morphologically rich language, we go beyond naive language-agnostic noising of text. A more detailed explanation of the Icelandic-specific noise is given in Appendix C. The noise methods are shown in Table 1 . This is by no means a finite list of linguistic variants or errors found in Icelandic texts, but constitutes examples chosen for studying the model performance on these more grammatically complex challenges.\nThe error generator allows for noising levels to be configured via hyperparameters. Experiments with different noise ratios in the synthetic data showed that highly noised text provided the best training examples, without the models learning to \"overcorrect\", i.e., to introduce false positives. Instead of producing even more synthetic data, we geared up the noise to produce highly error-dense examples, setting the random and more naive error noise to appear in 80% of cases, and the rule-based error noise to be used wherever possible. 2 Examples of parallel sentences with and without synthetic errors are shown in Table 2 . A total of 35M synthetic parallel sentences were generated using these methods. 2000/4000 lines were set aside for validation/testing, respectively, and special evaluation sets were generated for each error type (see Section 4).\n\nModels\nWe compared three model architectures to evaluate the differences between using subword tokenization and a byte-level method. Comparing models with different architectures calls for defining which factors are compared. In particular, byte sequences are longer than subword sequences when counting the number of tokens, roughly 4 times longer on average in the original multilingual mT5/ByT5 training data (Xue et al., 2021) . And as Xue et al. (2022) note, the mT5 architecture tends to need less training time than ByT5. We compared the models after an equal amount of training samples (100k updates). We also continued the training of the ByT5 model, using more than five times the number of updates.\n\nmBART\nWe continued training of the pretrained multilingual BART25 model (mBART) (Liu et al., 2020) , using the original pretraining objective on texts in Icelandic and English. The training of the model is detailed in Appendix A. This model, mBART-ENIS, was then finetuned on the synthetic error data, to teach it to correct errors in Icelandic texts.\nTraining on the synthetic error data was performed with an effective batch size of 3000 input tokens (roughly 60 sequences), a learning rate of 3e-5 with an inverse square root scheduler, 0.1 dropout, 0.3 attention dropout, 1000 warmup steps, 0.1 label smoothing, no weight decay, and using the Adam optimizer, for 100k updates on an A100 card for a day. 3 In addition to the above experiments, we conducted separate experiments using segmentation regularization (Kudo, 2018) to introduce more noise to the training examples and explore alternative measures to mitigate the subword tokenization problem. The BART architecture uses unigram subword units; we applied subword regularization with \u03b1 = 0.7 and keep all other parameters unchanged.\n\nByT5\nFor the byte-level approach we employed the ByT5base model (Xue et al., 2022) , which is based on the multilingual T5 model (Raffel et al., 2020) , but operates on bytes instead of subwords. The ByT5 model is pretrained on over 100 languages, but has only seen a limited amount of Icelandic. The mC4 dataset which is used to train ByT5 and mT5 is also lacking in quality for low-resource languages, in particular for Icelandic, as shown by Snaebjarnarson et al. (2022) .\nThe pretraining task in ByT5 has been adapted\nto a byte-level model, with span infilling based on bytes, not subwords. Apart from this, the main difference between the mT5 and ByT5 model architecture is the heavier encoder of ByT5.\nSequences in byte-level architectures are long and correspond more or less to the number of characters in Icelandic, resulting in increased training time. We trained the ByT5-base model using a maximum sequence length of 512 bytes, which was found to be a reasonable compromise, as most sentences in Icelandic texts are shorter than this.\nThe ByT5-base model was finetuned on the synthetic data with an effective batch size of 32 sequences (sentences). The learning rate was set to 2e-5 using the Adam optimizer with 1000 warmup steps and no weight decay. This model was further trained for a total of 550k updates, or 13 A100 card days. 4\n\nmT5\nFor a more direct comparison of byte-level and subword-level models, we also finetuned the mT5base (Xue et al., 2021) model on the same data, with the notable difference to the mBART model that it was not further trained on Icelandic. The mT5 models were pretrained on the same data as ByT5 and have a similar architecture, as described above, and are thus as comparable as subword and byte-level models can be. As previously mentioned, mT5-base is the base for the state-of-the-art gT5 (Rothe et al., 2021) model for multilingual GEC.\nWe finetuned the mT5-base model on the synthetic data using the same parameters as in our ByT5-base finetuning and evaluated it at 100k updates. 5 \n\nFinetuning on curated corpora\nUsing the curated error corpora (IceEC), we finetuned the byte-level and subword-level models to convergence. For the mBART model, this meant training with a learning rate of 2e-6 for 53k updates (67 epochs), with attention dropout set to 0.1, weight decay to 0.001 and other parameters being the same as during the synthetic finetuning.\nThe ByT5 and mT5 models were finetuned with a learning rate of 2e-6, other parameters were the same as during finetuning on the synthetic data. The ByT5 model had converged at 120k updates (60 epochs), while the mT5 was still improving on the validation data at 200k updates (100 epochs), but with time we found it forgot too much of the synthetic error correction task. We report evaluation scores at 130k.\nFor comparison, we also finetuned the different models (mBART-ENIS, mT5 and ByT5-base) on the IceEC data only, without the synthetic finetuning phase. This was done to examine how much the models learn from the added synthetic examples, and how far we can get using a small amount of hand-corrected examples. The mT5 and ByT5 models were trained for 100k updates and the mBART-ENIS model for 10k updates.\n\nResults\nDifferent metrics exist for evaluating GEC performance, but most are language-specific, and have not been adapted to Icelandic. Here we employ a language agnostic metric for scoring our models, the GLEU score (Napoles et al., 2015 (Napoles et al., , 2016)) . GLEU 6 is a variant of the BLEU (Papineni et al., 2002) score used to evaluate machine-translation output. It has been modified to account for both the source and the reference, by rewarding overlap between the source and the target sentence, and penalizing n-grams that should have been changed, but were not.\nWhen evaluating GEC for English, ERRANT (Bryant et al., 2017 ) is commonly used. It is a spanbased correction method that uses the F 0.5 metric, where precision weighs twice as much as recall. Though this metric has not been implemented for Icelandic, we also report ERRANT scores using a language-agnostic approach, disregarding error types and only reporting the span-based F 0.5 scores for each test set. These results are shown in Table 5 in Appendix E; they align well with the GLEU results in Table 3 which are described below.\nWe consider a variety of curated and synthetic test sets to get a good overview of the differences between the byte-level and subword-level approach for GEC. For the real errors, we report scores over the IceEC.test set, the test set from the IceEC, which contains around 5000 sentences. In con-trast, the dyslexic, L2 and children test sets contain 500 held-out sentences each from the respective specialized error corpora described in section 3.1 (only 100 examples were collected for the dativitis error type, a rarer occurrence in the data). We also annotated a small dataset (163 sentences) of data from an Icelandic news outlet (news), where each sentence contains at least one error; this is further described in Appendix B.\nFor the synthetic errors, we report GLEU scores over the test.synth set, which contains around 4000 held-out sentences from the synthetic data. Furthermore, we generated test sets of synthetic examples, each containing a particular error type in each sentence (dativitis, spaces, commas, dupl-words, mood, rand-noise, noun-case). This last group of test sets was generated using source texts that, while editorial, may include other errors, just like the synthetic training examples. The models, as they get better, learn to correct these errors as well. This may paradoxically lower the GLEU score as the corrected output deviates from the erroneous reference. These generated test sets still provide valuable information about what the models learn about each error type in isolation.\nTo understand what approach is best suited for GEC we trained the models on different data combinations and using different pre-trained models. The Synth-100k models are all trained for 100k updates on the same synthetic data, and the Synth-100k-EC models are additionally finetuned on the curated IceEC error corpus. To provide a baseline for the GLEU scores, we also report no_corr scores, where the source text is not corrected. This gives some idea of the noise level of the test sets, with test.synth being the noisiest and IceEC.test containing the least noise.\nThe GreynirCorrect (\u00d3lad\u00f3ttir et al., 2022) (GC) results were produced by applying GreynirCorrect to the test sets in its two configurations; correcting single-word errors on the one hand (spell.) and all errors found on the other (all). The GC system was developed in part to focus on the error categories and error frequencies defined in the IceEC training data, and this may be reflected in the scores for the IceEC test sets (asterisked in Table 3 ). The news test set ( \u2020) was created using only sentences flagged by GC (see Appendix B) and is therefore heavily biased towards that system. Models trained only on the synthetic data (Synth 100k) generally perform best on the synthetic er- ror corpora and thus solve the more simple and systematic spelling errors such as mistakes in punctuation, missing white space and word duplication. Their performance on the curated error corpora is somewhat lacking. In contrast, models trained only on the curated error corpora (EC) generally produce somewhat better GLEU scores on the curated error corpora than the Synth 100k models, but do not generalize to the error categories presented in the synthetic test sets. They are also unable to correct multiple errors in a single sentence (test.synth) .\nTraining on the synthetic data and then finetuning on the curated error corpora (Synth100k/ Synth550k+EC) performs best on the curated errors and retains much of the performance on the synthetic test sets. In all of these experiments, we can see that the ByT5 models generally perform better than the subword counterparts. This is also reflected in the ERRANT scores in Appendix E, Table 5 , where the ByT5 models score highest overall.\n\nDiscussion & Conclusion\nOur results show that the ByT5 models are the overall high-scorers on the real-world test sets, and on most of the synthetic ones. We include finetuning results on the ByT5 model that has been trained for longer on the synthetic data (550k updates) to compare how performance improves with time. We see the GLEU scores keep going up with time, and more importantly, when taking a close look at the actual generated output, this is the model that best corrects real-world errors. This makes it the most feasible option for use in real-world scenarios. A comparison of the output of the models trained on both data sources is shown in Appendix D.\nAn example from the test data is when the subword-tokenized model mBART-ENIS-Synth100k+EC incorrectly changes the name of a person from a rare name (\"L\u00e1retta\") to a more common one (\"L\u00e1ra\"). This kind of error is not seen in the byte-level model, which is quite conservative in its corrections of unknown entities. While this means ByT5 occasionally misses actual errors, we find that it is much better suited for production than a subword-level model that makes serious semantic errors. These more nuanced error correction examples may not be fully captured by the automated metrics, but are crucial for real-world use.\nThe subword regularization experiments are included as an alternative approach for mitigating the subword tokenization problem. The results are marginally better than the model without subword regularization when trained on the synthetic data, and the model performs better than the ByT5-Synth100k model in the case of dupli-cate words, which linguistically is a quite trivial task, and in more intricate mood errors. It however doesn't do any better than the mB-ISEN-Synth100k trained without subword regularization on the curated datasets, and this also holds when the model is finetuned additionally on curated data. The model finetuned on only the curated data with subword regularization (mB-ISEN-reg-EC) however performs consistently much better than its counterpart without subword regularization, often on par with or surpassing ByT5. This model has not seen any of the highly noised synthetic data, and thus has the most to gain from the subword noise. We speculate that this is one of the reasons we don't see more gains from adding subword regularization; the training examples are already so highly noised that there is not much to be learned from the added subword noise.\nThe IceEC finetuning data contain real-world errors which have been hand-corrected. These texts are somewhat different from the highly noised training examples with synthetic errors, have fewer errors on average and are more varied as they are naturally occurring. They also include stylistic edits from the reviewers, which improve the text's fluency, but in those cases the original is not necessarily incorrect as per the language standard. With these differences in mind, we expect the models to have to forget some of the synthetic error correction task in order to adapt to this \"new\" denoising task. We see this happen in the mBART-ENIS finetuning on the curated data, and to a lesser extent in the ByT5 finetuning. The denoising task performance on the synthetic errors from the previous step has in part been lost, which is expected, since some of these errors are not particularly common in real texts.\nFor the more grammatically complex error-types in the synthetic data (dativitis and changes to noun cases and verb moods), we find that the mBART-ENIS trained on synthetic data generally does well; for some subsets even surpassing the ByT5 counterpart that was finetuned on curated corpora. We suspect that this has to do with the linguistic knowledge the model has already gained during its pretraining on Icelandic texts, as explained in Appendix A. The ByT5 model that was trained for longer however manages to surpass it on the mood error type, indicating that it is still adapting to the Icelandic language, alongside its primary denoising task.\nThe models trained on only the finetuning data perform the worst throughout. The results show that they do not manage to correct the synthetic categories much beyond the baseline, except for mBART-ENIS in some cases. We expect that this has to do with their extra knowledge of Icelandic and the denoising objective used in the synthetic error correction finetuning. The results for these models on the curated in-domain test sets are in fact mostly on par with the models finetuned on the synthetic data only. Looking at the generated output, we see that the error types these models correct are not the same as those that the syntheticonly models are able to correct, which is expected, as they are trained on different data.\nWe conclude that adopting a byte-level approach rather than a subword approach leads to best results for the task of GEC, at the very least in the case of a morphologically rich language such as Icelandic. Finally, we find that the optimal way of capturing a wide range of errors is to train on a combination of synthetic and curated data, particularly when the curated data is limited.\n"}
{"question": "What did the human evaluations of SCoTD-generated chain-of-thoughts reveal?", "evidence": "  Q1: Does SCoTD result in higher-quality chainof-thoughts? Test: OPT-1.3B versus OPT-1.3B + SCoTD. Result: Yes.  We assess this hypothesis on two subsets of instances: 1) a pure random sample (N=900); and 2) a set of instances for which both models eventually predicted the correct label (N=654). The second setting focuses more closely on the chain-of-thoughts themselves rather than the predictive accuracy of the model. SCoTD is superior in both settings: for the random sample setting, SCoTD won in 59% of cases (p<.001), whereas in the correctness controlled setting, SCoTD won in 61% of cases (p<.001). Results hold with p < .05 for each QA dataset individually. ", "options": ["A. SCoTD chain-of-thoughts were rated lower in quality.", "B. SCoTD chain-of-thoughts outperformed those generated by OPT-1.3B.", "C. There was no significant difference in chain-of-thought quality.", "D. SCoTD chain-of-thoughts were only superior in a correctness-controlled setting."], "answer": "B", "content": "\nIntroduction\nEmpirical scaling laws suggest that the accuracy of Large Language Models (LLMs) on benchmark tasks can be improved by increasing model size and pre-training data volume (Hoffmann et al., 2022) . Beyond these training-time improvements, however, an inference-time strategy dubbed \"chain-ofthought\" (CoT) prompting, 1 i.e., eliciting verbalizations of predictive processes via key-phrases like \"Let's think step-by-step\" (Kojima et al., 2022) , can *Work done during an internship at AI2. Figure 1 : Symbolic Chain-of-thought Distillation (SCoTD) applied to a student model, ranging in size from 125M-1.3B parameters. We show that fine-tuning on a (diverse and potentially filtered) corpus of expert chain-of-thought demonstrations from a teacher model is an effective strategy to make smaller models capable of chain-of-thought reasoning.\nsimilarly improve performance, e.g., Suzgun et al. (2022) demonstrate additional performance gains on a hard subset of the BigBench tasks (BIG-bench collaboration, 2022) using chain-of-thought.\nHowever, chain-of-thought prompting has only been shown to be beneficial for models of sufficient scale (e.g., with more than 60B parameters (Wei et al., 2022b) ). In this work, we study whether small language models can be \"taught\" the capacity for chain-of-thought reasoning by larger language models. We adopt a simple strategy, which we call Symbolic Chain-of-thought Distillation (SCoTD): first, we sample chain-of-thought rationales from large language model given (unlabeled) input instances from a dataset; then, we train a smaller language model to predict the sampled rationale CommonsenseQA: Q: The man working in the attic swatted away a bee, but soon the single bee was an entire what? (a) swarm; (b) pack; (c) countryside; (d) soft drink; (e) field of flowers GPT-3: The answer must be related to bees, but also connected with being numerous. Of the above answers, only swarm fits the scenario. So the answer is: (a) OPT-1.3B: The answer must be a swarm of bees. Of the above choices, only soft drink is used to describe a swarm of bees. So the answer is: (c) OPT-1.3B + SCoTD: The answer must be the name of something that has bees. Of the above choices, only swarms have bees. So the answer is: (a) QuaRel: Q: Steve is waiting at the gym ... He starts hitting around a hockey puck with his stick. The gym has a carpeted floor and more friction than the ice rink he goes to. Which surface is rougher? (A) carpet (B) ice rink GPT-3: When something is smoother, it is easier to slide on and easier to pass through. So the carpet is rougher. So the answer is: (A) OPT-1.3B: _________ is rougher than carpet. Thus, the gym floor is rougher than the ice rink. So the answer is: (A) OPT-1.3B + SCoTD: When something is rougher, it has more friction. Thus, the gym has more friction than the ice rink he goes to. So the answer is: (A)\nOpenBookQA: Q: Magnets may be attracted to (a) clay pots (b) roofing nails (c) paper plates (d) plastic cutlery GPT-3: Magnets are attracted to metal objects. These objects include roofing nails. So the answer is: (b) OPT-1.3B: Magnets are attracted to clay pots, roofing nails, paper plates, plastic cutlery. So the answer is: (d) OPT-1.3B + SCoTD: Magnets may be attracted to some metals, but not to clay pots, roofing nails, paper plates or plastic cutlery. So the answer is: (b) Table 1 : Few-shot chain-of-thoughts produced by GPT-3 (code-davinci-002, the teacher model), OPT-1.3B (the un-distilled student model), and OPT-1.3B + SCoTD (ours), the student model trained using Symbolic Chainof-thought Distillation. \u00a73 shows this process significantly improves the task-accuracy of the student model in a variety of settings, and in \u00a73.1.1, human evaluations show that, even when the un-distilled student model happens to get the multiple choice question correct (see QuaRel example), humans tend to prefer OPT-1.3B + SCoTD. and sampled label. This process follows the \"symbolic knowledge distillation\" paradigm as in West et al. (2022) , wherein corpora are sampled from a larger language model to serve as training data for a smaller one.\nWe find that through SCoTD, smaller language models learn to self-rationalize and perform significantly better on 3 commonsense QA tasks compared to learning without rationalizations. This result holds for both supervised and few-shot settings, and across student models of varying scales (125M-1.3B parameters). Performance gains are especially pronounced when applying distilled chain-ofthought models to difficult scenarios like: contrast sets (Gardner et al., 2020) ( \u00a73.4 ; SCoTD significantly outperforms supervised learning on labels) and fully held-out tasks ( \u00a73.5; few-shot SCoTD significantly outperforms in-context learning).\nKey to the success of this process is sampling a relatively large number of rationales per example from the teacher model (e.g., 30 rationales/example) (Figure 2 ). This is different from many prior practices that train with one rationale per example (Camburu et al., 2018; Li et al., 2022a) . In ablation studies, we investigate several competing hypotheses for what are the most important factors within the corpus: we filter the corpus to CoTs that are assigned high probability by GPT-3 vs. filtering to CoTs that are diverse vs. filtering to CoTs that explain more open-ended input instances.\nWhile diversity and high probability are reasonable filters that on average perform well, the \"null hypothesis\" of random downsampling performs well, suggesting that the sheer volume of the rationales is also a key contributing factor.\nWe will release code and the corpus of sampled chain-of-thoughts at https://github.com/ allenai/cot_distillation.\n\nSymbolic Chain-of-Thought Distillation\nOur primary goal is to improve the accuracy of a (relatively small) student language model S on a target classification 2 task D Test = {(x i , y i )}. 3 We assume access to 1) (an unlabeled) training set D Train = {(x i )}; and 2) a large teacher language model T (e.g., GPT-3 (Brown et al., 2020) ), capable of generating chain-of-thoughts in a few-shot fashion.\nOur first step is to curate a set of labeled chainof-thoughts to serve as few-shot Prompts for T . For each target task, we sample a small number (e.g., 10) of examples x i from D Train , provide a gold classification label y i , and manually author a chain-of-thought z i for each to form the prompt set P = {(x i , y i , z i )} 4 . Then, for each x i in D Train , we sample N chainof-thoughts zi along with the resulting prediction \u1ef9i from the teacher model, i.e.,\n(\u1ef9 k i , zk i ) \u223c N T (y i , z i |x i , P).\nThe result of this sampling is a corpus\nC = {(x i , {(\u1ef9 k i , zk i )} N k=1\n)}, which contain teacherpredicted chain-of-thoughts/labels. Depending on the experimental setting (details in \u00a7 3), we sometimes filter the entries of C, e.g., in the fully supervised case where D Train instances have associated labels, we discard samples for which the sample the teacher model predicted an incorrect label. Next, we train the student model using the standard language modeling loss, i.e., we maximize\nE (x,\u1ef9,z) \u223c C [S(\u1ef9, z|x)].\nAfter fine-tuning the student model on the corpus sampled from the teacher, to evaluate the model on a test instance (x test , y test ) from the target task, we decode both a chain-of-thought ztest and a predicted label \u1ef9test from the student and evaluate \u1ef9test versus the true label y test . We consider two strategies for decoding. (1) Predict the most likely chain-of-thought and the label ztest , \u1ef9test = argmax z,y S(z, y|x test ). This can be approximated by greedy decoding or beam search. (2) There may be different valid chainof-thoughts for a given question and as a result, large language models distribute probability mass for a certain label across many diverse chain-of-thoughts (Wang et al., 2022b) . Thus, it is beneficial to marginalize out the reasoning paths to find the most consistent answer: \u1ef9test = argmax y E z\u223cS(z|xtest) S(y|z, x test ). This can be approximated by sampling multiple reasoning paths and take a majority vote among the predicted answers, dubbed \"self-consistency\" (Wang et al., 2022b) . We experiment with both approaches and conduct a discussion in \u00a73.2.\n\nExperiments\nWe evaluate primarily on 3 target tasks: 1) Com-monsenseQA (CSQA) (Talmor et al., 2019) , a 5way multi-choice dataset; 2) OpenBookQA (Mihaylov et al., 2018) , and 3) QuaRel (Tafjord et al., 2019) . While any model capable of few-shot chain-of-thought could be substituted, we use the thought prompts from prior work (Wei et al., 2022b; Wang et al., 2022b) code-davinci-002 version of GPT-3 5 (Brown et al., 2020) as our teacher model T . We use OPT (Zhang et al., 2022) as our student model S. Our standard student model is OPT-1.3B (though we explore a range of student model sizes in \u00a73.3).\nWe sample from GPT-3 with a temperature of T = 1.0. For each training example, we sample N = 30 rationales. OPT is fine-tuned with a batch size of 32 and a learning rate of 2 \u00d7 10 \u22125 . We use HuggingFace transformers (Wolf et al., 2019) , Pytorch (Paszke et al., 2019) , and Accelerate 6 for the implementation. Main experiments can be reproduced on one GPU with 48GB of memory.\n\nResults in Default SCoTD Setting\nWe first consider both a few-shot learning setting and a supervised setting. For the few-shot setting, the only labeled examples available to our teacher/student models are contained in the prompt set P (but we use the unlabeled examples and teacher-generated chain-of-thoughts/labels for training). 7 We also consider the supervised setting, where we assume access to labels in D Train . Supervised SCoTD involves simply discarding the samples within C that do not have the correct label prior to fine-tuning the student: for Common-\nCommonsenseQA QuaRel OpenBookQA\nFigure 2 : For three commonsense QA tasks, accuracy (y-axis) improves significantly as the student is trained on more chain-of-thoughts sampled from the teacher (x-axis). Oversampling chain-of-thoughts is sometimes required to improve student performance beyond the supervised label-only baseline, e.g., as in OpenbookQA.\nsenseQA, OpenBookQA, and QuaRel, this results in discarding 40.4%, 45.0%, 34.2% of chain-ofthoughts. For the few-shot setting, we decode with the self-consistency approach; for the supervised setting, we decode with greedy decoding (introduced in \u00a7 2; see an discussion in \u00a7 3.2). We compare SCoTD to 2 baselines: 1) Label-Only, the student is fine-tuned on just the label (in the few-shot setting, the label comes from the teacher and could be wrong; in the supervised setting, we use the gold label), instead of also with CoT; 2) Greedy-CoT, we decode a single-CoT per example (instead of N = 30 samples) from T for each training example instead of sampling. For additional reference, Table 2 (a) reports the performance of the student (and teacher) in a variety of few-shot settings prior to applying any distillation: No CoT = few shot prompting with labeled instances from P but no z i , Greedy and Self-Consistency are prompting with CoT but with different decoding strategies ( \u00a7 2).\nTable 2 (b) gives the performance of the student model after distillation in the supervised and fewshot settings. In all cases, distillation significantly improves the student model, and in all-but-one case, learning with CoT outperforms the label-only distillation baseline. While the student model initially fails to perform CoT through prompting (Table 2 (a)) it learns to do so through distillation.\nThe number of samples. In our default setting, to serve as our distillation corpus C, we sample N = 30 rationales from the teacher T for each (unlabelled) training instance. Figure 2 shows the performance of the student model when it is trained on corpora with fewer sampled CoT per instance: results suggest that learning with multiple sampled (albeit nosier) rationales/chain-of-thoughts per example is more beneficial than learning with one (most likely) rationale. Will more rationales bring more performance improvement? We sampled more rationales from GPT-3 to train the student model; however, this does not bring more performance gains. When N = 50, the performance is similar to N = 30: the model achieves 67.0 in accuracy on OpenBookQA (v.s. 67.0), 67.2 on CommonsenseQA (v.s. 67.0), 84.9 on QuaRel (v.s. 83.8).\n\nHuman Evaluations\nWhile SCoTD improves task accuracy significantly, we additionally conduct human evaluations to assess the generated chain-of-thoughts themselves (see Table 1 for samples). We sample instances from the CommonsenseQA, OpenBookQA, and QuaRel validation sets (300 instances per dataset), and conduct head-to-head human evaluations 8 to assess:\nQ1: Does SCoTD result in higher-quality chainof-thoughts? Test: OPT-1.3B versus OPT-1.3B + SCoTD. Result: Yes. We assess this hypothesis on two subsets of instances: 1) a pure random sample (N=900); and 2) a set of instances for which both models eventually predicted the correct label (N=654). The second setting focuses more closely on the chain-of-thoughts themselves rather than the predictive accuracy of the model. SCoTD is superior in both settings: for the random sample setting, SCoTD won in 59% of cases (p<.001), whereas in the correctness controlled setting, SCoTD won in 61% of cases (p<.001). Results hold with p < .05 for each QA dataset individually.\nQ2: Does a SCoTD student surpass the much larger teacher? Test: OPT-1.3B + SCoTD versus text-davinci-002. While the task accuracy of the teacher is still higher in most cases, the studentgenerated CoT are comparable. 9 We again evaluate on: 1) a pure random sample (N=900); and 2) a correctness-controlled setting (N=659). The 100x smaller SCoTD's generations are competitive in both cases; we can't reject the null hypothesis of the crowd having equal preferences (OPT-1.3B + SCoTD wins in 47% and 51% of cases respectively, p > .01). Results hold for each dataset individually, as well.\n\nSelf-Consistency for the Student\nWang et al. (2022b) find that, for chain-of-thought prompted models, taking a majority vote over a large set of sample of predicted labels (resulting from a diverse range of CoTs) can improve performance. Our results regarding the effectiveness of sampling N = 30 rationales from the teacher during SCoTD are similar-in-spirit: i.e., we also show performance gains from sampling multiple rationalization chains per instance.\n9 See \u00a76 for more discussion about the disparity between CoT-quality and task accuracy. A natural question is, does the student model S exhibit the same phenomenon, i.e., can we sample multiple chain-of-thoughts from it and take a majority vote? We find that the student model can benefit from \"self-consistency,\" but not in all cases. In Table 3 , we report the performance with/without self-consistency (majority vote among 30 sampled reasoning paths with a temperature of 0.7). When training with filtered CoTs (Table 3 (a) bottom rows) or training with few CoTs per example (Table 3 (b), when #CoTs/Example is small), the student model does not benefit from self-consistency. Only when we train with multiple rationales per example without filtering (the few-shot setting), self-consistency is beneficial on CSQA and Open-BookQA. Overall, the results show that student models benefit from being shown a diverse/noisy set of rationales, and that self-consistency can be effectively applied after distillation.\n\nSCoTD across Model and Dataset Sizes\nWe also verify the effectiveness of SCoTD across model and dataset sizes; in these experiments, we consider the supervised setting. Data scaling. Figure 3 shows the effect of varying the size of D Train (for simplicity, we show only performance on CSQA as an example). Learning with CoTs is beneficial under all data scales. Interestingly, SCoTD, trained with access to only 40% of the labelled data, can surpass the direct supervised label-only model with 100% of the labelled corpus; this result aligns with the argument in Zaidan et al. (2007) -providing more explanations from the teacher model could be more beneficial than providing more labels.\nStudent model size scaling. Figure 4 presents results when varying the size of the student model from 125M to 1.3B parameters for CSQA. For all model three model sizes, SCoTD outperforms the standard supervised fine-tuning baseline (Label Only). Sampling multiple rationales per input instance is an effective strategy for all model sizes.\n\nSCoTD on Challenging Contrast Sets\nCan learning with explanations help generalization, as hypothesized by (Zaidan et al., 2007) ? As a preliminary study, we show that SCoTD enables better generalization to contrast sets. Contrast sets (Gardner et al., 2020) are proposed to evaluate a model's robustness to perturbations around the decision boundary, by asking annotators to modify the original test instances in small but meaningful ways that (typically) change the gold label.\nWe experiment on the IMDB (Maas et al., 2011 ) sentiment analysis task in the supervised setting; we consider the corresponding contrast set of IMDB proposed by Gardner et al. (2020) . We train two models on the training set of IMDB: Label-Only and SCoTD. For efficiency, we sub-sample 100K examples from the training set of IMDB and truncate input sequences to 700 tokens. As shown in Figure 5 , while both models with/without SCoTD achieve high performance on the original IMDB test set (96.1% v.s. 95.5%, with the Label-Only model performing slightly better), the model with SCoTD achieves significantly higher performance on the contrast set: 92.0% vs. 81.6%. This result supports the hypothesis of (Zaidan et al., 2007) ; that explanations can support more robust generalization.\n\nSCoTD on Unseen, Out-of-domain Tasks\nLarge language models can perform few-shot, incontext learning with chain-of-thought prompting, i.e., generating reasonable chain-of-thoughts on unseen tasks with a few demonstrations (Suzgun et al., 2022) . We conduct a preliminary experiment, inspired by Min et al. (2021) 's MetaICL, to test whether student models trained with SCoTD acquire the same ability. We train a supervised SCoTD model on ANLI, CommonsenseQA, and OpenBookQA, and evaluate it on SST-2 (Socher et al., 2013) , a sentiment analysis task.\nThe SCoTD model achieves a few-shot accuracy of 79.6% on the validation set (an example prediction is shown in Figure 6 ). 10 Compared to a baseline model that learns with no CoT(i.e., a re-implementation of MetaICL trained on 3 source tasks); the baseline fails to recognize the input/output format of the new task and predicts answers out of the desired label set. It achieves (an effective) 0% accuracy on SST-2. This suggests the potential of including CoTs during instruction/incontext tuning (Wei et al., 2022a; Min et al., 2021) .\n\nWhat Factors are Important for\nDistillation?\nAn important factor underlying the performance gains highlighted in \u00a73 was the number of chain-ofthoughts we sampled from the teacher model perinstance (more samples = better; Figure 2 ). Here we ask: is data volume the key contributing factor to the performance improvement? Or, are specific aspects of chain-of-thought samples key for the performance improvements?\nWe design several filters to identify potentially important examples/CoTs among the correct rationales. We apply designed filters (to be introduced) to C \u2032 , the corpus sampled from the teacher (with wrong CoTs dropped), that operationalize different hypotheses about what factors are important to distill. We control for dataset size when filtering, i.e., \n\nLabel Only SCoTD\nThe author said that they love this movie and they are never tired of watching it.\nThey say that the movie is wonderful and they are grateful to see such an outstanding picture. So the answer is: positive\nThis was a wonderfully clever and entertaining movie that I shall never tire of watching many, many times\u2026 I can only be grateful when I see such an outstanding picture for most of the motion pictures made more\nThis was a wonderfully thick as two short planks and soul-destroying movie that I shall never watch any number of times\u2026 I can only be sorry when I see such an abysmal picture just as most of the motion pictures \u2026\n\nIMDB Dataset\nThe author said that the movie was 'thick as two short planks and souldestroying', implying that the movie is bad. So the answer is: negative all filtered corpora have the same number of training CoTs. We downsample with a budget of 5 CoT per instance on average 11 . Then, we train the same student model on each of the filtered corpora, and compare on downstream tasks. If a student model trained on filtered corpus A tends to outperform the student model trained on filtered corpus B, then we argue that the property that produced corpus A is more important. The hypotheses we consider are:\nNull hypothesis: data volume. As a null hypothesis, we randomly sub-sample 5 CoT per instance; this filter operationalizes the assumption that an arbitrary set of samples is sufficient.\n\nDiversity.\nFor each instance, we compute S-BERT (Reimers and Gurevych, 2019) embed-11 In rare cases, we may end up with less as there are less than 5 correct CoTs for the instance. dings 12 of each of the chain-of-thoughts, and cluster the resulting embeddings using hierarchical clustering into k = 5 clusters. Then, we randomly sample a single instance from each cluster: the resulting sample covers all clusters, and thus represents a diverse+representative sample.\nTeacher likelihood. For each instance, we keep the 5 CoT samples with the highest per-token loglikelihood according to the teacher model.\nOpen-endedness. Some instances in each dataset lead to a broader range of chain-of-thought samples than others. For example, on CommonsenseQA, the question \"What form of alcohol is made from grapes?\" leads to a narrower range of rationalizations vs. \"Why might someone purposefully be going into trance?\"\nWe hypothesize that openended instances could benefit from relatively more sampled rationales. We sort instances into quintiles based on the unique bi-grams in their corresponding 30 CoTs; for high-ranking instances (more unique CoT bi-grams, like the \"trance\" example above), we keep more rationales and for low-ranking instances, we keep less rationales. We keep 1, 3, 5, 7, 9 rationales for instances of different bins (thus controlling for the total number of CoT).\nResults Figure 7 reports the accuracy of the student model when fine-tuned on the different subsampled corpora for the three tasks we consider. Overall, random subsampling is a strong baseline, but, we see some evidence that diversity among the rationales is important. None of the models trained on the sub-sampled data could approach the model trained on the full 30x/instance CoT set. This suggests that the sheer volume of the CoTs is a key driving force for the performance improvement.\n\nRelated Work\nChain-of-thought prompting. As an extension of few-shot prompting (Brown et al., 2020) Learning with explanations. Hase and Bansal (2022) discuss how explanations can serve as inputs (Talmor et al., 2020) , targets (Hendricks et al., 2016; Fidler et al., 2017; Camburu et al., 2018; Zhou et al., 2020; Narang et al., 2020; Kayser et al., 2021; Wiegreffe et al., 2022) , and priors (Zhang et al., 2016; Srivastava et al., 2018) for machine learning models. Chain-of-thought extends earlier efforts which treat explanations as intermediate structures, generated at inference time (Rajani et al., 2019) . Most related to our work is Li et al. (2022a) , who do also learn with GPT-3 generated explanations; we show multiple samples improve significantly over their single-sample method, and also use chain-of-thought prompting at inference time vs. predicting explanations+labels via independent multitasking.\nKnowledge distillation. Recent work, inspired by Knowledge Distillation (Hinton et al., 2015) , has considered symbolic knowledge distillation, (West et al., 2022) , i.e., instead of distilling from soft representations like logits, large language model serve as training data generators (Xiong et al., 2019; Petroni et al., 2019; Schick and Sch\u00fctze, 2021; West et al., 2022; Liu et al., 2022; Meng et al., 2022; Bhagavatula et al., 2022) ; this paper continues this line of work.\nContemporaneous work. There are several contemporaneous papers: Huang et al. (2022) , Magister et al. (2022), and Ho et al. (2022) all show that smaller models can benefit from large models' chains of thought. We contributes beyond these by: 1) showing that sampling a large number of chain-of-thoughts is paramount; 2) exploring transfer performance to challenge sets/unseen tasks; and 3) analysis that address what factors are important in the teacher corpus.\n\nConclusion\nWe demonstrate the effectiveness of Symbolic Chain-of-thought Distillation (SCoTD): a method that enables smaller language models to effectively use chain-of-thought-style reasoning. We demonstrate the method's effectiveness across several downstream tasks, different student model sizes, different levels of supervision, and in difficult settings (challenge sets, unseen tasks). Our ablations shed light on what factors are particularly important to distill in these chain-of-thoughts. Our concrete recommendations are: 1) sampling multiple and diverse CoTs for each input instance, and 2) performing self-consistency when the teacher CoTs are noisy. Several promising av-enues for future work include:\n1. Exploring SCoTD for generation tasks in addition to classification tasks;\n2. Scaling up the number of source tasks in \u00a7 3.5 to generalize to more tasks;\n3. Using the down-sampling setup introduced in \u00a74 to explore additional hypotheses about what other factors may be of importance in CoTs.\n"}
{"question": "What did we mainly do in our work?", "evidence": "  In this paper, we introduce TOME, a innovative two-stage model-based retrieval approach. To implement our approach, we make two major technical contributions in the design of the identifier and the architecture of two-stage generation. Moreover, we also employ a number of training strategies to better optimize our proposed architecture, especially on large-scale corpora. Extensive results demonstrate the effectiveness of TOME. Furthermore, we perform a thorough analysis and summarize the scaling law for the proposed method. We believe such an idea itself is worthwhile for exploring in designing new model-based retrieval architecture.\n ", "options": ["A. Introduce a innovative two-stage model-based retrieval approach--TOME. ", "B. Design the identifier", "C. Design the architecture of two-stage generation", "D. Thoroughly analyse and summarize the scaling law for the proposed method. "], "answer": "A", "content": "\nIntroduction\nInformation retrieval systems have undergone continuous development over the past few decades, with the aim of obtaining relevant resources, such as documents, in response to a user query from a vast collection. With the recent success of Pretrained Language Models (PLMs) (Devlin et al., 2019; Raffel et al., 2020; Zhao et al., 2023) , researchers have developed PLM-based dense retrievers (Lin et al., 2021; Zhao et al., 2022) , which utilize dual-encoders and nearest neighbor search index for retrieval and achieve significant improvements over sparse retrievers.\nMore recently, a new retrieval paradigm, known as model-based retrieval (Tay et al., 2022; Zhou et al., 2022c) , has been introduced by developing an alternative architecture for retrieval. In contrast to traditional retrieval methods, it does not explicitly maintain a corpus index, thereby simplifying the classic index-retrieve-rerank process. Typically, a model-based retrieval system is built based on a sequence-to-sequence generation model with an encoder-decoder architecture, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) . It accepts a query as input and directly generates the corresponding document identifier via the generation model.\nDespite its attractive benefits in simplifying the retrieval pipeline, model-based retrieval still faces following major challenges.\n\u2022 Firstly, since the retrieval task is framed as a prediction task of document identifiers, making it crucial to design document identifiers that are well-suited to the underlying generative PLM. However, this issue is rarely discussed in prior research, and most existing approaches employ manually or randomly constructed identifiers (i.e., docids) as generation targets. Such docids are not adequately captured in the pretraining stage of the generative PLM, thus limiting PLM's capabilities for generative prediction (e.g., unseen docids during pre-training). This creates a discrepancy between the pre-training and fine-tuning phases.\n\u2022 Secondly, there is a discrepancy between training and inference in the single-model generative architecture. While most existing studies incorporate multi-task learning (Tay et al., 2022) and auxiliary pre-training tasks (Zhou et al., 2022b) to model both documents and queries during training, the model only processes queries dur- ing inference, resulting in a gap between the training and inference stages.\nTo this end, in this paper, we propose a novel TwO-stage Model-based rEtrieval approach, TOME (as illustrated in Figure 1 ), which makes two major technical contributions.\n\u2022 Firstly, we suggest using tokenized URLs (or URIs) as text identifiers, which are widely available for web pages or Wikipedia pages 1 . By using URL-based identifiers, the tokenized symbols are well aligned with the vocabulary of the generative PLM, thereby enhancing the generative capacity of the PLM. URLs are typically comprised of normal text, as opposed to manually or randomly constructed identifiers. As a result, such an identifier design can be used to help alleviate the gap between pre-training and fine-tuning.\n\u2022 Secondly, our approach decomposes the prediction task into two consecutive stages, namely passage generation and URL generation, which are fulfilled by two separate T5-based generation models, respectively. The first stage aims to generate a relevant passage in the corpus based on the query, while the second stage aims to generate the corresponding URL of the generated passage from the first stage. This two-stage architecture can reduce the discrepancy between training and inference. In addition, the entire generation process is progressive. Consequently, the second stage is capable of tolerating errors that may be introduced by the preceding stage and generates correct URLs.\nMoreover, we discover that optimizing modelbased retrieval becomes a challenging task when dealing with a vast corpus. As a result, we propose a number of improved training strategies to optimize the generation models, including query augmentation, passage length reduction, and model scaling.\nTo verify the effectiveness of TOME, we conduct extensive experiments on the publicly available MS MARCO and NQ datasets. Experimental results demonstrate the effectiveness of the proposed method, including the URL identifier design and the two-stage generation process. Additionally, case studies indicate that the second stage can tolerate errors induced by the first stage. Furthermore, we investigate the scaling laws of TOME by examining different model sizes, corpus sizes, and text lengths. We anticipate that these experimental results will facilitate further research on model-based retrieval.\n\nRelated Works\nText Retrieval. Text retrieval endeavors to find textual information related to a query from a large candidate corpus. Early studies on sparse retrieval focused on term matching by utilizing sparse representations and inverted indices, such as BM25 (Robertson et al., 2009) . In recent years, with the resurgence of neural networks and the emergence of pre-trained language models (PLMs) (Devlin et al., 2019; Raffel et al., 2020) , dense retrieval achieves better performance beyond traditional sparse retrieval on multiple tasks (Khattab and Zaharia, 2020; Karpukhin et al., 2020; Xiong et al., 2021; Qu et al., 2021) . The dense retrieval and the technique of approximate nearest neighbor search have been widely adopted in various applications (Oguz et al., 2020; Ren et al., 2021a,b; Asai et al., 2021; Ren et al., 2022; Zhou et al., 2022a) . Recently, Zhao et al. (2022) have made a very comprehensive survey about the recent progress of dense retrieval based on PLMs, and we refer the readers to this survey paper for more details.\nModel-based Retrieval. Both sparse retrieval and dense retrieval rely on explicit indices. Recently, researchers have proposed model-based retrieval (a.k.a., generative retrieval) models (Metzler et al., 2021; Tay et al., 2022) . These methods consider model parameters as retrieval indices and directly generate the identifiers of related documents. Such an idea is initially proposed for entity retrieval (Cao et al., 2021) , which autoregressively generates unique entity identifiers. Following this approach, researchers have introduced sequenceto-sequence encoder-decoder architecture for document retrieval (Zhou et al., 2022c; Bevilacqua et al., 2022; Zhuang et al., 2022; Wang et al., 2022; Lee et al., 2022; Chen et al., 2022; Zhou et al., 2022b) . As discussed in the previous section, there still remain issues with model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. Our work tries to deal with these issues with a two-stage generation architecture with URL identifiers.\n\nApproach\nIn this section, we first introduce the task formulation, followed by the description of the proposed two-stage generation approach TOME.\n\nTask Formulation\nIn this work, we consider the task of text retrieval, which aims to find relevant text resources (e.g., documents) related to a query from a large corpus. We further assume that these texts can be accessed by an associated URL 2 (or URI).\nTo develop our approach, we adopt the recently proposed model-based paradigm for text retrieval (Tay et al., 2022; Zhuang et al., 2022) . For retrieval, a model-based retrieval model takes a query q as input and uses the text-to-text model to generate the identifier y (length n) of the relevant document in an autoregressive manner, with the conditional probability:\nEQUATION\nwhere y i denotes the i-th output token in the identifier y, y <i denotes the previous tokens y 1 , . . . , y i\u22121 , and M represents the PLM. The identifier can be an atomic token or a string (Tay et al., 2022) . In our setting, it is assigned to an associated URL of a text (refer to Section 3.2.1). Typically, a generative pre-trained language model (PLM) with an encoder-decoder architecture is employed to implement the text-to-text model (e.g., T5), which is typically optimized by a cross-entropy loss as follows:\nL(M) = \u2212 log Pr M (y|q) = \u2212 n i=1 log Pr M (y i |y <i , q) . (2)\nThe key to model-based retrieval is to design a generative architecture that employs suitable document identifiers, and to develop effective training methods that can effectively associate queries to the identifiers of documents. Next, we expound our approach in detail.\n\nModel Architecture\nIn this section, we first introduce the design of document identifiers, and then present the two-stage generation architecture.\n\nIdentifier Design\nExisting studies typically use docids to represent a document (Tay et al., 2022; Zhuang et al., 2022) . These docids are often randomly generated or manually constructed, which may not exist in realworld text corpora. However, the generative PLM is pre-trained based on large-scale text corpora, leading to a discrepancy between pre-training and fine-tuning.\nDifferent from previous approaches, we consider a tokenized form of URLs as the docids. We directly treat the URL as a text string and tokenize it into a sequence of tokens using a T5 tokenizer. For instance, a sample URL 'https://en.wikipedia.org/wiki/Nevada' can be tokenized to {'https', '://', 'en', '.', 'wikipedia', '.', 'org', '/', 'wiki', '/', 'N', 'e', 'vada'}. We use the token sequence as the prediction target of the generative PLM, following the generation formula of Equation (1). It is worth noting that Ultron (Zhou et al., 2022b ) also uses URLs as identifiers, where a URL is reversed and only used as part of an identifier (also involving titles and domains). As a comparison, we solely utilize tokenized URLs as the identifier, without any additional processing.\nCompared to non-linguistic docids, URLs typically contain more meaningful tokens in the form normal text and widely exist in real-world text corpora, making them more suitable to modeling and prediction using generative PLMs. During decoding, we can directly adopt the general text decoding method to generate the URL, without resorting to limited search strategies such as constrained beam search (Tay et al., 2022; Bevilacqua et al., 2022) . Since these tokenized symbols often overlap among different URLs (e.g., web pages from the same domains), they naturally derives semantic strings as the clustering method in DSI (Tay et al., 2022) .\n\nTwo-stage Generation Architecture\nThe objective of the generative model for retrieval is to establish a correlation between a query and its corresponding docid (i.e., URL). However, owing to the scarcity of annotated data, various improved strategies such as multi-task learning (Tay et al., 2022) or pre-training (Zhou et al., 2022b) have been proposed. Typically, a model processes both documents and queries during training, while it processes only queries during inference, resulting in the discrepancy between training and inference. To tackle this issue, we propose a two-stage generation approach with two different generation models: one for passage generation and the other for URL generation, as shown in Figure 1 .\nPassage Generation. In the first stage, we employ a T5-based passage generation model to map an input query to the passage content according to Equation (1). The generated passage is anticipated as a relevant passage in the corpus that can provide an answer to the query. The objective of the passage generation model is to memorize the passages in the corpus, so as to generate the passages with utmost precision. It is trained with query-passage pairs, where each pair comprises a query and a passage from the document, along with the corresponding labeled URL. Different from existing methods (Tay et al., 2022; Bevilacqua et al., 2022) , we do not utilize any data structure to restrict the decoding process and simply use greedy search to generate an individual result for a query in an autoregressive manner, which has a high decoding efficiency. By incorporating the intermediate passage generation, our approach can mitigate the training-inference discrepancy that the query encoder also needs to process documents (Tay et al., 2022) . URL Generation. In the second stage, another T5-based PLM is employed to predict the corresponding URL as the retrieval result, utilizing the passage generated by the passage generation model as input. The URL is generated by means of greedy search decoding in a similar manner as in Equa-tion (1). The URL generation model is trained with passage-URL pairs, where each pair comprises a passage and its corresponding URL. The objective of the URL generation model is to memorize all the URLs in the corpus, so as to map a generated passage related to a query to a corresponding URL. Meanwhile, even if the generated passages contain some irrelevant content or noise, this stage can still make reliable predictions since it can employ long passages as the context, rather than short queries.\nOverall, such a two-stage generation approach can more effectively capture the semantic relatedness between queries and identifiers by both reducing the training-inference discrepancy and enriching the generation context, which is specifically tailored for model-based retrieval.\n\nTraining\nFor both the passage generation model and the URL generation model, we optimize them independently by utilizing the cross-entropy loss for optimizing standard T5 models, as shown in Equation (2). Nevertheless, optimizing model-based retrieval approaches (Zhuang et al., 2022; Wang et al., 2022 ) is a challenging task as they essentially require memorizing the corpus information, and generating long text also poses challenges in model convergence. In this part, we further propose several strategies for improving the training of our approach.\nQuery Augmentation. Generating pseudo queries is proven to be effective in improving the performance of model-based retrieval (Wang et al., 2022; Zhuang et al., 2022) . Here, we utilize query generation for constructing the training data for passage generation. Specifically, we take the passage collection as the corpus, and use an existing query generation model (i.e., DocT5query (Nogueira et al., 2019) ) trained on the labeled dataset to generate multiple pseudo queries for each passage in the corpus. Following DSI-QG (Zhuang et al., 2022) , we use the top-k sampling strategy for query generation, and set k up to 20. The generated pseudo queries and their corresponding passages are then used to construct query-passage pairs as the training data for the passage generation model. Such a query augmentation method can significantly increase the availability of training data, and also enhance the generalization capability of the model for different queries.\nReducing the Passage Length. Since passages are much longer than URLs, passage generation is more complicated than URL generation. In the generation task, a more extensive generation target results in larger search space, which typically leads to a decrease in efficiency and effectiveness. While, in our approach, passage generation serves as an indirect step for predicting the URL, so that we consider reducing the passage length for improving the training efficiency. For this purpose, we shorten the maximum truncation length of the passage, from 128 to 32. However, reducing the passage length will probably results in a information loss, thus hurting the generation performance. As the solution, we concatenate the title (a short text) and the shortened passage for enhancing the contained semantics. We also add prompts before titles and passage contents like \"title:\" or \"passage:\" for better generation performance.\nIncreasing Model Scale. Model-based retrieval requires a strong memorization capacity from the generative PLM, especially for our approach that involves a passage generation stage. Besides, scaling up the text corpus will significantly increase the difficulty of corpus memorization, and the PLM with a small parameter scale will have a limited memorization capacity when the data scale reaches a certain level. Considering the two aspects, we scale the model size accordingly and employ a larger PLM when necessary. Specifically, we use T5-large (the first stage is more difficult) and T5base for the two stages of our approach on a small corpus (e.g., subsets of MS MARCO), respectively. Further, we increase them to T5-3B and T5-large accordingly on a large corpus (e.g., the full set of MS MARCO). Besides the improved capacity, we find that using a larger model size is also useful in improving the convergence rate (as detailed in Section 5.4).\n\nExperimental Settings\nThis section describes the major experimental settings, including datasets, evaluation metrics, baselines and implementation details.\n\nDatasets and Evaluation Metrics\nDatasets. We conduct experiments on two public available datasets, namely MS MARCO (Nguyen et al., 2016) Passage Ranking and Natural Questions (NQ) (Kwiatkowski et al., 2019) . (1) MS MARCO contains Bing search queries as well as passages from web documents, making it one of the largest web search datasets to date, with a full corpus of over 8.8 million passages. In addition, we also consider two subsets, each containing 100K and 1M passages, by following (Tay et al., 2022; Zhuang et al., 2022) . Based on the MS MARCO Question Answering dataset, we extract the URLs associated with the passages, selecting a random URL if a passage contains multiple URLs (2) The NQ dataset is a question answering dataset where the query data is collected from Google search logs, and the document data is from Wikipedia. We use the NQ320K version by following NCI (Wang et al., 2022) , which contains 320K labeled querydocument pairs and 100K documents. We collect abstracts of documents as intermediate-generated passages.\nEvaluation Metric. Following previous works, we adopt Hits@1 as the evaluation metric. This metric is calculated as the percentage of queries to which the top-1 generation result is positive. Since the outputs of models at different stages are either passage texts or URL texts, unlike the conventional MS MARCO evaluation by determining whether the retrieved identifiers are in the identifier label list, we evaluate the results by determining whether it is an exact match to the label text.\n\nBaselines\nFor comparison, we chose the following baselines including sparse retrieval, dense retrieval, and model-based retrieval.\nBM25 (Robertson et al., 2009 ) is a classical sparse retriever that uses the inverted index to find relevant passages by term overlap. DPR (Karpukhin et al., 2020) and ANCE (Xiong et al., 2021) are two representative dense retrievers that adopts dual-encoder architecture. For modelbased retrievers, DSI (Tay et al., 2022 ) is a pioneer work for model-based retrieval that uses a sequence-to-sequence model to map the input query to the relevant docid. We use the open-source code released by DSI-QG for reproducing DSI baseline on MS MARCO. SEAL (Bevilacqua et al., 2022) is proposed to generate multiple ngrams for a query with an auxiliary Ferragina Manzini index. DSI-QG (Zhuang et al., 2022) proposes to improve DSI with augmented data constructed by query generation. NCI (Wang et al., 2022 ) also utilizes pseudo queries for improving model-based retrieval with tailored architecture. Due to the different experimental settings of different methods, we copy the performance values for some baselines on NQ in NCI and reproduce all of the baselines on MS MARCO under the same evaluation strategy. All the model-based retrieval baselines adopt the \"large\" version of PLMs.\n\nImplementation Details\nWe conduct our experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) and natural language processing toolkit PaddleNLP (Contributors, 2021) on up to 32 NVIDIA Tesla A100 GPUs (with up to 80G RAM).\nPLM. The generation models adopted in our work are initialized with different parameter scales of T5 (Raffel et al., 2020) . In the passage generation model, we use T5-3B for initialization on MS MARCO Full, and other models are initialized with T5-large. In the URL generation model, we use T5large for initialization on MS MARCO Full, and other models are initialized with T5-base.\nHyper-parameters. We adopt Adam optimizer with a learning rate of 5e-5, and train the models for a maximum of 3M steps with bf16 mixed precision strategy. The batchsize is set up to 128, 384 and 80 for T5-base, T5-large and T5-3B, respectively. The maximal length of queries, passages and URLs are set as 32, 32 and 80, respectively. The warm-up step is set as 100K and 10K for passage and URL generation task, respectively. Query Augmentation. We adopt the existing docT5query-large (Nogueira et al., 2019) model that trained on MS MARCO training set, and generate 20 and 15 queries per passage for MS MARCO and NQ, respectively. For training data, we only use pseudo-labeled data constructed by query generation on MS MARCO, and use both pseudolabeled data and labeled data on NQ.\n\nExperimental Results and Analysis\nIn this section, we report the experimental results of our proposed approach and conduct comprehensive empirical analysis. (Karpukhin et al., 2020) 71.84 52.52 29.54 DSI (Tay et al., 2022) 11.75 --DSI-QG (Zhuang et al., 2022) Table 1 : The Hits@1 results of different methods on variant corpus scales of MSMARCO.\n\nMethods\nHits@1 BM25 (Yang et al., 2017) 15.11 ANCE (Xiong et al., 2021) 52.63 DSI (Tay et al., 2022) 35.60 SEAL (Bevilacqua et al., 2022) 59.93 NCI (Wang et al., 2022) 66.23 DSI-QG (Zhuang et al., 2022) 61.34 Comparison with Model-based Retrievers. We observe that TOME consistently outperforms model-based retrievers on three subsets of MS MARCO and NQ320K datasets, thereby demonstrating the effectiveness of the proposed method. Moreover, NCI is a competitive baseline on NQ320K, which uses tailored decoder architecture, preprocessed semantic docid, and regularization on top of DSI-QG, while our method is simply trained with the standard T5 configuration without any additional processing. We also discover that DSI-QG is unable to effectively converge when trained on the MS MARCO Full. We speculate that random non-linguistic docids become a bottleneck as the corpus scales up, while the loss can normally converge when using normal text (e.g., URL) as a generation target.\nEffect of Two-stage Generation Architecture. By simply substituting the generation target of DSI-QG from random string docids to URLs (singlestage of our method), the performance has been improved (refer to DSI-QG and TOME single-stage in Table 1 and 2 ), indicating that natural language identifiers are more suitable for model-based retrieval tasks than non-linguistic docids. Furthermore, if we employ the two-stage generation that includes an intermediate step to generate passages before generating URLs, the performance will be further improved (refer to TOME single-stage and TOME two-stage in Table 1 and 2 tion demonstrates that integrating passage generation in the process of model-based retrieval leads to better performance.\nComparison with Dense Retrievers. By adopting a series of training strategies, we successfully train TOME on large-scale corpora. However, although TOME outperforms dense retrieval methods on MS MARCO 100K and NQ320K, there still remains a performance gap when compared to DPR on larger corpora such as MS MARCO 1M and Full. This indicates that our method still has gaps compared to advanced dense retrieval methods when the corpus scales up. Since the model-based method necessitates complete memorization of the entire corpus, it inherently possesses a disadvantage in larger-scale corpora when compared to dense retrievers, which needs to be further explored.\n\nAblation Study\nIn this section, we conduct an ablation study to examine the effectiveness of strategies in TOME.\nWe report the results on MS MARCO 100K and NQ320K. Here, we consider three variants based on TOME for comparison: (a) w/o prompt removes the prompts before titles and passages; (b) w/ increased maxlen increases the maximum truncated length of passage from 32 to 128; (c) w/ reduced pseudo query reduces the amount of pseudo query to 10 per passage. Table 3 presents the results for variants of TOME. We can observe the following findings: (a) The performance drops in w/o prompt, demonstrating that adding prompts for identifying the title and passage is helpful for generating better results. (b) The performance drops in w/ increased maxlen, demonstrating that due to various training strategies, shortening the maximum truncated passage length does not bring performance loss but reduces the difficulty of training. (c) The performance drops in w/ reduced pseudo query, demonstrating the effectiveness of generating a large number of pseudo queries for data augmentation.\n\nAnalysis on Two-stage Generation\nIn this section, we investigate the generation results of the passage generation model quantitatively and qualitatively to showcase the superiority of the proposed two-stage generation approach.\n\nQuantitative Analysis\nWe quantitatively analyze the generation results on MSMARCO dev set with the passage generation models trained on MS MARCO 100K.\nFirst, we are surprised to find that on the entire dev set, the proportion of generated passages are the passages exist in the corpus is about 95%. In cases where the model failed to generate labels correctly, about 85% of the generated passages still exist in the corpus. This result indicates that the model is capable of memorizing the corpus precisely and is able to generate a retrieval-like result. Moreover, previous studies of dense retrieval reveal that there are a lot of false negatives in MS-MARCO (Qu et al., 2021) . We also observe that approximately 80% of the generation results that are not labeled as positives but appear in the corpus are false negatives, showing that model-based retrieval suffers from the same issue of false negatives as dense retrieval. Despite this, the passage generation model actually has strong generation capability.\n\nQualitative Analysis\nTo explore the generative capabilities of TOME, we conduct a case study on MSMARCO 100K, utilizing a maximum truncation length of 128 for better illustration.\nTable 4 gives two sampled queries, along with their corresponding label passages, evidence passages (if available) and generated passages. With respect to the first query, the generated passage is not exactly the same as the labeled passage. In comparison with the labeled positive passage, the second half of the generated passage is altered. Despite the alteration in the generation passage, the URL generation model is still able to accurately map it to the correct URL, indicating that the URL generation model can tolerate changes introduced by the passage generation model. In the second example, the model extracts relevant content from both the label passage and the evidence passage, and then combines the contents to create the generated passage. It is interesting Ginger is an analgesic (a pain-killer) that may alleviate the pain associated with a sore throat. It is also a good antibacterial and antifungal and can help fight the infection causing your sore throat. . . . Table 4 : The comparison of the labeled passages and generated passages. The evidence passages are not manually labeled but contain relevant content. The italic words with underline represents the different parts of two passages, the ::::::::::::::::::::::::::\nitalic words with wavy underline and bold words with underline in different passages represent the reference parts. to observe that the passage generation model is capable of summarizing multiple passages.\n\nAnalysis on Scaling\nWe observe that long text generation poses a challenge to the convergence of loss, so we investigate the training efficiency and capability of the model under varying conditions. In particular, we use the same computing resource and conduct training on the passage generation stage (i.e., the first stage) of TOME. Considering that the trend is similar in the second stage, it has been omitted here due to limited space.\nEffect on Data Scale. We investigate the impact of expanding the corpus on model training and examine whether the model capacity is insufficient when dealing with a large corpus. We fix the T5-large model and conduct training on MSMARCO 100K, 1M and Full datasets, respectively, without shortening the length of passages. We use perplexity (PPL) to estimate the model capacity and monitor how perplexity changes as training steps increase. The results are shown in Figure 2 (a). It can be observed that the perplexity of the T5-large model fails to converge to a lower level after corpus scale expansion, which illustrates that under this task, a certain amount of data will lead to the capacity bottleneck of the model. In addition, the decline rate of perplexity slows down on larger corpora, indicating that models with the same parameter size have low learning efficiency on a large-scale corpus.. Effect on Passage Length. In order to investi-gate the effect of reducing the length of generated passages, we fixed the model as T5-large, and conducted experiments on passages with different maximum truncated lengths as generation targets on MSMARCO 1M. Figure 2 shows that after reducing the maximum truncated length of the generated passage, the perplexity significantly decreases, indicating that such a strategy is beneficial to mitigate the difficulty of the passage generation task. Moreover, the model exhibited enhanced efficiency when generating shorter passages.\n\nConclusion\nIn this paper, we introduce TOME, a innovative two-stage model-based retrieval approach. To implement our approach, we make two major technical contributions in the design of the identifier and the architecture of two-stage generation. Moreover, we also employ a number of training strategies to better optimize our proposed architecture, especially on large-scale corpora. Extensive results demonstrate the effectiveness of TOME. Furthermore, we perform a thorough analysis and summarize the scaling law for the proposed method. We believe such an idea itself is worthwhile for exploring in designing new model-based retrieval architecture.\n"}
{"question": "What method do we proposed? ", "evidence": " We propose a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions.  We propose to introduce semantics into SSL speech encoders by using ASR-U to interface with LLMs. Section 3.2 describes how to use ASR-U to link a speech encoder with a LLM. Section 3.3 describes how to combine both acoustic and semantic information and deal with ASR transcriptions errors. Finally, Section 3.4 describes how to align LLMs with the speech encoder for downstream tasks.\n ", "options": ["A. We propose a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. ", "B. We propose introducing semantics into SSL speech decoders by using ASR-U to interface with LLMs. ", "C. We propose how to combine both textual and semantic information and deal with ASR transcriptions errors.", "D. We propose how to align LLMs with the speech decoder for downstream tasks."], "answer": "A", "content": "\nIntroduction\nRealizing artificial intelligence (AI) that can understand and respond to spoken language is a north star for many speech and natural language processing (NLP) researchers. A particularly effective framework for this is the encoder-decoder architecture, where an encoder represents input audio signals as high-dimensional embeddings and a decoder converts said embeddings to outputs for different downstream tasks. Benchmarks for such systems include spoken language understanding, where intent, named entities, or slot values are predicted from input utterances (Yang et al., 2021; Bastianelli et al., 2020; Shon et al., 2022) , and spoken question answering, where the start and end frames of an input audio passage answering an input audio question are predicted (Lin et al., 2022a) .\nA particularly notable setup of the encoderdecoder framework is the universal representation setup (Yang et al., 2021) , where a shared selfsupervised speech encoder is pretrained upstream once and frozen for all downstream tasks, then a different lightweight decoder is fine-tuned on each downstream task. This setup is appealing for building speech systems as maintaining a separate large specialized model for every task is not computationally efficient. The universal representation setup has been widely adopted in other areas of research, such as computer vision (Goyal et al., 2019; Ericsson et al., 2021) and NLP (Rogers et al., 2020; Qiu et al., 2020) , and production when there are many downstream tasks or domains (Molino et al., 2019) . The current state-of-the-art speech encoders under this setup are W2V2 and HUBERT (Yang et al., 2021; Baevski et al., 2020; Hsu et al., 2021) , which are transformer-based models trained with self-supervised learning (SSL) on raw audio and have achieved impressive performance on various tasks.\nRecently, analytical works found SSL speech encoders capture primarily acoustic, not semantic, information (Pasad et al., 2021) . Thus, researchers proposed end-to-end systems (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) that introduce semantic information through large language models (LLMs), such as ROBERTA (Liu et al., 2019) or BART (Lewis et al., 2019) , which are pretrained to capture language semantics (Clark et al., 2019) . This is typically accomplished by the pipeline approach (Bastianelli et al., 2020) , which passes audio input through the SSL speech encoder, then bridge module, then LLM. The bridge module converts speech encoder embedding outputs into LLM token inputs (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) .\nUnsupervised ASR models (ASR-U) (Liu et al., 2020b; Baevski et al., 2021; Liu et al., 2022) have also seen recent success. The state-of-the-art ASR-U model uses generative adversarial networks (GANs) (Goodfellow et al., 2020) to generate text transcription from input audio (Liu et al., 2022) .\nCurrent works combining SSL speech encoders and LLMs do not satisfy the universal representation framework, as they either (1) rely on ASR data on the downstream task, which is expensive to collect and maintain, (2) are not lightweight, requiring training the whole system end-to-end, or (3) are not general, as they do not consider a wide variety of downstream tasks (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) . Similarly, ASR-U was proposed for speech recognition and the focus is not improving SSL speech encoders (Baevski et al., 2021; Liu et al., 2022) .\nWe propose introducing Semantics into Speech Encoders, SSE, a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. Concretely, SSE adopts the pipeline approach to obtain semantic embeddings, with an ASR-U bridge connector to extract information from LLMs. As ASR-U is inherently noisy, SSE introduces attention residual connection (He et al., 2016; Vaswani et al., 2017) between the speech encoder and LLM. SSE also efficiently aligns the LLM with the speech encoder through adapter modules (Houlsby et al., 2019) . SSE improves W2V2 (Baevski et al., 2020) and HUBERT (Hsu et al., 2021) on 3 SLU tasks across 3 datasets, all under the universal representation setup. SSE also outperforms state-of-the art no-ASR method, DUAL (Lin et al., 2022a) , in SQA.\nWhile recent works use ASR-U to augment existing speech encoders with phoneme-level LLMs (Feng et al., 2022; Meng et al., 2022; Shi et al., 2022; Hsu et al., 2022) , subword-level LLMs contain much more pertinent and measurable semantic information (Clark et al., 2019) . Other works in SQA rely on clustering to assign audio frames to frequent subword tokens, but this requires heavy finetuning on the downstream task (Lin et al., 2022a) .\nTo the best of our knowledge, we are the first to propose a task-agnostic SSL speech encoder which directly interfaces with subword-based LLMs, unblocking many other applications and future work in this domain. To this end, attention residual con-nections and adapters are essential to successfully extracting semantic information from noisy intermediary transcriptions. We summarize our contributions below:\n\u2022 We propose using ASR-U components to augment SSL speech encoders for generating subword tokens with semantic information.\n\u2022 The augmented SSL speech encoders can be connected with powerful LLMs seamlessly and yields state-of-the-art performance under the universal representation setup.\n\u2022 We show attention residual connections and adapters are essential to combining and aligning speech and text encoders.\n2 Related Works 2.1 Self-Supervised Speech Encoders SSL speech encoders (Liu et al., 2020a; Chung et al., 2020a; Ling and Liu, 2020; Liu et al., 2021 Liu et al., , 2020c;; Chung et al., 2019; Baevski et al., 2019; Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Qian et al., 2022; Zhang et al., 2022) are trained to learn and reconstruct pooled clustered representations of input audio from the original audio. The intuition for this objective comes from linguistics, where speech can be broken down into phoneme groups, where different chunks of input audio represent different phoneme groups.\nW2V (Schneider et al., 2019) trains a convolutional neural network model to reconstruct the quantized cluster representations. W2V2 (Baevski et al., 2020) uses transformers and a discrete codebook quantization module. HUBERT (Hsu et al., 2021) improves W2V2 by disentangling the clustering and SSL objectives and using a BERT-style encoder (Devlin et al., 2018) . The speech processing universal performance benchmark (SU-PERB) (Yang et al., 2021; Lin et al., 2022b; Tsai et al., 2022) shows SSL speech encoders are the most effective method for solving multiple downstream tasks with minimal fine-tuning. A recent analytical work finds SSL speech encoders successfully encode acoustic information, but lack semantic information (Pasad et al., 2021) . In response, CONTENTVEC (Qian et al., 2022) propose disentangling the speaker and semantic content of audio via an SSL objective. SPEECHLM (Zhang et al., 2022) propose training a multi-modal speech and text encoder.\n\nLarge Language Models\nIn contrast to speech encoders, pretrained LLMs are shown to capture rich semantic information (Clark et al., 2019) . These methods optimize variants of the masked language modeling (MLM) objective to train a large transformer model. BERT (Devlin et al., 2018) uses MLM to learn a transformer encoder. ROBERTA (Liu et al., 2019) introduces dynamic masking and a larger text corpus. BART (Lewis et al., 2019) supports generative modeling and adds a denoising objective, making it less susceptible to noisy text inputs. LONG-FORMER (Beltagy et al., 2020) is pretrained for long documents by increasing the document length limit during pretraining. LLMs have been successfully integrated with speech models for specific semantic tasks (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) , but not under the universal representation framework.\n\nTask-Specific Speech Models\nTask-specific SLU systems outperform generic SSL speech encoders typically by using a LLM. These systems rely on ASR data to reliably interface the LLM. LUGOSCH (Lugosch et al., 2019) trains a LSTM bridge module to convert audio features into phonemes then text. CTI's (Seo et al., 2022) bridge module uses ASR logits to compute a weighted average of token embeddings. In addition to improving the bridge module, other works attempt to also distill LLM embeddings into speech representations (Chung et al., 2020b; Cha et al., 2021; Kim et al., 2021; Agrawal et al., 2022) . For optimizing targeted metrics, researchers have also experimented with reinforcement learning (Rao et al., 2021) . While combinations of these methods achieve impressive performance, they do not satisfy the universal representation setup.\n\nUnsupervised ASR\nRecent work show the viability of unsupervised speech recognition. W2V2-U (Baevski et al., 2021) accomplished this by running Principal Component Analysis (PCA), k-means clustering, and mean pooling to convert W2V2 (Baevski et al., 2020) features into phoneme-granularity features, then trains a GAN model to output phoneme text from the post-processed model (Baevski et al., 2021) . The state-of-the-art method for phoneme-level unsupervised ASR is W2V2-U2.0 (Liu et al., 2022) which directly trains a CNN to output phonemes from W2V2 features and uses a reconstruction loss to tie the input audio with corresponding generated text. Both methods use WFSTs to decode the phonemes into raw text. While there have been preliminary attempts (Feng et al., 2022; Meng et al., 2022) to use W2V2-U2.0 with phoneme language models 1 , we are the first to combine it with semantically-rich subword-based LLMs.\n\nAdapters\nAdapters are intermediary layers added to a large pretrained encoder. Adapter weights are learned during fine-tuning while the rest of the pretrained model is frozen. Adapters serve the dual purpose of efficient fine-tuning and preventing overfitting. First used by computer vision researchers (Rebuffi et al., 2017) , adapters now enjoy much success in the natural language processing community by efficiently tuning LLMs (Houlsby et al., 2019) . In particular, the multilingual speech translation community found that adapters can effectively align SSL speech encoders and LLMs for spoken translation tasks (Li et al., 2020; Le et al., 2021) .\n\nProposed Method\nWe propose to introduce semantics into SSL speech encoders by using ASR-U to interface with LLMs. Section 3.2 describes how to use ASR-U to link a speech encoder with a LLM. Section 3.3 describes how to combine both acoustic and semantic information and deal with ASR transcriptions errors. Finally, Section 3.4 describes how to align LLMs with the speech encoder for downstream tasks.\n\nProblem Setting\nFollowing the universal representation framework (Yang et al., 2021) , our model consists of a large speech encoder, E : X \u2192 Z, mapping input audio, X \u2208 X , to embeddings, Z \u2208 Z, and a light-weight task decoder,\nD \u03c9 : Z \u2192 Y \u03c9 , mapping embeddings to downstream task outputs, Y \u03c9 \u2208 Y \u03c9 .\nThe speech encoder, E, is pretrained once, then shared on all downstream tasks. The task decoder, D \u03c9 , is fine-tuned on its respective task, \u03c9 \u2208 \u2126.\nDuring fine-tuning, the majority of model weights are frozen. This ensures the model can be efficiently stored and deployed.\nDuring pretraining, the speech encoder is trained on unlabelled audio, X \u2208 X , and unlabeled text, T u \u2208 T u . During finetuning, the model is trained on the labelled downstream dataset, (X, Y \u03c9 ) \u2208 X \u00d7 Y \u03c9 . Notice, costly labelled ASR data is not required during pretraining or finetuning.\n\nUnsupervised Semantic Representation as a Bridge\nTo incorporate semantic information into SSL speech encoders, E : X \u2192 Z, we wish to leverage subword-based LLMs, M : S \u2192 Z, that capture language semantics (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Beltagy et al., 2020) . The major challenge is the mismatch of input spaces. Speech encoders take raw audio as input, X \u2208 X . LLMs take subword tokens as input, S \u2208 S. SSE uses W2V2-U2.0 (Liu et al., 2022) as a bridge module (Seo et al., 2022) , B : Z \u2192 S, to convert speech encoder embedding output into LLM subword tokens in a pipelined approach,\nE SSE = E \u2022 B \u2022 M.\nFollowing W2V2-U2.0, the bridge module, B uses a GAN (Goodfellow et al., 2020) We also add an upsampling layer, U : Z \u2192 Z to make the sequence length of the LLM output match the speech encoder output, such that E and E SSE share the same output space.\nWe choose the 15th layer of the W2V2 (Baevski et al., 2020) as our speech encoder, as the last layers overfit the self-supervised training objective hence providing worse acoustic representations (Fan et al., 2020; Baevski et al., 2021; Pasad et al., 2021) . We choose BART (Lewis et al., 2019) as our LLM, as it is trained to denoise noisy input subword tokens, and we expect the bridge module to introduce some noise. We call this version of our model SSE-BASE. A depiction can be found in Figure 1a .\n\nCombining Semantics and Acoustics with Residual Attention\nWe hypothesize certain tasks may require more acoustic information than others. to implicitly transcribe parts of the input speech, a primarily acoustic task. Since the pipelined model may suffer from transcription errors introduced by ASR-U, naively using the pipelined approach introduces an information bottleneck at the bridge module. Hence, we propose adding a residual connection (He et al., 2016) between SSE-BASE and the speech encoder, E. This can be done in two ways: (1) upsampling semantic embeddings and concatenating with speech embeddings, Z = [Z E ||U(Z M )], or (2) using multihead attention (Vaswani et al., 2017) to merge the two embeddings, Z =\n[Z E ||MHA(Z E , Z M , Z M )],\nwhere Z E \u2208 Z is the output of the W2V2L15 (Baevski et al., 2020) and Z M \u2208 Z is the output of BART (Lewis et al., 2019) . The former is a simpler but more naive method. The latter is more effective as the attention layers are able to learn the alignment between speech and semantic embeddings. Notice, (2) introduces more learnable parameters to the finetuning-step, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder.\n\nAligning Pretrained Text Model with Adapters\nInspired by works from speech translation (Li et al., 2020; Le et al., 2021) , we hypothesize that the LLM can easily be adapted for speech tasks through the use of adapters. We adopt the general recipe for adapters, where an adapter (Houlsby et al., 2019) , composed of a LayerNorm and 2-layer ReLU neural network, is added to the end of each feed forward layer in the LLM and finetuned on downstream tasks. This introduces additional parameters to finetuning, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder. We call the model using both residual attention and adapters SSE-TUNE, and outline it in Figure 1b .\n\nExperiments 4.1 Dataset\nTo show the effectiveness of introducing semantics into speech encoders, we evaluate 3 SLU tasks, intent classification (IC), slot filling (SF), and named entity recognition (NER), and SQA \n\nSpoken Language Understanding\nTo show SSE improves SSL speech encoders, we augment two state-of-the art speech encoders under the universal representation setup: W2V2 and HUBERT. Following prior works that found intermediary layers of W2V2 contain better representations (Pasad et al., 2021; Baevski et al., 2021) , we consider the 15th layer and the last layer of W2V2, named W2V2L15 and W2V2L24 respectively. As mentioned in Section 3, we show 2 versions of our model, SSE-BASE and SSE-TUNE. The former uses the pipelined approach to connect W2V2L15 with BART (Lewis et al., 2019) with no additional modifications. The latter introduces an attention residual connection and learnable adapters to combine acoustics and semantics together and align the LLM with the speech encoder respectively. We either connect the residual connection to the output of W2V2L15, yielding SSE-TUNE (W2V2L15), or to the output of HU-BERT, yielding SSE-TUNE (HUBERT).\nTo show the importance of using LLMs, we compare against 2 very recent approaches for improving SSL speech encoders without LLMs, SPEECHLM (Zhang et al., 2022) and CON-TENTVEC (Qian et al., 2022) . As HUBERT-BASE was used as the base speech encoder by both baselines, we also provide results where SSE-TUNE is used to augment HUBERT-BASE.\n\nSpoken Question Answering\nTo show the effectiveness of SSE, we compare it against DUAL (Lin et al., 2022a) , the state-ofthe-art SQA model which does not use ASR data. While both SSE and DUAL obtain frame-level tokens from speech input, SSE uses ASR-U to obtain its tokens, whereas DUAL uses clustering. As a result, SSE's output tokens exists in the LLM's existing vocabulary, whereas DUAL's output tokens does not. Hence, DUAL must retrain the LLM on its output tokens.\nWe compare DUAL to the closest analogous SSE model, which is SSE-BASE but with adapter layers, SSE-BASE (ADAP). Similar to DUAL, both methods modify the LLM weights. Unlike DUAL, SSE-BASE (ADAP) is lightweight, tuning only around 10% of the total parameters. To produces framelevel predictions, we remove the upsampling layer from SSE-BASE (ADAP). We choose W2V2L15 as our speech model and BART as our LLM, as it is robust to ASR errors.\nWe also show a PIPELINE model, which trains a W2V2 model on ASR data and a LONGFORMER LLM on text-only question answering data. It is worth noting that since evaluation is based on the frame-level, SSL speech encoders are not a baseline since they operate at the audio level.\n\nDecoder Setup\nTo satisfy the universal representation setup, we adopt lightweight SLU decoders from SU-PERB (Yang et al., 2021) is sum pooling followed by a multilayer perceptron classifier trained with cross entropy loss. For the SF and NER tasks, the decoder is recursive neural network (RNN) that transcribes input audio into text. The decoder identifies named entities or slot values by surrounding them with named special tokens and is trained with connectionist temporal classification loss. For SQA, we adopt the same decoder as DUAL (Lin et al., 2022a) , which is a linear layer classifying each subword embedding as the start or end or neither of an answer span.\n\nImproving SSL Speech Encoders\nAs seen in Table 2 , SSE significantly improves the SLU performance of both W2V2 and HU-BERT, confirming that including semantic information drastically improves existing SSL speech encoder performance. Specifically, SSE-TUNE (W2V2L15) improves W2V2L15 on all tasks. SSE-TUNE (HUBERT) improves HUBERT on 3 out of 4 tasks, and is the best performing model overall. Comparing SSE-TUNE with SSE-BASE shows residual attention and adapters effectively counteracts bridge module transcription errors. The relative performance gain for IC is more than SF or NER. Unlike IC, both SF and NER require the speech encoder to transcribe identified audio snippets, and transcription is a primarily acoustic task. Hence SF and NER require less semantic information than IC. Nevertheless, combining both acoustic and semantic information, as done by SSE-TUNE, provides the most consistent performance improvement, since the skip connection can learn which type of information is more needed.\n\nImportance of LLMs\nAs seen in Table 2 , SSE-TUNE (HUBERT-BASE) outperforms alternative approaches augmenting speech encoders, SPEECHLM (HUBERT-BASE) and CONTENTVEC (HUBERT-BASE). Unlike these alternative approaches, SSE-TUNE incorporate information from LLMs, which we found to be very beneficial for capturing semantic information as they are carefully pretrained objectives on large amounts of unlabelled text data.\nIt is noteworthy that SSE-TUNE is a general framework which can augment any speech encoder of our choice, including SPEECHLM and CON-TENTVEC. Similarly, SSE-TUNE can directly integrate new LLMs without costly pretraining. We leave incorporating such encoders into SSE-TUNE as future work.\n\nSpoken Question Answering\nAs seen in Table 3 , SSE outperforms recent unsupervised clustering-based approaches, DUAL. In contrast to DUAL's HUBERT cluster tokens, SSE's ASR-U tokens are better aligned with LLMs and share the same space. Thus, SSE can better utilizes pretrained LLMs. Furthermore, SSE does not require carefully tuning the number of HUBERT cluster counts, as the vocabulary size of the LLM is fixed and consistent with ASR-U.\n\nChoice of Language Model\nWe find subword-based LLMs contain more information than phoneme-based LLMs (Clark et al., 2019) . We empirically verify this by replacing our subword-based LLM, BART (Lewis et al., 2019) , with popular character-based LLM, ByT5 (Xue et al., 2022) , and phoneme-based LLM, T5lephone (Hsu et al., 2022) in SSE-BASE. As seen in Table 4 , the subword-based LLM perform the best as each subword token is more semantically meaningful than a phoneme or character. We believe T5lephone outperforms the Byt5 as it has better robustness to ASR-U errors. Overall, subword-based LLMs are the best choice for embedding semantic information in transcribed text.\n\nResidual Attention and Adapters\nTo more carefully analyze the affect of residual attention and adapters in SSE-TUNE, we run experiments on all SLU datasets with and without each component. We denote these two design choices as (ResAtt) and (Adap) respectively. As seen in Table 4, both components provide ample performance improvement over SSE-BASE.\nWe also try the naive residual connection approach described in Section 3.3 by directly concatenating the LLM upsampled semantic embeddings to the speech embeddings. We call this approach SSE-BASE (RES). This method is less effective than SSE-BASE (RESATT) as it does not learn how to align speech and semantic embeddings, but improves SSE-BASE, further validating our hypothesis that merging acoustic and semantic information is beneficial.\nAs seen in parameter breakdown for the SSE-TUNE (W2V2L15) model in Table 1 , the number of new learnable parameters introduced by (Re-sAtt) and (Adap) are unsubstantial compared to the size of the lightweight downstream decoder. Specifically, the downstream task decoder accounts for 9.60% of the total model parameters. SSE-TUNE introduces only 10.47% more parameters than SSE-BASE during fine-tuning and 0.91% to the total model parameter count, but often provides significant performance improvement.\n\nComparison with Supervised ASR Methods\nTo quantify the effect of transcription errors introduced by the bridge module, we compute the word error rate (WER) of the bridge connector in SSE-TUNE, and compare it against standard W2V2 supervised ASR models (Baevski et al., 2020) trained on 10 minutes, 100 hours, and 960 hours of labeled ASR data. indicating the effectiveness of the bridge module.\nOn SLURP and SLUE, the relative drop in WER (> 20%) is substantially more than the relative drop in downstream performance (< 5%), verifying SSE-TUNE's tolerance to noisy transcriptions. The robustness to ASR errors come from our choice of LLM, BART, which is trained to handle noisy inputs, residual connection to acoustic embeddings, and LLM alignment with adapters.\n\nComparison to Specialized SLU Models\nTo better quantify the performance improvement introduced by SSE, we compare against 2 specialized SLU models that do not abide by the universal representation framework: Kaldi+HerMiT, which is a pipelined Kaldi ASR (Povey et al., 2011) and HerMiT NLU (Vanzo et al., 2019) model reported in the SLURP paper (Bastianelli et al., 2020) , and CTI (Seo et al., 2022) , which is an end-to-end pipelined W2V2 (Baevski et al., 2020) ASR and ROBERTA (Liu et al., 2019) NLU model. To the best of our knowledge, CTI is the state-of-the-art SLU model. In addition to unlabelled text, unlabelled audio, and downstream data, both Kaldi+HerMiT and CTI require 40 hours of downstream SLURP ASR data (Bastianelli et al., 2020) . Kaldi+HerMiT requires an additional 24,000 hours of ASR data (Povey et al., 2016) . CTI requires an additional 960 hours of ASR data (Panayotov et al., 2015) . Neither use lightweight fine-tuning. Thus, such specialized SLU models are less general, more expensive, and require much more data. As seen in Table 6 , SSE helps bridge the gap between tailormade models and more practical SSL speech encoders. We believe ASR-U errors plays a major role in the remaining gap, as the ASR-supervised Kaldi+HerMiT and CTI models have WER of 16.20% and 16.67% respectively, compared to A mix-up is when the model either misclassifies label \"A\" as \"B\" or misclassifies label \"B\" as \"A\". For each mix-up, we compute the percentage of less mistakes made by SSE-TUNE (HUBERT) than HUBERT. For example, SSE-TUNE (HUBERT) misclassifies calendar_set as calendar_query or vice-versa 20% less frequently than HUBERT. The \"general-quirky\" label is assigned to Out-of-Distribution inputs.\nSSE's ASR-U bridge with a WER of 51.51%.\n\nError Analysis\nTo better understand the semantic information captured by SSE, we study predictions made by both HUBERT and SSE-TUNE (HUBERT) on SLURP-IC's test set. We find HUBERT errors are made primarily between intents within the same or similar domains (e.g. calendar_set vs calendar_query).\nThe performance bottleneck lies with distinguishing finer-grained in-domain intents. Table 7 shows that SSE-TUNE is better at differentiating finergrained intents. SSE-TUNE's misclassifications come primarily from errors made by its ASR-U bridge component. As seen in Table 8 , the ASR-U WER of incorrect predictions made by HUBERT is much lower than that of incorrect predictions made by SSE-TUNE. When ASR-U returns resonable transcriptions (typically <50% WER), SSE-TUNE can correctly classify inputs that HUBERT cannot. Hence, the effectiveness of SSE is tightly coupled with the effectiveness of ASR-U.\n\nRepresentation Visualization\nTo better see the impact of including semantic representations, we visualize the pooled audio snippet embedding for intent classification on SLURP-IC using t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton, 2008) or incorrectly (\u2717). We denote the number of pairs belonging to each subset, in the thousands, in parentheses. denote the ground truth label of each audio snippet by the color of its pooled embedding. As seen in Figure 2 , the clusters produced by semantic embeddings are more spread out and better separated than those produced by just acoustic speech embeddings, indicating that SSE introduces new semantic information that existing speech encoders lack.\n\nConclusion\nWe presented a compelling case for introducing semantics into SSL speech encoders and an effective method of doing so. Our approach boosts the performance of existing speech encoders on multiple SLU and SQA tasks and datasets. We provide reasoning for what tasks may benefit more or less from incorporating semantics. Furthermore, our approach is task agnostic and can augment any existing SSL speech encoder. With SSE-TUNE, we show merging acoustic and semantic information and effectively aligning LLMs to the speech encoder on downstream tasks can further boost performance with minimal parameter overhead. As it can generalize to many downstream tasks, SSE provides an important step towards AI that can understand and respond to spoken language.\n"}
{"question": "What evaluation metric is used to assess unsupervised parsing performance in the paper?", "evidence": "  We use unlabeled corpus-level F1 to evaluate unsupervised parsing performance, reporting both overall F1 and discontinuous F1 (DF1). ", "options": ["A. Precision", "B. Perplexity", "C. F1 score", "D. BLEU score "], "answer": "C", "content": "\nIntroduction\nUnsupervised parsing aims to induce hierarchical linguistic structures given only the strings in a language. A classic approach to unsupervised parsing is through probabilistic grammar induction (Lari and Young, 1990) , which learns a probabilistic grammar (i.e., a set of rewrite rules and their probabilities) from raw text. Recent work has shown that neural parameterizations of probabilistic contextfree grammars (PCFG), wherein the grammar's rule probabilities are given by a neural network over shared symbol embeddings, can achieve promising results on unsupervised constituency parsing (Kim et al., 2019; Jin et al., 2019 Jin et al., , 2021;; Yang et al., 2021b Yang et al., , 2022)) .\nHowever, context-free rules are not natural for modeling discontinuous language phenomena such as extrapositions, cross-serial dependencies, and Code: https://github.com/sustcsonglin/TN-LCFRS. wh-movements. Mildly context-sensitive grammars (Joshi, 1985) , which sit between context-free and context-sensitive grammars in the classic Chomsky-Sch\u00fctzenberger hierarchy (Chomsky, 1959; Chomsky and Sch\u00fctzenberger, 1963 ), 1 are powerful enough to model richer aspects of natural language including discontinuous and non-local phenomena. And despite their expressivity they enjoy polynomial-time inference algorithms, making them attractive both as cognitively plausible models of human language processing and as targets for unsupervised learning.\nThere are several weakly equivalent formalisms for generating the mildly context-sensitive languages which might serve as potential targets for grammar induction: tree adjoining grammars (Joshi, 1975) , head grammars (Pollard, 1985) , combinatory categorial grammars (Steedman, 1987) , and linear indexed grammars (Gazdar, 1988) . In this paper we work with linear context-free rewriting systems (LCFGS, Vijay-Shanker et al., 1987) , which generalize the above formalisms and are weakly equivalent to multiple context-free grammars (Seki et al., 1991) . Derivation trees in an LCFRS directly correspond to discontinuous constituency trees where each node can dominate a non-contiguous sequence of words in the yield, as shown in Fig. 1 .\nWe focus on the LCFRS formalism as it has previously been successfully employed for supervised discontinuous constituency parsing (Levy, 2005; Maier, 2010; van Cranenburgh et al., 2016) . The complexity of parsing in a LCFRS is O(\u2113 3k |G|), where \u2113 is the sentence length, k is the fan-out (the maximum number of contiguous blocks of text that can be dominated by a nonterminal), and |G| is the grammar size. While polynomial, this is too high to be practical for unsupervised learning on real-world data. We thus restrict ourselves to LCFRS-2, i.e., binary LCFRS with fan-out two, which has been shown to have high coverage on discontinuous treebanks (Maier et al., 2012) . Even with this restriction LCFRS-2 remains difficult to induce from raw text due to the O(\u2113 6 |G|) dynamic program for parsing and marginalization. However Corro (2020) observe that a O(\u2113 5 |G|) variant of the grammar that discards certain rules can still recover 98% of real world treebank constituents. Our approach uses with this restricted variant of LCFRS-2 (see Sec 2.2). Finally, following recent work which finds that that overparameterizing deep latent variable models is beneficial for unsupervised learning (Buhai et al., 2020; Yang et al., 2021b; Chiu and Rush, 2020; Chiu et al., 2021) , we scale LCFRS-2 to a large number of nonterminals by adapting tensor-decomposition-based inference techniques-originally developed for PCFGs (Cohen et al., 2013; Yang et al., 2021b Yang et al., , 2022)) -to the LCFRS case.\nWe conduct experiments German and Dutchboth of which have frequent discontinuous and non-local language phenomena and have available discontinuous treebanks-and observe that our approach is able to induce grammars with nontrivial performance on discontinuous constituents. Rabanser et al., 2017) to decompose the 3D binary rule probability tensor T \u2208 R m\u00d7m\u00d7m as,\n\nApproach\nT = r q=1 u q \u2297 v q \u2297 w q ,\nwhere u q , v q , w q \u2208 R m , r is the tensor rank (a hyperparameter), and \u2297 is the outer product. Letting U, V, W \u2208 R r\u00d7m be the matrices resulting from stacking all u q , v q , w q , Cohen et al. ( 2013) give the following recursive formula for calculating the inside tensor \u03b1 \u2208 R (\u2113+1)\u00d7(\u2113+1)\u00d7m for a sentence of length \u2113:\n\u03b1 L i,j = V \u03b1 i,k , \u03b1 R j,k = W \u03b1 k,j , \u03b1 i,j = U T j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k .\nHere 2021b) and further pre-compute matrices J = V U T , K = W U T to rewrite the above recursive formula as:\n\u03b1 L , \u03b1 R \u2208 R (\u2113+1)\u00d7(\u2113+1\n\u03b1 L i,j = J\u03b1 \u2032 i,j ,\u03b1 R i,j = K\u03b1 \u2032 i,j \u03b1 \u2032 i,j = j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k\nwhere \u03b1 \u2032 \u2208 R (n+1)\u00d7(n+1)\u00d7r is an auxiliary inside score tensor. The resulting complexity of this approach is O(\u2113 3 r + \u2113 2 r 2 ), which is smaller than O(\u2113 3 r + \u2113 2 mr) when r \u226a m, i.e., in the setting with a large number of nonterminals whose probability tensor is of low rank. In this paper we adapt this low rank neural parameterization to the LCFRS case to scale to a large number of nonterminals.\n\nRestricted LCFRS\nIn an LCFRS, a single nonterminal node can dominate a tuple of strings that need not be adjacent in the yield. The tuple size is referred to as the fanout. We mark the fan-out of each non-leaf node in Fig. 1 . The fan-out of an LCFRS is defined as the maximal fan-out among all its nonterminals, and influences expressiveness and parsing complexity. For a binary LCFRS (i.e., LCFRS with derivation rules that have at most two nonterminals on the right hand side) with fan-out k, the parsing complexity for a sentence of length \u2113 is O(\u2113 3k ). 2 In this paper we work with binary LCFRS with fanout 2 (Stanojevi\u0107 and Steedman, 2020, LCFRS-2) , which is expressive enough to model discontinuous constituents but still efficient enough to enable practical grammar induction from natural language data. This choice is also motivated by Maier et al. (2012) who observe that restricting the fan-out to two suffices for capturing a large proportion of discontinuous constituents in standard treebanks. 3 However, LCFRS-2's inference complexity of O(\u2113 6 |G|) is still too expensive for practical unsupervised learning. We thus follow Corro (2020) and discard all rules that require O(\u2113 6 ) time to parse, which reduces parsing complexity to O(\u2113 5 |G|). 4 Formally, this restricted LCFRS-2 is a 6-tuple G = (S, N 1 , N 2 , P, \u03a3, R) where: S is the start symbol; N 1 , N 2 are a finite set of nonterminal symbols of fan-out one and two, respectively; P is a finite set of preterminal symbols; \u03a3 is a finite set of terminal symbols; and R is a set of rules of the following form (where M \u225c N 1 \u222a P):\nS(x) \u2192 A(x) A \u2208 N 1 A(xy) \u2192 B(x)C(y) A \u2208 N 1 , B, C \u2208 M A(yxz) \u2192 B(x)C(y, z) A \u2208 N 1 , B \u2208 M, C \u2208 N 2 A(x, y) \u2192 B(x)C(y) A \u2208 N 2 , B, C \u2208 M\n2 A binary CFG is thus a special case of a binary LCFRS with fan-out one, and parsing in this case reduces to the classic CKY algorithm.\n3 For instance, Stanojevi\u0107 and Steedman (2020) report that LCFRS-2 can cover up to 87% of the gold discontinuous constituents in the NEGRA treebank. We refer readers to Table 1 of Corro (2020) for more details. 4 These correspond to rules (d), (i), (j), (k), and (l) in Figure 3 of Corro (2020) .\n\nItem form:\n[A, i, j]: fan-out-1 node A spanning [i, j)\n[A, i, j, k, n]: fan-out-2 node A spanning [i, j), [k, n) Axioms: [A, i, i + 1], 0 \u2264 i < \u2113 + 1, A \u2208 N 1 Goals: [S, 0, n] Deductive rules: [B, i, k] [C, k, j] [A, i, j] A(xy) \u2192 B(x)C(y) i < k < j 1a [B, i, j] [C, m, n] [A, i, j, m, n] A(x, y) \u2192 B(x)C(y) i < j < m < n 1b [B, m, n] [C, i, m, n, j] [A, i, j] A(yxz) \u2192 B(x)C(y, z) i < m < n < j 2a [B, i, k] [C, k, j, m, n] [A, i, j, m, n] A(xy, z) \u2192 B(x)C(y, z) i < k < j < m < n 2b [B, k, j] [C, i, k, m, n] [A, i, j, m, n] A(yx, z) \u2192 B(x)C(y, z) i < k < j < m < n 2c [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, xz) \u2192 B(x)C(y, z) i < j < m < k < n 2d [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, zx) \u2192 B(x)C(y, z) i < j < m < k < n 2e\nTable 1 : Chart parsing algorithm described in the parsing-asdeduction framework. Here \u2113 is the sentence length and we use interstice indices (not word indices) as in Corro (2020) .\nA(xy, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(yx, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, xz) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, zx) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M T (w) \u2192 w, T \u2208 P, w \u2208 \u03a3.\nHere A(x) indicates that A has a fan-out 1; A(x, y) indicates that A has a fan-out 2 and x and y are nonadjacent contiguous strings in the yield of A. \n\u2192 B(x)C(y, z) above. B is a fan-out-1 node whose yield is x = w i \u2022 \u2022 \u2022 w k\u22121 and C is a fan-out-2 node whose first span is y = w k \u2022 \u2022 \u2022 w j\u22121 and whose second span is z = w m \u2022 \u2022 \u2022 w n\u22121 .\nA is the parent node of B, C, and inherits the yields of B and C, where x is concatenated with y to form a contiguous span and z is a standalone span.\nParsing. Table 1 gives the parsing-asdeduction (Pereira and Warren, 1983 ) description of the CKY-style chart parsing algorithm of our restricted LCFRS-2.\n\nTensor decomposition-based neural parameterization\nWe now describe a parameterization of LCFRS-2 that combines a neural parameterization with tensor decomposition, which makes it possible to scale LCFRS-2 to thousands of nonterminals.\nLet\nm 1 = |N 1 |, m 2 = |N 2 |, p = |P|, and m = m 1 + p.\nThe rules involving A \u2208 N 1 on the left hand side are 1a and 2a , whose probabilities can be represented by 3D tensors\nC 1 \u2208 R m 1 \u00d7m\u00d7m and D 1 \u2208 R m 1 \u00d7m\u00d7m 2 . For A \u2208 N 2 , the relevant rules are 1b , 2b , 2c , 2d , 2e , whose proba- bilities can be represented by 3D tensors C 2 \u2208 R m 2 \u00d7m\u00d7m and D 3 , D 4 , D 5 , D 6 \u2208 R m 2 \u00d7m\u00d7m 2 . We stack D 3 , D 4 , D 5 , D 6 into a single 4D tensor D 2 \u2208 R m 2 \u00d7m\u00d7m 2 \u00d74\nto leverage the structural similarity of these rules. Since these tensors are probabilities, we must have\nEQUATION\nEQUATION\nTensor decomposition. To scale up the LCFRS-2 to a large number of nonterminals, we first apply CPD on all the binary rule probability tensors,\nC 1 = r 1 \u22121 q=0 U 1 :,q \u2297 V 1 :,q \u2297 W 1 :,q C 2 = r 2 \u22121 q=0 U 2 :,q \u2297 V 2 :,q \u2297 W 2 :,q D 1 = r 3 \u22121 q=0 U 3 :,q \u2297 V 3 :,q \u2297 W 3 :,q D 2 = r 4 \u22121 q=0 U 4 :,q \u2297 V 4 :,q \u2297 W 4 :,q \u2297 P :,q\nwhere U :,q denotes the q-th column of U . The dimensions of these tensors are\nU 1 \u2208 R m 1 \u00d7r 1 , V 1 , W 1 \u2208 R m\u00d7r 1 , U 2 \u2208 R m 1 \u00d7r 2 , V 2 \u2208 R m\u00d7r 2 , W 2 \u2208 R m 2 \u00d7r 2 , U 3 , W 3 \u2208 R m 2 \u00d7r 3 , U 4 , W 4 \u2208 R m 2 \u00d7r 4 , V 3 \u2208 R m\u00d7r 3 , V 4 \u2208 R m\u00d7r 4\n, and P \u2208 R 4\u00d7r 4 . Here r 1 , r 2 , r 3 , r 4 are the ranks of the tensors that control inference complexity. To ensure these factorizations lead to valid probability tensors, 1), we additionally impose the following restrictions: (1) all decomposed matrices are non-negative; (2) P, V i , W i are column-wise normalized where i \u2208 {1, 2, 3, 4};\n(3) \u2200i, j U 1 ij + k U 2 ik = 1; and (4) \u2200i, j U 3 ij + k U 4 ik = 1.\nIt is easy to verify that Eq. 1 and 2 are satisfied if the above requirements are satisfied.\nRank-space dynamic programming. For unsupervised learning, we need to compute the marginal likelihood of a sentence p(w 1 w 2 \u2022 \u2022 \u2022 w \u2113 ). We give the rank-space dynamic program (i.e., the inside algorithm) for computing p(w\n1 w 2 \u2022 \u2022 \u2022 w \u2113 ) in this tensor decomposition-based LCFRS-2 in App. A.\nThe resulting complexity is dominated by O(\u2113 5 r 4 + \u2113 4 (r 3 +r 4 )(r 2 +r 4 )). We thus set r 4 to a very small value, which greatly improves runtime.\nParameterization. Following prior work on neural parameterizations of grammars (Jiang et al., 2016; Kim et al., 2019) , we parameterize the component matrices to be the output of neural networks over shared embeddings.\nThe symbol embeddings are given by: E 1 \u2208 R m\u00d7d where the first m 1 rows correspond to fanout-1 nonterminal embeddings and the last p rows are the preterminal embeddings; E 2 \u2208 R m 2 \u00d7d for the fan-out-2 nonterminal embedding matrix; r \u2208 R d for the start symbol embedding. We also have four sets of \"rank embeddings\"\nR 1 \u2208 R r 1 \u00d7d , R 2 \u2208 R r 2 \u00d7d , R 3 \u2208 R r 3 \u00d7d\n, and R 4 \u2208 R r 4 \u00d7d . Given this, the entries of the U, V, W matrices are given by,\nU o ij \u221d exp{(R o j ) \u22a4 f o U (E 1 i )}, o \u2208 {1, 2} U o ij \u221d exp{(R o j ) \u22a4 f o U (E 2 i )}, o \u2208 {3, 4} V o ij \u221d exp{(R o j ) \u22a4 f o V (E 1 i )}, o \u2208 {1, 2, 3, 4} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 1 i )}, o \u2208 {1, 2} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 2 i )}, o \u2208 {3, 4} where f o U , f o V , f o W are one-layer ReLU MLPs with output size d. U o , V o , W o\nare normalized according to the requirements described in the previous subsection. We share the parameters of the following MLP pairs:\n(f 1 U , f 2 U ), (f 3 U , f 4 U ), (f 1 V , f 3 V ), (f 2 V , f 4 V ), (f 1 W , f 3 W ), (f 2 W , f 4 W )\nas they play similar roles (e.g., f 1 V and f 3 V are both applied to left children). For the D 2 tensor we also require the matrix P \u2208 R 4\u00d7r 4 , and this is given by P \u22a4 = f P (R 4 ), where f P is a one-layer residual network with output size 4 that is normalized via a softmax across the last dimension.\nFinally, for the starting and the terminal distributions we have\ns = f s (r), Q = f Q (E 1 m 1 :\n), which results in s \u2208 R m 1 (i.e., the probability vector for rules of the form S \u2192 A) and Q \u2208 R p\u00d7v (i.e., probability matrix for rules of the form T (w) \u2192 w). Here E 1 m 1 : is the last p rows of E 1 , and f s and f Q are residual MLPs with softmax applied in the last layer to ensure that s and Q are valid probabilities.\nDecoding. While the rank-space inside algorithm enables efficient computation of sentence likelihoods, direct CKY-style argmax decoding in this grammar requires instantiating the full probability tensors and is thus computationally intractable. We follow Yang et al. (2021b) and use Minimal Bayes Risk (MBR) decoding (Goodman, 1996) . This procedure first obtains the posterior probability of each span's being a constituent via the inside-outside algorithm (which has the same complexity as the inside algorithm). Then, these posterior probabilities are used as input into CKY in a grammar that only has a single nonterminal. The complexity of this approach is thus independent of the number of nonterminals in the original grammar, and takes O(\u2113 5 ). This strategy can be seen as finding the tree that has the largest number of expected constituents (Smith and Eisner, 2006) . See App. A for details.\n\nEmpirical Study\nData. We conduct experiments with our Tensor decomposition-based Neural LCFRS (TN-LCFRS) on German and Dutch, where discontinuous phenomena are more common (than in English). For German we concatenate TIGER (Brants et al., 2001) and NEGRA (Skut et al., 1997) as our training set, while for Dutch we use the LASSY Small Corpus treebank (van Noord et al., 2013) . The data split can be found in App. B.1. For processing we use disco-dop 5 (van Cranenburgh et al., 2016) and discard all punctuation marks. We further take the most frequent 10,000 words for each language as the vocabulary, similar to the standard setup in unsupervised constituency parsing on PTB (Shen et al., 2018 (Shen et al., , 2019;; Kim et al., 2019) .\nGrammar size. To investigate the importance of using a large number of latent variables (which has previously been shown to be helpful for structure induction (Buhai et al., 2020; Yang et al., 2021b )), we train TN-LCFRSs of varying sizes. We first choose the number of preterminals |P| \u2208 {45, 450, 4500} and set the number of fan-out one and fan-out two nonterminals to be\n|N 1 | = |N 2 | = 1 3 |P|.\nThe rank of the probability tensors are set to r 1 = r 3 = 400, r 2 = r 4 = 4, and the dimensionality of the Baselines. Our baselines include: the neural PCFG (N-PCFG) and the compound PCFG (C-PCFG) (Kim et al., 2019) , which cannot directly predict discontinuous constituents 6 but still serve as strong baselines for overall F1 since the majority of spans in these treebanks are continuous; and their direct extensions, neural LCFRS (N-LCFRS) and compound LCFRS (C-LCFRS), which do not employ the tensor-based low-rank factorization. These non-low-rank models have high computational complexity and hence we set |P| = 45 for these models. When |P| = 4500, we also compare against the tensor decompositional-based neural PCFG (TN-PCFG) from Yang et al. (2021b) .\nEvaluation. We use unlabeled corpus-level F1 to evaluate unsupervised parsing performance, reporting both overall F1 and discontinuous F1 (DF1). For all experiments, we report the mean results and standard deviations over four runs with different random seeds. See App. B.2 for further details.\n\nMain results\nTable 2 shows the main results. With smaller grammars (|P| = 45), we find that both neural/compound LCFRSs have lower F1 than their PCFG counterparts, despite being able to predict discontinuous constituent spans. On the other hand, TN-LCFRS achieves better F1 than N-LCFRS even though it is a more restricted model (since it assumes that the rule probability tensors are of low rank), showing the benefits of parameter sharing through low rank factorizations. As we scale up TN-LCFRSs with |P| \u2208 {45, 450, 4500} we observe continuous improvements in performance, with TN-LCFRS 4500 achieving the best F1 and DF1 on all three datasets. These results all outperform trivial (left branching, right branching, and random tree) baselines.\nAs an upper bound we also train a supervised model with TN-LCFRS 4500 . 7 We also show the maximum possible performance with oracle binary trees with this optimal binarization. While the discontinuous F1 of our unsupervised parsers are nontrivial, there is still a large gap between the unsupervised and supervised scores (and also between the supervised and the oracle scores), indicating opportunities for further work in this area.\n\nAnalysis\nRecall by constituent label. Table 3 shows the recall by constituent tag for the different models averaged over four independent runs. Overall the unsupervised methods do well on noun phrases (NP), prepositional phrases (PP) and proper nouns (PN), with some of the models approach the supervised baselines. Verb phrases (VP) and adjective dynamic programming to sum out all possible nonterminals for each node, resulting in the joint log probability of unlabeled binarized tree and sentence. This was then maximized during training. As for the oracle bound, we emphasize that the gold trees are nonbinary while our model can only predict binary trees. Approximation error. Approximation error in the context of unsupervised learning arises due to the mismatch between the EM objective (i.e., log marginal likelihood) and structure recovery (i.e., F1), and is related to model misspecification (Liang and Klein, 2008) . Figure 2 (left column) plots the training/dev perplexity as well as the dev F1/DF1 as a function of the number of epochs. We find that larger grammars result in better performance in terms of both perplexity and structure recovery, which ostensibly indicates that the unsupervised objective is positively correlated with structure induction performance. However, when we first perform supervised learning on the log joint likelihood and then switch to unsupervised learning with log marginal likelihood (Figure 2 , right), we find that while perplexity improves when we switch to the unsupervised objective, structure induction performance deteriorates. 8 Still, the difference in F1 before and after switching to the unsupervised objective is less for larger models, confirming the benefits of using larger grammars.\n\nEven more restricted LCFRS formalisms.\nThere are even more restricted versions of LCFRSs which have faster parsing (e.g. O(\u2113 3 ), O(\u2113 4 )) but 8 It is worth noting that the phenomenon of mismatch between log marginal likelihood objective and parsing accuracy is quite common in unsupervised grammar induction (and latent variable modeling approaches to structured induction more generally). Many previous works have observed this phenomenon, e.g., Merialdo (1994) in the context of HMMs, and Johnson et al. (2007) and Liang and Klein (2008) in the context of PCFGs. This is partially attributed to the fact that generative grammars often make some unreasonable independence assumptions to make the training process tractable, which does not fully comply with the true generative process of human languages and their underlying structures. 45.4 0.9 44.5 0.5\n\nModel\nTable 5 : Ablation studies on the German (TIGER) treebank.\ncan still model discontinuous constituents. In the supervised case, these restricted variants have been shown to perform almost as well as the more expressive O(\u2113 5 ) and O(\u2113 6 ) variants (Corro, 2020) .\nIn the unsupervised case however, we observe in Table 5 that disallowing O(\u2113 5 ) rules ( 2b , 2c , 2d , 2e ) significantly degrades discontinuous F1 scores. We posit that this phenomena is again related to empirical benefits of latent variable overparameterization-while in theory it is possible to model most discontinuous phenomena with more restricted rules, making the generative model more expressive via \"overparameterizing\" in rule expressivity space (i.e., using more flexible rules than is necessariy) empirically leads to better performance.\nParameter sharing. As shown in Table 5 , it was important to share the symbol embeddings across the different rules. Sharing the parameters of the MLPs as described in Sec. 2.3 was also found to be helpful. This highlights the benefits of working with neural parameterizations of grammars which enable easy parameter sharing across rules that share symbols and/or have similar shapes.\nQualitative analysis. In Fig. 3 , we show some examples trees in German. For each sentence, we show the gold, TN-LCFRS 4500 , and TN-PCFG 4500 trees. In the first sentence, the crossing dependency occurs due to the initial adverb (\"So\")'s being analyzed as a dependent of the non-finite verb phrase at the end of the sentence which occurs due to German V2 word order. Our parser correctly predicts this dependency, although the subject NP (which itself is correctly identified) has the wrong internal structure. relative clause a part of the non-finite verb complex, which does not conform to the annotation guidelines but resembles an alternative analysis that has been proposed for extraposed relative clauses (Baltin, 1983) . Sentence initial adverbs in the context of auxiliary verb constructions and right-extraposed relative clauses describe two common instances of discontinuous phenomena in German. Wh-questions constitute another potential class of discontinuous phenomena; however, these are not treated as discontinuous in TIGER/NEGRA. See App. D for more examples trees (including on Dutch).\n\nRelated work\nMildly context-sensitive grammars. Given the evidence against the context-freeness of natural language (Shieber, 1985) , mildly context-sensitive grammars such as tree adjoining grammars were thought to be just flexible (but still constrained) enough to model natural language (Joshi, 1985) . Prior work on inducing mildly context-sensitive grammars has generally focused on combinatory categorial grammars (Bisk and Hockenmaier, 2012, 2013) , and we are unaware of any work on in-ducing LCFRSs from observed yields alone. Our work is also related to the rich line of work on supervised discontinuous parsing (Kallmeyer and Maier, 2010; Maier et al., 2012; Maier, 2015; Corro, 2020; Vilares and G\u00f3mez-Rodr\u00edguez, 2020; Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez, 2020 , 2021 , 2023) , though we are unaware of any prior work on unsupervised discontinuous parsing.\nNeural grammars. Early work on probabilistic approaches to grammar induction was largely negative (Lari and Young, 1990; Carroll and Charniak, 1992) . However, recent work has shown that neural parameterizations of classic grammars can greatly improve structure induction. Our work adds to the line of work on neural parameterizations of dependency models (Jiang et al., 2016; Han et al., 2017; He et al., 2018; Yang et al., 2020) , context-free grammars (Kim et al., 2019; Jin et al., 2019; Zhu et al., 2020; Yang et al., 2021a) , and synchronous grammars (Kim, 2021; Wang et al., 2022; Friedman et al., 2022) . Neural parameterizations make it easy to share parameters and condition on additional side information (images/audio/video) which has shown to be particularly useful for multimodal grammar induction (Zhao and Titov, 2020; Jin and Schuler, 2020; Su et al., 2021; Hong et al., 2021; Zhang et al., 2021) .\nScaling latent variable models. Buhai et al. (2020) study the empirical benefits of overparameterization in learning latent variable models. Other works have explored parameterizations of latent variable models that make it especially amenable to scaling (Chiu and Rush, 2020; Chiu et al., 2021; Yang et al., 2021b Yang et al., , 2022)) . Relatedly, Peharz et al. (2020) and Liu et al. (2022) show the benefits of scaling probabilistic circuits (Choi et al., 2020) .\n\nConclusion\nThis work studied unsupervised discontinuous constituency parsing with mildly context-sensitive grammars, focusing on the formalism of linear context-free rewriting systems. By using a tensor decomposition-based neural parameterization of linear context-free rewriting systems, our approach was able to induce grammars that had nontrivial discontinuous parsing performance on German and Dutch. Whether even more expressive grammars will eventually lead to models learn linguistically meaningful structures and are at the same time competitive with pure neural language models (as a language model) remains an open question.\n"}
{"question": "What is the main challenge addressed by the MixDA approach?", "evidence": "  Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. it is still unclear how to regularly, and inexpensively inject domain knowledge into PLMs for different domain-specific tasks. ", "options": ["A. Improving general language understanding", "B. Reducing the number of parameters in pre-trained models", "C. Efficiently injecting domain-specific knowledge into pre-trained models", "D. Tuning all parameters of the pre-trained models "], "answer": "C", "content": "\nIntroduction\nPre-trained language models (PLMs) have achieved a multitude of successful applications in natural language understanding (Devlin et al., 2018; Liu et al., 2019; He et al., 2021b) and generation (Lewis et al., 2019; Zhang et al., 2020; Yang et al., 2020; Brown et al., 2020) . The predominant methodology for domain adaptation is fine-tuning on labeled domain-specific data or continued pre-training (Gururangan et al., 2020) on unlabeled domain-specific data. Although effective, both fine-tuning and continued pre-training methods require tuning all the parameters of a PLM, raising high costs beyond many institutions' reach. To mitigate this, multiple parameter-efficient fine-tuning (PEFT) methods are proposed, including prompt-based tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . However, they are more concerned about task adaptation and it is still unclear how to regularly, and inexpensively inject domain knowledge into PLMs for different domain-specific tasks. Moreover, directly tuning PLMs on a domain-specific corpus with PEFT methods will lead to the catastrophic forgetting problem (Yogatama et al., 2019; Gururangan et al., 2020) . These limitations highlight an important research question: how to adapt PLMs with the new domain knowledge while keeping the old-domain knowledge unmodified?\nInspired by the recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022 ) that found knowledge is stored in feed-forward networks (FFNs), we decouple the FFNs into two parts: the original pre-trained FFNs to maintain the olddomain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Specifically, we propose Mixture-of-Domain-Adapters (MixDA), a mixture of several domain adapters to inject domain-specific knowledge without affecting the old-domain knowledge. Our model has two stages: piq domain-specific tuning multiple knowledge adapters on unlabeled data and then piiq task-specific tuning adapters on labeled data. In the first stage, we train several domain adapters on both domain-specific corpus and pre-training corpus simultaneously while keeping the original feed-forward networks unchanged. In the second stage, we train a mixture-of-adapters gate to dynamically select the desired knowledge adapter and a task-specific adapter for task adaptation.\nWe conduct experiments on a broad range of tasks, including 4 out-of-domain datasets, 9 in-domain datasets, and 2 knowledge-intensive datasets. Our experimental results demonstrate the effectiveness of MixDA on 15 datasets, spanning biomedical, computer science publications, news, and reviews. Further analysis displays three key properties of our proposed approach: piq Reliability: it shows superior performance on both in-domain and out-of-domain tasks. piiq Scalability: it scales well to the increasing number of domains. piiiq Efficiency: it adds only a small number of parameters per domain. We claim that these properties are helpful for language models as a service, where a frozen PLM is served, and multiple adapters are inserted to support different customized services.\n\nRelated Work\nIn this section, we will review four research lines related to injecting domain knowledge into pretrained language models: knowledge injection, domain adaptation, parameter-efficient fine-tuning, and mixture-of-adapters.\n\nKnowledge Injection\nKnowledge can be injected into PLMs by pretraining or fine-tuning, each corresponding to a separate research direction. During pre-training, the knowledge carried by knowledge graphs (Zhang et al., 2019; He et al., 2020) , entities (Sun et al., 2019; Xiong et al., 2020) , n-grams (Diao et al., 2020) , knowledge embedding (Wang et al., 2021b) , synonym and hyponym-hypernym relations in WordNet (Lauscher et al., 2019) , word-supersense knowledge (Levine et al., 2020) , and knowledge bases (Peters et al., 2019) can be injected into PLMs by feeding knowledge inputs and designing new objectives. However, pre-training-based methods are costly, making the application to huge PLMs (e.g., models with 175 Billion parameters) impossible. Fine-tuning-based methods only require an additional fine-tuning process. Some studies inject extra information into the input sentences, like knowledge triples from knowledge graphs (Liu et al., 2020) and knowledge context (Faldu et al., 2021) , while other studies explored specific model and training designs, like knowledge adapter networks (Wang et al., 2021a) , graph convolutional networks and LSTMs (Lin et al., 2019) , and metalearning (Sinitsin et al., 2020) . Zhu et al. (2020) formulated knowledge injection as a constrained optimization problem by adding a constraint on the loss on the unmodified facts. Recent studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) reveal that knowledge is stored in the feed-forward networks in PLMs. Inspired by these studies, we propose a new efficient tuning method to inject domain knowledge into feed-forward networks with minimal costs.\n\nDomain Adaptation\nPrevious studies have observed that language models suffer from a significant performance drop during the domain shift (Beltagy et al., 2019; Alsentzer et al., 2019; Huang et al., 2019; Lee et al., 2020; Ke et al., 2022b) . Effective strategies that can bridge the domain gap are introduced. Pre-training language models from scratch is effective but costly, like SciBERT (Beltagy et al., 2019) , BioBERT (Lee et al., 2020) , and ClinicalBERT (Alsentzer et al., 2019) . Recent studies explored continued pretraining (Gururangan et al., 2020) and adapter networks (Diao et al., 2021) to save time by training on unlabeled downstream task data. In this paper, we introduce plug-in domain adaptors for domain adaptation, which are effective and mitigate catastrophic forgetting issues because of the explicit learning strategy and efficient model architecture.\n\nParameter-Efficient Fine-tuning\nAnother relevant research direction is parameterefficient fine-tuning (PEFT), which only fine-tunes a small number of parameters. Existing works solve this problem from two perspectives: promptbased tuning (Gao et al., 2021; Liu et al., 2021b; Schick and Sch\u00fctze, 2021; Li and Liang, 2021; Liu et al., 2021a) , and adapter-based tuning (Houlsby et al., 2019; Pfeiffer et al., 2020b; Hu et al., 2021) . Several works in adapter-based tuning are closely related to ours. AdapterFusion (Pfeiffer et al., 2021) aims to combine multiple task adapters but does not offer specific architecture or training strategies to learn external knowledge. DEMix (Gururangan et al., 2022) and MixDA both train adapters that specialize in domains and use mechanisms to route different adapters, but differ in routing methods, base models, and training strategies. K-Adapter (Wang et al., 2021a ) is re-stricted by its training on T-REx triples and lacks the flexibility to train on unstructured knowledge. Similar to MixDA, CPT (Ke et al., 2022a) integrates domain knowledge into LMs, but it employs a different approach. While MixDA uses domain adapters to substitute FFN layers and task adapters to perform end tasks, CPT adds CL-Plugins that learn domain knowledge. Recent work by He et al. (2021a) presents a unified framework that establishes connections across different PEFT methods. Our work can leverage any PEFT method and complement them.\n\nMixture-of-Experts\nMixture-of-Experts (MoE) (Shazeer et al., 2017) is introduced with several expert networks, gating networks, and load-balancing techniques. The following studies improve MoE on initialization and training schemes (Fedus et al., 2022) , routing mechanisms (Zuo et al., 2021; Yang et al., 2021) , and load-balancing issues (Lewis et al., 2021; Roller et al., 2021) . AdaMix (Wang et al., 2022) proposed a mixture of adapters to improve the downstream task performance. Instead of mixing different designs of adapters, our domain adapter is a feedforward network specifically designed for domain knowledge.\n\nApproach\nGiven a pre-trained language model M, the input is a sentence X \" t 1 t 2 \u00a8\u00a8\u00a8t i \u00a8\u00a8\u00a8t T (t i indicates the i-th token) and the output is the representation of each token. The overall architecture of our model is shown in Figure 1 . The training process is divided into two-stage. In Stage 1 (Figure 1 (a)), we inject new feed-forward networks (FFNs) (namely domain-adapter) paralleled to the original pre-trained FFNs in some Transformer layers, acting as a key-value memory. The newly injected domain-adapter is trained on both domain-specific unlabeled data and original pre-training unlabeled data to store new factual associations while keeping old-domain ones. All modules are frozen except domain-adapter in this stage. In Stage 2 (Figure 1 (b)), we train a mixture-of-adapters (MoA) gate and a task-adapter on downstream tasks with labeled data, and only these two new modules are updated. The MoA gate receives outputs from the old-domain FFNs and domain-adapter, then outputs a weighted sum of them. An additional taskadapter is inserted in each Transformer block to facilitate downstream tasks. Figure 1 (c) shows the structures of the domain-adapter and the MoA gate.\nIn this section, we first introduce domainadapter, which learns and stores domain-specific knowledge, and then describe task-adapters that perform the downstream task. Finally, we discuss how the MoA gate integrates the outputs from the FFN and the domain-adapter.\n\nDomain-Adapter\nPrevious studies (Geva et al., 2021; Cao et al., 2021; Meng et al., 2022) suggest that factual associations are stored in the FFNs of some Transformer layers. To help models learn domain-specific knowledge, we propose a lightweight domain-adapter that works parallel to the FFNs, and a training method to learn domain-specific knowledge alongside keeping old-domain ones. Domain-adapter has a simple bottleneck architecture consisting of a down projection layer, a nonlinearity (such as ReLU (Agarap, 2018)), and an up projection layer. This helps keep the parameter size low (Houlsby et al., 2019) with competitive performance.\nIn Stage 1, the domain-adapter is trained with the domain-specific and old-domain datasets in one batch. Note that all other parameters are frozen except the domain-adapter at this stage. Let L K denote the knowledge loss related to domain-specific knowledge, and L S denote the sampling loss related to old-domain knowledge. The knowledge loss is a cross-entropy loss on predicting masked tokens, and the sampling loss is designed to align the latent spaces of the old-domain knowledge and new domain-specific knowledge. The total loss L is given by a weighted sum of the two, that is:\nEQUATION\nwhere \u03bb is a weight for the knowledge loss.\nThe knowledge loss is implemented by using cross-entropy loss. Given a sentence with M mask tokens whose answers are m 1 , m 2 , \u00a8\u00a8\u00a8, m M , respectively, the knowledge loss L K is given by\nEQUATION\nwhere ppm i q is the probability for token m i output by M. 2016)), we translate each relation into a sentence, and then mask out its object. For example, the relation \"the Eiffel tower-/r/LocatedAt-Paris\" is translated into \"The Eiffel Tower is located at Paris.\", then \"Paris\" is substituted with the mask token, and the model is trained to fill the mask. \u201a Unstructured knowledge For unstructured knowledge (e.g., downstream unlabeled texts), we use the masked language model (MLM) similar to RoBERTa pretraining. Some tokens are randomly sampled from the input sentence and replaced with the special token <mask>, and the model is trained to predict the masked token. The cross-entropy loss is calculated to optimize the model. For old-domain knowledge and sampling loss, we train the model on general corpora including Wikipedia and BookCorpus (Zhu et al., 2015) . Specifically, for each batch, sentences randomly sampled from the dataset are input into the model. Given L layers that have domain-adapters installed, for each such layer l, we collect token representations from the FFN F l , and representations from the domain-adapter K l . The goal is to keep them as similar as possible. Thus, we calculate the sampling loss L S with L2 loss:\nL S \" 1 L L \u00ff l\"1 ||F l \u00b4Kl || 2 2 .\n(3)\n\nTask-Adapter\nAfter training domain-adapters, the model is aware of the domain knowledge, which is not directly related to downstream tasks though. Therefore, we add task-adapters on top of the domain-adapter to adapt to downstream tasks. For example, a domainadapter trained in biomedical knowledge can sup- \n\nMixture-of-Adapters Gate\nOn downstream tasks, it is possible that the output from the FFN, or a weighted sum of the two, produces better results. Therefore, in Stage 2, we train an additional mixture-of-adapters (MoA) gate simultaneously. The MoA gate receives the outputs from the attention layer q, the domain-adapter K, and the FFN F . q is first sent into a multi-layer perceptron (MLP):\nEQUATION\n)\nThe MLP is composed of a down-projection layer W d and an up-projection layer W u , and h \" W u \u03c3pW d qq, where \u03c3 is the nonlinearity function.\nThen, h is input into a Sigmoid layer to generate the weights of the FFNs and other domain-adapters:\nw \" Sigmoidphq.\n(5)\nThe final output o is a weighted sum of the outputs of the FFNs and the domain-adapter:\nEQUATION\nwhere r; s denotes matrix concatenation.\n\nExperimental Settings\nIn this section, we first introduce the datasets, then the baseline models, the evaluation metrics, and implementation details in the following four subsections, respectively.\n\nDatasets\nWe conduct experiments on three types of datasets: in-domain (ID) tasks that require general-domain knowledge; out-of-domain (OOD) tasks that require domain-specific knowledge; knowledgeintensive (KI) tasks that require commonsense knowledge.\n\u201a ID: GLUE Benchmark (Wang et al., 2018) including MNLI (Williams et al., 2017) , CoLA (Warstadt et al., 2019) , MRPC (Dolan and Brockett, 2005) , SST-2 (Socher et al., 2013) , RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) , STS-B (Cer et al., 2017) , WNLI (Levesque et al., 2012) , QNLI (Rajpurkar et al., 2016) , and QQP (Iyer et al., 2017) . \u201a OOD: ChemProt (Kringelum et al., 2016) , RCT (Dernoncourt and Lee, 2017) , IMDB (Maas et al., 2011) , and Amazon (He and McAuley, 2016) . ChemProt is a manually annotated chemical-protein interaction dataset extracted from 5,031 abstractions. RCT is a dataset based on PubMed for sentence classification. IMDB provides 25,000 movie reviews for sentiment analysis. Amazon is a dataset containing product reviews from Amazon, annotated with user ratings. \u201a KI: FEVER (Thorne et al., 2018) and Common-senseQA (CSQA) (Talmor et al., 2019) . FEVER consists of 185,445 claims that correspond to Wikipedia articles and are classified as supported, refuted, and not enough information. Common-senseQA consists of 12,247 questions with 5 choices and requires commonsense knowledge to predict the correct answers.\nFor Stage 1, we train the domain-adapter with unstructured knowledge related to the dataset following Section 3.1. The unstructured knowledge used is listed in Table 1 . We also experiment with structured knowledge in Section 6.2. For Stage 2, we adopt the true few-shot setting following (Perez et al., 2021) to demonstrate the effectiveness of MixDA. For each dataset class, we randomly sample K \" 16 examples from the original training set as the new training set, and another different K \" 16 examples as the validation set. The original validation set will be used as the test set. The Pfeiffer adapter is used in Stage 2 unless stated otherwise.\n\nBaselines\nIn our experiments, we use the following models as the main baselines. For convenience, we refer to them with the abbreviations in the parentheses later. \u201a HOULSBY (HO): Houlsby adapter (Houlsby et al., 2019) Prefix-Tuning trains a number of prompt embeddings for each task and pre-pends it before tokens. \u201a FINE-TUNING (FT): Fine-tuning all of the parameters of the RoBERTa-large model on downstream tasks.\n\nEvaluation Metrics\nWe adopt the Pearson correlation for STS-B since it is a regression task. The remaining are text classification tasks. Following Wang et al. (2018) ; Gururangan et al. ( 2020); Diao et al. (2021) , we adopt macro-F1 for MRPC and QQP, and micro-F1 for others as evaluation metrics. Macro-F1 computes the F1 independently for each metric, while micro-F1 computes an average metric of all classes. To account for the instability of small datasets, we report the average performance and the standard deviation of 3 runs with different random seeds.\n\nImplementation\nWe implement our RoBERTa-large model based on the Transformers library from HuggingFace 2 . The Houlsby adapter, the Pfeiffer adapter, and Prefix-Tuning are implemented based on the adaptertransformers library (Pfeiffer et al., 2020a) . LoRA is implemented based on OpenDelta (Ding et al., 2022) . During Stage 1, we train the domain-adapter with learning rate 1e-4, batch size 20, and weight decay 0.05. The knowledge loss factor \u03bb is set to 0.5. We train the 7 and 11 layers of RoBERTa-large with domain-adapter in 10 epochs. In Stage 2, we use the Pfeiffer adapter as the default task-adapter and train 20 epochs. All the experiments are conducted on Nvidia 2080Ti GPUs. We find the best hyper-parameters through grid search and the best results are listed in Appendix A. The computation time can be found in Appendix B.\n\nExperimental Results\nWe compare the performance of MixDA with our baselines on 15 datasets. First, we train the domainadapter for each domain individually and then perform each task with its corresponding domainadapter, which shows significant improvement over our baselines. Next, we plug in several domainadapters trained on different domains parallelly to verify the scalability of our model. detect the chemical-protein interaction. For example, MixDA shows more familiarity with words associated with that field, such as \"gefitinib\" and \"tyrosine kinase inhibitor\". In contrast, MixDA falters on STS-B, falling behind Pfeiffer by 0.8%. This is because the knowledge in Stage 1 is not effectively utilized. STS-B consists of sentence pairs like \"The cat sat on the mat\" and \"The cat did not sit on the mat\", with little need for additional knowledge. Across the three task domains, MixDA has an average improvement of 4.8% over RoBERTa + Pfeiffer on out-of-domain tasks, 2.5% on indomain tasks, and 5.0% on knowledge-intensive tasks. It shows that MixDA is not only effective for out-of-domain tasks and knowledge-intensive tasks that require additional knowledge but is helpful for general-domain language tasks as well, demonstrating its ability to excel at both in-domain and out-of-domain tasks (reliability).\n\nParallel Domain Adapters\nIn the previous section, we explored using a single domain-adapter for each downstream task. Next, we show the scalability of MixDA by using parallel domain-adapters and only train the MoA layer and task-adapters in Stage 2. The training process in Stage 2 follows the previous experiments. Table 3 shows the comparison across single domainadapter, parallel domain-adapters, and RoBERTa + Pfeiffer on 7 datasets. On average, parallel domainadapters show an improvement of 0.6% over vanilla RoBERTa + Pfeiffer, even though they fall behind the single domain adapter by 1.9%. This could be attributed to the MoA gate choosing the suboptimal domain-adapter for some test data. Still, considering its improvement over Pfeiffer, the MoA gate chooses the correct domain-adapter in most cases. Therefore, MixDA demonstrates its scalability, allowing end users to train Stage 1 on different datasets and combine them later. Overall, in both single and parallel situations, MixDA significantly improves upon the vanilla RoBERTa + Pfeif- fer model with a small increase in model size. This is due to the ability of MixDA to capture knowledge and the MoA to select useful knowledge for downstream tasks.\n\nAnalysis\nIn this section, we analyze the respective contributions of each part of MixDA through detailed analysis, including the Stage 1 training, task-adapters in Stage 2, and the mixture-of-adapters gate.\n\nAblation Study\nIn this section, we conduct an ablation study to reveal the contributions of each part of the model. There are three variants: (1) We remove the MoA gate and choose the domain-adapter instead of the RoBERTa feed-forward layer (-MoA). ( 2 \n\nStructured and Unstructured Knowledge\nIn Section 5, the MixDA is only trained on unstructured knowledge. As a comparison, we also train the domain adapter on ConceptNet, a structured knowledge dataset, and then attach both the unstructured and structured to our model and train the MoA layer and the task-adapter during Stage 2.\nTable 5 shows the result of combining structured and unstructured knowledge in Stage 1. FEVER and CSQA, two knowledge-intensive tasks, have the greatest improvement: 10.3% for FEVER and 1.2% for CSQA. This is because ConceptNet stores commonsense knowledge that can help both tasks. Meanwhile, MRPC and STS-B also obtain improvement, showing that ConceptNet can benefit general language tasks as well. In conclusion, the experiment demonstrates the ability of MixDA to utilize structured knowledge, the extensibility of our model, and the possible benefits of structured knowledge.\n\nEffectiveness of Task-Adapters\nIn most experiments of this paper, we adopt Pfeiffer as the task-adapter unless otherwise specified. In this section, we test the performance of MixDA combined with other kinds of task-adapters, including Houlsby, Prefix-Tuning, LoRA, and Pfeiffer. parameters compared to Houlsby, making it the optimal choice of task-adapters in our experiment.\n\nConclusion\nIn this paper, we proposed MixDA, a mixture of adapters for domain adaptation. We first decouple the knowledge modules (i.e., FFNs) into the old-domain and domain-specific FFNs. Then we propose a two-stage adapter tuning strategy: first tuning the domain adapter on each domain and then tuning the task adapter on each task. Moreover, our model could be scaled to multiple domains easily with the introduction of the mixture-of-adapters gate. Empirically, MixDA achieved significant improvement over in-domain tasks, out-of-domain tasks, and knowledge-intensive tasks. Further analyses demonstrate the reliability, scalability, and efficiency of our method.\n"}
{"question": "What graphs are not available for VisText in the From Graphs to Text Statista dataset?", "evidence": "   This focus is informed by recent work in the data visualization research community which has chosen single-series line charts as the target of study for natural language descriptions (Kim et al., 2021; Stokes et al., 2022) .  VisText also includes single-series bar and area charts as they typically exhibit similar perceptual features to line charts. For each data table, we iterate through pairs of univariate fields. If the pair contains a temporal field, we randomly generate an area or line chart;\n\uf0b7  ", "options": ["A.  Multi-series bar chart", "B.  single-series bar", "C.  Area chart", "D.  single-series line charts"], "answer": "A", "content": "\nIntroduction\nStudies have shown that captions can improve the recall and comprehension of the data that charts depict (Hegarty and Just, 1993; Large et al., 1995) . For instance, when a caption emphasizes visually prominent features of a chart, like a peak or a sharply declining trend, readers treat this information as the key takeaway (Kim et al., 2021) . Moreover, for people with visual disabilities, captions (or equivalent descriptions such as alt text) are often the only means of accessing the presented data. However, as evidenced by numerous guidelines (Jung et al., 2021) , producing high-quality * Both authors contributed equally to this work. chart captions is a non-trivial and laborious manual process. Thus, despite these advantages, charts are only rarely captioned in practice (Lundgard and Satyanarayan, 2022) .\nTo bridge this gap, several research communities have begun to explore methods for automatically generating chart captions, including using templates and heuristics (Demir et al., 2008; Srinivasan et al., 2019) , adapting image captioning techniques (Balaji et al., 2018; Chen et al., 2019a) , or via data-to-text machine translation (Kantharaj et al., 2022; Obeid and Hoque, 2020) . While promising, these approaches have largely produced captions that either describe a chart's construction (e.g., \"The graph is plot between 'Number of people' x-axis over 'Movie Genres' y-axis\" (Balaji et al., 2018) ) or provide statistical summaries (e.g., \"Machinery and equipment was the most valuable commodity for Singapore in 2019\" (Kantharaj et al., 2022) ). However, these captions do not articulate the perceptual and cognitive features that make charts a distinctive and compelling medium for communicating data (e.g., \"Prices of Big Tech corporations seem to fluctuate but nevertheless increase over time\" (Lundgard and Satyanarayan, 2022) ). Indeed, as Lundgard and Satyanarayan (2022) find, both sighted and blind readers strongly prefer captions that express this type of content.\nTo automatically produce such semantically richer captions, we introduce VisText: a benchmark dataset of 12,441 pairs of charts and captions. VisText makes two key extensions over prior approaches. First, VisText offers three representations of charts: a rasterized image and backing data table, as in previous work; and a scene graph, a hierarchical representation akin to a web page's Document Object Model (DOM), that presents an attractive midpoint between the affordances of chart-as-image and chart-as-data-table. Second, for each chart, VisText provides a synthetically generated caption detailing its construction as well as a crowdsourced caption describing its statistical, perceptual, and cognitive features. These crowdsourced captions represent a substantial increase in data over prior comparable datasets (Mahinpei et al., 2022; Kantharaj et al., 2022) .\nTo demonstrate the possible uses of the VisText dataset, we train three classes of models -textbased caption models, image-guided captioning models, and semantic prefix-tuning. Text-based captioning models fine-tune large language models for VisText's chart captioning task, revealing that both data table and scene graph representations can produce compelling and semantically rich captions. Following recent advancements in image-guided translation (Sulubacak et al., 2020) , we leverage the additional visual affordances in chart images to develop image-guided chart captioning models. Finally, since users often have varying preferences about the type of semantic content in their captions (Lundgard and Satyanarayan, 2022) , we apply semantic prefix-tuning to each of our models, enabling them to output customizable captions.\nOur models generate coherent, semantically rich captions across the VisText charts. Evaluating against standard machine translation and text generation metrics reveals that our models consistently output captions that accurately describe the chart's construction, such as its chart type, title, and axis ranges. Through qualitative analysis of our model's captions, we find that our model competently outputs semantically rich captions that describe data trends and complex patterns. Further, we categorize six common captioning errors that can inform the future development of chart captioning models on the VisText dataset.\nThe VisText dataset and source code are available at: https://github.com/mitvis/ vistext.\n\nRelated work\nHeuristic-Based Chart Captioning. Automatically generating natural language descriptions of data tables dates back to Reiter and Dale (1997) . Demir et al. (2008 Demir et al. ( , 2010 Demir et al. ( , 2012) ) survey this early work and describe the process of extracting insights from a chart by evaluating a list of propositions and composing selected propositions together to produce a natural language summary. More recently, data visualization researchers have explored heuristics that calculate summary statistics and templates to assemble natural language \"data facts\" (Srini-vasan et al., 2019) or descriptions (Cui et al., 2019) . While useful, these approaches yield standardized descriptions that lack the variation and linguistic construction that characterize semantically rich captions (Lundgard and Satyanarayan, 2022) .\nChart Captioning as Image Captioning. With rapid advances of neural image captioning (Vinyals et al., 2015; Anderson et al., 2018) , researchers have begun to adapt these methods for captioning charts. For instance, Balaji et al. (2018) develop a deep learning pipeline that ingests a PNG chart image, classifies the chart type, detects and classifyies textual content present in the chart, and uses this information to generate a textual description. Chen et al. (2019a Chen et al. ( ,b, 2020) ) propose a simpler workflow using ResNet to encode the chart image and an LSTM with Attention to decode it into a natural language description. Both approaches share a pair of limitations. The captions they produce convey relatively simplistic information about the chart (e.g., title, axis labels, etc.) or articulate concepts in visual rather than data terms (e.g., \"Dark Magenta has the lowest value\"). While both approaches contribute associated datasets, their charts and captions are synthetically generated and may not represent real-world counterparts. SciCap (Hsu et al., 2021) addresses this limitation by scraping real-world charts from 290,000 arXiv papers; however, the baseline models trained on this dataset struggle to generate semantically rich captions.\nChart Captioning as Text Translation. Perhaps closest to our contribution is recent work modeling chart captioning as a data-to-text problem. For instance, Spreafico and Carenini (2020) train an encoder-decoder LSTM architecture to generate a natural language caption from time series data. Similarly, Obeid and Hoque (2020) and Kantharaj et al. (2022) explore how transformer architectures can translate tabular structures into captions. These data-to-text methods are more successful than chart-as-image captioning, yielding captions that better capture relevant information from the charts and have higher BLEU scores. Nevertheless, we observe two limitations with these data-to-text approaches that motivate our contribution. First, data-to-text methods are heavily reliant on access to a chart's data table. In practice, data tables are only rarely published alongside charts and methods that recover equivalent information via OCR experience a significant drop in performance (Kantharaj et al., 2022) . Second, the associated datasets do not contain sufficient training examples of captions that express semantically rich insights about the depicted data (i.e., the perceptual and cognitive phenoma that distinguish charts as a medium as distinct from data tables (Lundgard and Satyanarayan, 2022) ). As a result, while the generated captions are compelling, they are largely limited to reporting statistics which sighted and blind readers prefer less than captions that convey complex trends and patterns (Lundgard and Satyanarayan, 2022) .\n\nThe VisText Dataset\nWe designed the VisText dataset in response to two limitations existing datasets present for generating semantically rich chart captions. First, existing datasets represent charts as either rasterized images or as data tables. While useful, these representations trade off perceptual fidelity and chart semantics in mutually exclusive ways -images capture the perceptual and cognitive phenomena that are distinctive to charts (e.g., trends or outliers) but pixels cannot express the rich semantic relationships between chart elements (e.g., estimating plotted data values using axis labels). While the vice-versa is true (Lundgard and Satyanarayan, 2022) , tables also present additional caveats. There is not always a one-to-one relationship between the semantics of a data table and chart (i.e., one data table may be the source for several distinctly different charts). Moreover, data tables are rarely published alongside charts; and, automatic data table extraction is error-prone due to the diversity of chart types and visual styles as well as the difficulty of reasoning about visual occlusion (Kantharaj et al., 2022; Luo et al., 2021; Jung et al., 2017) ).\nSecond, if existing datasets provide captions that describe perceptual or cognitive features, these captions comprise only a small portion of the dataset. At best, LineCap (Mahinpei et al., 2022) offers 3,528 such captions for line charts only, while Chart-to-Text (Kantharaj et al., 2022) estimates that roughly 15% of the sentences in its captions across a variety of chart types express such content.\nIn contrast, VisText provides 12,441 crowdsourced English captions that articulate statistical, perceptual, and cognitive characteristics of bar, line, and area charts. In VisText, charts are available as not only data tables and rasterized images but also as scene graphs. Scene graphs are hierarchical representations that better preserve perceptual fidelity and chart semantics, are often the format for publishing web-based charts, and can be recovered from chart images (Poco and Heer, 2017) .\n\nData Table Collection\nThe data tables found in VisText are sourced from the Statista dataset of the Chart-to-Text benchmark (Kantharaj et al., 2022) . The tables were collected by crawling Statista.com in December 2020 and contain real-world data related to technology, trade, retail, and sports. We process these tables to make them amenable for chart generation, including stripping formatting symbols (e.g., $ and %), standardizing data strings, and identifying the measure type of each column (i.e., quantitative, categorical, or temporal). Data tables are discarded if they do not contain at least one quantitative field and one categorical or temporal field, or if other errors occur during the processing steps. We further down select to data tables containing between 2 to 20 columns and 10 to 500 rows. If a data table has over 500 rows, we randomly sample rows. In larger data tables, this step potentially affects how salient a trend is.\n\nChart Generation and Representation\nCharts in the Chart-to-Text Statista dataset all feature the same layout and visual appearance. In contrast, we aim for richer visual diversity by generating charts using the Vega-Lite visualization library (Satyanarayan et al., 2016) via the Python Altair package (VanderPlas et al., 2018) . To facilitate collecting high-quality captions, we focus on univariate charts: charts that depict one quantitative observation against a categorical or temporal variable. This focus is informed by recent work in the data visualization research community which has chosen single-series line charts as the target of study for natural language descriptions (Kim et al., 2021; Stokes et al., 2022) . VisText also includes single-series bar and area charts as they typically exhibit similar perceptual features to line charts.\nFor each data table, we iterate through pairs of univariate fields. If the pair contains a temporal field, we randomly generate an area or line chart; if the pair contains a categorical field, we randomly generate a horizontal or vertical bar chart. For diversity in layout and visual appearance, we randomly rotate axis labels and apply one of fourteen themes provided by the Vega-Lite library. These themes mimic the visual style of common chart platforms or publishers (e.g., ggplot2 or the LA Times). \n\nGenerated L1 Caption\nHere is a area chart is labeled Cumulative number of patients diagnosed with coronavirus (COVID-19) in Japan as of December 4, 2020, by place of infection. On the x-axis, Month is measured with a categorical scale starting with April and ending with October. There is a linear scale with a minimum of 0 and a maximum of 150,000 along the y-axis, labeled Patients within Japan.\n\nCrowdsourced L2/L3 Caption\nBy December 4th 2020, approximately 160,000 people in Japan had been diagnosed with COVID-19. The first person diagnosed with COVID-19 in Japan was diagnosed in March 2020. The greatest increase in cumulative number of patients in Japan diagnosed with COVID-19 occurred between November and December 2020. In VisText, each chart is represented as a rasterized image, stored as an RGBA-encoded PNG file, as well as a scene graph. A scene graph is a textual representation of the rendered chart similar to a web page's Document Object Model (DOM). Scene graphs encode the position, value or content, and semantic role of all visual elements within a chart, including the individual marks (i.e., bars or points along the line), titles, axes gridlines, etc. Thus, scene graphs express the perceptual features of rasterized images in a more computationallytractable form.\n\nCumulative number of patients diagnosed with coronavirus (COVID-19) in\nScene graphs are a standard data structure for representing vector-based graphics -the most common format for publishing visualizationsand, thus, can be trivially recovered (e.g., by traversing the SVG text string). We extract the scene graph directly from the rendered chart using the Vega-Lite API. As most text generation models expect a linear set of input tokens, we flatten the scene graph via a depth-first traversal. To scale to large language models, we need to further reduce the size of the scene graph. Thus, we preserve the following elements which we hypothesize as being most critical for generating semantically rich captions: title, title coordinates, axis labels, axis label coordinates, axis tick coordinates, mark coordinates, and mark sizes. VisText includes both the original (hierarchical) and reduced (linearized) scene graphs.\n\nCaption Generation and Collection\nOur captioning process is guided by the framework developed by Lundgard and Satyanarayan (2022) , which identifies four levels of semantic content: L1 content enumerates aspects of the chart's construction (e.g., axis ranges); L2 content reports summary statistics and relations (e.g., extrema); L3 content synthesizes perceptual and cognitive phenomena (e.g., complex trends); and, L4 content describes domain-specific insights (e.g., sociopolitical context). In subsequent studies, the authors find that while sighted readers typically prefer higher levels of semantic content, blind readers are split about the usefulness of L1 and L4 content. Thus, given these differing preferences, we define a single caption to express multiple levels of content separated across clauses or sentences. We only consider the first three levels of this model, and leave L4 content to future work. Following guidelines prescribed by the National Center for Accessible Media (NCAM), our captions begin with L1 content and then turn to L2 and L3 content (Gould et al., 2008) .\nWe algorithmically generate L1 content and use a crowdsourced protocol to collect L2 and L3 content. This approach follows (Lundgard and Satyanarayan, 2022)'s computational considerations as well as results from Morash et al. (2015) who find that, even with instructions and guidelines, crowd workers do not describe a chart's structural elements sufficiently for blind readers. Thus, synthetically generating L1 content allows us to ensure that captions convey complete descriptions of the chart's structural elements. L1 content comprises 1 sentence conveying the chart type and title, and then 1 -2 sentences describing the axes (including the titles, ranges, and scales). We use template randomization to generate a diverse range of L1 captions to mimic human variability and reduce the capacity of the model to overfit to a single L1 style. Three templates are defined for the first sentence and twenty-six template combinations for the subsequent sentences. During generation, we randomly select a pair of templates and fill in in- formation from the abstract chart specification. For additional diversity, we randomly drop scale information and swap template words with synonyms. Templates and synonym replacements are listed in Appendix E.2.\nTo crowdsource L2 and L3 content, we extend the protocol used by Lundgard and Satyanarayan (2022) . After soliciting consent, we introduce the task: participants are presented with a chart image and corresponding L1 description; they are asked to write a description about the trends and patterns they observe without drawing on background knowledge or repeating L1 information. The introduction provides examples and explanations of valid and invalid responses. After acknowledging these examples, participants are asked to complete 5 random iterations of the task. To maximize the quality of our crowdsourced captions, we manually curated the charts and L1 descriptions used in the study. We discarded any charts that were challenging to read (e.g., colors were too similar, marks were not easily readable, etc.). Participants were recruited on the Prolific.co platform, took approximately 14 minutes to complete the study, and were compensated $3.25 ($14/hour). Additional details on our crowdsourcing process are in Appendix E.3.\nWe manually verified charts where participants failed an attention check and discarded invalid descriptions. Additionally, we manually inspected captions for personally identifiable information or offensive content. Using heuristics, we removed captions where respondents described charts as unclear or illegible and replaced newline characters with spaces. Although we attempted to fix incorrect spelling and casing errors using a similar heuristic-based approach, we observed that this process could improperly affect axis and chart names. As a result, these errors remain in our dataset.\n\nDataset Analysis\nFigure 2 shows the distribution and means of the lengths of chart representations and captions. Synthetically generated L1 captions have roughly 1.5x more characters than crowdsourced L2/L3 captions (\u00b5 = 255 vs. \u00b5 = 177) but the average number of sentences are comparable (2.5 vs. 2). The VisText dataset consists of captions for 3,189 area charts, 6,238 bar charts, and 3,014 line charts -the roughly twice-as-many bar charts as area or line charts corresponds to the randomization of temporal fields during chart generation (Sec. 3.2). As some charts have multiple crowdsourced captions, we randomly split our dataset into training, validation, and test sets using the chart IDs to prevent data leakage across sets. This resulted in an approximate ratio of 80:10:10.\nFinally, to understand the distribution of semantic content, we manually coded 2% (230) of crowdsourced captions. We followed a protocol inspired by Lundgard and Satyanarayan (2022) by breaking sentences down into independent statements and mapping these statements to their semantic content level. We marked statements as not categorizable if they did not map to the framework -for instance, if captions expressed commentary from crowd workers such as \"this chart is hard to read.\" Our analysis revealed 11 L1 statements (2.4%), 180 L2 statements (39.7%), 253 L3 statments (55.7%), and 10 not categorizable statements (2.2%). While a handful express L1 content, the bulk of statements (95%) express L2 or L3 content, with approximately 1.4x L3 statements than L2.\n\nChart Captioning Models\nTo demonstrate the affordances of the VisText dataset, we train three classes of models. First, we fine-tune large language models to translate from textual chart representations to natural lan-guage captions. These models evaluate the feasibility and impact of scene-graph models compared to prior data-table approaches (Kantharaj et al., 2022) . Second, as VisText provides multiple chart representations, we adapt image-guided translation (Sulubacak et al., 2020; Cho et al., 2021) to develop two multimodal chart captioning models: image-scene-graph and image-data-table. Finally, since VisText offers captions at different semantic levels and prior work has shown significant differences in readers' preferences (Lundgard and Satyanarayan, 2022) , we explore prefix-tuned models that selectively output L1, L2/L3, or L1+L2/L3 captions. Training details are in Appendix D.\n\nText-Based Chart Captioning\nInformed by prior work (Kantharaj et al., 2022) , we investigate text translation models for generating chart captions. In particular, Kantharaj et al. found that models that translate data tables to chart captions significantly outperform image captioning models. However, when data tables were not available, the authors found a significant drop in their models' ability to extract relevant information from the chart -an effect that was only slightly ameliorated by using OCR methods to extract text from chart images. In contrast, VisText's scene graphs can be more readily recovered from charts when data tables are not available -for instance, by processing the SVG format of web-based visualizations. Moreover, scene graphs offer a potentially richer source of information than data tables as they encode visual properties of the chart (e.g., coordinates and colors) and are less noisy than tokens recovered via OCR. Thus, to evaluate the feasibility and efficacy of scene graphs, we train a scene-graph text translation model and a baseline data-table model for comparison.\nFor each model, we fine-tune a pretrained ByT5 transformer model (Xue et al., 2022) on the Vis-Text dataset. We choose ByT5 over T5 transformers (Raffel et al., 2020) because it uses a token-free, byte-encoding that eliminates the use of a tokenizer. As a result, it is robust to noisy inputs, minimizes the need for text preprocessing, and eliminates the out-of-dictionary problem. This allows our model to handle common typographical and chart reading errors in the crowdsourced L2 and L3 captions and increases generalizability to previously-unseen words that could be present in chart and axes titles.\n\nImage-Guided Chart Captioning\nFollowing recent advancements in image-guided machine translation (Sulubacak et al., 2020) , we train image-guided captioning models using the VisText dataset. Images have improved text-based machine translation models by providing visual information complementary to natural language inputs. Similarly, chart images can contain visuals complementary to the textual specification. For instance, visual affordances that are important for perceiving a trend (e.g., gestalt relations, relative sizes/areas, etc.) may be obfuscated in the scene graph but better captured in the chart image.\nWe train three image-guided chart captioning models: image, image-scene-graph, and image-data-table. All models leverage the vision-language transformer model VL-T5 (Cho et al., 2021) . VL-T5 is pretrained on image captioning and visual grounding tasks and was successfully applied to machine translation, making it suitable for chart captioning. We extract visual features for each VisText chart image using a Bottom-Up Feature Extractor (Anderson et al., 2018) . To explore the value of images to chart captioning, our image model only takes in the image features, while image-scene-graph and image-data-table concatenate the image features with the chart's textual representations (scene graph or data table).\n\nSemantic Prefix-Tuning\nIn real-world chart captioning settings, users want to vary the level of semantic content in their captions. For instance, while some blind users want verbose captions that describe the chart visuals, sighted users may only want captions that help them expose data trends (Lundgard and Satyanarayan, 2022) . To develop models capable of such customization, we leverage prefix-tuning strategies alongside VisText's semantic caption breakdown. Prefix-tuning specifies a task alongside the input, permitting a single large language model to perform many different tasks. In our setting, we use prefix-tuning to specify the level of semantic content to include in the caption (Li and Liang, 2021) .\nWe train each of our models with and without semantic prefix-tuning. With semantic prefix-tuning, we treat chart captioning as a multi-task fine-tuning problem, where the model is trained to generate the L1 and L2/L3 captions separately. In every epoch, the model sees each VisText chart twice, once with the L1 prefix and caption and once with the L2/L3 prefix and caption.\n\nEvaluation and Results\nTo evaluate the VisText dataset and our chart captioning models, we measure the readability and accuracy of generated captions and their similarity to the VisText target caption. We also qualitatively analyze the descriptiveness of generated L2/L3 captions and categorize common errors.\n\nQuantitative Model Performance\nWe evaluate the results of our text-based and imageguided captioning models with and without prefixtuning. We also compare to a current state-of-theart chart captioning model that uses data table chart representations and a T5 generation model (Kantharaj et al., 2022) . To measure the quality of output captions, we evaluate each model on machine translation and language generation metrics (Table 1 ).\nChart images do not support captioning. The image model performs the worst of all the chart captioning models. Its low perplexity and high error rates indicate it is highly confident in its inaccurate captions. While chart images contain the same information encoded in the chart's textual representations, it is presumably not adequately extracted by the model. Both the image model backbone (Cho et al., 2021) and the visual feature extractor (Anderson et al., 2018) are trained on natural images, making chart images out-of-distribution inputs that are likely to be poorly represented by these vision models. As the chart captioning task grows, model backbones, architectures, and feature extractors could be customized to chart images, which may improve image-based chart captioning.\nAll models produce high quality L1 captions. In our chart captioning setting, relation generation (Wiseman et al., 2017) measures how often the chart title, axis names, and axis scales in the input appear in the caption. Every model (except image) achieves a similarly-high relation generation score, indicating that every model can generate detailed L1 captions.\nScene graphs perform as well as data tables. Models trained on scene graph representations achieve similar performance across the evaluative metrics to models trained on data tables. As scene graphs can be more easily extracted from web-based charts images, they may be the preferred representation for future chart captioning models.\nImage-guiding does not improve captioning. Our image-guided captioning models do not experience the significant increase in performance other image-guided translation tasks report. While in image-guided translation, images contain substantial additional information beyond the text, the image and textual representations in chart captioning often contain highly similar information. The small amount of additional information in images might benefit complex captioning tasks on multivariate charts or infographics; however, the current VisText captions rarely reference visual information not present in the scene graph or data table.\nPrefix-tuning is free. Adding semantic prefixtuning to our models does not significantly change their performance. Models trained with and without prefix-tuning are exposed to the same set of charts, so it is consistent that prefix-tuning would not impact the quality of output captions. Given prefix-tuned models are able to output L1, L2/L3, and L1+L2/L3 captions, prefix-tuning may be preferred if users require semantic customization.\n\nQualitative Caption Evaluation\nTo augment our quantitative evaluation, we qualitatively assess the descriptiveness and accuracy of the generated chart captions. Since L1 caption accuracy can be measured at scale via relation generation, we focus our evaluation on L2/L3 predictions.\nPrior analysis tasked annotators with comparing the accuracy, coherence, and fluency of generated captions compared to a target caption (Kantharaj et al., 2022) . Instead, our approach follows an inductive qualitative data analysis approach: iteratively analyzing captions in a \"bottom-up\" fashion to identify emergent patterns in how generated captions compare to the ground truth (Bingham and Witkowsky, 2021). We randomly sample 176 generated captions from the scene-graph model with prefix-tuning and break them into their independent L2 and L3 statements, resulting in 181 (48.27%) L2 statements and 194 (51.73%) L3 statements.\nApproximately half (241 / 512) of the L2 and L3 statements made in the generated captions are factually accurate. Moreover, many of the full sentences are written in a natural, human-like manner and generated captions frequently include both compound and complex sentences. On average, every generated caption has one L3 statement and zero to et al., 2022) . We evaluate each model using machine translation and text generation metrics, including BLEU (Papineni et al., 2002) , Perplexity, Relation Generation (RG) (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD) (Kusner et al., 2015) , and Translational Error Rate (TER) (Snover et al., 2006) . We report the mean and standard deviation of three independent models. Darker colors indicate better scores.\nInput PT BLEU \u2191 Perplexity \u2193 RG \u2191 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193 Kantharaj et\ntwo L2 statements. Often this takes the form of a L3 general trend statement (e.g., \"The median annual family income in Canada has increased from 2000 to 2018\") accompanied by an L2 minimum and maximum statement (\"The highest was in 2015 at 80k and the lowest was in 2000\"). For the remaining half of analyzed captions, we identified the following recurring types of errors:\nIdentity Errors. We identify 86 identity errors (22.93% of analyzed statements). An identity error occurs when an L2 or L3 statement incorrectly reports the independent variable for a given (often correctly identified) trend. For bar charts, this error means incorrectly reporting the categorical label associated with a bar (e.g., in Appendix Figure 5c : \"The most popular music activity is vinyl albums and vinyl singles\" should be \"The most popular music activity is tickets for festivals\"). For area and line charts, this error means incorrectly identifying the temporal point or range of the trend. With bar charts, in particular, we observed that the identities were often \"off-by-one\" (i.e., identifying a minimum or maximum value, but attributing it to the second-highest or second-lowest category).\nValue Errors. A value error occurs when the quantitative data value of a statement is incorrect.\nOf the captions we analyzed, 3.20% (12) of statements contained a value error. For instance, as shown in Appendix Figure 4c , for the caption \"The total gate revenue from sporting events worldwide by region from 2006 to 2015 has increased from around 15 billion dollars to around 15 billion dollars\", the value should be around 18 billion dollars.\nIf it is ambiguous whether an error is an Identity or Value Error, we classify it as the former.\nDirection Errors. A direction error occurs when the direction (which can be increasing, decreasing, or stable) of a trend in an L3 statement is incorrect. We uncovered 32 direction errors (8.53% of analyzed statements). For instance, in the caption \"The per capita consumption of sweet corn in the US has increased from 2000 to 2019\" (Appendix Figure 3c ), the trend is actually decreased. In most direction errors, the identity (i.e., temporal range) is correct.\nStability Errors. A stability error occurs when the magnitude of a direction or the variance in a trend is incorrect. This can often refer to how much a trend is increasing or decreasing, such as rapidly or slowly, as well as whether it's a steady change or highly-fluctuating change. Looking ahead, while accessibility remains a key domain that would benefit from automated chart captioning, and deploying automated chart captioning models into the field is an exciting prospect, we believe the most promising approach for future work lies in \"mixed-initiative\" (i.e., human + AI) chart authoring systems. In particular, as we describe in our Ethics Statement below, chart captioning models are currently prone to make a number of factual inaccuracies which can have severe harmful consequences. On the other hand, by integrating these models into chart authoring systems (e.g., Tableau, Charticulator, Data Illustrator, or Lyra), chart authors can intervene and make any necessary corrections. Indeed, such integration offers exciting opportunities to develop novel interactive methods for verifying generated captions. For instance, models like ours could generate an initial caption (or set of captions) based on the chart currently being authored; as the system has access to all three representations of the chart (the back-ing data table, chart image, and structured scene graph), it might automatically segment the caption into independent \"data segments\" and interactively link and map them to rows in the table or regions on the chart, akin to Kori (Latif et al., 2021) .\n\nLimitations\nComputational Constraints. Despite using modern GPUs, with large amounts of memory, we were forced to use the smallest-parameter variants of T5 and ByT5 as we encountered out-of-memory errors with the larger alternatives. More problematically, the quadratic relationship between sequence length and time/space complexity of transformer architectures (Vaswani et al., 2017) , especially when using byte-level sequences (Xue et al., 2022), has had a significant impact on our model performance.\nIn particular, to be computationally tractable, we were forced us to truncate our input and output sequences to, at most, 1,024 and 512 characters respectively (1,024 coming from the underlying ByT5 architecture (Xue et al., 2022)).\nThese character thresholds have likely had an outsized effect on scene-graph models. For instance, due to these character limits, we reduced scene graph sequences to only a minimal set of visual characteristics; VisText also includes the raw, unprocessed scene graphs which offer a richer source of information about the visual features that are important to how people decode charts (e.g., bounding boxes, color) but were unavailable to our models. Moreover, as Figure 2 shows, even with this reduced representation, the mean length of scene graph sequences is 948 characters (cf. 426 characters for data tables) with a wide distribution. Thus, despite scene-graph models achieving comparable performance to data-table models, the former saw a much smaller proportion of complete sequences as compared to the latter. This truncation step additionally negatively impacts charts with long titles or axis names -in such cases, we observed that the L2 or L3 caption would be altogether truncated before generation.\n\nChart Types and the Visualization Design Space.\nVisText is scoped to only univariate bar, area, and line charts. We chose to begin with these chart types informed by data visualization research that has focused on studying natural language descriptions of single-series line charts -a basic, but commonly occurring chart type that offers a compelling target of study as it most visibly surfaces any poten-tial trends in the data (Kim et al., 2021; Stokes et al., 2022) . Future work can now begin to consider more complex chart forms in a step-by-step manner. For instance, moving from univariate bar, area, and line charts to multivariate versions of these chart types (i.e., stacked bars and areas, grouped bars, and multi-series line charts). From there, work can also consider chart types that surface perceptual and cognitive phenomena in visually distinct ways (e.g., scatterplots, where trends appear as clusters of points; heatmaps, where color saturation often encodes a trend; or maps, where color or other layered elements such as symbols are used to represent data values). Finally, automated methods for captioning visualizations may eschew chart typologies altogether in favor of visualization grammars -by offering a more composable and combinatorial approach to the design space (Wilkinson, 2012) , learning over visualization grammars may offer a more robust approach to captioning highly customized or unique visual forms.\nFor each future work direction, we anticipate scene graph representations to prove more fruitful than the data table. As the complexity of the visualization increases, its relationship to the data table only grows more ambiguous; the scene graph, on the other hand, directly encodes the visual form and thus remains faithful to it. As a result, to support such future work, VisText provides the raw specifications used to produce our charts (via the Vega-Lite visualization grammar (Satyanarayan et al., 2016) ) as well as the raw, hierarchical scene graphs prior to our linearization and reduction step.\n\nEthics Statement\nThe Consequences of Incorrect Captions. Weidinger et al. ( 2021) comprehensively survey the risks associated with the large language models (LLMs) that underlie our contribution. Of the six categories of risk they identify, harms stemming from models producing factually incorrect statements are not only most pertinent to our work, but are likely heighted as compared to general uses of LLMs given the context we are addressing: automatically captioning charts. In particular, people most often consume charts and visualizations in order to make data-driven decisions (Keim et al., 2008) -for instance, about whether to evacuate ahead of a hurricane (Padilla et al., 2018) , or health & safety during the pandemic (Shneiderman, 2020) . Moreover, recent results have shown that readers not only fixate for longer and are more likely to recall the textual content of and around visualizations (Borkin et al., 2015) but this textual content can strongly influence the takeaway message readers leave with even when it is at odds with the depicted data (Kong et al., 2018 (Kong et al., , 2019)) . Finally, these issues are exacerbated by the persuasive and rhetorical force of data and charts (Kennedy et al., 2016; Hullman and Diakopoulos, 2011) , that often project a sense of authority and certainty (Correll, 2019) . As a result, readers may not think to double check the accuracy of chart captions, and inaccurate statements that models may produce could lead to harmful downstream decisions.\nTo proceed ethically with this line of research, we believe that advances in data and modeling need to be closely followed by attention devoted to mitigating the risks of incorrect statements. At base, automatically generated captions should be identified as such at the forefront to raise readers' awareness about the potential for incorrect statements. And, interactive visual linking strategies (such as those explored by Kong and Agrawala (2012) ; Kim et al. ( 2018)) could be deployed to help readers manually verify the constituent statements of a caption against the chart. These strategies, however, place the burden of harm mitigation on readers. Thus, an alternate approach might never surface automatically generated captions to readers directly but instead use them as part of mixed-initiative systems for jointly authoring visualization and text, such as Kori (Latif et al., 2021) . In such systems, automated chart captioning models would help to accelerate the authoring process -combatting the blank slate problem by providing an initial summary of the chart -and chart authors would make any necessary corrections prior to publication.\nBesides these human-computer interaction (HCI) approaches for mitigating harm, an equally important direction for future work should leverage interpretability techniques to more deeply study what the models are learning. To what degree are chart captioning models stochastic parrots (Bender et al., 2021) , and how much do they understand the information charts depict? Automated Captioning for Accessibility. Although accessibility is a guiding motivation for the bulk of work in automated captioning (be it image captioning or, as in our case, chart captioning), studies find mixed reactions, at best, about these approaches among people with disabilities (PWDs).\nFor instance, accessibility educator and researcher Chancey Fleet described Facebook's automatic image descriptions as \"famously useless in the Blind community\" despite \"garner[ing] a ton of glowing reviews from mainstream outlets\" (Fleet, 2021; Hanley et al., 2021) . This disconnect appears to stem from a more fundamental mismatch between what PWDs describe as their captioning needs, and what the research community -particularly through its automatic, quantitative evaluationsprioritizes (Jandrey et al., 2021) . In particular, surveys with PWDs repeatedly surface the contextual nature of captions. Bennett et al. (2021) find that the context of use shapes the degree to which PWD are comfortable with captions describing people's race, gender, and disabilities -for instance, changing their preferences if they were in a white, cisgender, nondisabled, and professional company versus their own community. Similarly, Jung et al. (2022) find shifting preferences for the content image descriptions should convey across different photo activites -for example, when viewing or taking photos, participants wished for descriptions that conveyed spatial cues whereas when searching or reminiscing about photos, participants hoped for descriptions to connect to personal data or differentiating details.\nIn contrast, quantitative metrics of model performance compare generated captions to a single \"ground truth\" caption. This framing of success not only makes it difficult to develop contextuallyvarying caption generation but can actively penalize such investigations. For instance, with our work, we explored how prefix-tuning can be used to develop models that are responsive to users' preferences about semantic content. However, as described in Sec. 5.1, existing quantitative metrics of model performance (e.g., BLEU, ROUGE, WMD, and TER) show a drop in model performance despite our qualitative analysis indicating that these captions are indeed high quality.\nFinally, our exploration of semantic prefixtuning represents only a very preliminary step towards addressing the contextual captioning needs of PWDs. In particular, the semantic labels Vis-Text assigns to captions were derived from prior work (Lundgard and Satyanarayan, 2022) that only explored natural language descriptions when consuming presentations of visualizations -one task from a broader palette (Brehmer and Munzner, 2013) . Future work might instead extend the Vis-Text dataset -and corresponding models -to consider captions for a broader range of tasks including consuming visualizations for scientific discovery, enjoyment or, producing, searching, or querying visualizations (Brehmer and Munzner, 2013) . \n\nModel Generated L1 Caption\nAverage spending per consumer on selected music activities in the United States as of July 2018 is a bar graph. The x-axis measures Response while the y-axis measures $40 to $99.99.\n\nModel Generated L2/L3 Caption\nThe most popular music activity is vinyl albums and vinyl singles.\nThe least popular music activity is vinyl albums. (b) Model results using the L2/L3 captions.\nTable 2 : We separately evaluate our L1 and L2L3 captions on all the same metrics except for Relation Generation.\nIn general, we observe that L1 captions perform better than the L2/L3 captions. Our models generate verbose L1 captions that are similar to the structure of our L1 templates, while the L2/L3 captions are human-generated and contain more variability. Darker colors indicate better scores.\n\nB Additional Evaluations\nB.1 Independent L1 and L2/L3 Caption Evaluation\nTo better understand how our models generate varying levels of semantic content, we separately evaluate our prefix-tuned models on L1 captioning and L2/L3 captioning tasks. Each prefix-tuned model can output an L1 or an L2/L3 caption for each chart. We evaluate these captions to their respective L1 or L2/L3 ground truth captions and report the results in Table 2 . Since we compute Relation Generation using only the L1 chart fields (e.g., chart title, axis scale, etc.), we do not report the results separately for L1 versus L2/L3 captioning. There is no direct Relation Generation analog for L2/L3 captions, since they are human-generated and do not follow a specific template. The Relation Generation for L1 captions is identical to the Relation Generation for L1/L2/L3 captions reported in Table 1 .\n\nB.2 Evaluation Details\nQuantitative Model Performance Metrics. We evaluate our models using NLP and machine translation metrics, including BLUE (Papineni et al., 2002; Lin and Och, 2004) , Perplexity, Relation Generation (Wiseman et al., 2017) , ROUGE (Lin, 2004) , Word Mover's Distance (WMD), and Translation Edit Rate (TER) (Snover et al., 2006; Post, 2018) . We implement Relation Generation per Wiseman et al. (2017) , use the Gensim implementation of WMD, and use the Hugging Face implementation (Wolf et al., 2019) for the remaining metrics.\n\u2022 BLEU: BLEU requires several gold standard references. In our evaluation setup, we use the test set caption as a single reference.\n\u2022 Perplexity: We use a pretrained GPT-2 Medium model to compute Perplexity.\n\u2022 Relation Generation: The fields we evaluate on are the chart title, axis names, and axis scales (if any).\n\u2022 Translation Edit Rate (TER): Edits consist of deletions, additions, and substitutions, as present in SacreBLEU.\nQualitative Caption Evaluation. To produce our qualitative evaluation results (Sec. 5.2), we iteratively evaluated randomly sampled captions until there was no more marginal information about they types of errors to be gained from evaluating more captions. For each L2/L3 caption, we assess the number of independent, mutually-exclusive L2 and L3 claims/statements that are being made. In comparison to evaluating at a sentence-level, this allows us to take a more nuanced approach that isn't limited by where the model has generated a full-stop. This approach allows us to more-accurately evaluate factual precision without overly-penalizing for a single mistake. An example might take the form of \"The lowest value is X (claim 1), the highest value is Y (claim 2), and the second highest is Z (claim 3). Overall, it is increasing over time (claim 4).\" We observe that the first sentence is a compound sentence that consists of three independent clauses, each with a single factual L2 claim, while second sentence is a single factual L3 claim. Let us assume that claim 1 was factually incorrect. If we evaluate at a sentence-level, then the entire first sentence comprising of claim 1, claim 2, and claim 3 would be incorrect. However, by breaking this caption into independent, mutually-exclusive claims, we can more precisely calculate the factual precision of our text generation. \n\u2193 ROUGE-1 \u2191 ROUGE-2 \u2191 ROUGE-L \u2191 ROUGE-L SUM \u2191 WMD \u2193 TER \u2193\nTransformer Backbone T5-small L2/L3 0.06 \u00b1 2.67e\u22123 35.81 \u00b1 4.13e+0 0.25 \u00b1 6.43e\u22123 0.09 \u00b1 3.43e\u22123 0.22 \u00b1 5.73e\u22123 0.22 \u00b1 5.60e\u22123 0.99 \u00b1 8.70e\u22123 113.33 \u00b1 2.94e+0 Ours (ByT5-small) L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\nL1 Generation new-seed L2/L3 0.08 \u00b1 5.93e\u22123 20.96 \u00b1 2.71e+0 0.29 \u00b1 5.77e\u22123 0.11 \u00b1 2.33e\u22123 0.25 \u00b1 5.30e\u22123 0.25 \u00b1 5.27e\u22123 0.91 \u00b1 1.83e\u22123 116.36 \u00b1 1.11e+1 original-seed L2/L3 0.07 \u00b1 8.07e\u22123 18.81 \u00b1 3.74e+0 0.28 \u00b1 1.65e\u22122 0.11 \u00b1 9.43e\u22123 0.25 \u00b1 1.02e\u22122 0.244 \u00b1 1.02e\u22122 0.92 \u00b1 8.90e\u22123 120.62 \u00b1 6.72e+0\n(c) Ablation study results using the L2/L3 captions. \n\nC Ablation Studies\nTo evaluate our modeling and dataset design choices, we run ablation studies measuring the impact of our transformer model backbones and stochastic data generation pipeline. We report the results in Table 3 .\nTransformer Backbone. To understand the impact of our token-free, byte-to-byte architecture ByT5 model backbone, we explore other large language models. Specifically, we compare our 300M parameter ByT5-small model (Xue et al., 2022) with a 60M parameter T5-small (Raffel et al., 2020) and 140M parameter BART-base model (Lewis et al., 2020) . We also apply prefix-tuning to the ByT5 and T5 models. We cannot apply prefix-uning to BART because BART does not support multi-task learning. Quantitatively, using ByT5 does not appear to significantly improve upon T5. However, we theorize that ByT5's token-free paradigm increases the input sequence length by compressing more input text into fewer input tokens.\nL1 Caption Generation. Since we generate L1 captions stochastically, we evaluate whether our initial randomization impacted the model's results. We compare generate a second set of L1 captions using a different random seed. We see the results are nearly identical across all metrics, indicating our dataset captures a diverse set of L1 captions. We estimate that we trained each model between 5 to 10 times to achieved our final results.\n\nD.3 Ablation Models\nWe train our ablation models using the same parameters as our default models, only varying the parameter of interest. We train them on 16 virtual CPU cores on Xeon E5 hypervisors with 128GB of memory and PCI pass-through access to eight NVidia Titan XP GPUs with 12GB of memory.\n\nD.4 Notable Package Versions\nPackage versions are listed in Table 5 .\n\nE Additional VisText Dataset Details E.1 Licensing\nOur use of the raw Statista data from Kantharaj et al. ( 2022) is consistent with its intended use case. The data was licensed under the GNU General Public License v3.0. We release our data and code under GNU General Public License v3.0.\n\nE.2 L1 Caption Generation Process\nThe Level 1 captions are generated from a random process that chooses from 3 title templates and 6 axis templates. The title templates we use are:\n\u2022 This is a [chart-type] titled [chart-title]\n\u2022 This [chart-type] is titled [chart-title]\n\u2022 [chart-title] is a [chart-type]\nThe axis templates we use for each axis are:\n\u2022 For each axis template, we randomly choose whether to include the axis scale. Furthermore, within each template, we further randomly swap words with synonyms. A list of words and their possible synonym substitutions are:\n\u2022 this: here, a\n\u2022 chart: graph, diagram, plot\n\u2022 titled: called, named, labeled\n\u2022 on: along\n\u2022 plotted: defined, measured, drawn, shown\n\u2022 plots: measures, shows\n\u2022 with: using, on, along, as\n\u2022 found: seen\n\u2022 labeled: marked\n\nE.3 Crowdsourced Study Protocol\nFigures 6-10 screenshot the introduction, eligibility and consent statements, instructions, and a task from our crowdsourced study. We recruited participants on the Prolific.co crowdsourcing platform, following conventions in the data visualization research community 3 and recent research results (Tang et al., 2022) that suggest Prolific yields higher quality results than Amazon Mechanical Turk. We conducted multiple pilot runs to calibrate the amount of time it would take participants to complete the study, and found that most participants were able to successfully do so within 14 minutes. Following Silberman et al. (2018) , who advocate for paying workers at least minimum wage at your location, we choose to pay our participants $3.25 -a roughly $14/hour rate in line with the $14.25/hour minimum wage in Massachusetts at the time the study was conducted.\nOur study was determined to be exempt by MIT's institutional review board (IRB). Participants had to explicitly provide their consent in order to proceed with the study -if participants did not consent, they were redirected back to the Prolific platform. The consent statement (Fig. 8 ) reminded participants of their rights (including that their participation is voluntary and consent could be revoked at any time), and encouraged participants to contact either the study PI or IRB board directly should they have any concerns. We constrained our participant pool (and eligibility requirements) to people living within the United States or United Kingdom who self-reported as being sighted with no vision or color impairments. We did not collect any additional demographic data from participants as we did not determine this to bias or otherwise affect the content we hoped to collect. Each task (an example of which is shown in Fig. 10 ) included an attention check where participants were asked to correctly identify the chart type shown. If participants failed more than two attention checks, their submission was flagged for manual review -in practice, the bulk of participants who failed attention checks nevertheless produced valid captions and, thus, were paid fully. The task asked participants to complete a free response question to describe as completely as they could the trends and patterns observed, emphasizing that their response would be evaluated for correctness and completeness. Despite best practices suggesting a more structured, querying approach (called QID) can yield higher quality captions (Morash et al., 2015) , we opted for our free-response approach as the benefits of QID (namely, in expressing the chart type, title, and axes units) would already be captured by our synthetically generated L1 captions. Moreover, in contrast to the templatized output produced by QID, we hoped that our free-response responses would yield more \"natural\" articulations of perceptual and cognitive trends, following the Lundgard and Satyanarayan (2022) framework.\n\n\nhttps://github.com/mitvis/vistext 2 https://github.com/j-min/VL-T5\n"}
{"question": "How do models trained on a combination of synthetic and curated data perform in Icelandic error correction task?", "evidence": "  The results for these models on the curated in-domain test sets are in fact mostly on par with the models finetuned on the synthetic data only We conclude that adopting a byte-level approach rather than a subword approach leads to best results for the task of GEC, at the very least in the case of a morphologically rich language such as Icelandic ", "options": ["A. Perform the best", "B. Perform well", "C. Perform average", "D. Perform poorly "], "answer": "B", "content": "\nIntroduction\nSpelling mistakes due to typos and rushed writing, nonstandard punctuation and spelling, and grammatical and stylistic issues are common to almost everyone who writes any kind of text. This applies in any language and can distract the reader or make the communication miss its mark. This can hinder people who have difficulties writing text conforming to a particular language standard, be it due to disability, dyslexia, linguistic background, limited access to education or any other reason. Prejudice against people whose writing deviates from the standard can make some shy away from communicating with others, leaving their voices out of important discussions and restricting their opportunities (Alexander-Passe, 2015) .\nGrammatical error correction (GEC) is the task of adjusting a text's spelling, grammar, and linguistic style to conform to an approved language standard or convention (Rauf et al., 2017) . While the latest work on GEC is based on Transformer models (Vaswani et al., 2017) , the subword tokenization methods commonly used in these models are a source of problems when it comes to typos and other variants (Schmaltz et al., 2017) . Subword tokenization (Sennrich et al., 2016; Kudo, 2018) was presented as a solution to the open vocabulary problem, as it is a compromise between character encoding and whole word encoding, enabling unknown words to be represented using known subwords.\nHowever, a significant downside of subword tokenization is how it is affected by noisy input; if a word contains a typo or other spelling variants, this can completely shift its representation. In addition, in languages with rich morphology, a word can have many different surface forms, some rarer than others, that all carry the meaning of the base word, but appear in different syntactic contexts. A subword-tokenized model may struggle to capture the nuances of such a language effectively since it may need several different subwords to represent a single word, depending on spelling and context. When an unfamiliar variant of the word appears in unseen text, the model is challenged to decode it correctly, even when it results in uncommon subword units.\nOur motivation is that a byte or character-level approach should intuitively be more robust to spelling or morphology variations, as it is not constrained by the subword vocabulary. We explore using a byte-level architecture, ByT5 (Xue et al., 2022) , for correcting everything from typos to com-Figure 1 : Overview of training data and comparison of output. The Icelandic-English mBART-ENIS model and the multilingual ByT5 and T5 models are first trained on generated parallel error corpora before being adapted on curated (collected) true error corpora in Icelandic. The final models are compared on an error correction (EC) task in Icelandic. The example demonstrates how the byte-level model performs well while the subword model cannot see the individual characters in every word, leading to degraded performance. plex grammatical issues in text. The language studied is Icelandic, a highly-inflected North Germanic language. For instance, the morphological complexity in Icelandic means that nouns can have up to 16 different surface forms, and adjectives over 50. GEC for a morphologically complex language needs to go beyond correcting only single words or limited phrases; it needs to consider the syntax of the whole sentence. This is the case for Icelandic but also for other languages with rich morphology, such as Arabic, Hebrew, Polish, Basque, Lithuanian and Hungarian, to name a few.\nWe compare the performance of the byte-level architecture to two subword-based architectures; ByT5's predecessor, mT5 (Xue et al., 2021) , and an mBART (Liu et al., 2020) model that has been pretrained further on both Icelandic and English. We employ real and synthetic error data for training, and present models and a framework for error generation methods that can be adapted to other languages. For under-resourced languages such as Icelandic (R\u00f6gnvaldsson, 2022) , using synthetic training data makes neural training for GEC a viable option.\nOur main contributions include a comparison between subword tokenization and byte-level tokenization for GEC when training over a combination of curated and synthesized data. We demonstrate how byte-level models not only bypass subword-related issues, but can also correct long-range errors in text. We release our error generation framework as well as models for GEC using byte-level and subword tokens in Icelandic. While our work focuses on the Icelandic language, we have no reason to believe that similar results do not hold for other languages, particularly those similar to Icelandic in terms of morphological complexity.\n\nRelated work\nThe bulk of research on grammatical error detection and correction has been focused on English and English learner texts, due to existing training data and benchmarks, and the large market of English learners worldwide who benefit from an automatic language correction tool (N\u00e1plava and Straka, 2019) . While spelling and grammar errors appear in every language, each language has its own set of error types that are more common than others, due to different phonetic, morphological and syntactic characteristics.\n\nSynthetic data generation for GEC\nThe problem of data scarcity in GEC, when approached as a sequence-to-sequence task, is typically addressed with synthetic data generation (Stahlberg and Kumar, 2021) . One approach to cre-ating ungrammatical sentences uses random character noise and simple rules to manipulate the text. Another approach is using a spell checker in reverse to noise text (Grundkiewicz and Junczys-Dowmunt, 2019) . In contrast, others have used probabilities derived from an annotated corpus of naturally occurring errors to corrupt text (Felice and Yuan, 2014) . Recent efforts widely employ neural networks to create synthetic errors (Stahlberg and Kumar, 2021) ; many use methods derived from machine translation (Junczys-Dowmunt et al., 2018) , for example, by creating worse text using deliberately bad translation models (Xie et al., 2018; Zhou et al., 2020) or roundtrip translations between languages (Lichtarge et al., 2019) . Yet another option is to leverage available resources with edits, such as Wikipedia edit histories, to generate corrupted corpora (Grundkiewicz and Junczys-Dowmunt, 2014) .\n\nSequence segmentation for GEC\nGEC can essentially be considered the task of generating grammatical target text from an ungrammatical source, similar to machine translation. The idea of approaching GEC as a machine translation problem dates back to 2006 (Brockett et al., 2006) , and this approach has since become the most prevalent method of GEC, with the focus shifting from statistical machine translation (SMT) to neural methods as they developed (Yuan and Briscoe, 2016; Ji et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018) . However, phrase-based SMT continued to be the state-of-the-art for GEC for longer than in the field of interlingual neural machine translation (NMT) (Junczys-Dowmunt et al., 2018) . This is partly because of the data scarcity problem and partly because of the tokenization methods typically used in transformer models. Breaking words up into subword units decreases the vocabulary size while addressing the out-of-vocabulary problem (Sennrich et al., 2016) . A prominent drawback of this approach is that the fixed subword vocabulary makes the models sensitive to noise in the text (Tay et al., 2021; Eger and Benz, 2020) .\nIn a subword-based GEC model, when a word contains a typo or is spelled unconventionally, it may look like an unknown word, for which no known representation exists. The model may then segment the word differently from what was seen during training, causing mispredictions. If the subword representation for \"different\" is [_diff, er, ent] , but the word is misspelled as \"diffirent\", its subwords might be [_diffi, ren, t], and a subword-based GEC model might correct the typo by outputting a different word, [_diffi, cul, t] . This issue is highlighted in Figure 1 , also showing how a byte-based approach is not limited by this issue.\nThis is also true for unseen words that are correctly spelled, such as foreign-named entities, which can lead to the subword-based GEC model \"correcting\" a perfectly spelled word it has not seen before, by replacing it with the most likely candidate. In the sentence \"The tournament was held in Espoo, Finland.\", the place name \"Espoo\" may be represented by a single subword token Espoo. Since this token is unfamiliar, the model finds the most likely subword token for this particular sentence, Helsinki. 1 This changes the semantics of the sentence and can introduce serious errors. The result is a grammatically correct and meaningful sentence, but the semantics have drifted away from the original text.\nDue to this known shortcoming of subword tokenization (Schmaltz et al., 2017) , efforts have been made to design architectures where the characters or the underlying bytes are used directly as input tokens. Byte and character-level models inevitably result in much longer sequences than subword models, making them more costly to train, and slower in inference. Some truly token-free general-purpose architectures that are increasingly competitive to token-based models have emerged recently, including CANINE (Clark et al., 2022 ) (character-level), PIXEL (Rust et al., 2023 ) (text-to-image), and ByT5 (Xue et al., 2022) (byte-level) . The training of ByT5 is based on the subword-based multilingual mT5 (Xue et al., 2021) approach, but in comparison, the model is equipped with a heavier encoder (three times the depth of the decoder). Compared to mT5, ByT5 is more robust to noisy input, but inference is slower (1.5 to 2.6 times slower on average on a transliteration task, and up to 9 times longer on tasks with longer input sequences) (Xue et al., 2022) . Another approach to making subword models more robust is using subword regularization to produce multiple segmentations of the same word (Kudo, 2018; Provilkov et al., 2020) . This is commonly used for addressing the open vocabulary problem and noisy data, such as ungram-matical text.\nDespite the reported advantage of character or byte-based Transformer models on noisy text (Libovick\u00fd et al., 2022) , work using this approach to GEC is not very common. A notable exception is work for GEC in the Lithuanian language, where ByT5 has been used for diacritics restoration (Stankevi\u010dius et al., 2022) and limited GEC (Stankevi\u010dius and Luko\u0161evi\u010dius, 2022) . They generate synthetic data by applying noise (common typographical errors, swapping letters for similar sounding ones, other non-grammatical word-level noise) to a crawled and filtered corpus and compare results when training with T5 and ByT5. Their findings agree with ours that the byte-level model outperformed the subword model.\nOur work deviates from that of Stankevi\u010dius et al. (2022) in the following key ways: We (a) generate more sophisticated and realistic errors using grammatical information from part-of-speech (PoS) tags and using custom rules based on empirical findings; (b) combine generated error data with true error corpora from a wide range of demographic sources; and (c) explore in detail which error and text types benefit the most from finetuning on true error corpora, as opposed to training on synthetic data. As far as we are aware, no attempt at bytelevel Transformer-based GEC, trained on synthetic and real error corpora, has been published. Concurrent work using ByT5 for Icelandic is (Jasonarson et al., 2023) , where errors in the OCR output of historical Icelandic texts are corrected using a generated corpus of errors extracted from real data.\nCurrent state-of-the-art in GEC is based on sequence-tagging methods (Omelianchuk et al., 2020) , which instead of generating whole sequences, tag the erroneous sentence with its corrections, and on sequence-to-sequence methods, as has been described. Further work has explored automatic character transformations for GEC tagging (Straka et al., 2021) to better handle character-level errors. One of the current highest-scoring models on English GEC benchmarks is gT5 (Rothe et al., 2021) , which is based on the mT5 model.\n\nPrior work for Icelandic\nApart from some rule-based spell checkers that don't make use of the full context, one rule-based correction system exists for Icelandic, based around parse trees, GreynirCorrect (\u00d3lad\u00f3ttir et al., 2022) . This system is contingent on the sentence parsing according to a pre-defined context-free grammar, and can only handle issues that fit pre-defined rules. This setup is both a strength and a weakness as the system is highly configurable and capable of many things, such as detecting syntactic inconsistencies and errors, and can give the user useful information on the errors found. Still, when a text has many errors, complexity builds up and rules can start interfering. Sentences containing many issues, such as from users with dyslexia, generally have lower accuracy using this method.\nThe work presented here is the first where neural networks are used in GEC for Icelandic. Snaebjarnarson et al. (2022) use neural methods for detecting such errors, but not for correcting them.\n\nCurated dataset\nA single collection of parallel error corpora exists for Icelandic, the Icelandic Error Corpus (IceEC). The corpora are annotated and corrected by language experts (Arnard\u00f3ttir et al., 2021) . The dataset is highly granular in its categories, containing hundreds of labels, but with a limited number of highlevel groups (coherence, grammar, orthography, style and vocabulary).\nThe IceEC is split into a larger general corpus and three specialized corpora (Arnard\u00f3ttir et al., 2022) . The general one contains 58,239 sentences of student essays, online news texts and Wikipedia articles. This corpus is annotated with around 50k errors of different categories. The three specialized corpora are much smaller and contain texts from Icelandic language learners (6270 sentences), dyslexic native speakers (1362 sentences), and children (2070 sentences), volunteered by the users themselves. These smaller corpora contain more errors per sentence than the general one, and add diversity to the training data.\nThis curated error data was used for finetuning our models, and combined into one training dataset for a total of 64k input sequences (single sentences), after setting aside validation and test data. The general IceEC also includes a 5.3k sentence test set used for evaluation.\n\nSynthetic dataset\nWe applied a diverse set of methods for error generation, both using linguistic knowledge and random noising methods. This rule-based approach to synthetic data generation gave us control over the types of noise applied, and allowed us to generate evaluation data for each error type.\nAs our basis of correct text to be noised, we used the Icelandic Gigaword Corpus (IGC) (Steingr\u00edmsson et al., 2018) , a collection of Icelandic editorial texts. These are mostly news articles, published literature and legal texts. We selected from this corpus those text sources that are the most likely to have been reviewed as part of the editorial process of each publication/source (literature, journals, news, laws, adjudications and transcribed parliamentary speeches). Some of these texts still have their share of typos and other errors and inconsistencies, especially the news articles, which were deemed important training data because of their general vocabulary, not found in the more formal text sources. As a preprocessing step, we filtered out lower-quality and irrelevant sentences, by removing sentences containing mostly foreign texts, illegal characters and words with known misspellings, sourced from lists of common misspellings. The corpus was tokenized using the Greynir Tokenizer (\u00deorsteinsson et al., 2019) and PoS tagged using the GreynirSeq tagger (Snaebjarnarson et al., 2022) .\nWe generated three categories of errors: 1) noise within words; 2) noise at the sequence level; and 3) grammatical and morphological modifications. The first two resemble those used when noising backtranslation data (Edunov et al., 2018) . The third type is based on using available tools and linguistic knowledge to create errors that are unlikely to be formed randomly, but resemble those of human writers.\nIn order to explore to what extent subword and byte-level models can learn and generalize grammatically complex issues in a morphologically rich language, we go beyond naive language-agnostic noising of text. A more detailed explanation of the Icelandic-specific noise is given in Appendix C. The noise methods are shown in Table 1 . This is by no means a finite list of linguistic variants or errors found in Icelandic texts, but constitutes examples chosen for studying the model performance on these more grammatically complex challenges.\nThe error generator allows for noising levels to be configured via hyperparameters. Experiments with different noise ratios in the synthetic data showed that highly noised text provided the best training examples, without the models learning to \"overcorrect\", i.e., to introduce false positives. Instead of producing even more synthetic data, we geared up the noise to produce highly error-dense examples, setting the random and more naive error noise to appear in 80% of cases, and the rule-based error noise to be used wherever possible. 2 Examples of parallel sentences with and without synthetic errors are shown in Table 2 . A total of 35M synthetic parallel sentences were generated using these methods. 2000/4000 lines were set aside for validation/testing, respectively, and special evaluation sets were generated for each error type (see Section 4).\n\nModels\nWe compared three model architectures to evaluate the differences between using subword tokenization and a byte-level method. Comparing models with different architectures calls for defining which factors are compared. In particular, byte sequences are longer than subword sequences when counting the number of tokens, roughly 4 times longer on average in the original multilingual mT5/ByT5 training data (Xue et al., 2021) . And as Xue et al. (2022) note, the mT5 architecture tends to need less training time than ByT5. We compared the models after an equal amount of training samples (100k updates). We also continued the training of the ByT5 model, using more than five times the number of updates.\n\nmBART\nWe continued training of the pretrained multilingual BART25 model (mBART) (Liu et al., 2020) , using the original pretraining objective on texts in Icelandic and English. The training of the model is detailed in Appendix A. This model, mBART-ENIS, was then finetuned on the synthetic error data, to teach it to correct errors in Icelandic texts.\nTraining on the synthetic error data was performed with an effective batch size of 3000 input tokens (roughly 60 sequences), a learning rate of 3e-5 with an inverse square root scheduler, 0.1 dropout, 0.3 attention dropout, 1000 warmup steps, 0.1 label smoothing, no weight decay, and using the Adam optimizer, for 100k updates on an A100 card for a day. 3 In addition to the above experiments, we conducted separate experiments using segmentation regularization (Kudo, 2018) to introduce more noise to the training examples and explore alternative measures to mitigate the subword tokenization problem. The BART architecture uses unigram subword units; we applied subword regularization with \u03b1 = 0.7 and keep all other parameters unchanged.\n\nByT5\nFor the byte-level approach we employed the ByT5base model (Xue et al., 2022) , which is based on the multilingual T5 model (Raffel et al., 2020) , but operates on bytes instead of subwords. The ByT5 model is pretrained on over 100 languages, but has only seen a limited amount of Icelandic. The mC4 dataset which is used to train ByT5 and mT5 is also lacking in quality for low-resource languages, in particular for Icelandic, as shown by Snaebjarnarson et al. (2022) .\nThe pretraining task in ByT5 has been adapted\nto a byte-level model, with span infilling based on bytes, not subwords. Apart from this, the main difference between the mT5 and ByT5 model architecture is the heavier encoder of ByT5.\nSequences in byte-level architectures are long and correspond more or less to the number of characters in Icelandic, resulting in increased training time. We trained the ByT5-base model using a maximum sequence length of 512 bytes, which was found to be a reasonable compromise, as most sentences in Icelandic texts are shorter than this.\nThe ByT5-base model was finetuned on the synthetic data with an effective batch size of 32 sequences (sentences). The learning rate was set to 2e-5 using the Adam optimizer with 1000 warmup steps and no weight decay. This model was further trained for a total of 550k updates, or 13 A100 card days. 4\n\nmT5\nFor a more direct comparison of byte-level and subword-level models, we also finetuned the mT5base (Xue et al., 2021) model on the same data, with the notable difference to the mBART model that it was not further trained on Icelandic. The mT5 models were pretrained on the same data as ByT5 and have a similar architecture, as described above, and are thus as comparable as subword and byte-level models can be. As previously mentioned, mT5-base is the base for the state-of-the-art gT5 (Rothe et al., 2021) model for multilingual GEC.\nWe finetuned the mT5-base model on the synthetic data using the same parameters as in our ByT5-base finetuning and evaluated it at 100k updates. 5 \n\nFinetuning on curated corpora\nUsing the curated error corpora (IceEC), we finetuned the byte-level and subword-level models to convergence. For the mBART model, this meant training with a learning rate of 2e-6 for 53k updates (67 epochs), with attention dropout set to 0.1, weight decay to 0.001 and other parameters being the same as during the synthetic finetuning.\nThe ByT5 and mT5 models were finetuned with a learning rate of 2e-6, other parameters were the same as during finetuning on the synthetic data. The ByT5 model had converged at 120k updates (60 epochs), while the mT5 was still improving on the validation data at 200k updates (100 epochs), but with time we found it forgot too much of the synthetic error correction task. We report evaluation scores at 130k.\nFor comparison, we also finetuned the different models (mBART-ENIS, mT5 and ByT5-base) on the IceEC data only, without the synthetic finetuning phase. This was done to examine how much the models learn from the added synthetic examples, and how far we can get using a small amount of hand-corrected examples. The mT5 and ByT5 models were trained for 100k updates and the mBART-ENIS model for 10k updates.\n\nResults\nDifferent metrics exist for evaluating GEC performance, but most are language-specific, and have not been adapted to Icelandic. Here we employ a language agnostic metric for scoring our models, the GLEU score (Napoles et al., 2015 (Napoles et al., , 2016)) . GLEU 6 is a variant of the BLEU (Papineni et al., 2002) score used to evaluate machine-translation output. It has been modified to account for both the source and the reference, by rewarding overlap between the source and the target sentence, and penalizing n-grams that should have been changed, but were not.\nWhen evaluating GEC for English, ERRANT (Bryant et al., 2017 ) is commonly used. It is a spanbased correction method that uses the F 0.5 metric, where precision weighs twice as much as recall. Though this metric has not been implemented for Icelandic, we also report ERRANT scores using a language-agnostic approach, disregarding error types and only reporting the span-based F 0.5 scores for each test set. These results are shown in Table 5 in Appendix E; they align well with the GLEU results in Table 3 which are described below.\nWe consider a variety of curated and synthetic test sets to get a good overview of the differences between the byte-level and subword-level approach for GEC. For the real errors, we report scores over the IceEC.test set, the test set from the IceEC, which contains around 5000 sentences. In con-trast, the dyslexic, L2 and children test sets contain 500 held-out sentences each from the respective specialized error corpora described in section 3.1 (only 100 examples were collected for the dativitis error type, a rarer occurrence in the data). We also annotated a small dataset (163 sentences) of data from an Icelandic news outlet (news), where each sentence contains at least one error; this is further described in Appendix B.\nFor the synthetic errors, we report GLEU scores over the test.synth set, which contains around 4000 held-out sentences from the synthetic data. Furthermore, we generated test sets of synthetic examples, each containing a particular error type in each sentence (dativitis, spaces, commas, dupl-words, mood, rand-noise, noun-case). This last group of test sets was generated using source texts that, while editorial, may include other errors, just like the synthetic training examples. The models, as they get better, learn to correct these errors as well. This may paradoxically lower the GLEU score as the corrected output deviates from the erroneous reference. These generated test sets still provide valuable information about what the models learn about each error type in isolation.\nTo understand what approach is best suited for GEC we trained the models on different data combinations and using different pre-trained models. The Synth-100k models are all trained for 100k updates on the same synthetic data, and the Synth-100k-EC models are additionally finetuned on the curated IceEC error corpus. To provide a baseline for the GLEU scores, we also report no_corr scores, where the source text is not corrected. This gives some idea of the noise level of the test sets, with test.synth being the noisiest and IceEC.test containing the least noise.\nThe GreynirCorrect (\u00d3lad\u00f3ttir et al., 2022) (GC) results were produced by applying GreynirCorrect to the test sets in its two configurations; correcting single-word errors on the one hand (spell.) and all errors found on the other (all). The GC system was developed in part to focus on the error categories and error frequencies defined in the IceEC training data, and this may be reflected in the scores for the IceEC test sets (asterisked in Table 3 ). The news test set ( \u2020) was created using only sentences flagged by GC (see Appendix B) and is therefore heavily biased towards that system. Models trained only on the synthetic data (Synth 100k) generally perform best on the synthetic er- ror corpora and thus solve the more simple and systematic spelling errors such as mistakes in punctuation, missing white space and word duplication. Their performance on the curated error corpora is somewhat lacking. In contrast, models trained only on the curated error corpora (EC) generally produce somewhat better GLEU scores on the curated error corpora than the Synth 100k models, but do not generalize to the error categories presented in the synthetic test sets. They are also unable to correct multiple errors in a single sentence (test.synth) .\nTraining on the synthetic data and then finetuning on the curated error corpora (Synth100k/ Synth550k+EC) performs best on the curated errors and retains much of the performance on the synthetic test sets. In all of these experiments, we can see that the ByT5 models generally perform better than the subword counterparts. This is also reflected in the ERRANT scores in Appendix E, Table 5 , where the ByT5 models score highest overall.\n\nDiscussion & Conclusion\nOur results show that the ByT5 models are the overall high-scorers on the real-world test sets, and on most of the synthetic ones. We include finetuning results on the ByT5 model that has been trained for longer on the synthetic data (550k updates) to compare how performance improves with time. We see the GLEU scores keep going up with time, and more importantly, when taking a close look at the actual generated output, this is the model that best corrects real-world errors. This makes it the most feasible option for use in real-world scenarios. A comparison of the output of the models trained on both data sources is shown in Appendix D.\nAn example from the test data is when the subword-tokenized model mBART-ENIS-Synth100k+EC incorrectly changes the name of a person from a rare name (\"L\u00e1retta\") to a more common one (\"L\u00e1ra\"). This kind of error is not seen in the byte-level model, which is quite conservative in its corrections of unknown entities. While this means ByT5 occasionally misses actual errors, we find that it is much better suited for production than a subword-level model that makes serious semantic errors. These more nuanced error correction examples may not be fully captured by the automated metrics, but are crucial for real-world use.\nThe subword regularization experiments are included as an alternative approach for mitigating the subword tokenization problem. The results are marginally better than the model without subword regularization when trained on the synthetic data, and the model performs better than the ByT5-Synth100k model in the case of dupli-cate words, which linguistically is a quite trivial task, and in more intricate mood errors. It however doesn't do any better than the mB-ISEN-Synth100k trained without subword regularization on the curated datasets, and this also holds when the model is finetuned additionally on curated data. The model finetuned on only the curated data with subword regularization (mB-ISEN-reg-EC) however performs consistently much better than its counterpart without subword regularization, often on par with or surpassing ByT5. This model has not seen any of the highly noised synthetic data, and thus has the most to gain from the subword noise. We speculate that this is one of the reasons we don't see more gains from adding subword regularization; the training examples are already so highly noised that there is not much to be learned from the added subword noise.\nThe IceEC finetuning data contain real-world errors which have been hand-corrected. These texts are somewhat different from the highly noised training examples with synthetic errors, have fewer errors on average and are more varied as they are naturally occurring. They also include stylistic edits from the reviewers, which improve the text's fluency, but in those cases the original is not necessarily incorrect as per the language standard. With these differences in mind, we expect the models to have to forget some of the synthetic error correction task in order to adapt to this \"new\" denoising task. We see this happen in the mBART-ENIS finetuning on the curated data, and to a lesser extent in the ByT5 finetuning. The denoising task performance on the synthetic errors from the previous step has in part been lost, which is expected, since some of these errors are not particularly common in real texts.\nFor the more grammatically complex error-types in the synthetic data (dativitis and changes to noun cases and verb moods), we find that the mBART-ENIS trained on synthetic data generally does well; for some subsets even surpassing the ByT5 counterpart that was finetuned on curated corpora. We suspect that this has to do with the linguistic knowledge the model has already gained during its pretraining on Icelandic texts, as explained in Appendix A. The ByT5 model that was trained for longer however manages to surpass it on the mood error type, indicating that it is still adapting to the Icelandic language, alongside its primary denoising task.\nThe models trained on only the finetuning data perform the worst throughout. The results show that they do not manage to correct the synthetic categories much beyond the baseline, except for mBART-ENIS in some cases. We expect that this has to do with their extra knowledge of Icelandic and the denoising objective used in the synthetic error correction finetuning. The results for these models on the curated in-domain test sets are in fact mostly on par with the models finetuned on the synthetic data only. Looking at the generated output, we see that the error types these models correct are not the same as those that the syntheticonly models are able to correct, which is expected, as they are trained on different data.\nWe conclude that adopting a byte-level approach rather than a subword approach leads to best results for the task of GEC, at the very least in the case of a morphologically rich language such as Icelandic. Finally, we find that the optimal way of capturing a wide range of errors is to train on a combination of synthetic and curated data, particularly when the curated data is limited.\n"}
{"question": "What is the second step of our CAT framework?", "evidence": "  Our framework can be summarized into four steps. Train teacher models for both event conceptualization and triple conceptualization on the labeled dataset D l h and D l t , respectively. Use the two teachers to assign pseudo labels to unlabeled datasets.\n(2) Conduct alternative conceptualization or instantiation on labeled and pseudo-labeled data.\n(3) Bootstrap (aggregate) the alternative concepts and instances in the second step using natural language prompt templates and train student models on both labeled and pseudo-labeled data.  (4) Use the student models to refine the pseudo labels and then re-train the student models. ", "options": ["A. Use the student models to refine the pseudo labels and then re-train the student models.", "B. Bootstrap (aggregate) the alternative concepts and instances in the second step using natural language prompt templates and train student models on both labeled and pseudo-labeled data. ", "C. Conduct alternative conceptualization or instantiation on labeled and pseudo-labeled data.", "D. Train teacher models for both event conceptualization and triple conceptualization on the labeled dataset D l h and D l t , respectively. Use the two teachers to assign pseudo labels to unlabeled datasets."], "answer": "C", "content": "\nIntroduction\n\"Concepts are the glue that holds our mental world together.\"- Murphy (2004) Commonsense reasoning is a crucial ability for machines to make situational presumptions and draw inferences from the knowledge that reflects our humans' understanding of situations and common facts (Davis, 1990; Davis and Marcus, 2015) . It has gained increasing popularity in the Natural Language Processing (NLP) community with the emergence of CommonSense Knowledge Bases (CSKB) (Sap et al., 2019a; Speer et al., 2017;  Figure 1 : A demonstration of commonsense reasoning on an unknown situation, PersonX plays with his dog, with the aid of abstract commonsense knowledge. Decontextualized conceptualization, such as observe, may yield wrong abstract commonsense knowledge that cannot be instantiated within the corresponding context. Hwang et al., 2021) and large language models (Bosselut et al., 2019; Rajani et al., 2019; Liu et al., 2022b; Su et al., 2022; Yu et al., 2022b) . However, when encountering situations beyond the data given, more abstract background knowledge must be acquired and generalized to assist the reasoning (Tenenbaum et al., 2011) , and language models trained with an autoregressive language modeling objective do not explicitly leverage such abstract knowledge during inference.\nInstead, humans rely on conceptual induction and deduction (Murphy, 2004) to make inferences on novel situations without the need to memorize all special cases. As shown in Figure 1 , humans can derive conceptualizations based on the assertion that \"PersonX watches a football game, as a result, he feels relaxed\" to infer that \"relaxing events can make someone feel relaxed,\" where the acquired abstract commonsense knowledge can be further used as general knowledge to perform reasoning on similar or associated situations. A new commonsense knowledge \"PersonX plays with his dog, as a result, he feels happy and relaxed\" can be deduced by instantiating relaxing events to playing with his dog.\nAs the cornerstone of generalizable commonsense reasoning, such a process is extremely challenging for machines to replicate due to the absence of contextualized conceptualizations and abstract commonsense knowledge in CSKBs and a lack of relevant methodologies.\nYet, existing works address the process of induction and deduction separately via conceptualization and instantiation. Several methods performing conceptualization are proposed with a specific focus on entity-level (Durme et al., 2009; Song et al., 2011; Gong et al., 2016; He et al., 2020; Peng et al., 2022; Song et al., 2015) and event-level (Chen et al., 2020; He et al., 2022) semantics. Instantiation (Allaway et al., 2023) , as the process that simulates conceptual deduction, is tackled separately and not leveraged by these methods. Though abstract commonsense knowledge can be derived by using existing conceptualization methods to abstract a certain instance from factual commonsense knowledge, several limitations still exist.\nFirst, the plausibility of abstract commonsense knowledge banks on both the correctness of conceptualization and proper contextualization under specific assertions. The latter one, which is an essential step for the deduction of abstract knowledge, is missing from current methodologies. Take Figure 1 as an example, the concept observe will not necessarily lead to the result of \"feeling relaxed\", as observe omits the entertaining property of the original instance as a cost of abstraction. Second, instantiating abstract commonsense knowledge can yield much more and diverse concrete commonsense knowledge that can serve as an augmentation of the training dataset, while current methods undervalue such a process and only focus on conceptualization. Finally, the complex contextualization and conceptualization of commonsense knowledge can easily bring more than two orders of magnitude of data on top of the original dataset. This makes current labeled data scarce and infeasible for practitioners to annotate all of them, leaving a large amount of unlabeled data.\nTo fill in these research gaps, we propose CAT (Contextualized ConceptuAlization and InsTantiation), a semi-supervised learning framework that unites event conceptualization and instantiation in cascade to conceptualize CSKBs and acquire abstract commonsense knowledge to aid commonsense reasoning. Inspired by how humans learn with concepts (Carey, 2004) , we design a novel bootstrapping 1 method to enhance conceptualizations and abstract commonsense knowledge verification with the help of similar conceptualizations and instantiations as a reference. We demonstrate the effectiveness of CAT by using the acquired abstract commonsense knowledge to train COMET (Bosselut et al., 2019) , a commonsense inference language model that generates if-then commonsense knowledge, and showing that our derived abstract commonsense knowledge can significantly improve commonsense inference modeling.\nOur contributions are three-fold: (1) We introduce a semi-supervised learning framework, CAT, to conceptualize CSKBs with the assistance of progressively bootstrapping similar abstract concepts or instantiations in the conceptualization process.\n(2) We use CAT to acquire abstract commonsense knowledge at scale with high quality, which can be used for commonsense inference modeling. (3) We demonstrate the effectiveness of our framework by achieving state-of-the-art performance on two CSKB conceptualization tasks and remarkably improving commonsense inference modeling with our derived abstract commonsense knowledge.\n\nRelated Works\nConceptualization and Instantiation. Many existing works have studied conceptualization and instantiation separately. Durme et al. (2009) first attempted to derive more general knowledge by abstracting over large sets of factoids obtained from WordNet (Miller, 1995) synsets. Song et al. (2011 Song et al. ( , 2015) ) and Gong et al. (2016) proposed to turn instances in a sentence into concepts via weight matching from Probase (Wu et al., 2012) . Recently, Liu et al. (2022c) proposed a taxonomy-guided induction method to mine verb-oriented commonsense knowledge from verb phrases. Peng et al. (2022) constructed a conceptual knowledge benchmark to evaluate language models with three zeroshot probing tasks. While these works focus on the conceptualization of entities, He et al. (2022) constructed an event conceptualization benchmark based on ATOMIC (Sap et al., 2019a ) by combining syntactic parsing, semantically heuristic matching, and human annotation. Besides, the line of works focusing on ultra-fine entity typing (Choi et al., 2018; Dai et al., 2021; Li et al., 2022) similar objectives of typing named entities, nominal nouns, and pronouns into a set of free-form phrases. Instantiation was attempted by Allaway et al. (2023) , who proposed a controllable generative framework to probe valid instantiations for abstract knowledge automatically. Though Porada et al. (2021) and Peng et al. (2022) both proved that existing pretrained language models lack conceptual knowledge, none of existing works explicitly combine both techniques to derive abstract knowledge that is context-sensitive and generalizable.\nCommonsense Reasoning. Endowing NLP systems with the ability to perform commonsense reasoning is an elusive goal of artificial intelligence (Sap et al., 2020) . A diverse collection of commonsense reasoning tasks have been proposed as evaluation benchmarks (Talmor et al., 2019; Omura et al., 2020; Ponti et al., 2020; Fang et al., 2021a) . Among them, Bosselut et al. (2019) proposed a generative model, COMET, to learn to produce if-then commonsense knowledge as an effective approach toward modeling commonsense inference that can be applied in various commonsense reasoning tasks (Talmor et al., 2019) .\nSemi-Supervised Learning. Semi-supervised learning (SSL) aims at taking advantage of unlabeled data to equip models with stronger generalization ability (van Engelen and Hoos, 2020) . The most common approach is using pseudo labels (Iscen et al., 2019; Wang et al., 2022) to expose more unseen data to the student model. It has been applied in various machine learning tasks such as image classification (Liu et al., 2022a; Hu et al., 2021 ), text classification (Li et al., 2021; Meng et al., 2019; Xiao et al., 2019) , commonsense knowledge base population (Fang et al., 2022) , and named entity recognition (Liu et al., 2021; Chen et al., 2021) .\n\nProblem Definition\nDefinition. Conceptualizing an event-centric CSKB to derive abstract commonsense knowledge comprises two steps (He et al., 2022) : event conceptualization and triple conceptualization.\n\nDenote the triples in the original CSKB as\nD o = {(h o , r, t)|h o \u2208 H o , r \u2208 R, t \u2208 T }, where H o , R,\nand T are the set of heads, relations, and tails in the original CSKB. The first step only operates on head events without considering the context in r and t. The goal of event conceptualization is to produce conceptualized head event h a from the original head h o to represent an abstraction of h o . In the second step, the task is to verify whether the conceptualized head h a still makes sense in the context of r and t, as r and t will further restrict the level of abstractness in h a . As shown in Figure 1 , conceptualizing watch football game to observe is wrong within the context of having feel relaxed as a result. Plausible (h a , r, t) triples will be considered as valid abstract commonsense knowledge. Specifically, in the first step, there are two ways of conceptualizing head events alone: a retrievalbased discriminative way and a generative way. The retrieval-based discriminative paradigm identifies and links a component i in h o to a concept c in a concept taxonomy C to form a conceptualization h a by replacing i with c. The model needs to verify whether h a is a valid conceptualization of h o . The generative paradigm aims to generate a h a directly given h o and the designated component i in h o .\nFormally, denote the annotated dataset in the first step, event conceptualization, as\nD l h = {(h o , h a , y)|h o \u2208 H o , h a \u2208 H a , y \u2208 {0, 1}},\nwhere h o is an original head event without conceptualization, h a is a corresponding conceptualization of h o , and y is the human-annotated label indicating whether such a conceptualization is plausible or not. The labeled dataset in the second step, triple conceptualization, is denoted as\nD l t = {(h, r, t, y)|h \u2208 H a , r \u2208 R, t \u2208 T, y \u2208 {0, 1}},\nwhere h is a conceptualized head event from the first step, r and t are a relation and a tail from the original CSKB accompanied with the corresponding original head h o , and y is the human-annotated label indicating whether such abstract commonsense knowledge, in the form of a conceptualized triple, is plausible or not. Besides labeled datasets, unlabeled datasets are defined similarly as D u h and D u t only with the difference that labels y are missing. Thus, the task objective for discriminative event conceptualization is to determine whether a h o can be properly abstracted using h a , where h a is derived by replacing a component i \u2282 h o with its linked concept c from a concept taxonomy C. The task objective for generative event conceptualization is to generate h a directly from h o with text generation models. For the triple conceptualization task, the objective is to distinguish whether a conceptualized triple (h a , r, t), representing abstract commonsense knowledge, is plausible or not.\nDataset. To study conceptualization over CSKBs, we use the AbstractATOMIC dataset provided by He et al. (2022) as the benchmark. In Ab-stractATOMIC, ATOMIC is used as the original CSKB. And the event conceptualization adopts a discriminative way, where a syntactic parsing schema is defined to identify the components i in h o to be heuristically linked to concept taxonomies Probase (Wu et al., 2012) and WordNet (Miller, 1995) to form conceptualized h a . Such a heuristic can produce over 32 times more candidate conceptualized head events and over 10 times more conceptualized triples compared with the original ATOMIC, as the number of retrieved concepts from the concept taxonomy C can be manually controlled to acquire a large number of conceptualizations. Triple conceptualization is defined as predicting the plausibility of the triples whose head is conceptualized. Only 131K (26%) conceptualizations of 7K (45%) ATOMIC head events and 81K (1.3%) conceptualized triples are manually anno-tated as D l h and D l t , while others remain unlabeled D u h and D u t . The trn/dev/tst partition follows the same split as in the original ATOMIC. Statistics and more detailed explanations of AbstractATOMIC are shown in Table 1 and Appendix A.\n\nCAT Framework\nThis section introduces our proposed Contextualized ConceptualizAtion and InsTantiation (CAT) framework for conceptualizing commonsense knowledge bases and acquiring abstract commonsense knowledge. An overview is presented in Figure 2 . Our motivation is two-fold: first, adding instantiation after conceptualization to form a cycle can strongly benefit two conceptualization tasks simultaneously. On the one hand, instantiating conceptualized triple relies on the correctness of event conceptualization. On the other hand, properly conceptualized triples can benefit event conceptualization via instantiation by providing more context brought by (r, t). Second, to address the lack of annotations, we resort to pseudo labeling, a typical semi-supervised learning approach to automatically assign pseudo labels to the vast majority of unlabeled data using a teacher model.\nFollowing He et al. (2022) , we study the retrieval-based discriminative paradigm of event conceptualization and leave the generative paradigm as an intrinsic evaluation. In CAT, we unify event conceptualization and triple conceptualization into one cycle and make them mutually benefit each other through instantiation and conceptualization. Our framework can be summarized into four steps:\n(1) Train teacher models for both event conceptualization and triple conceptualization on the labeled dataset D l h and D l t , respectively. Use the two teachers to assign pseudo labels to unlabeled datasets.\n(2) Conduct alternative conceptualization or instantiation on labeled and pseudo-labeled data.\n(3) Bootstrap (aggregate) the alternative concepts and instances in the second step using natural language prompt templates and train student models on both labeled and pseudo-labeled data. (4) Use the student models to refine the pseudo labels and then re-train the student models.\n\nTeacher Model Training\nTwo teacher models on both event and triple conceptualization tasks are trained separately on the labeled dataset D l h and D l t . As both tasks are inherently text/triple classification, we adopt KG-BERT (Yao et al., 2019) as the skeleton of our models. The event conceptualization model determines whether h a is a valid conceptualization of h o , and the triple conceptualization model determines whether a conceptualized triple (h a , r, t) is plausible or not. The two models \u03b8 are trained on annotated examples x i with a cross-entropy loss (Eq. 1) and used to provide pseudo labels to instances from the unlabeled datasets D u h and D u t . Two thresholds, T + and T \u2212 , are set to determine the pseudo labels of unlabeled examples with high confidence. Examples with a pseudo-labeled score higher than T + will be labeled y i = 1, and those lower than T \u2212 will be labeled y i = 0. The rest will be discarded.\nL(x i , \u03b8) = \u2212 |x| i=1 y i log(\u03b8(x i ))\n(1)\n\nAlternative Conceptualization and Instantiation\nAccording to Murphy (2004) , when humans learn a new concept, we pre-extract similar known concepts in our minds and infer possibly equivalent unknown concepts on the fly. Inspired by this theory, we retrieve additional abstract concepts or instantiated events to help discriminate conceptualizations and abstract commonsense knowledge. For event conceptualization, we retrieve some alternative possible conceptualizations of h o to accompany the learning of h a . Additional conceptualizations of h o from both labeled and pseudo-labeled examples are predicted again by the teacher model and ranked according to their plausibility score prediction. And top m conceptualizations are retrieved with m being a hyperparameter to control the number of retrievals. For triple conceptualization, we perform instantiation in cascade to instantiate c to some concrete instances to assist the learning process.\nPossible instantiations of c are extracted from annotated and pseudo-labeled event conceptualizations by searching for conceptualized events h \u2032 a \u2208 H a other than h a with c as the concept and extracting their corresponding instances i \u2282 h \u2032 a . Similarly, the instances are then scored by the teacher model, and the top n of them are retrieved. Intuitively, alternative event conceptualizations can serve as hints for discriminating the correctness of the target conceptualization, and instantiations can carry additional contextualized information to help verify the plausibility of a conceptualized triple, which meets the objective of deriving abstract commonsense knowledge that is context-sensitive.\n\nPrompt Aggregation\nWe then bootstrap the retrieved alternative conceptualizations/instantiations via natural language prompts. Here bootstrap (Carey, 2004 ) can be understood as binding the alternative retrievals and the target concept/triple together to strengthen the discrimination of the target concept/triple. As shown in Figure 2 step (3), the initially given input and retrieved concepts/instances are concatenated via human-defined prompts for both conceptualization tasks. Alternative concepts/instances are sorted in the order of their plausibility score ranking. Two student models S h and S t for both tasks are trained using the modified text with such prompts as inputs. They are expected to learn the bootstrapping connectionism between the target and the additional retrievals we provided. More detail about the prompt design is in Appendix B.\n\nPseudo-Label Refinement\nAll pseudo labels, initially derived by a teacher model trained on the original labeled dataset, are relabeled according to the plausibility score predicted by our newly enhanced student models S h and S t . Similar to the teacher model, two thresholds, T + and T \u2212 , are applied to distinguish positive and negative examples for both tasks. In addition, negative Table 2 : Performance (%) by our CAT framework on the discriminative event conceptualization and triple conceptualization tasks. We report the average AUC score and standard deviation across experiments with three random seeds. The best performances within each framework are underlined, and the best among all models are bold-faced. labels are assigned to triples whose conceptualized head events are predicted as wrong conceptualizations by S h , as wrong conceptualizations will not yield plausible abstract commonsense knowledge.\n\nApplication and Evaluation of CAT\nThe resulting models of CAT include an event conceptualization model and a triple conceptualization model, both fine-tuned on the refined pseudo labels and the labeled data. These two models can be used to conceptualize ATOMIC to a larger commonsense knowledge base on a more abstract level. We further conduct intrinsic evaluations on the acquired event conceptualization model under a generative event conceptualization paradigm and extrinsic evaluations on the resulting conceptualized CSKB with commonsense inference modeling task (COMET; Bosselut et al. (2019) ) in Section 5. Here we select COMET as the representative because it is a general commonsense model that can be applied to various downstream commonsense reason-ing tasks such as SocialIQA (Sap et al., 2019b) , self-talk (Shwartz et al., 2020) , and CSKB completion (Malaviya et al., 2020) . Meanwhile, generative event conceptualization enables performing automatic conceptualization scalably. Both are important applications and evaluations of CAT.\n\nExperiments\nWe conduct conceptualization experiments using CAT in Section 5.1 and generative experiments as evaluations in Section 5.2. These experiments demonstrate that CAT has a strong capability in conceptualizing CSKBs, and better conceptualization modeling can help populate more novel and diverse commonsense knowledge and thus help commonsense modeling (COMET).\n\nCSKB Conceptualization\nBaselines. We collectively introduce the baselines for both event and triple conceptualization tasks, as they are inherently classification tasks. Table 3 : Performance (%) of GPT2 (XL) on the generative event conceptualization task. D l h stands for annotated labeled data, and D u stands for the data acquired by CAT. The underfoot value indicates the threshold for selecting plausible pseudo labels. The best performances are bold-faced, and the second-best ones are underlined.\nAUC is used as the evaluation metric. Under a supervised learning setting, we apply KG-BERT (Yao et al., 2019) model with BERT (Devlin et al., 2019) , BART (Lewis et al., 2020) , RoBERTa (Liu et al., 2019 ), DeBERTa (He et al., 2021 , 2023 ), and ELECTRA (Clark et al., 2020) as the backbone language models. We also attempt to leverage supervised generative language models as baselines. GPT2 (Radford et al., 2019) models are trained with a text generation objective only on positive examples, and we use perplexity as the prediction scores to calculate AUC. For the semi-supervised learning baselines, we leverage UDA (Xie et al., 2020a) , NoisyStudent (Xie et al., 2020b), and Pseu-doReasoner (Fang et al., 2022) with RoBERTalarge being the backbone model. Additional explanations can be found in Appendix C.1.1.\nDiscriminative Results. The results for both tasks are presented in Table 2 . Under a supervised learning setting, KG-BERT family mostly performs better on both tasks than GPT2 due to the fact that GPT2 is only fine-tuned on positive examples and thus cannot learn from negative examples that contain wrong conceptualizations and implausible abstract commonsense knowledge. As for the semi-supervised learning setting, previous SSL baselines are rather limited in improving the performance against supervised learning. The best Pseu-doReasoner only improves by 0.5% and 0.3% on the test set for both tasks compared with supervised RoBERTa-large models. Instead, models trained with CAT can outperform all other training methodologies. Comparing the test set performance with PseudoReasoner, small backbone models (BERTbase) can improve by 3.4% and 2.2%, and large models (RoBERTa-large) can be improved by 2.1% and 2.2%. This shows pipelining two-step concep-tualizations as a loop and leveraging our proposed bootstrapping-based method can yield a larger performance gain compared with simply applying a semi-supervised learning strategy. Due to limited space, ablation studies on framework components and the semi-supervised learning paradigm of CAT are conducted in Appendix C.1.4. For example, the results indicate that bootstrapping alternative conceptualization and instantiation plays the most important role in assisting learning conceptualization among all components of CAT. Additional results and a computational cost study can be found in Appendix C.1.3 and Appendix D.\n\nApplication and Evaluation of CAT\nAs CAT is a framework for acquiring conceptualized commonsense knowledge, including both conceptualized head events (from h o to h a ) and abstract commonsense triples (h a , r, t), we assess these pseudo-labeled outcomes via two generative tasks with various threshold tuning as evaluations.\nGenerative Event Conceptualization. To intrinsically evaluate the effectiveness of CAT's event conceptualization, we use the acquired conceptualized head events as training data to learn a generative event conceptualizer. Specifically, the models are trained with instance-conceptualizations pairs in the format of \"<instance> is an instance of <concept>\". At the evaluation phase, the model is prompted with \"<instance> is an instance of [GEN]\" where <instance> is the instance to be conceptualized and [GEN] is the generation token. We then retrieve the top-1 generation and compare it against the target set from the evaluation dataset to compute four NLG metrics, as listed in Appendix C.2.1. These scores can be regarded as an approximation of the top-1 generations' recall. 40.0 40.3 27.1 27.8 20.0 20.8 16.5 17.5 16.1 16.3 35.3 35.7 31.6 31 Additionally, we uniformly sample 500 generations from each evaluation split and conduct expert annotations on the plausibility of each conceptualization to ensure that out-of-domain concepts can be properly evaluated. The experts are asked to determine whether each top-1 generation is indeed a plausible conceptualization or not, such that the top-1 generations' precision is reflected. Thus, current evaluation measures jointly evaluate the top-1 generations' precision and recall, which makes it robust and non-easy to be impacted by repetition problems (Li et al., 2020) . Zero-shot GPT2 and GPT2 fine-tuned on the originally labeled event conceptualizations in D l h are used as baselines. We also study the effect of the threshold T + that selects plausible conceptualized heads, where higher thresholds indicate higher plausibility regarded by CAT. The results are presented in Table 3 . With a relatively high threshold, generators trained on a mixture of pseudo-labeled data by CAT and annotated concepts significantly outperform the baselines in every automated metric. A plausible rate of 93.3% is maximally achieved on the test set, which is 11.8% higher than the baseline. Gradually reducing the threshold also decreases the performance, indicating abstract heads with lower plausibility scores can be of poorer quality. Such results indicate that CAT can produce high-quality event conceptualizations for generative models to learn better conceptualizers without the need to annotate a large number of data.\n\nCommonsense Inference Modeling (COMET).\nThe second component of CAT produces triple-level abstract commonsense knowledge. We evaluate these abstract commonsense triples with a commonsense inference task that generates commonsense tails given heads and relations as inputs, as in COMET (Bosselut et al., 2019) . Following He et al. ( 2022), we apply the same training and evaluation process to the models. The base training data we use are a subset of ATOMIC triples corresponding to those annotated abstract triples in D l t , which contains 17K (3.7%) among the original ATOMIC. We derive abstract commonsense knowledge using CAT from a subset of D u t where the heads correspond to those in the ATOMIC subset to ensure no data leakage, denoted as D u CAT . GPT2 is fine-tuned on the ATOMIC subset, the annotated abstract triples D l t , the abstract knowledge verified by CAT, or their combinations. The commonsense generation results are presented in Table 4 . Similar to COMET (Bosselut et al., 2019) , all models are evaluated on the original ATOMIC's full validation and testing sets. The best result is achieved using a mixture of the ATOMIC subset and abstract triples pseudo-labeled by our framework, with 0.95 as the threshold for selecting plausible triples. This indicates high-quality abstract commonsense triples can indeed provide a more general view of the original commonsense knowledge, thus helping commonsense inference. Additionally, training with our pseudo-labeled examples outperforms training with those annotated triples in AbstractATOMIC, which also validates the effectiveness of our model that leverages a large amount of unlabeled data. To further investigate how conceptual knowledge 5HWULHYDO1XPEHU (YHQW&RQFHSWXDOL]DWLRQ$8& improves commonsense inference modeling, we conduct more empirical analysis in Section 5.4. Additional experiment results with other thresholds and case studies can be found in Appendix C.2.3 and Appendix E, respectively.\n\nNumber of Retrieved Alternative\nConceptualizations and Instantiations.\nWe then study the ablation of bootstrapping different numbers of alternative conceptualizations/instantiations (denoted as #retrieval) in our CAT framework. For simplicity, when tuning the #retrieval for one task, the #retrieval of the other task is fixed at the best value we acquired. We plot the test AUC score with #retrieval from 0 to 11 using BERT-base as the backbone model in Figure 3 . #retrieval=0 refers to training with a simple student-teacher framework without bootstrapping alternative conceptualizations and instantiations. For event conceptualization, the performance generally positively correlates with the number of retrievals, while it starts dropping after 9. A reversed trend is observed for triple conceptualization, where using only two instances achieves the best performance. One possible reason is that in triple conceptualization, the retrieved instances are events and much longer than the retrieved concepts in event conceptualization, and aggregating various alternative events for a triple will cause language models to be less sensitive to the semantics of the original triple (Holtzman et al., 2020) .\n\nThe Effect of Abstract Knowledge\nWe finally study the effect of abstract commonsense knowledge acquired by CAT by studying the semantic overlaps between training and testing data. We sort the test set by the BERTScore (Zhang \n\nConclusion\nIn conclusion, this paper proposes CAT, a semisupervised learning framework for commonsense reasoning, by leveraging the power of abstract commonsense knowledge. By achieving state-of-theart performances in CSKB conceptualization tasks, we remarkably improve modeling commonsense inference, as an important cornerstone of many commonsense reasoning tasks. Our analysis also demonstrates that high-quality abstract commonsense knowledge can benefit commonsense inference modeling by providing more generalizability on hard commonsense knowledge. We hope this work can draw insights toward commonsense reasoning from a conceptualization perspective.\n"}
{"question": "Which aspect of political actor modeling is a novel feature of UPPAM?", "evidence": "  UPPAM is novel in three points. (1) We learn the mapping from statements to the representation of political actors, instead of directly learning actor representations. By doing so, the mapping parameters can be transferred to any downstream tasks easily, learning representations for unseen political actors based on their statements. (2) We propose several self-training tasks to inject general knowledge in the political scenarios into mapping parameters in the pre-training stage. (3) We propose a multigranular actor representation model, that can capture nuances of both general ideology and specific preferences between different political actors.  ", "options": ["A. Learning actor representations directly from statements.", "B. Incorporating information of political scenarios into mapping parameters.", "C. Utilizing multigranular actor representation to capture nuances.", "D. Achieving competitive results compared to task-specific models."], "answer": "C", "content": "\nIntroduction\nPolitical actors are shaping our attitudes, opinions, and decisions toward public issues. For instance, on social platforms, politicians can select and emphasize certain aspects of content to bias the discussion, through which they can derive an opinion climate from user engagement and acquire direct feedback from potential voters and opinion leaders (Bene, 2017; Heiss et al., 2019) . Political actor modeling is essential for quantitative political science and has applications in various downstream tasks such as roll call vote prediction (Yang et al., 2020) , frame detection (Johnson et al., 2017) and bias detection (Baly et al., 2020) .\nData-driven approaches utilize different kinds of information to profile political actors, including public statements, legislative behaviors and social Figure 1 : An illustration of political actors. They not only participate in legislative activities, but also form relationships with others, and convey opinions through tweets, speeches and etc. We propose to represent political actors based on their statements and learn the mapping from language to their representations using social networks and behaviors as self-constructed supervision.\nnetworks (Figure 1 ). Early research analyzes roll call data to estimate the ideology of political actors. Ideal point model (Clinton et al., 2004 ) is one of the most widely used approaches for votebased analysis that reveals how cleavages between legislators reflect partisan affiliation. Researchers further incorporate texts of bills to enhance the ideal point model (Gerrish and Blei, 2011, 2012; Kraft et al., 2016) and develop multidimensional vectors to replace one-dimension points. Recently, more abundant information has been considered to learn effective representations for political actors, such as co-sponsorship network (Yang et al., 2020) , relations of contributors (Davoodi et al., 2020) , stakeholders (Davoodi et al., 2022) , mention in documents (Pujari and Goldwasser, 2021) , and expert knowledge (Feng et al., 2021 (Feng et al., , 2022)) .\nGenerally speaking, previous research aims to learn representations for a certain group of political actors using supervision from specific downstream tasks as objectives. Although they report positive results on target tasks, their models lack generalization ability in two aspects. (1) Representations are learned on labeled data from specific tasks, e.g., state-level vote prediction, therefore they cannot be easily transferred to other tasks or scenarios. (2) The model is limited to the training setting and can not be adapted to dynamic social contexts. In other words, it's hard for the model to estimate new legislators, non-voting candidates and other political actors unseen.\nRecently, large-scale pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019b; Brown et al., 2020) have demonstrated a strong generalization ability and achieved excellent performance in many language modeling tasks. Motivated by PLMs, we explore representing political actors based on their statements and propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM) 1 . We employ a two-stage training procedure following the fashion of PLMs. Firstly, we pre-train our model to learn the mapping from statements to actor representation. We propose a multigranular method to represent political actors based on language, and information of political scenarios is further injected into our model via proposed structure-aware contrastive learning and behaviordriven contrastive learning tasks. Secondly, we fine-tune the model for downstream tasks using the corresponding supervised objectives.\nUPPAM is novel in three points. (1) We learn the mapping from statements to the representation of political actors, instead of directly learning actor representations. By doing so, the mapping parameters can be transferred to any downstream tasks easily, learning representations for unseen political actors based on their statements. (2) We propose several self-training tasks to inject general knowledge in the political scenarios into mapping parameters in the pre-training stage. (3) We propose a multigranular actor representation model, that can capture nuances of both general ideology and specific preferences between different political actors. We evaluate our approach on three types of tasks in quantitative political science, i.e., profile of actors, prediction of behaviors and analysis of languages. UPPAM outperforms general PLMs and other political domain-specific PLMs on these tasks. Our task-agnostic model also achieved competitive results compared to the task-specific models that employ architectures crafted for the 1 We have made our code publicly available at https:// github.com/xymou/UPPAM. vote prediction task. Further analysis shows the effectiveness and robustness of UPPAM in few-shot settings and different aggregation settings.\n\nMultigranular Actor Representation\nPolitical actors manifest themselves in political activities in multiple granularities. On the one hand, they hold a general ideology or bias, which is long-term and stable. On the other hand, when discussing or taking action on different issues, they hold specific positions (Gerrish and Blei, 2012) , which are the result of long-term bias and shorttime interests (Spell et al., 2020) . Based on this, we propose to represent political actors in two granularities to model both broad ideology and specific preferences for various downstream scenarios.\n\nGeneral and Specific Statements Collection\nIn practice, we use all statements a political actor has posted to get his general representation, characterizing the broad political leaning. Furthermore, issue-related content is adopted to help capture specific attitudes. Concretely, we use a handcrafted information retriever (see more details in Appendix A.2), to collect statements related to the queried policy area as input to encode the specific representation.\nStatements Aggregator Since a political actor can post thousands of statements, the first challenge is how to aggregate one's statements to get his representation. It is too expensive in time and computation cost to combine full sentences. Instead, we identify indicator words from statements for information aggregation. According to the framing theory (Entman, 1993) , entities and subjective content an author uses can implicitly reflect his political leaning. Following this, we identify entities, frame and sentiment words as indicators. We sort them by TFIDF (Jones, 1972) scores and keep indicators with the highest values to form an indicator sequence. In this case, for each political actor, we can get two kinds of indicator sequences, given a query about policy area j:\nS g i = w g 1 , w g 2 , ...w g N (1) S p j i = w p j 1 , w p j 2 , ...w p j M (2)\nwhere S g i is calculated from all the statements made by political actor i, S related to policy area j, and we reserve top N and M indicators with highest TFIDF value, where N and M are pre-defined hyper-parameters.\nIn subsequent pre-training and downstream tasks, we use general sequences as input when the goal is to profile the characters broadly, e.g., estimating ideology. And we input both sequences and average the representation when specific attitudes are required in tasks, as shown in Figure 2 . Note that even if the issue-related content can not be retrieved, we can use the general sequence as a substitute, to ensure input compatibility.\n\nMultidimensional Pre-training for Political Actor Modeling\nTo inject general knowledge of the political landscape into the mapping from statements to representation, we construct self-supervised tasks based on structural and behavioral information.\n\nStructure-aware Contrastive Learning (SCL)\nIn terms of structural information, we mainly focus on the relationship formed between political actors. Previous studies have revealed that homophily exists in political communities, where people with similar ideologies form a link with each other (Barber\u00e1, 2015) . We use two parts of links, namely party affiliation and co-sponsorship in voting. We treat party affiliation as a coarse relationship and cosponsorship as a fine relationship respectively. By doing this, the model can further capture nuances across parties as well as inside the same party.\nParty Affiliation Link We compare statements of legislators from different parties. We choose a legislator as the anchor, and then take another legislator with the same party affiliation as the positive sample, while those from the opposite party are regarded as negative samples. By comparing general statement sequences of legislators from different parties, the model can learn the differences in the languages of different ideologies.\nCo-sponsorship Link In the legislative process, a bill is initialized by a sponsor and several cosponsors. We assume that the more two legislators collaborate, the more they are alike since they reach agreements on many occasions (Yang et al., 2020; Mou et al., 2021) . Given an anchor legislator, other legislators are divided into three categories based on the number of times they co-sponsored with the anchor legislator: G 1 (the co-sponsorship times are above the average); G 2 (the co-sponsorship times are below the average); G 3 (they have never cosponsored). And we further sample positive and negative samples with the rule of\nG 1 < G 2 < G 3 .\nBased on the triplets constructed in the above two ways, the structure-aware contrastive objective is formulated as follows:\nLSCL = t\u2208T SCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4SCL + (3)\nwhere T SCL is the set of legislator triplets, t (a) , t (p) and t (n) are actor representation encoded by general sequences of anchor, positive and negative sample in triplet t, \u03b4 SCL is a hyperparameter and\n[\u2022] + is max(\u2022, 0).\nNotably, this task endows the model to capture general ideology of speakers from their languages.\n\nBehavior-driven Contrastive Learning (BCL)\nWhen it comes to behavioral information, we pay attention to the most common and important actions, i.e., voting. Specifically, we sample triplets consisting of an anchor bill and a pair of legislators, where the positive legislator p votes yea on the given bill and the negative one n votes nay.\nDifferent from the ideology cleavages modeled in Sec 2.2.1, the divergence of specific preferences is supposed to be reflected in the languages here. Thus, for each legislator, we extract statements about the policy area of the anchor bill as the specific sequence, input with the general sequence, as we mentioned in Sec 2.1. In this way, the behaviordriven contrastive objective is as follows:\nLBCL = t\u2208T BCL t (a) \u2212 t (p) 2 \u2212 t (a) \u2212 t (n) 2 + \u03b4BCL + (4)\nwhere T BCL contains all vote triplets, and \u03b4 BCL is a hyperparameter. t (a) is the bill representation, t (p) and t (n) are the average of representation of the general sequence and the specific sequence, for the positive and negative legislators respectively.\nIt's noticeable that this pattern is not limited to the roll-call vote scenarios, instead, it can be applied to model the preferences towards any bills, events, or targets with a text description.\n3 Pre-training Process\n\nLanguage Model Co-training\nAs mentioned in Sec 2.2.2, modeling political actors in political scenarios inevitably requires encoding textual information of the bills and issues they interact with, e.g., Equation 4. Meanwhile, it is important to understand their opinions in a single discourse without context. Thus, we incorporate additional modules to model political texts. Specifically, as shown in Figure 2 , we have two FFN layers in parallel in each transformer layer, to handle text and actor sequences separately. Given a sequence of input x = {x 1 , ..., x n }, the model first performs multi-head self-attention and then the corresponding module FNN k obtains the required representation:\nh k = FNN k ( Self-Attention ({x 1 , . . . , x n }))\n(5) where k \u2208 {0, 1} indicates the modules of actor and text respectively.\nWe adopt a masked language model objective to pre-train the language model. As mentioned before, political bias and framing effect are often reflected in the selection and mention of specific entities, subjective content, and emphasized frames. Thus, we take a masking strategy that upsamples entity tokens, sentiment words (Wilson et al., 2005) and frame indicators (Roy and Goldwasser, 2020) to be masked for the MLM objectives, with a 30% probability. More details can be found in Appendix B.\n\nOverall Pre-training\nSince the indicator sequence is not a normal sentence, we don't train the MLM task with contrastive learning together. Instead, the pre-training process is divided into two stages. In the first stage, we adopt the MLM task on the original statement sentences and activate text modules, to urge the model to understand the political text. Then, based on this checkpoint, we further conduct the multidimensional pre-training for political actor modeling by combining the objectives:\nEQUATION\n)\nwhere \u03b1 is hyperparameters.\n\nExperiment Setup\nWe fine-tune our model on different kinds of downstream tasks in quantitative political science. We then compare it with prior general PLMs and political domain-specific PLMs.\n\nPre-training Datasets\nCompared to other political actors, congress legislators are more typical and they generate massive content every day. Thus, we start with legislators to construct our pre-training datasets. Overall, we get 887 legislators and delete the meaningless tweets including self-promotion advertisements, notifications, etc., using regular expressions. Finally, the cleaned data contains 2,020,938 tweets, covering discussions of events in various areas. We keep 10K held-out tweets as the validation set.\n\nLegislative Context\nWe collect the party affiliation, sponsorship lists of bills, bills, and corresponding voting records from VoteView 2 and the website of U.S. Congress 3 . Each bill belongs to a specific policy area and has textual information of title and description. We get bills of 112th and 113th for pre-training and reserve those of 114th and 115th for the formulation of downstream tasks. In the pre-training stage, 1,045 bills and 375,440 voting records are involved.\nTo correlate legislators' votes to their statements in the related policy area, we filtered each legislator's tweets in each policy area by the handcrafted information retriever mentioned in Sec 2.1. We finally acquire 1,142,587 tweets, and the details can be found in Appendix A.2. The distribution of the policy agenda of bills and the percentage of legislators whose related tweets can be retrieved in each policy area are shown in Figure 3a and Figure 3b . Over 90% of legislators can be retrieved with relevant statements in most policy areas.\n\nImplementation Details\nUPPAM is produced via continued pre-training on RoBERTa-base model (Liu et al., 2019b) , where we add parallel FFN modules in each transformer layer with the same initialization as the original one. In the first stage, the model is trained on tweets, to minimize the MLM loss with AdamW (Loshchilov and Hutter, 2018) optimizer. In the second stage, the model is further trained on indicator sequences and bill texts, to minimize the L CL . We evaluate the model every 200 training steps on the validation set and keep the best checkpoint. The pre-training procedure takes around 96 hours on 4 Tesla V100-SXM2 GPUs. More details and hyperparameters can be found in Appendix B.\n\nDownstream Tasks and Datasets\nWe evaluate the models on three types of tasks, namely actor profiling, behavior prediction and language analysis. Notably, datasets include not only congress legislators but also other political actors such as journalists, news media, and even anonymous users, to validate the model's generalization capability.\n\nActor Profiling\nThis type of task can be formulated as a user-level classification task, where we aggregate multiple statements to predict the speaker's attribute.\nIdeology Detection is the main task to profile actors broadly, aiming to predict political leaning. Models are evaluated on the following datasets.\n\u2022 CongS (Gentzkow et al., 2018) collects speeches from US congressional records. \u2022 celeb (Wojcieszak et al., 2022) contains tweets of celebrities (journalists, politicians and media). We convert the ideology scores into labels according to the signs. \u2022 Reddit (Kitchener et al., 2022) collects comments of common users in non-political subreddits, and labels the users with ideology in the economic dimension. \u2022 PEM (Xiao et al., 2022) collects tweets of legislators, news outlets and cabinet of President Obama and President Trump. \u2022 TIMME (Xiao et al., 2020) includes Twitter accounts with location information and selfidentified political-polarity labels. These accounts are not run by politicians.\n\nBehavior Prediction\nThis type of task can be regarded as a relation prediction task, where we predict a political actor's attitude or action towards a given target with a piece of text description.\nVote Prediction tasks aim to predict votes of legislators towards bills with stances of yea or nay. We follow two configurations in (Mou et al., 2021) . \u2022 VoteIn refers to the in-session setup, where we randomly split the bills in the same congress session, i.e., the 114th session. \u2022 VoteOut refers to the more challenging outof-session setup, where we use data in the 114th session for training and validation while testing on the 115th session.\nGrade Prediction tasks are designed as classification tasks for ratings in a certain issue, given a politician's statements and background description of the given issue. We include datasets as follows:\n\u2022 NRA Grades (Pujari and Goldwasser, 2021) provides politicians' grades {A, B, C, D & F} assigned by National Rifle Association and their statements on guns, as well as background information of guns from ontheissues.org. \u2022 LCV Grades (Pujari and Goldwasser, 2021) is similar to NRA Grades, but it's about the scores in the environment area.\n\nLanguage Analysis\nIn addition to the overall characterization of political actors, we also test models' ability to understand individual discourses. We apply stance detection and frame detection as downstream tasks, which can be formulated as sentence-level classification tasks.\nStance detection tasks aim to predict one's stance towards a given target. The tasks take a 3-way label (favor, against, and neutral) or binary label (favor, against). We test on these datasets.\n\u2022 poldeb (Somasundaran and Wiebe, 2010) provides opinion-target pairs from several debating platforms covering different domains. \u2022 election (Kawintiranon and Singh, 2021) contains tweets related to the 2020 US presidential election, expressing stances towards President Trump and Biden.\n\u2022 SEval (Mohammad et al., 2016 ) is a shared task to detect stances in public tweets.\nFrame detection tasks aim to detect which frame dimensions are employed in a piece of text. It's a multi-label classification task with a pre-defined label set. We test on these datasets.\n\u2022 twitter (Johnson et al., 2017) annotates tweets of politicians with 17 general frames. \u2022 gvfc (Liu et al., 2019a) collects news headlines about gun violence, and annotates them with 9 issue-specific frame dimensions. \u2022 immi (Mendelsohn et al., 2021) collects immigration-related tweets posted by the public, annotated with 14 general frames.\n5 Experiment Results\n\nMain Results\nThe compared general PLMs include BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) . We also compare our model with available PLMs for social science texts-SsciBERT (Shen et al., 2022) , and for the political domain: POLI-TICS (Liu et al., 2022) and PoliBERTweet (Kawintiranon and Singh, 2022). We fine-tune all the PLMs in the same settings, and we select the best fine-tuned model on validation sets using macro F1.\nThe implementation details and hyperparameters can be found in Appendix C.2. Table 1 presents macro F1 scores on the downstream tasks.\nActor Profiling Our model shows superior performance on various political actor modeling tasks.\nResults of ideology detection tasks indicate that our model can not only characterize the ideology of legislators but is also good at modeling other roles like journalists in the celeb dataset and cabinet in the PEM dataset, demonstrating the transferability of using languages to represent characters. The reason for not performing best on the Reddit dataset may be the gap between the expression habits of common users and that of politicians. Nevertheless, we still outperform the majority of baselines.\nBehavior Prediction All the models show excellent performance on vote prediction and grade prediction tasks, using languages to represent political actors. It indicates that it's a feasible scheme to infer political actors' behaviors from their languages. Among all the PLMs, our model is the best. We attribute the performance gain to our proposed behavior-driven pre-training task.\nLanguage Analysis Moreover, our model also achieves competitive performance on tasks of analyzing individual text including stance detection and frame detection, indicating that the ability to understand political languages is preserved while the model is learning to profile actors, benefiting from the co-training process in Sec 3.1.\n\nAblation Study\nTo explore the effects of different components, we conduct ablation studies and results are reported in Table 2 . Removing SCL or BCL mainly hurts the performance of actor profiling tasks. Removing the text modules results in the most loss in language analysis tasks, especially the frame detection task. This demonstrates the necessity of separate modules to guarantee the ability to model political text.\n\nFurther Analysis\nFew-shot Learning We fine-tune PLMs on different numbers of samples. Figure 4 tasks. Benefiting from the pre-training stages, our model can better capture ideology and preference differences, even when using only 16 samples.\nCompare with Task-specific Models Taking the vote prediction task as an example, we compare our model with previous task-specific models, where particular meta-data and structural information is crafted for the task. Table 3 shows that UPPAM achieves competitive results, indicating that we can deduce political actors' votes from languages. Additionally, our method can be used to analyze nonvoting actors, relieving the cold-start problem.\n\nMethods of Statements Aggregation\nWe show the impact of statements aggregation methods on ideology detection in fine-tuning. We mainly compare our method with concat (Table 4 ) and mean pooling (Table 5 ). concat means to concatenate each speaker's political statements into a flat sequence and then encode it. mean pooling encodes each sentence individually and uses the averaged representation as the final representation. We further discuss the impact of the number of aggregated sentences in Appendix C.2.2. Results illustrate that our model shows robustness in several settings and our aggregator is more effective and efficient.\n\nVisualization\nGeneral Ideology We perform Principle Component Analysis (PCA) on political actor representation generated by our model for the CongS dataset. As shown in Figure 5a , our method can well separate politicians of different ideologies.\nIndividual Specific Preferences We also visualize specific representation in different policy areas for individuals. Figure 5b shows the representation in several highly-discussed policy areas, learned by different models from the tweets of Rep. Rooney. We can observe that Rep. Rooney behaves conservatively in immigration, but expresses left-wing views on environment (Pujari and Goldwasser, 2021) . While most of our baselines fail to capture this nuance, UPPAM can well compare the relative polarity in each area.\n\nRelated Work\nPolitical Actor Modeling focuses on modeling attributes and behaviors of political actors, with special attention to estimating the ideology. Because of the publicity and typicality, politicians like legislators have been the research subject for most work.\nThe most widely used approach to estimate the ideology of legislators is ideal point model (Clinton et al., 2004 ) that represents legislators and bills as points in a one-dimension latent space from the rollcall data. After that, researchers further incorporate texts of bills (Gerrish and Blei, 2011; Gu et al., 2014) to enhance the model, solving the problem of prediction on new bills. Some embedding methods are also proposed to promote learning of legislators (Kraft et al., 2016; Kornilova et al., 2018) . More recently, external information including cosponsorship (Yang et al., 2020) , donors (Davoodi et al., 2020) , relevant stakeholders (Davoodi et al., 2022) and expert knowledge (Feng et al., 2021 (Feng et al., , 2022) ) is used to better learn legislator representation. They follow a mixed structure of textual encoder and graph encoder, to explicitly combine textual and structural information. Despite outstanding performance on target tasks, these methods are limited to certain settings or data, behaving inefficient in dynamic political scenarios. Thus they are hard to be transferred to all actors. By contrast, methods relying on texts (Vafa et al., 2020) provide more possibility for generalization.\n\nDomain-specific Pre-training\nBased on continued pre-training on domain-specific data, domainspecific Pre-trained Language Models have shown superiority on many NLP tasks. Domain-specific PLMs have been investigated in many areas including medical (Zhang et al., 2021) and financial (Araci, 2019) \n\nConclusion\nIn this paper, we propose to learn political actors from languages and inject multidimensional domain knowledge into the PLMs through structureaware contrastive learning and behavior-driven con-trastive learning. Experimental results validate the effectiveness and generalization capability of our approach.\n"}
{"question": "what is the main challenge of Multi-Modal Machine Translation (MMT)?", "evidence": "  One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data. ", "options": ["A. The main challenge of MMT is the lack of specific data available for evaluation and training.", "B. The main challenge of MMT is resolving ambiguity in images.", "C. The main challenge of MMT is establishing effective cross-modal representations.", "D. The main challenge of MMT is improving the quality of text translation."], "answer": "C", "content": "\nIntroduction\nMultimodal machine translation (MMT) typically refers to the use of additional non-textual data in text-based machine translation (MT). Here, we focus on the case where source texts are accompanied by images, the idea being to exploit visual data to improve the translation of ambiguous sentences. For example, in Figure 1 , the English word glasses can either be translated as French verres 'drinking vessels' or lunettes 'spectacles', an ambiguity which is resolved using the image.\nA main research direction of MMT has been how to best exploit image representations and combine the image and text modalities (Yin et al., 2020; Caglayan et al., 2021; Calixto et al., 2017; Li et al., 2022) . It has typically been difficult to surpass strong text-only baselines, the image modality often being ignored (Wu et al., 2021) . A major issue holding back progress is that most current stateof-the-art MMT models (Yin et al., 2020; Elliott and K\u00e1d\u00e1r, 2017; Wu et al., 2021; Li et al., 2022) are trained solely on the \u223c30k examples of the Multi30k dataset (Elliott et al., 2016) , comprising image captions and their translations. This causes two issues: (i) the models do not exploit the large amount of text-only data available and therefore perform poorly in comparison to state-of-the-art text-only MT systems, and (ii) we show that very few examples require images to be correctly translated, which means that the datasets are ill-adapted to evaluating the use of the image modality.\nIn this article, we aim to overcome these problems by proposing (i) a new MMT approach that is able to exploit (text-only) monolingual and parallel data as well as (multimodal) captioning data, and that reaches a good balance between maintaining high MT quality and effectively exploiting images, and (ii) a test set, CoMMuTE, containing contrastive evaluation pairs, where images provide the necessary context to disambiguate between multiple meanings of the same source sentence.\nOur suggested model is inspired by work on adapting frozen language models (LMs) to multimodal inputs (Sung et al., 2022; Yang et al., 2022; Eichenberg et al., 2021; Pfeiffer et al., 2022) ; we propose to adapt a strong MT model to multimodal inputs with lightweight modules (Houlsby et al., 2019) to exploit the large amount of textual data it was trained on. We also propose to better exploit the image by introducing guided self-attention and by combining the standard MMT objective with a visually-conditioned masked language modelling (VMLM) objective (Li et al., 2019; Lu et al., 2019; Su et al., 2020) . Our model obtains competitive results compared to strong text-only baselines on standard En\u2192{Fr,De,Cs} MMT benchmarks (Elliott et al., 2016 (Elliott et al., , 2017;; Barrault et al., 2018) and outperforms them and state-of-the-art MMT models on our lexically ambiguous contrastive test set. 3 2 Related Work Multimodal MT data. The reference dataset to train and evaluate MMT models is Multi30k (Elliott et al., 2016) . However, recent work has shown that most MMT systems trained and evaluated on it do not effectively exploit the image information; Elliott (2018) showed that replacing the ground truth image with a random one does not lead to the drop in performance that would be expected, while Wu et al. (2021) argued that the observed gain in performance was due to a regularisation effect. It is also notoriously difficult to beat text-only baselines on this benchmark (Barrault et al., 2018) . This may be due to (i) some subsets of Multi30k having been translated independently from the images (Elliott et al., 2016) and (ii) most of the time, the source text being sufficient in theory to produce a perfect translation (i.e. the image is not necessary; see Section 5.2 for our own analysis).\nBased on this, alternative test sets and evaluation methods have been proposed. Caglayan et al. (2019) proposed to probe the use of images in MMT models, while Li et al. (2021) proposed another training corpus and evaluation benchmark to evaluate MMT systems, but their work is only based on gender ambiguity and requires specific training data to train MMT models. Lala and Specia (2018) released a lexically ambiguous MMT evaluation dataset to evaluate models ability to disambiguate source sentences, but we found that text context is generally sufficient to translate the evaluation dataset correctly.\nContrastive MT datasets. Another means of evaluating (and the one we adopt here) is to target specific phenomena through the use of contrastive test sets. They involve evaluating models based on their ability to rank pairs of translations, where one is correct and the other incorrect. They have been used for the evaluation of different linguistic phenomena, including grammaticality (Sennrich, 2017) , multi-sense word disambiguation (Rios Gonzales et al., 2017; Raganato et al., 2019) , pronoun translation (M\u00fcller et al., 2018; Bawden et al., 2018; Voita et al., 2019) and lexical coherence/consistency (Bawden et al., 2018; Voita et al., 2019) . Bawden et al. (2018) introduced the idea of conditioning which of the translations is correct depending on linguistic context, and we adopt the same strategy here with our CoMMuTE dataset, composed of lexically ambiguous sentences whose translations are determined by the visual context.\n\nAdapting pretrained LMs to multimodal inputs.\nA lot of progress has been made through the use of pretrained LMs (Devlin et al., 2019; Conneau and Lample, 2019; Liu et al., 2020) , often trained on raw text for text-only models or image captioning data for multimodal ones (Radford et al., 2021; Alayrac et al., 2022; Chen et al., 2022) . One of the most efficient ways to learn multimodal LMs is the visually-conditioned masked language modelling (VMLM) objective (Chen et al., 2020; Lu et al., 2019; Su et al., 2020; Li et al., 2020; Zhou et al., 2021; Huang et al., 2021a; Li et al., 2019) . Inspired by the masked language modelling (MLM) objective (Devlin et al., 2019) , it consists in randomly masking input text tokens and predicting them conditionally based on the visual features. A lot of interest has also been shown in lightweight modules such as adapters (Houlsby et al., 2019) to adapt large frozen LMs to multimodal tasks (Eichenberg et al., 2021; Yang et al., 2022; Pfeiffer et al., 2022; Tsimpoukelli et al., 2021; Sung et al., 2022) in order to avoid catastrophic forgetting (De Lange et al., 2021) . Based on these approaches, we propose to adapt a strong text-only MT model with lightweight modules in order to exploit the large amount of data it previously learned.\n\nWhich type of visual features in MMT systems?\nIn terms of how images are represented in multimodal models, different strategies exist. Many works first proposed to incorporate global visual features from object recognition models pretrained Frozen with trainable adapters\nv n t 1 t n <DE> CLIP emb.\nTwo ___ wearing ___ .\n\nVisual projection Embedding men hats\nVisually Conditioned MLM\nv 1 v n t 1 t n\n\n<EN>\nFigure 2 : Overview of our approach, multimodal MT (MMT) (left) and visually-conditioned masked language modeling (VMLM) (right) objectives. We train VGAMT on both objectives jointly.\non ImageNet (Deng et al., 2009) , such as ResNet50 (He et al., 2016) , either in the form of a single vector or a set of features (Calixto et al., 2017; Elliott and K\u00e1d\u00e1r, 2017; Calixto and Liu, 2017; Yao and Wan, 2020; Helcl et al., 2018) . More recent global features extractor such as CLIP (Radford et al., 2021) exist, but to our knowledge have not been used in MMT models. Extending this idea, other works focused on entities in the image and extracted bounding boxes using a pretrained Faster R-CNN (Ren et al., 2015) in order to introduce more semantic visual information into MT (Gr\u00f6nroos et al., 2018; Ive et al., 2019; Caglayan et al., 2021) . Recent efforts have been made to only select parts of the image that are relevant to the translation of the sentence. Some proposed to use a more selective attention mechanism between modalities (Liu et al., 2021; Ye et al., 2022) , while others suggested extracting other types of visual features (Huang et al., 2021b; Fang and Feng, 2022) . Based on this, Yin et al. (2020) decided to exploit local image-text correspondences in their model Graph-MMT. Similar to their approach, we use a simpler method to extract relevant visual features, using the output queries from a state-of-the-art free-form text object detector MDETR (Kamath et al., 2021) as our local visual features (in addition to global features from CLIP). \n\nOur approach: VGAMT\nThe two main aims of our approach are to (i) exploit a maximum available data (not just multimodal parallel text data) and to (ii) provide an effective way to combine image and text modalities. Our approach, shown in Figure 2 , consists in taking a strong text-only MT model 4 and adapting it to multimodal MT. To adapt this strong text-only model to multimodal inputs, we add several lightweight modules-bottleneck adapters (Houlsby et al., 2019) and linear visual projection layers-to the otherwise frozen initial model. The bottleneck adapters are lightweight linear layers introduced after each attention block and each feedforward layer to project embeddings down before projecting them up.\nIn terms of representing visual information, we choose to use two types of representation. We concatenate local (MDETR) features and global (CLIP) features to the text inputs. We choose to use global features too, since the source sentence can describe more general aspects of the image than mere objects (such as scenes). We jointly train the non-frozen parts of our model on two distinct objectives: multimodal MT (MMT) and visuallyconditioned masked language modelling (VMLM), as described in Section 3.1. We also introduce a guided self-attention to exploit image information in a straightforward manner (see Section 3.2) in the encoder (while the decoder uses regular self-and cross-attentions and can only attend to embeddings related to text positions). We call our approach Visually Guided and Adapted Machine Translation (VGAMT).\n\nCombining training objectives\nAs shown in Figure 2 , we jointly train VGAMT on two objectives: visual masked language modelling (VMLM) and multimodal MT (MMT). VMLM (resp. MMT) consists in predicting masked tokens (resp. translating the sentence) conditioned on the image. 5 The use of the VMLM objective in addition to MMT ensures that the model does not learn to ignore the visual inputs when translating (since Multi30k is mainly composed of very standard and unambiguous parallel sentences). We make sure to mask a high percentage (25%) of the text inputs so that the model is forced to attend to the image when producing translations.\n\nGuided self-attention\nThe backbone of VGAMT is an encoder-decoder MT model, in which image features are concatenated to textual input embeddings and shared selfattention is used over the two input modalities (see Figure 2 ). Instead of using full self-attention (Caglayan et al., 2021) (connections between all image parts and all text tokens), we introduce guided self-attention. Guided self-attention consists in masking irrelevant connections between text and image representations; each text (resp. image) embedding can attend to itself and all other text (resp. image) positions, but can only attend to image (resp. text) positions conditioned on pre-extracted textimage alignments. We obtain these alignments (in the form of a cross-modal correspondence matrix) using MDETR (Kamath et al., 2021) , which detects image regions and corresponding text spans based on a free-form text (see Figure 3 and Appendix B for more details).\nConcretely, let Q, K and V denote the learnable query, key and value parameters of a standard self-attention mechanism. Attention can be defined as Attention(Q, K, V ) = A \u2022 V , where the attention matrix A = (a ij ) is defined as\nA = softmax QK T / \u221a d k\n, where d k is the dimension of the key vector, i.e.:\na ij = e Q i K T j / \u221a d k l e Q i K T l / \u221a d k (1)\nThe idea behind our guided self-attention mechanism is that we want to allow subwords to attend to all subwords, all bounding boxes to attend to all bounding boxes, but to only allow cross-modal attention between a subword and bounding boxes that are linked by MDETR (see Figure 3 ). We therefore define a binary masking matrix C = (c ij ) where (i) c ij = 1 if indices i and j correspond to embeddings coming from the same modality, and (ii) c ij is provided by the MDETR matrix otherwise: it is 1 if MDETR has created a link between subword (resp. bounding box) i and bounding box (resp. subword) j. Once this guiding matrix C is defined, we can replace the standard attention (1) with our guided attention:\nEQUATION\n)\nThe main advantage of guided self-attention over full self-attention is that the model does not have to learn to ignore irrelevant text-image correspondences since alignments are introduced as a prior.\n\nContrastive Multilingual Multimodal Translation Evaluation (CoMMuTE)\nTo overcome the flaws of existing benchmarks (see Section 5.2), we introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation dataset 6 . It is composed of 155 lexically ambiguous sentences in English, each associated with two translations corresponding to two of the possible meanings of each sentence and two images that determine which of the translations is correct. It covers English\u2192French, English\u2192German and English\u2192Czech. An example is given in Figure 4 . 2018), and we created the remaining ones. 7 We collected two images for each sentence under Creative Commons license (either Google Images or our own photos), so that the image illustrates without ambiguity one of the two meanings of the sentence. We do not restrict the image-text relation to be strictly descriptive (as for image captions) in order to have a more general evaluation dataset. Each sentence was translated into two possible translations (each corresponding to one of the images) by a native speaker of the target language.\nAppendix A provides some basic statistics.\nThe idea of CoMMuTE is to use MMT models to rank each of the two translations based on image information. The perplexity of a sentence for a given model is defined as: P P L q (y) = N i=1 q(y i ) \u2212 1 N , where q is the probability distribution output by the model, N is the sequence length and y 1 , . . . , y N is the sequence of tokens. Now, let y 1 , . . . , y N 1 be the sequence of tokens of the correct translation and y \u2032 1 , . . . , y \u2032 N 2 the sequence of tokens of the incorrect translation, a model makes a correct prediction if P P L q (y) \u2264 P P L q (y \u2032 ). i.e. the model considers the correct translation more likely than the incorrect one. For each example, we rank each of the translations based on each of the images (2 comparisons per example), and report the accuracy over all the examples. As CoMMuTE is perfectly balanced, a text-only model will get exactly 50% accuracy on this task. 5 Experiments\n\nText-only data\nAll our experiments are based on the strong MT model mBART 8 (Liu et al., 2020) , which we finetune on parallel text (see Table 1 ). We use Open-\n1 2\nWe'll have to get rid of that mole.\nIl va falloir enlever ce grain de beaut\u00e9. Subtitles2018 9 (Lison et al., 2018) , Wikipedia (Wo\u0142k and Marasek, 2014) , Ted Talks (Reimers and Gurevych, 2020) and the Books datasets (Tiedemann, 2012). We preprocess the data using Moses scripts (Koehn et al., 2007 ). 10\n\nMultimodal data\nTest2016 Test2017 MSCOCO Ambiguous (%) 21 (2.1%) 20 (2%) 6 (1.3%) (2017) and Barrault et al. (2018) released two additional related test sets (Test2017 and Ambiguous Coco). However, on analysis of these sets and as shown in Table 2 , we found that very few examples are image-dependent (i.e. the source sentence is ambiguous and the image is required to solve the ambiguity in the target language), 11 meaning that an MMT system is unlikely to perform better than a text-only system. Moreover, most of these ambiguities are semantically similar and they only cover a few multi-sense words. Although Ambiguous Coco (Elliott et al., 2017) is designed to be an ambiguous test set as it is built around multi-sense verbs, it was automatically created from sentences from MSCOCO (Lin et al., 2014) for which the textual context is often sufficient for disambiguation. These benchmarks remain useful to make sure MMT systems do not perform worse than text-only MT models on examples where images are not necessary to translate correctly. However, we consider them insufficient to assess how well MMT systems exploit images to improve translation.\nMonolingual multimodal data. For the VMLM objective, we train our model on the Conceptual Captions (CC) dataset (Sharma et al., 2018) composed of 3.3M 12 images aligned with English text.\n\nImplementation details\nFor all our experiments, we use the mBART implementation from Hugging Face (Wolf et al., 2020) . Experiments with adapters used bottleneck adapters (Houlsby et al., 2019) with a reduction factor of 8 and ReLU activation (Agarap, 2018).\nWe use the implementation provided by adaptertransformers (Pfeiffer et al., 2020) . We use a batch size of 512, the Adam optimiser (Kingma and Ba, 2014) with \u03b2 1 = 0.9, \u03b2 2 = 0.99 and a learning rate of 10 \u22124 for En\u2192Fr and 10 \u22125 for En\u2192{De,Cs}.\nWe also applied 0.1 label smoothing (Szegedy et al., 2016) during training. We selected our final model according to the best BLEU score (Papineni et al., 2002) on the Multi30k dev set after at least one full pass over the Multi30k and Conceptual Captions training sets. We ran each experiment 3 times with different seeds and report the average BLEU 13 (Papineni et al., 2002) and COMET (Rei et al., 2020 ) scores 14 and the standard errors. We also report METEOR scores (Banerjee and Lavie, 2005) in Appendix E. All experiments were carried out on 8 NVIDIA V100 GPUs for \u223c15h.\n\nBaselines\nWe consider several text-only and multimodal baselines. All baselines except the MT models finetuned from mBART were trained from scratch with the original codebases and features released by the papers' authors. Models trained on the (multimodal) MT objective only where trained on Multi30k, while models jointly trained on the (multimodal) MT and (V)MLM objectives were trained on Multi30k and Conceptual Captions.\nText-only. We trained a text-only Seq2seq Transformer (Vaswani et al., 2017) from scratch and a text-only Seq2Seq Transformer initialised from TLM weights (Conneau and Lample, 2019) . We refer to these models as Vanilla MT and TLM + MT respectively. We also trained several MT models initialised from pretrained mBART (Liu et al., 2020 ) and which we fine-tuned on parallel data (Lison et al., 2018; Wo\u0142k and Marasek, 2014) . We refer to these models as mBART + MT. 'w/ adapters' specifies that the model's weights are frozen except bottleneck adapters (Houlsby et al., 2019) .\nMultimodal. We trained several state-of-the-art multimodal MT models: Graph-MMT (Yin et al., 2020) , Gated Fusion (Wu et al., 2021) and a Seq2Seq Transformer trained from VTLM weights (Caglayan et al., 2021) (hereafter VTLM + MMT).\n\nResults and Analysis\nQuatre cyclistes font une course sur un parcours avec une foule en arri\u00e8re-plan.\nQuatre motards font une course sur un parcours avec une foule en arri\u00e8re-plan. Tables 3 and 4 show BLEU, COMET and accuracy scores for all models compared on several En\u2192{Fr,De,Cs} test sets including CoMMuTE. An initial observation is that the text-only model is a strong baseline on the three standard benchmarks (Test2016, Test2017 and MSCOCO). As mentioned in Section 5.2, most of these evaluation datasets do not need visual context to be correctly translated. Our model VGAMT is on average on par with its counterpart text-only mBART+MT w/ adapters baseline for all Multi30k En\u2192Fr test sets, while being on average just below this baseline on En\u2192{De,Cs} Multi30k benchmarks. It outperforms other MMT models with a large margin due to both the effective use of textual knowledge from the frozen MT model but also guided self-attention. Note that the scores reported for the baselines are lower than the ones reported in the original papers of the models for several reasons. First, we computed the scores on fully detokenised data to have a uniform evaluation between all models. We also report the average score from three different runs using different seeds and not the best score obtained over a single run. More importantly, our VGAMT obtains strong improvements over both text-only baselines and state-of-the-art MMT systems on CoMMuTE; our model can use visual context to disambiguate sentences. This can be seen in Figure 5 (one of the \n\nAblation Study\nTo better understand the role of VGAMT's components, we carry out several ablations for En\u2192Fr and report all results in Table 5 .\nAdapters versus Fine-tuning. We compare the results of fine-tuning an unfrozen VGAMT model (w/o adapters) in comparison to our frozen model with adapters (VGAMT), all other things remaining equal. The unfrozen version faces a drop in scores on all test sets except Test2017. Notably, the unfrozen model's accuracy score of 60.5 on CoM-MuTE is 6.6 points lower than our final VGAMT model. As well as providing a more lightweight solution that does not involve fine-tuning all parameters, using neural adapters and freezing other weights is useful in terms of performance.\nImpact of the VMLM objective. VMLM sampling probability and degree of masking. We ran experiments to vary the VMLM sampling probability (see Section 3.1) and the percentage of masked text inputs (see Figure 7 for results on CoMMuTE). For the sampling between VMLM and MMT objectives, the maximum value is reached for p =50%, i.e. equal sampling between VMLM and MMT objectives (Figure 7a ). Similar results are obtained for p = 75%, i.e. 3 VMLM batches for 1 MMT batch, but the translation quality is lower. For the percentage of masking, there is a peak at 25% masked text inputs and a constant decrease for higher values (Figure 7b ).\n\nConclusion\nWe propose a new MMT approach (VGAMT) based on (i) adapting a strong text-only MT model with lightweight adapters and (ii) introducing better use of the text and image modalities through a novel guided self-attention mechanism and joint MMT and VMLM training. We also introduce \n"}
{"question": "Why do we run experiments on all SLU datasets with and without each component?", "evidence": "  To better see the impact of including semantic representations, we visualize the pooled audio snippet embedding for intent classification on SLURP-IC using t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton, 2008) or incorrectly (\u2717).  To better understand the semantic information captured by SSE, we study predictions made by both HUBERT and SSE-TUNE (HUBERT) on SLURP-IC's test set.  To more carefully analyze the affect of residual attention and adapters in SSE-TUNE, we run experiments on all SLU datasets with and without each component.  To better quantify the performance improvement introduced by SSE, we compare against 2 specialized SLU models that do not abide by the universal representation framework. ", "options": ["A. To better see the impact of including semantic representations.", "B. To better understand the semantic information captured by SSE.", "C. To analyze the affect of residual attention and adapters in SSE-TUNE more carefully.", "D. To better quantify the performance improvement introduced by SSE."], "answer": "C", "content": "\nIntroduction\nRealizing artificial intelligence (AI) that can understand and respond to spoken language is a north star for many speech and natural language processing (NLP) researchers. A particularly effective framework for this is the encoder-decoder architecture, where an encoder represents input audio signals as high-dimensional embeddings and a decoder converts said embeddings to outputs for different downstream tasks. Benchmarks for such systems include spoken language understanding, where intent, named entities, or slot values are predicted from input utterances (Yang et al., 2021; Bastianelli et al., 2020; Shon et al., 2022) , and spoken question answering, where the start and end frames of an input audio passage answering an input audio question are predicted (Lin et al., 2022a) .\nA particularly notable setup of the encoderdecoder framework is the universal representation setup (Yang et al., 2021) , where a shared selfsupervised speech encoder is pretrained upstream once and frozen for all downstream tasks, then a different lightweight decoder is fine-tuned on each downstream task. This setup is appealing for building speech systems as maintaining a separate large specialized model for every task is not computationally efficient. The universal representation setup has been widely adopted in other areas of research, such as computer vision (Goyal et al., 2019; Ericsson et al., 2021) and NLP (Rogers et al., 2020; Qiu et al., 2020) , and production when there are many downstream tasks or domains (Molino et al., 2019) . The current state-of-the-art speech encoders under this setup are W2V2 and HUBERT (Yang et al., 2021; Baevski et al., 2020; Hsu et al., 2021) , which are transformer-based models trained with self-supervised learning (SSL) on raw audio and have achieved impressive performance on various tasks.\nRecently, analytical works found SSL speech encoders capture primarily acoustic, not semantic, information (Pasad et al., 2021) . Thus, researchers proposed end-to-end systems (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) that introduce semantic information through large language models (LLMs), such as ROBERTA (Liu et al., 2019) or BART (Lewis et al., 2019) , which are pretrained to capture language semantics (Clark et al., 2019) . This is typically accomplished by the pipeline approach (Bastianelli et al., 2020) , which passes audio input through the SSL speech encoder, then bridge module, then LLM. The bridge module converts speech encoder embedding outputs into LLM token inputs (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) .\nUnsupervised ASR models (ASR-U) (Liu et al., 2020b; Baevski et al., 2021; Liu et al., 2022) have also seen recent success. The state-of-the-art ASR-U model uses generative adversarial networks (GANs) (Goodfellow et al., 2020) to generate text transcription from input audio (Liu et al., 2022) .\nCurrent works combining SSL speech encoders and LLMs do not satisfy the universal representation framework, as they either (1) rely on ASR data on the downstream task, which is expensive to collect and maintain, (2) are not lightweight, requiring training the whole system end-to-end, or (3) are not general, as they do not consider a wide variety of downstream tasks (Lugosch et al., 2019; Rao et al., 2021; Lin et al., 2022a; Seo et al., 2022) . Similarly, ASR-U was proposed for speech recognition and the focus is not improving SSL speech encoders (Baevski et al., 2021; Liu et al., 2022) .\nWe propose introducing Semantics into Speech Encoders, SSE, a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. Concretely, SSE adopts the pipeline approach to obtain semantic embeddings, with an ASR-U bridge connector to extract information from LLMs. As ASR-U is inherently noisy, SSE introduces attention residual connection (He et al., 2016; Vaswani et al., 2017) between the speech encoder and LLM. SSE also efficiently aligns the LLM with the speech encoder through adapter modules (Houlsby et al., 2019) . SSE improves W2V2 (Baevski et al., 2020) and HUBERT (Hsu et al., 2021) on 3 SLU tasks across 3 datasets, all under the universal representation setup. SSE also outperforms state-of-the art no-ASR method, DUAL (Lin et al., 2022a) , in SQA.\nWhile recent works use ASR-U to augment existing speech encoders with phoneme-level LLMs (Feng et al., 2022; Meng et al., 2022; Shi et al., 2022; Hsu et al., 2022) , subword-level LLMs contain much more pertinent and measurable semantic information (Clark et al., 2019) . Other works in SQA rely on clustering to assign audio frames to frequent subword tokens, but this requires heavy finetuning on the downstream task (Lin et al., 2022a) .\nTo the best of our knowledge, we are the first to propose a task-agnostic SSL speech encoder which directly interfaces with subword-based LLMs, unblocking many other applications and future work in this domain. To this end, attention residual con-nections and adapters are essential to successfully extracting semantic information from noisy intermediary transcriptions. We summarize our contributions below:\n\u2022 We propose using ASR-U components to augment SSL speech encoders for generating subword tokens with semantic information.\n\u2022 The augmented SSL speech encoders can be connected with powerful LLMs seamlessly and yields state-of-the-art performance under the universal representation setup.\n\u2022 We show attention residual connections and adapters are essential to combining and aligning speech and text encoders.\n2 Related Works 2.1 Self-Supervised Speech Encoders SSL speech encoders (Liu et al., 2020a; Chung et al., 2020a; Ling and Liu, 2020; Liu et al., 2021 Liu et al., , 2020c;; Chung et al., 2019; Baevski et al., 2019; Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Qian et al., 2022; Zhang et al., 2022) are trained to learn and reconstruct pooled clustered representations of input audio from the original audio. The intuition for this objective comes from linguistics, where speech can be broken down into phoneme groups, where different chunks of input audio represent different phoneme groups.\nW2V (Schneider et al., 2019) trains a convolutional neural network model to reconstruct the quantized cluster representations. W2V2 (Baevski et al., 2020) uses transformers and a discrete codebook quantization module. HUBERT (Hsu et al., 2021) improves W2V2 by disentangling the clustering and SSL objectives and using a BERT-style encoder (Devlin et al., 2018) . The speech processing universal performance benchmark (SU-PERB) (Yang et al., 2021; Lin et al., 2022b; Tsai et al., 2022) shows SSL speech encoders are the most effective method for solving multiple downstream tasks with minimal fine-tuning. A recent analytical work finds SSL speech encoders successfully encode acoustic information, but lack semantic information (Pasad et al., 2021) . In response, CONTENTVEC (Qian et al., 2022) propose disentangling the speaker and semantic content of audio via an SSL objective. SPEECHLM (Zhang et al., 2022) propose training a multi-modal speech and text encoder.\n\nLarge Language Models\nIn contrast to speech encoders, pretrained LLMs are shown to capture rich semantic information (Clark et al., 2019) . These methods optimize variants of the masked language modeling (MLM) objective to train a large transformer model. BERT (Devlin et al., 2018) uses MLM to learn a transformer encoder. ROBERTA (Liu et al., 2019) introduces dynamic masking and a larger text corpus. BART (Lewis et al., 2019) supports generative modeling and adds a denoising objective, making it less susceptible to noisy text inputs. LONG-FORMER (Beltagy et al., 2020) is pretrained for long documents by increasing the document length limit during pretraining. LLMs have been successfully integrated with speech models for specific semantic tasks (Chung et al., 2020b; Kim et al., 2021; Qian et al., 2021; Le et al., 2022; Seo et al., 2022; Lin et al., 2022a) , but not under the universal representation framework.\n\nTask-Specific Speech Models\nTask-specific SLU systems outperform generic SSL speech encoders typically by using a LLM. These systems rely on ASR data to reliably interface the LLM. LUGOSCH (Lugosch et al., 2019) trains a LSTM bridge module to convert audio features into phonemes then text. CTI's (Seo et al., 2022) bridge module uses ASR logits to compute a weighted average of token embeddings. In addition to improving the bridge module, other works attempt to also distill LLM embeddings into speech representations (Chung et al., 2020b; Cha et al., 2021; Kim et al., 2021; Agrawal et al., 2022) . For optimizing targeted metrics, researchers have also experimented with reinforcement learning (Rao et al., 2021) . While combinations of these methods achieve impressive performance, they do not satisfy the universal representation setup.\n\nUnsupervised ASR\nRecent work show the viability of unsupervised speech recognition. W2V2-U (Baevski et al., 2021) accomplished this by running Principal Component Analysis (PCA), k-means clustering, and mean pooling to convert W2V2 (Baevski et al., 2020) features into phoneme-granularity features, then trains a GAN model to output phoneme text from the post-processed model (Baevski et al., 2021) . The state-of-the-art method for phoneme-level unsupervised ASR is W2V2-U2.0 (Liu et al., 2022) which directly trains a CNN to output phonemes from W2V2 features and uses a reconstruction loss to tie the input audio with corresponding generated text. Both methods use WFSTs to decode the phonemes into raw text. While there have been preliminary attempts (Feng et al., 2022; Meng et al., 2022) to use W2V2-U2.0 with phoneme language models 1 , we are the first to combine it with semantically-rich subword-based LLMs.\n\nAdapters\nAdapters are intermediary layers added to a large pretrained encoder. Adapter weights are learned during fine-tuning while the rest of the pretrained model is frozen. Adapters serve the dual purpose of efficient fine-tuning and preventing overfitting. First used by computer vision researchers (Rebuffi et al., 2017) , adapters now enjoy much success in the natural language processing community by efficiently tuning LLMs (Houlsby et al., 2019) . In particular, the multilingual speech translation community found that adapters can effectively align SSL speech encoders and LLMs for spoken translation tasks (Li et al., 2020; Le et al., 2021) .\n\nProposed Method\nWe propose to introduce semantics into SSL speech encoders by using ASR-U to interface with LLMs. Section 3.2 describes how to use ASR-U to link a speech encoder with a LLM. Section 3.3 describes how to combine both acoustic and semantic information and deal with ASR transcriptions errors. Finally, Section 3.4 describes how to align LLMs with the speech encoder for downstream tasks.\n\nProblem Setting\nFollowing the universal representation framework (Yang et al., 2021) , our model consists of a large speech encoder, E : X \u2192 Z, mapping input audio, X \u2208 X , to embeddings, Z \u2208 Z, and a light-weight task decoder,\nD \u03c9 : Z \u2192 Y \u03c9 , mapping embeddings to downstream task outputs, Y \u03c9 \u2208 Y \u03c9 .\nThe speech encoder, E, is pretrained once, then shared on all downstream tasks. The task decoder, D \u03c9 , is fine-tuned on its respective task, \u03c9 \u2208 \u2126.\nDuring fine-tuning, the majority of model weights are frozen. This ensures the model can be efficiently stored and deployed.\nDuring pretraining, the speech encoder is trained on unlabelled audio, X \u2208 X , and unlabeled text, T u \u2208 T u . During finetuning, the model is trained on the labelled downstream dataset, (X, Y \u03c9 ) \u2208 X \u00d7 Y \u03c9 . Notice, costly labelled ASR data is not required during pretraining or finetuning.\n\nUnsupervised Semantic Representation as a Bridge\nTo incorporate semantic information into SSL speech encoders, E : X \u2192 Z, we wish to leverage subword-based LLMs, M : S \u2192 Z, that capture language semantics (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Beltagy et al., 2020) . The major challenge is the mismatch of input spaces. Speech encoders take raw audio as input, X \u2208 X . LLMs take subword tokens as input, S \u2208 S. SSE uses W2V2-U2.0 (Liu et al., 2022) as a bridge module (Seo et al., 2022) , B : Z \u2192 S, to convert speech encoder embedding output into LLM subword tokens in a pipelined approach,\nE SSE = E \u2022 B \u2022 M.\nFollowing W2V2-U2.0, the bridge module, B uses a GAN (Goodfellow et al., 2020) We also add an upsampling layer, U : Z \u2192 Z to make the sequence length of the LLM output match the speech encoder output, such that E and E SSE share the same output space.\nWe choose the 15th layer of the W2V2 (Baevski et al., 2020) as our speech encoder, as the last layers overfit the self-supervised training objective hence providing worse acoustic representations (Fan et al., 2020; Baevski et al., 2021; Pasad et al., 2021) . We choose BART (Lewis et al., 2019) as our LLM, as it is trained to denoise noisy input subword tokens, and we expect the bridge module to introduce some noise. We call this version of our model SSE-BASE. A depiction can be found in Figure 1a .\n\nCombining Semantics and Acoustics with Residual Attention\nWe hypothesize certain tasks may require more acoustic information than others. to implicitly transcribe parts of the input speech, a primarily acoustic task. Since the pipelined model may suffer from transcription errors introduced by ASR-U, naively using the pipelined approach introduces an information bottleneck at the bridge module. Hence, we propose adding a residual connection (He et al., 2016) between SSE-BASE and the speech encoder, E. This can be done in two ways: (1) upsampling semantic embeddings and concatenating with speech embeddings, Z = [Z E ||U(Z M )], or (2) using multihead attention (Vaswani et al., 2017) to merge the two embeddings, Z =\n[Z E ||MHA(Z E , Z M , Z M )],\nwhere Z E \u2208 Z is the output of the W2V2L15 (Baevski et al., 2020) and Z M \u2208 Z is the output of BART (Lewis et al., 2019) . The former is a simpler but more naive method. The latter is more effective as the attention layers are able to learn the alignment between speech and semantic embeddings. Notice, (2) introduces more learnable parameters to the finetuning-step, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder.\n\nAligning Pretrained Text Model with Adapters\nInspired by works from speech translation (Li et al., 2020; Le et al., 2021) , we hypothesize that the LLM can easily be adapted for speech tasks through the use of adapters. We adopt the general recipe for adapters, where an adapter (Houlsby et al., 2019) , composed of a LayerNorm and 2-layer ReLU neural network, is added to the end of each feed forward layer in the LLM and finetuned on downstream tasks. This introduces additional parameters to finetuning, but we find the number of new parameters inconsequential compared to the size of the lightweight decoder. We call the model using both residual attention and adapters SSE-TUNE, and outline it in Figure 1b .\n\nExperiments 4.1 Dataset\nTo show the effectiveness of introducing semantics into speech encoders, we evaluate 3 SLU tasks, intent classification (IC), slot filling (SF), and named entity recognition (NER), and SQA \n\nSpoken Language Understanding\nTo show SSE improves SSL speech encoders, we augment two state-of-the art speech encoders under the universal representation setup: W2V2 and HUBERT. Following prior works that found intermediary layers of W2V2 contain better representations (Pasad et al., 2021; Baevski et al., 2021) , we consider the 15th layer and the last layer of W2V2, named W2V2L15 and W2V2L24 respectively. As mentioned in Section 3, we show 2 versions of our model, SSE-BASE and SSE-TUNE. The former uses the pipelined approach to connect W2V2L15 with BART (Lewis et al., 2019) with no additional modifications. The latter introduces an attention residual connection and learnable adapters to combine acoustics and semantics together and align the LLM with the speech encoder respectively. We either connect the residual connection to the output of W2V2L15, yielding SSE-TUNE (W2V2L15), or to the output of HU-BERT, yielding SSE-TUNE (HUBERT).\nTo show the importance of using LLMs, we compare against 2 very recent approaches for improving SSL speech encoders without LLMs, SPEECHLM (Zhang et al., 2022) and CON-TENTVEC (Qian et al., 2022) . As HUBERT-BASE was used as the base speech encoder by both baselines, we also provide results where SSE-TUNE is used to augment HUBERT-BASE.\n\nSpoken Question Answering\nTo show the effectiveness of SSE, we compare it against DUAL (Lin et al., 2022a) , the state-ofthe-art SQA model which does not use ASR data. While both SSE and DUAL obtain frame-level tokens from speech input, SSE uses ASR-U to obtain its tokens, whereas DUAL uses clustering. As a result, SSE's output tokens exists in the LLM's existing vocabulary, whereas DUAL's output tokens does not. Hence, DUAL must retrain the LLM on its output tokens.\nWe compare DUAL to the closest analogous SSE model, which is SSE-BASE but with adapter layers, SSE-BASE (ADAP). Similar to DUAL, both methods modify the LLM weights. Unlike DUAL, SSE-BASE (ADAP) is lightweight, tuning only around 10% of the total parameters. To produces framelevel predictions, we remove the upsampling layer from SSE-BASE (ADAP). We choose W2V2L15 as our speech model and BART as our LLM, as it is robust to ASR errors.\nWe also show a PIPELINE model, which trains a W2V2 model on ASR data and a LONGFORMER LLM on text-only question answering data. It is worth noting that since evaluation is based on the frame-level, SSL speech encoders are not a baseline since they operate at the audio level.\n\nDecoder Setup\nTo satisfy the universal representation setup, we adopt lightweight SLU decoders from SU-PERB (Yang et al., 2021) is sum pooling followed by a multilayer perceptron classifier trained with cross entropy loss. For the SF and NER tasks, the decoder is recursive neural network (RNN) that transcribes input audio into text. The decoder identifies named entities or slot values by surrounding them with named special tokens and is trained with connectionist temporal classification loss. For SQA, we adopt the same decoder as DUAL (Lin et al., 2022a) , which is a linear layer classifying each subword embedding as the start or end or neither of an answer span.\n\nImproving SSL Speech Encoders\nAs seen in Table 2 , SSE significantly improves the SLU performance of both W2V2 and HU-BERT, confirming that including semantic information drastically improves existing SSL speech encoder performance. Specifically, SSE-TUNE (W2V2L15) improves W2V2L15 on all tasks. SSE-TUNE (HUBERT) improves HUBERT on 3 out of 4 tasks, and is the best performing model overall. Comparing SSE-TUNE with SSE-BASE shows residual attention and adapters effectively counteracts bridge module transcription errors. The relative performance gain for IC is more than SF or NER. Unlike IC, both SF and NER require the speech encoder to transcribe identified audio snippets, and transcription is a primarily acoustic task. Hence SF and NER require less semantic information than IC. Nevertheless, combining both acoustic and semantic information, as done by SSE-TUNE, provides the most consistent performance improvement, since the skip connection can learn which type of information is more needed.\n\nImportance of LLMs\nAs seen in Table 2 , SSE-TUNE (HUBERT-BASE) outperforms alternative approaches augmenting speech encoders, SPEECHLM (HUBERT-BASE) and CONTENTVEC (HUBERT-BASE). Unlike these alternative approaches, SSE-TUNE incorporate information from LLMs, which we found to be very beneficial for capturing semantic information as they are carefully pretrained objectives on large amounts of unlabelled text data.\nIt is noteworthy that SSE-TUNE is a general framework which can augment any speech encoder of our choice, including SPEECHLM and CON-TENTVEC. Similarly, SSE-TUNE can directly integrate new LLMs without costly pretraining. We leave incorporating such encoders into SSE-TUNE as future work.\n\nSpoken Question Answering\nAs seen in Table 3 , SSE outperforms recent unsupervised clustering-based approaches, DUAL. In contrast to DUAL's HUBERT cluster tokens, SSE's ASR-U tokens are better aligned with LLMs and share the same space. Thus, SSE can better utilizes pretrained LLMs. Furthermore, SSE does not require carefully tuning the number of HUBERT cluster counts, as the vocabulary size of the LLM is fixed and consistent with ASR-U.\n\nChoice of Language Model\nWe find subword-based LLMs contain more information than phoneme-based LLMs (Clark et al., 2019) . We empirically verify this by replacing our subword-based LLM, BART (Lewis et al., 2019) , with popular character-based LLM, ByT5 (Xue et al., 2022) , and phoneme-based LLM, T5lephone (Hsu et al., 2022) in SSE-BASE. As seen in Table 4 , the subword-based LLM perform the best as each subword token is more semantically meaningful than a phoneme or character. We believe T5lephone outperforms the Byt5 as it has better robustness to ASR-U errors. Overall, subword-based LLMs are the best choice for embedding semantic information in transcribed text.\n\nResidual Attention and Adapters\nTo more carefully analyze the affect of residual attention and adapters in SSE-TUNE, we run experiments on all SLU datasets with and without each component. We denote these two design choices as (ResAtt) and (Adap) respectively. As seen in Table 4, both components provide ample performance improvement over SSE-BASE.\nWe also try the naive residual connection approach described in Section 3.3 by directly concatenating the LLM upsampled semantic embeddings to the speech embeddings. We call this approach SSE-BASE (RES). This method is less effective than SSE-BASE (RESATT) as it does not learn how to align speech and semantic embeddings, but improves SSE-BASE, further validating our hypothesis that merging acoustic and semantic information is beneficial.\nAs seen in parameter breakdown for the SSE-TUNE (W2V2L15) model in Table 1 , the number of new learnable parameters introduced by (Re-sAtt) and (Adap) are unsubstantial compared to the size of the lightweight downstream decoder. Specifically, the downstream task decoder accounts for 9.60% of the total model parameters. SSE-TUNE introduces only 10.47% more parameters than SSE-BASE during fine-tuning and 0.91% to the total model parameter count, but often provides significant performance improvement.\n\nComparison with Supervised ASR Methods\nTo quantify the effect of transcription errors introduced by the bridge module, we compute the word error rate (WER) of the bridge connector in SSE-TUNE, and compare it against standard W2V2 supervised ASR models (Baevski et al., 2020) trained on 10 minutes, 100 hours, and 960 hours of labeled ASR data. indicating the effectiveness of the bridge module.\nOn SLURP and SLUE, the relative drop in WER (> 20%) is substantially more than the relative drop in downstream performance (< 5%), verifying SSE-TUNE's tolerance to noisy transcriptions. The robustness to ASR errors come from our choice of LLM, BART, which is trained to handle noisy inputs, residual connection to acoustic embeddings, and LLM alignment with adapters.\n\nComparison to Specialized SLU Models\nTo better quantify the performance improvement introduced by SSE, we compare against 2 specialized SLU models that do not abide by the universal representation framework: Kaldi+HerMiT, which is a pipelined Kaldi ASR (Povey et al., 2011) and HerMiT NLU (Vanzo et al., 2019) model reported in the SLURP paper (Bastianelli et al., 2020) , and CTI (Seo et al., 2022) , which is an end-to-end pipelined W2V2 (Baevski et al., 2020) ASR and ROBERTA (Liu et al., 2019) NLU model. To the best of our knowledge, CTI is the state-of-the-art SLU model. In addition to unlabelled text, unlabelled audio, and downstream data, both Kaldi+HerMiT and CTI require 40 hours of downstream SLURP ASR data (Bastianelli et al., 2020) . Kaldi+HerMiT requires an additional 24,000 hours of ASR data (Povey et al., 2016) . CTI requires an additional 960 hours of ASR data (Panayotov et al., 2015) . Neither use lightweight fine-tuning. Thus, such specialized SLU models are less general, more expensive, and require much more data. As seen in Table 6 , SSE helps bridge the gap between tailormade models and more practical SSL speech encoders. We believe ASR-U errors plays a major role in the remaining gap, as the ASR-supervised Kaldi+HerMiT and CTI models have WER of 16.20% and 16.67% respectively, compared to A mix-up is when the model either misclassifies label \"A\" as \"B\" or misclassifies label \"B\" as \"A\". For each mix-up, we compute the percentage of less mistakes made by SSE-TUNE (HUBERT) than HUBERT. For example, SSE-TUNE (HUBERT) misclassifies calendar_set as calendar_query or vice-versa 20% less frequently than HUBERT. The \"general-quirky\" label is assigned to Out-of-Distribution inputs.\nSSE's ASR-U bridge with a WER of 51.51%.\n\nError Analysis\nTo better understand the semantic information captured by SSE, we study predictions made by both HUBERT and SSE-TUNE (HUBERT) on SLURP-IC's test set. We find HUBERT errors are made primarily between intents within the same or similar domains (e.g. calendar_set vs calendar_query).\nThe performance bottleneck lies with distinguishing finer-grained in-domain intents. Table 7 shows that SSE-TUNE is better at differentiating finergrained intents. SSE-TUNE's misclassifications come primarily from errors made by its ASR-U bridge component. As seen in Table 8 , the ASR-U WER of incorrect predictions made by HUBERT is much lower than that of incorrect predictions made by SSE-TUNE. When ASR-U returns resonable transcriptions (typically <50% WER), SSE-TUNE can correctly classify inputs that HUBERT cannot. Hence, the effectiveness of SSE is tightly coupled with the effectiveness of ASR-U.\n\nRepresentation Visualization\nTo better see the impact of including semantic representations, we visualize the pooled audio snippet embedding for intent classification on SLURP-IC using t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton, 2008) or incorrectly (\u2717). We denote the number of pairs belonging to each subset, in the thousands, in parentheses. denote the ground truth label of each audio snippet by the color of its pooled embedding. As seen in Figure 2 , the clusters produced by semantic embeddings are more spread out and better separated than those produced by just acoustic speech embeddings, indicating that SSE introduces new semantic information that existing speech encoders lack.\n\nConclusion\nWe presented a compelling case for introducing semantics into SSL speech encoders and an effective method of doing so. Our approach boosts the performance of existing speech encoders on multiple SLU and SQA tasks and datasets. We provide reasoning for what tasks may benefit more or less from incorporating semantics. Furthermore, our approach is task agnostic and can augment any existing SSL speech encoder. With SSE-TUNE, we show merging acoustic and semantic information and effectively aligning LLMs to the speech encoder on downstream tasks can further boost performance with minimal parameter overhead. As it can generalize to many downstream tasks, SSE provides an important step towards AI that can understand and respond to spoken language.\n"}
{"question": "Which of the following thing can\u2019t be guaranteed by utilizing the next sentence of r n\u22121 in the context as the current rationale r n", "evidence": "  Existing answer-unaware CQG models (Pan et al., 2019a; Do et al., 2022) commonly utilize the next sentence of r n\u22121 in the context as the current rationale r n .  To facilitate the models in selecting the current rationale and target answer appropriately and further improve the semantic diversity of dialogue flow, we design a what-to-ask module, which consists of two components: semantic graph construction and graph traversal algorithm. ", "options": ["Although such heuristics can guarantee that the flow of the generated questions is consistent with the narrative in context, the generated conversation may not always be as natural as in reality. "], "answer": "C", "content": "\nIntroduction\nBuilding systems that can comprehend human speech and provide assistance to humans through conversations is one of the main objectives in AI. Asking questions during a conversation is a crucial conversational behavior that helps AI agents communicate with humans more effectively (Allen et al., 2007; Li et al., 2016b) . This line of research is known as Conversational Question Generation (CQG), which targets generating questions given the context and conversational history (Nakanishi et al., 2019; Pan et al., 2019a; Gu et al., 2021; Do et al., 2022) . Compared to traditional single-turn question generation (Pan et al., 2019b) , CQG is more challenging as the generated multi-turn questions in a conversation need not only to be coherent but also follow a naturally conversational flow.\nGenerally, there are two main settings for the CQG task: answer-aware and answer-unaware. In the answer-aware setting, the expected answers of the (to be) generated questions are exposed to the models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . In reality, however, the answers are only \"future\" information that are unknown beforehand. Thus, growing attention has been on the more realistic answer-unaware setting, in which the answers are unknown to the CQG model (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nPrior studies either attempt to ask the questions first, and compute the reward function to evaluate their answerability (Pan et al., 2019a) or informativeness (Qi et al., 2020) ; or they extract the answer spans from the context as the what-to-ask first, and generate the questions based on them (Nakanishi et al., 2019; Do et al., 2022) . However, it has been argued that the former approach tends to generate repetitive questions (Qi et al., 2020; Do et al., 2022) . For the latter approach, Do et al. (2022) recently proposed a selection module to shorten the context and history of the input and achieved stateof-the-art performance. Nonetheless, it simply employs a naive heuristic to select the earliest forward sentence (without traceback) in the context as the rationale to extract the answer span. Although such heuristics ensure the flow of the generated questions is aligned with the context, we argue that the resulting conversations may not be natural enough, because, in reality, the interlocutors often talk about the relevant parts that may not form a sequential context. Furthermore, previous studies (Gao et al., 2019; Do et al., 2022) trained the models to decide the type of the question (boolean/span-based) to be generated implicitly. We argue that modeling question type explicitly is critical since in this setting, the answer, which hints the models to generate a boolean or span-based question, is unavailable.\nTo address the above problems, we propose a two-stage CQG framework based on a semantic graph, SG-CQG, which consists of two main components: what-to-ask and how-to-ask. In particular, given the referential context and dialog history, the what-to-ask module (1) constructs a semantic graph, which integrates the information of coreference, co-occurrence, and named entities from the context to capture the keyword chains for the possible \"jumping\" purpose; (2) traverses the graph to retrieve a relevant sentence as the rationale; and\n(3) extracts the expected answer span from the selected rationale (Section 3.1). Next, the how-to-ask module decides the question type (boolean/spanbased) via two explicit control signals and conducts question generation and filtering (Section 3.2).\nIn order to exhaustively assess the quality of the generated question-answer pairs, we propose a set of metrics to measure the diversity, dialog entailment, relevance, flexibility, and context coverage through both standard and human evaluations. Compared with the existing answer-unaware CQG models, our proposed SG-CQG achieves state-ofthe-art performance on the standard benchmark, namely the CoQA dataset (Reddy et al., 2019) .\nOur contributions can be summarized as follows:\n(1) We propose SG-CQG, a two-stage framework, which consists of two novel modules: whatto-ask encourages the models to generate coherent conversations; and how-to-ask promotes generating naturally diverse questions. Our codes will be released at https://github.com/ dxlong2000/SG-CQG.\n(2) SG-CQG achieves state-of-the-art performance on answer-unaware CQG on CoQA.\n(3) To the best of our knowledge, we are the first to propose a set of criteria to comprehensively evaluate the generated conversations. Moreover, we propose Conv-Distinct to measure the diversity of the generated conversation from a context, which takes the context coverage into account.\n(4) We conduct thorough analysis and evaluation of the questions and answers of our generated conversations, which can bring some inspiration for future work on the answer-unaware CQG.\n\nRelated Work\nOur work is closely related to two lines of prior work. Extended related work is in Appendix A.1.\n\nConversational Question Generation\nQuestion Generation has gained much attention from the research community over the years (Pan et al., 2019b; Lu and Lu, 2021) . Despite such intensive exploration, much less attention has been drawn to Conversational QG or CQG. Generally, CQG has been considered in two main settings: answer-aware and answer-unaware. In the answeraware setting, the expected answers are revealed to models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . However, this is not always the case in reality, as the answers are \"future information\". The answer-unaware setting; therefore, receives growing interests recently (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nTo tackle the what-to-ask problem, prior studies (Pan et al., 2019a; Do et al., 2022) selected the next sentence in the context as the rationale. Do et al. (2022) extract the target answer span from the rationale, while Pan et al. (2019a) generate the question, and compute a reward function to fine-tune the model by reinforcement learning. The howto-ask challenge was simply formulated as that in the answer-aware setting. In contrast, we attempt to model the rationale selection in a more coherent way by constructing and traversing a semantic graph, which simulates the keyword chains. We further propose control signals to promote diversity and fluency in question generation.\n\nKnowledge-grounded Conversation Generation\nLeveraging graphs to enhance dialog response generation has received growing interest (Moghe et al., 2018; Liu et al., 2019b; Xu et al., 2020 Xu et al., , 2021)) .\nIn particular, Xu et al. (2020) proposed to extract event chains (Mostafazadeh et al., 2016) , and utilised them to help determine a sketch of a multi-turn dialog. Nonetheless, the situation differs significantly when it comes to the CQG task. The responses in the dialog response generation task are normally full sentences with enough relevant mentions. However, in CQG, the questions and answers are mostly short and lack clear keywords, which makes the existing keyword-graph not applicable. We thus present a semantic graph, which incorporates the coreference, co-occurrence, and named entities information from the context.\n\nSG-CQG\nWe formulate the answer-unaware conversational question generation (CQG) task as:\ngiven the referential context C = {s 1 , s 2 , ..., s m } with s i being the i-th sentence in context, and the conversational history H n = {(q 1 , a 1 ), (q 2 , a 2 ), ..., (q n\u22121 , a n\u22121 )} with (q i , a i ) being the i-th turn of the question-answer pairs, as input D n = {C, H n }, the model learns to generate the current question q n and answer a n .\nFigure 1 demonstrates an overview of our proposed framework. It consists of two main components: (1) A what-to-ask module aims to select a reasonable sentence in the referential context C as the current rationale r n and thereby a span in r n as the target answer a n , given D n . (2) A how-to-ask module aims to generate the question q n , guided by the rationale r n and target answer a n .\n\nWhat-to-ask Module (WTA)\nExisting answer-unaware CQG models (Pan et al., 2019a; Do et al., 2022) commonly utilize the next sentence of r n\u22121 in the context as the current rationale r n . Although such heuristics can guarantee that the flow of the generated questions is consistent with the narrative in context, the generated conversation may not always be as natural as in reality, since human speakers often jump back and forth across the relevant but not sequential contents in context. To facilitate the models in selecting the current rationale and target answer appropriately and further improve the semantic diversity of dialogue flow, we design a what-to-ask module, which consists of two components: semantic graph construction and graph traversal algorithm.\nSemantic Graph Construction (SGC) Figure 1 shows an example of our semantic graph. Each node is displayed as a textual span and the index of the sentence it belongs to. To construct the semantic graph G = {V, E}, we first obtain the corefer-ence clusters from the context C by AllenNLP (Shi and Lin, 2019) and build the set of initial nodes from phrases in the clusters. We then connect all the nodes in the same cluster as a chain: each node in the cluster (except the one that appears last in the context) is connected to the nearest forward one in the context. We denote this type of relation as Coreference. To enhance the connectedness of G, we extract all named entities by spaCy 1 and add them as additional nodes if they are not in any clusters. We then connect all the nodes in the same sentence in the context in the same chaining style and name those edges as Same Sentence. Finally, we add a type of Extra edges between all connected subgraphs to make G fully-connected. Since those Extra edges do not bring any semantic relation to the graph, our objective is to minimize the number of those edges. Specifically, we gradually select, and connect two sentences such that their nodes are in different connected components and have the smallest indexes with the smallest difference, until the graph is fully-connected. To connect two sentences, we add an Extra edge between the last phrase in the smaller-index sentence and the first phrase in the remaining sentence. The adding-Extra-edges algorithm is in Appendix A.4.\nGraph Traversal Algorithm (GTA) Given the conversational history H n and the semantic graph G, we create a queue q to store nodes for traversing. We first add the nodes that appear in any previous turn' rationale to q in the index order 2 . We then traverse G by popping the nodes in q until it becomes empty. For each node, we retrieve the sentence that contains it as the rationale r n . If the model can generate a valid question from r n and any answer span extracted from r n , we add all unvisited neighbors of the current node to the beginning of q. A question is considered being valid if it passes the QF module (Section 3.2). Prepending the neighbors to queue is to prioritize the nodes that are connected so that the generated conversation can be formed from a chain of relevant sentences, which consolidates the coherence of the conversation. If the model cannot generate any valid q n by the current node, we add its unvisited neighbors to the end of q. The pseudocode of our proposed Graph Traversal Algorithm is described in Appendix A.2. et al. (2022) to design the answer span extractor module. In particular, a T5 model is trained on SQuAD (Rajpurkar et al., 2016) to predict the target answer span (a), given its original sentence in context (r). We use this pretrained model to extract a n from r n . Note that we also deselect the answer spans that are the same as those of previous turns.\n\nHow-to-ask Module (HTA)\nA high ratio of boolean questions in conversational datasets such as CoQA (Reddy et al., 2019) (around 20%) is one of the main challenges for current CQG studies (Gao et al., 2019; Pan et al., 2019a; Gu et al., 2021) . To the best of our knowledge; however, there is no up-to-date work which attempts to tackle this challenge. This problem is even worse in the answer-unaware setting since there is no\nYes/No answer to be provided to guide the generation of the models. Previous studies (Pan et al., 2019a; Do et al., 2022) simply train the CQG models to let them implicitly decide when to generate the boolean and span-based questions without any explicit modeling of the question type. We argue that explicitly modeling the question type is critical, as the models will gain more control on generating diverse questions, thus making the conversation become more natural. To this end, we introduce two control signals as the additional input to the QG model, and develop a simple mechanism to select the signal for the current turn.\nQuestion Type Classifier (QTC) We design two control signals to guide the QG model: <BOOLEAN> is prepended to the textual input if we expect the model to generate a boolean question, and <NORMAL> otherwise. To classify which signal should be sent to the QG model, we train a RoBERTa (Liu et al., 2019a) as our Question Type Classifier. This binary clasifier takes the rationale r n and the answer span a n generated from what-toask module, the context and the shortened conversational history as the input, and generates the label 0/1 corresponding to <NORMAL>/<BOOLEAN>. We conduct additional experiments to discuss why the control_signals work in Section 6.3.\nRewriting and Filtering (RF) Our RF module serves two purposes. Firstly, following Do et al.\n(2022), we train a T5 model on CoQA (Reddy et al., 2019) as our CQA model to answer the generated questions. A question is passed this filtering step if the answer generated by the CQA model has a fuzzy matching score greater or equal to 0.8 with the input answer span. Secondly, when invigilating the generated conversations, we observe multiple other errors that the blackbox model encounters, as shown in Table 1 . We thus propose extra post-processing heuristics to filter out the gen-erated questions and try to avoid the following issues: (1) Wrong answer. Unlike Do et al. ( 2022) that took the extracted spans as the conversational answers, we rewrite the extracted answer spans for the boolean questions by selecting the answers generated from the CQA model;\n(2) Irrelevant. For each generated question, we remove stopwords and question marks only for filtering purpose, and we check if all the remaining tokens exist in the context C;\n(3) Uninformative. To remove the turns like (\"Who woke up?\", \"Justine\"), we check validity if no more than 50% of the tokens of r n exist in any previously generated QA pairs; (4) Redundant. Unlike previous studies (Qi et al., 2020; Do et al., 2022) which only considered the redundant information from the generated answers, for each generated question that has more than 3 tokens, we filter it out if it has a fuzzy matching score >= 0.8 with any of the previously generated questions.\nQuestion Generation (QG) We fine-tune a T5 model (Raffel et al., 2020) to generate conversational questions. We concatenate the input\nD a n = {C, H n , a n , r n , control_signal} in the for- mat: Signal: control_signal Answer: a n , r n Context: C [SEP] H sub , where H sub \u2208 H n .\nThe model then learns to generate the target question q n . In our experiments, H sub is the shortened H n , in which we keep at most three previous turns. It was shown to improve upon training with the whole H n significantly (Do et al., 2022) . The performance of the QG model is in Appendix A.3.\n\nExperimental Settings\nDataset We use CoQA (Reddy et al., 2019) , a large-scale CQA dataset, in our experiments. Each conversation includes a referential context and multiple question-answer pairs, resulting in a total of 127k question-answer pairs. Among them, around 20% of questions are boolean, which makes this dataset become challenging for the CQG task (Pan et al., 2019a; Gu et al., 2021) . Since the test set of CoQA is unavailable, we follow Do et al. (2022) to keep the original validation set as our test set and randomly sample 10% of the original training set as our new validation set.\nAutomatic Evaluation We utilise BERTScore (Zhang et al., 2020) as our dialog entailment metric (BERTScore-entailment), a generalization of Dziri et al. (2019) . It considers the generated re-sponse (question/answer) as the premise, and the utterances in the conversational history as the hypothesis, and measures their similarity score as the topic coherence score. This property is crucial as the questions/answers should focus on the same topic as the previous turn(s). In our experiment, we measure the dialog entailment score with 1, 2, and all previous turn(s). To measure the relevance between the generated conversation and the context, we concatenate the generated QA pairs and compute the BERTScore. It provides how the generated conversation is explicitly relevant to the context.\nWe observe short conversations with very few generated turns tend to yield very high scores on the available diversity measurement metrics such as Distinct (Li et al., 2016a) . Since the conversation is generated from a given context, we argue that how much information from the given context the generated conversation covers should be taken into account. To this end, we introduce Context Coverage (CC) to measure the percentage of the sentences in the context that are the rationales of generated QA pairs. Our proposed Conv-Distinct of a generated conversation is then computed by multiplying the Distinct score of the generated conversation with its CC score, to measure the diversity of the turns generated from a given context:\nConv-Distinct = CC * Distinct (1)\nWe further provide Jumping Score (JS) to measure the flexibility of the generated conversation. JS is defined as the percentage of turns in which the model jumps back to any previous content of their previous turn (i.e. trace-back). It is worth noting that we do not rank the models based on JS score. Details of proposed metrics are in Appendix A.7.\nHuman Evaluation Human evaluation is critical to evaluate the quality of the generated conversations since the CQG model may generate reasonable conversations but unmatched well with the provided ground-truth ones. We randomly select 25 contexts in our test set and take the first five generated turns from the output of each model to compare, resulting in 125 samples in total. We hire three annotators who are English native speakers. Each generated question is rated by annotators on a 1-3 scale (3 is the best). We follow Do et al. (2022) to utilize three criteria: (1) Factuality measures the factual correctness and meaning of generated questions, (2) Conversational Alignment measures how aligned the generated questions are with the history, (3) Answerability measures how answerable the generated questions are by the given context. Given the fact that LMs can generate fluent texts, we omit using Fluency and Grammaticality.\nWe measure the annotators' agreement by Krippendorff's alpha (Krippendorff, 2011) . Our human rating instructions are in Appendix A.9.\nImplementation Details We fine-tune a RoBERTa large (Liu et al., 2019a) as our binary Question Type Classifier with the pretrained checkpoints from fairseq (Ott et al., 2019) on CoQA. We use a learning rate of 1e-5, a window size of 512, a batch size of 4, and AdamW (Loshchilov and Hutter, 2019) as our optimizer.\nOur classifier achieves an accuracy of 95.6%. The model is finetuned on a P40 Colab GPU for 10 epochs. Details of the input format are in Appendix A.5. We initialise SG-CQG with pretrained checkpoints of T5 base model (Raffel et al., 2020) from Huggingface (Wolf et al., 2020) . We also use AdamW (Loshchilov and Hutter, 2019) as our optimizer with a warmup of 0.1 and an initial learning rate of 1e-4. We train the model for 100k iterations with a standard window size of 512, a batch size of 4, and use a Beam search decoding strategy with a beam size of 4. \n\nMain Results\nTo evaluate the performance of SG-CQG on the answer-unaware CQG task, we employ 4 baselines for comparison, as shown in Table 2 .\n(1) T5 base (Raffel et al., 2020) , (2) BART base (Lewis et al., 2020) , (3) GPT-2 (Radford et al., 2019) , which are fine-tuned to generate conversational questionanswer pairs end-to-end, and (4) CoHS-CQG (Do et al., 2022) which adopts a strategy to shorten the context and history of the input, achieves the SoTA performance on CoQA in answer-aware and answer-unaware CQG. Firstly, we observe that SG-CQG outperforms other methods on most of the metrics, except Distinct and BERTScore. The reason is that BART and T5 often generate short QA pairs (the CC scores are 8.62% and 23.33% on average, respectively), and copy more from the context, thus they get higher scores on Distinct and BERTScore. Secondly, the metric Conv-Distinct reasonably penalizes models that generate too short conversations, on which SG-CQG achieves the best results. Thirdly, by allowing the model to jump back and forth across the relevant contents in the context by the semantic graph, SG-CQG outperforms other methods significantly on BERTScore-entailment, which indicates that conversational coherence is indeed im-proved. Furthermore, SG-CQG achieves the highest JS score, which demonstrates that the whatto-ask module allows our model to be most flexible in selecting rationales compared to the baselines. SG-CQG also achieves a significantly higher Context Coverage (CC) score compared to CoHS-CQG. Finally, compared with the results of Oracle, which are from the human-generated conversations, SG-CQG achieves commensurate performance on BERTScore-entailment and BERTScore. It demonstrates that our generated conversations are as closely coherent as human-generated ones.\nQuestion Generation Evaluation We compare the generated conversational questions of our model with 4 baselines: (1) ReDR (Pan et al., 2019a) is an encoder-decoder framework which incorporates a reasoning procedure to better understand what has been asked and what to ask next about the passage; (2) T5 base (Raffel et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . For T5, GPT-2 and CoHS-CQG, we extract the generated questions from the generated conversations for comparison. We measure the diversity of the generated questions by Distinct (Li et al., 2016a) and our proposed Conv-Distinct. Table 3 shows evaluation results of the generated conversational questions. We observe that SG-CQG achieves the best performance on Conv-Distinct, which takes the context coverage into account.\n\nAnswer Span Extraction Evaluation\nWe further evaluate the generated conversational answers of our model with 4 baselines: (1) T5 base (Raffel et al., 2020) ; (2) BART base (Lewis et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . We extract the generated conversational answers from the generated conversations of the models for comparison. We train another T5 base model on CoQA for the CQA task (see Appendix A.6) and utilize it to generate the groundtruth answers for the generated questions of the models. We then evaluate the quality of the generated conversational answers by measuring the Exact Match (EM) and F1 scores with the groundtruth ones. Table 4 shows the evaluation results. We observe that the generated conversational answers extracted by SG-CQG achieve the best EM and F1 scores, which are significantly higher than the other baselines.\n\nHuman Evaluation\nThe results of the human evaluation are present in et al., 2022) . Compared to CoHS-CQG, it achieves higher scores on all metrics except the Context Coverage (CC), which reflects that the quality of the generated conversations is indeed improved. These improvements are expected as the model in this case gains more control over generating boolean questions and has a stricter filtering process. This stricter filtering process also explains why it gets a lower CC score compared to CoHS-CQG.\n\nAblation of Question Type Classifier (QTC)\nWe conduct an ablation study of the Question Type Classifier (QTC) module. We name this experiment SG-CQG + w/o QTC. Table 2 shows the evaluation results of generated question-answer pairs. Compared with SG-CQG, the performance of SG-CQG + w/o QTC drops slightly on nearly all metrics (except Distinct), which consolidates our hypothesis that explicitly modeling the question type improves the overall coherency of the conversation. Furthermore, Table 3 shows that QTC enhances the diversity of the generated questions, while Table 4 illustrates that QTC improves the quality of the 2 ) and questions (Table 3 ) significantly. Notably, without the RF module, the extracted answer spans by SG-CQG + w/o RF can be very different from the true conversational answers, resulting in very low F1 and EM scores (Table 4 ). Although the CC score is perfect, the generated question-answer pairs from this experiment are of bad-quality.\n\nCase Study\nWe present one conversation generated by our proposed SG-CQG in Table 6 . We observe that the rationale of Q2-A2 is the 3-rd sentence in the context, and the rationale of Q3-A3 is the 8-th sentence, which is a forward jump of the model. On the other hand, the rationale of the Q4-A4 is the 7-th sentence, which is a traceback. Such a traceback enhances reasonable coherence between Q3-A3 and Q4-A4. Furthermore, Q5-A5 to Q6-A6 is also a traceback, and especially, Q6 is a boolean question. More case studies are shown in Appendix A.10.\n\nWhy Do Control Signals Work?\nExperimental Settings We design the experiments to verify the helpfulness of our two proposed control_signals: <BOOLEAN> and <NORMAL>.\nIn particular, we train a T5 model (Raffel et al., 2020 ) in the answer-aware setting. Given the input D a n = {C, H n , a n , r n } with C, H n , a n , r n as the context, ground-truth conversational history, ground-truth answer, and round-truth rationale, respectively, we conduct three experiments in Table 9 : original input with Yes/No keyword (With Y/N ), original input without Yes/No keyword (W/o Y/N ), original input without Yes/No and with the ground-truth control_signal (W/o Y/N + control_signal). Note that we train the model with the whole context, and a maximum of three previous history turns, as discussed in Appendix A.3. We measure the performance of the answer-aware CQG model separately on two types of questions: boolean and span-based by ROUGE-L (Lin, 2004) and BERTScore (Zhang et al., 2020) .\nObservations Table 9 shows the experimental results. We derive two main observations. Firstly, without knowing the keyword Yes/No (W/o Y/N ) -this is the case in the answer-unaware setting, the model performs worse. This decrease shows that the Yes/No keyword is indeed helpful in hinting the model towards generating the correct questions. Secondly, by inputting the groundtruth control_signal into the model (W/o Y/N + control_signal), the performance is improved by a large margin compared to (W/o Y/N ). We obtain three implications from the above improvement. Firstly, it consolidates our hypothesis that inputting the ground-truth control_signal is truly helpful. Secondly, by training with the control_signal, the performance of the model is even higher than with Y/N in the span-based cases, which indicates that training the model with control_signal makes it more stable to generate the correct questions. Thirdly, the performance of (W/o Y/N + control_signal) is lower than (With Y/N ) in boolean cases. The reason is <BOOLEAN> only informs the model to generate a boolean question without informing to generate an Yes or No one.\n\nConclusion\nThis paper presents SG-CQG, a two-stage framework for the CQG task in the answer-unaware setting. Firstly, the what-to-ask module aims to select a sentence as the rationale by the proposed semantic graph and extract the answer span from it. The how-to-ask module classifies the type of the question before generating and filtering it. Additionally, we propose a set of automatic evaluation criteria for answer-unaware CQG, especially a novel metric, Conv-Distinct, to evaluate the generated conversation from a context. Extensive automatic evaluation and human evaluation show that our method achieves state-of-the-art performances in the answer-unaware setting on CoQA, with a significant improvement in the conversational alignment property compared to previous frameworks. In the future, we will focus on how to reason over our semantic graph to select the rationale, and further improve the performances of how-to-ask module.\n"}
{"question": "What is the main objective of CQG in AI?", "evidence": "  This line of research is known as Conversational Question Generation (CQG), which targets generating questions given the context and conversational history (Nakanishi et al., 2019; Pan et al., 2019a; Gu et al., 2021; Do et al., 2022).   ", "options": ["Asking questions during a conversation is a crucial conversational behavior that helps AI agents communicate with humans more effectively (Allen et al., 2007; Li et al., 2016b). ", "Compared to traditional single-turn question generation (Pan et al., 2019b), CQG is more challenging as the generated multi-turn questions in a conversation need not only to be coherent but also follow a naturally conversational flow."], "answer": "B", "content": "\nIntroduction\nBuilding systems that can comprehend human speech and provide assistance to humans through conversations is one of the main objectives in AI. Asking questions during a conversation is a crucial conversational behavior that helps AI agents communicate with humans more effectively (Allen et al., 2007; Li et al., 2016b) . This line of research is known as Conversational Question Generation (CQG), which targets generating questions given the context and conversational history (Nakanishi et al., 2019; Pan et al., 2019a; Gu et al., 2021; Do et al., 2022) . Compared to traditional single-turn question generation (Pan et al., 2019b) , CQG is more challenging as the generated multi-turn questions in a conversation need not only to be coherent but also follow a naturally conversational flow.\nGenerally, there are two main settings for the CQG task: answer-aware and answer-unaware. In the answer-aware setting, the expected answers of the (to be) generated questions are exposed to the models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . In reality, however, the answers are only \"future\" information that are unknown beforehand. Thus, growing attention has been on the more realistic answer-unaware setting, in which the answers are unknown to the CQG model (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nPrior studies either attempt to ask the questions first, and compute the reward function to evaluate their answerability (Pan et al., 2019a) or informativeness (Qi et al., 2020) ; or they extract the answer spans from the context as the what-to-ask first, and generate the questions based on them (Nakanishi et al., 2019; Do et al., 2022) . However, it has been argued that the former approach tends to generate repetitive questions (Qi et al., 2020; Do et al., 2022) . For the latter approach, Do et al. (2022) recently proposed a selection module to shorten the context and history of the input and achieved stateof-the-art performance. Nonetheless, it simply employs a naive heuristic to select the earliest forward sentence (without traceback) in the context as the rationale to extract the answer span. Although such heuristics ensure the flow of the generated questions is aligned with the context, we argue that the resulting conversations may not be natural enough, because, in reality, the interlocutors often talk about the relevant parts that may not form a sequential context. Furthermore, previous studies (Gao et al., 2019; Do et al., 2022) trained the models to decide the type of the question (boolean/span-based) to be generated implicitly. We argue that modeling question type explicitly is critical since in this setting, the answer, which hints the models to generate a boolean or span-based question, is unavailable.\nTo address the above problems, we propose a two-stage CQG framework based on a semantic graph, SG-CQG, which consists of two main components: what-to-ask and how-to-ask. In particular, given the referential context and dialog history, the what-to-ask module (1) constructs a semantic graph, which integrates the information of coreference, co-occurrence, and named entities from the context to capture the keyword chains for the possible \"jumping\" purpose; (2) traverses the graph to retrieve a relevant sentence as the rationale; and\n(3) extracts the expected answer span from the selected rationale (Section 3.1). Next, the how-to-ask module decides the question type (boolean/spanbased) via two explicit control signals and conducts question generation and filtering (Section 3.2).\nIn order to exhaustively assess the quality of the generated question-answer pairs, we propose a set of metrics to measure the diversity, dialog entailment, relevance, flexibility, and context coverage through both standard and human evaluations. Compared with the existing answer-unaware CQG models, our proposed SG-CQG achieves state-ofthe-art performance on the standard benchmark, namely the CoQA dataset (Reddy et al., 2019) .\nOur contributions can be summarized as follows:\n(1) We propose SG-CQG, a two-stage framework, which consists of two novel modules: whatto-ask encourages the models to generate coherent conversations; and how-to-ask promotes generating naturally diverse questions. Our codes will be released at https://github.com/ dxlong2000/SG-CQG.\n(2) SG-CQG achieves state-of-the-art performance on answer-unaware CQG on CoQA.\n(3) To the best of our knowledge, we are the first to propose a set of criteria to comprehensively evaluate the generated conversations. Moreover, we propose Conv-Distinct to measure the diversity of the generated conversation from a context, which takes the context coverage into account.\n(4) We conduct thorough analysis and evaluation of the questions and answers of our generated conversations, which can bring some inspiration for future work on the answer-unaware CQG.\n\nRelated Work\nOur work is closely related to two lines of prior work. Extended related work is in Appendix A.1.\n\nConversational Question Generation\nQuestion Generation has gained much attention from the research community over the years (Pan et al., 2019b; Lu and Lu, 2021) . Despite such intensive exploration, much less attention has been drawn to Conversational QG or CQG. Generally, CQG has been considered in two main settings: answer-aware and answer-unaware. In the answeraware setting, the expected answers are revealed to models (Gao et al., 2019; Gu et al., 2021; Shen et al., 2021; Do et al., 2022) . However, this is not always the case in reality, as the answers are \"future information\". The answer-unaware setting; therefore, receives growing interests recently (Wang et al., 2018; Pan et al., 2019a; Nakanishi et al., 2019; Qi et al., 2020; Do et al., 2022) .\nTo tackle the what-to-ask problem, prior studies (Pan et al., 2019a; Do et al., 2022) selected the next sentence in the context as the rationale. Do et al. (2022) extract the target answer span from the rationale, while Pan et al. (2019a) generate the question, and compute a reward function to fine-tune the model by reinforcement learning. The howto-ask challenge was simply formulated as that in the answer-aware setting. In contrast, we attempt to model the rationale selection in a more coherent way by constructing and traversing a semantic graph, which simulates the keyword chains. We further propose control signals to promote diversity and fluency in question generation.\n\nKnowledge-grounded Conversation Generation\nLeveraging graphs to enhance dialog response generation has received growing interest (Moghe et al., 2018; Liu et al., 2019b; Xu et al., 2020 Xu et al., , 2021)) .\nIn particular, Xu et al. (2020) proposed to extract event chains (Mostafazadeh et al., 2016) , and utilised them to help determine a sketch of a multi-turn dialog. Nonetheless, the situation differs significantly when it comes to the CQG task. The responses in the dialog response generation task are normally full sentences with enough relevant mentions. However, in CQG, the questions and answers are mostly short and lack clear keywords, which makes the existing keyword-graph not applicable. We thus present a semantic graph, which incorporates the coreference, co-occurrence, and named entities information from the context.\n\nSG-CQG\nWe formulate the answer-unaware conversational question generation (CQG) task as:\ngiven the referential context C = {s 1 , s 2 , ..., s m } with s i being the i-th sentence in context, and the conversational history H n = {(q 1 , a 1 ), (q 2 , a 2 ), ..., (q n\u22121 , a n\u22121 )} with (q i , a i ) being the i-th turn of the question-answer pairs, as input D n = {C, H n }, the model learns to generate the current question q n and answer a n .\nFigure 1 demonstrates an overview of our proposed framework. It consists of two main components: (1) A what-to-ask module aims to select a reasonable sentence in the referential context C as the current rationale r n and thereby a span in r n as the target answer a n , given D n . (2) A how-to-ask module aims to generate the question q n , guided by the rationale r n and target answer a n .\n\nWhat-to-ask Module (WTA)\nExisting answer-unaware CQG models (Pan et al., 2019a; Do et al., 2022) commonly utilize the next sentence of r n\u22121 in the context as the current rationale r n . Although such heuristics can guarantee that the flow of the generated questions is consistent with the narrative in context, the generated conversation may not always be as natural as in reality, since human speakers often jump back and forth across the relevant but not sequential contents in context. To facilitate the models in selecting the current rationale and target answer appropriately and further improve the semantic diversity of dialogue flow, we design a what-to-ask module, which consists of two components: semantic graph construction and graph traversal algorithm.\nSemantic Graph Construction (SGC) Figure 1 shows an example of our semantic graph. Each node is displayed as a textual span and the index of the sentence it belongs to. To construct the semantic graph G = {V, E}, we first obtain the corefer-ence clusters from the context C by AllenNLP (Shi and Lin, 2019) and build the set of initial nodes from phrases in the clusters. We then connect all the nodes in the same cluster as a chain: each node in the cluster (except the one that appears last in the context) is connected to the nearest forward one in the context. We denote this type of relation as Coreference. To enhance the connectedness of G, we extract all named entities by spaCy 1 and add them as additional nodes if they are not in any clusters. We then connect all the nodes in the same sentence in the context in the same chaining style and name those edges as Same Sentence. Finally, we add a type of Extra edges between all connected subgraphs to make G fully-connected. Since those Extra edges do not bring any semantic relation to the graph, our objective is to minimize the number of those edges. Specifically, we gradually select, and connect two sentences such that their nodes are in different connected components and have the smallest indexes with the smallest difference, until the graph is fully-connected. To connect two sentences, we add an Extra edge between the last phrase in the smaller-index sentence and the first phrase in the remaining sentence. The adding-Extra-edges algorithm is in Appendix A.4.\nGraph Traversal Algorithm (GTA) Given the conversational history H n and the semantic graph G, we create a queue q to store nodes for traversing. We first add the nodes that appear in any previous turn' rationale to q in the index order 2 . We then traverse G by popping the nodes in q until it becomes empty. For each node, we retrieve the sentence that contains it as the rationale r n . If the model can generate a valid question from r n and any answer span extracted from r n , we add all unvisited neighbors of the current node to the beginning of q. A question is considered being valid if it passes the QF module (Section 3.2). Prepending the neighbors to queue is to prioritize the nodes that are connected so that the generated conversation can be formed from a chain of relevant sentences, which consolidates the coherence of the conversation. If the model cannot generate any valid q n by the current node, we add its unvisited neighbors to the end of q. The pseudocode of our proposed Graph Traversal Algorithm is described in Appendix A.2. et al. (2022) to design the answer span extractor module. In particular, a T5 model is trained on SQuAD (Rajpurkar et al., 2016) to predict the target answer span (a), given its original sentence in context (r). We use this pretrained model to extract a n from r n . Note that we also deselect the answer spans that are the same as those of previous turns.\n\nHow-to-ask Module (HTA)\nA high ratio of boolean questions in conversational datasets such as CoQA (Reddy et al., 2019) (around 20%) is one of the main challenges for current CQG studies (Gao et al., 2019; Pan et al., 2019a; Gu et al., 2021) . To the best of our knowledge; however, there is no up-to-date work which attempts to tackle this challenge. This problem is even worse in the answer-unaware setting since there is no\nYes/No answer to be provided to guide the generation of the models. Previous studies (Pan et al., 2019a; Do et al., 2022) simply train the CQG models to let them implicitly decide when to generate the boolean and span-based questions without any explicit modeling of the question type. We argue that explicitly modeling the question type is critical, as the models will gain more control on generating diverse questions, thus making the conversation become more natural. To this end, we introduce two control signals as the additional input to the QG model, and develop a simple mechanism to select the signal for the current turn.\nQuestion Type Classifier (QTC) We design two control signals to guide the QG model: <BOOLEAN> is prepended to the textual input if we expect the model to generate a boolean question, and <NORMAL> otherwise. To classify which signal should be sent to the QG model, we train a RoBERTa (Liu et al., 2019a) as our Question Type Classifier. This binary clasifier takes the rationale r n and the answer span a n generated from what-toask module, the context and the shortened conversational history as the input, and generates the label 0/1 corresponding to <NORMAL>/<BOOLEAN>. We conduct additional experiments to discuss why the control_signals work in Section 6.3.\nRewriting and Filtering (RF) Our RF module serves two purposes. Firstly, following Do et al.\n(2022), we train a T5 model on CoQA (Reddy et al., 2019) as our CQA model to answer the generated questions. A question is passed this filtering step if the answer generated by the CQA model has a fuzzy matching score greater or equal to 0.8 with the input answer span. Secondly, when invigilating the generated conversations, we observe multiple other errors that the blackbox model encounters, as shown in Table 1 . We thus propose extra post-processing heuristics to filter out the gen-erated questions and try to avoid the following issues: (1) Wrong answer. Unlike Do et al. ( 2022) that took the extracted spans as the conversational answers, we rewrite the extracted answer spans for the boolean questions by selecting the answers generated from the CQA model;\n(2) Irrelevant. For each generated question, we remove stopwords and question marks only for filtering purpose, and we check if all the remaining tokens exist in the context C;\n(3) Uninformative. To remove the turns like (\"Who woke up?\", \"Justine\"), we check validity if no more than 50% of the tokens of r n exist in any previously generated QA pairs; (4) Redundant. Unlike previous studies (Qi et al., 2020; Do et al., 2022) which only considered the redundant information from the generated answers, for each generated question that has more than 3 tokens, we filter it out if it has a fuzzy matching score >= 0.8 with any of the previously generated questions.\nQuestion Generation (QG) We fine-tune a T5 model (Raffel et al., 2020) to generate conversational questions. We concatenate the input\nD a n = {C, H n , a n , r n , control_signal} in the for- mat: Signal: control_signal Answer: a n , r n Context: C [SEP] H sub , where H sub \u2208 H n .\nThe model then learns to generate the target question q n . In our experiments, H sub is the shortened H n , in which we keep at most three previous turns. It was shown to improve upon training with the whole H n significantly (Do et al., 2022) . The performance of the QG model is in Appendix A.3.\n\nExperimental Settings\nDataset We use CoQA (Reddy et al., 2019) , a large-scale CQA dataset, in our experiments. Each conversation includes a referential context and multiple question-answer pairs, resulting in a total of 127k question-answer pairs. Among them, around 20% of questions are boolean, which makes this dataset become challenging for the CQG task (Pan et al., 2019a; Gu et al., 2021) . Since the test set of CoQA is unavailable, we follow Do et al. (2022) to keep the original validation set as our test set and randomly sample 10% of the original training set as our new validation set.\nAutomatic Evaluation We utilise BERTScore (Zhang et al., 2020) as our dialog entailment metric (BERTScore-entailment), a generalization of Dziri et al. (2019) . It considers the generated re-sponse (question/answer) as the premise, and the utterances in the conversational history as the hypothesis, and measures their similarity score as the topic coherence score. This property is crucial as the questions/answers should focus on the same topic as the previous turn(s). In our experiment, we measure the dialog entailment score with 1, 2, and all previous turn(s). To measure the relevance between the generated conversation and the context, we concatenate the generated QA pairs and compute the BERTScore. It provides how the generated conversation is explicitly relevant to the context.\nWe observe short conversations with very few generated turns tend to yield very high scores on the available diversity measurement metrics such as Distinct (Li et al., 2016a) . Since the conversation is generated from a given context, we argue that how much information from the given context the generated conversation covers should be taken into account. To this end, we introduce Context Coverage (CC) to measure the percentage of the sentences in the context that are the rationales of generated QA pairs. Our proposed Conv-Distinct of a generated conversation is then computed by multiplying the Distinct score of the generated conversation with its CC score, to measure the diversity of the turns generated from a given context:\nConv-Distinct = CC * Distinct (1)\nWe further provide Jumping Score (JS) to measure the flexibility of the generated conversation. JS is defined as the percentage of turns in which the model jumps back to any previous content of their previous turn (i.e. trace-back). It is worth noting that we do not rank the models based on JS score. Details of proposed metrics are in Appendix A.7.\nHuman Evaluation Human evaluation is critical to evaluate the quality of the generated conversations since the CQG model may generate reasonable conversations but unmatched well with the provided ground-truth ones. We randomly select 25 contexts in our test set and take the first five generated turns from the output of each model to compare, resulting in 125 samples in total. We hire three annotators who are English native speakers. Each generated question is rated by annotators on a 1-3 scale (3 is the best). We follow Do et al. (2022) to utilize three criteria: (1) Factuality measures the factual correctness and meaning of generated questions, (2) Conversational Alignment measures how aligned the generated questions are with the history, (3) Answerability measures how answerable the generated questions are by the given context. Given the fact that LMs can generate fluent texts, we omit using Fluency and Grammaticality.\nWe measure the annotators' agreement by Krippendorff's alpha (Krippendorff, 2011) . Our human rating instructions are in Appendix A.9.\nImplementation Details We fine-tune a RoBERTa large (Liu et al., 2019a) as our binary Question Type Classifier with the pretrained checkpoints from fairseq (Ott et al., 2019) on CoQA. We use a learning rate of 1e-5, a window size of 512, a batch size of 4, and AdamW (Loshchilov and Hutter, 2019) as our optimizer.\nOur classifier achieves an accuracy of 95.6%. The model is finetuned on a P40 Colab GPU for 10 epochs. Details of the input format are in Appendix A.5. We initialise SG-CQG with pretrained checkpoints of T5 base model (Raffel et al., 2020) from Huggingface (Wolf et al., 2020) . We also use AdamW (Loshchilov and Hutter, 2019) as our optimizer with a warmup of 0.1 and an initial learning rate of 1e-4. We train the model for 100k iterations with a standard window size of 512, a batch size of 4, and use a Beam search decoding strategy with a beam size of 4. \n\nMain Results\nTo evaluate the performance of SG-CQG on the answer-unaware CQG task, we employ 4 baselines for comparison, as shown in Table 2 .\n(1) T5 base (Raffel et al., 2020) , (2) BART base (Lewis et al., 2020) , (3) GPT-2 (Radford et al., 2019) , which are fine-tuned to generate conversational questionanswer pairs end-to-end, and (4) CoHS-CQG (Do et al., 2022) which adopts a strategy to shorten the context and history of the input, achieves the SoTA performance on CoQA in answer-aware and answer-unaware CQG. Firstly, we observe that SG-CQG outperforms other methods on most of the metrics, except Distinct and BERTScore. The reason is that BART and T5 often generate short QA pairs (the CC scores are 8.62% and 23.33% on average, respectively), and copy more from the context, thus they get higher scores on Distinct and BERTScore. Secondly, the metric Conv-Distinct reasonably penalizes models that generate too short conversations, on which SG-CQG achieves the best results. Thirdly, by allowing the model to jump back and forth across the relevant contents in the context by the semantic graph, SG-CQG outperforms other methods significantly on BERTScore-entailment, which indicates that conversational coherence is indeed im-proved. Furthermore, SG-CQG achieves the highest JS score, which demonstrates that the whatto-ask module allows our model to be most flexible in selecting rationales compared to the baselines. SG-CQG also achieves a significantly higher Context Coverage (CC) score compared to CoHS-CQG. Finally, compared with the results of Oracle, which are from the human-generated conversations, SG-CQG achieves commensurate performance on BERTScore-entailment and BERTScore. It demonstrates that our generated conversations are as closely coherent as human-generated ones.\nQuestion Generation Evaluation We compare the generated conversational questions of our model with 4 baselines: (1) ReDR (Pan et al., 2019a) is an encoder-decoder framework which incorporates a reasoning procedure to better understand what has been asked and what to ask next about the passage; (2) T5 base (Raffel et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . For T5, GPT-2 and CoHS-CQG, we extract the generated questions from the generated conversations for comparison. We measure the diversity of the generated questions by Distinct (Li et al., 2016a) and our proposed Conv-Distinct. Table 3 shows evaluation results of the generated conversational questions. We observe that SG-CQG achieves the best performance on Conv-Distinct, which takes the context coverage into account.\n\nAnswer Span Extraction Evaluation\nWe further evaluate the generated conversational answers of our model with 4 baselines: (1) T5 base (Raffel et al., 2020) ; (2) BART base (Lewis et al., 2020) ;\n(3) GPT-2 (Radford et al., 2019) ; (4) CoHS-CQG (Do et al., 2022) . We extract the generated conversational answers from the generated conversations of the models for comparison. We train another T5 base model on CoQA for the CQA task (see Appendix A.6) and utilize it to generate the groundtruth answers for the generated questions of the models. We then evaluate the quality of the generated conversational answers by measuring the Exact Match (EM) and F1 scores with the groundtruth ones. Table 4 shows the evaluation results. We observe that the generated conversational answers extracted by SG-CQG achieve the best EM and F1 scores, which are significantly higher than the other baselines.\n\nHuman Evaluation\nThe results of the human evaluation are present in et al., 2022) . Compared to CoHS-CQG, it achieves higher scores on all metrics except the Context Coverage (CC), which reflects that the quality of the generated conversations is indeed improved. These improvements are expected as the model in this case gains more control over generating boolean questions and has a stricter filtering process. This stricter filtering process also explains why it gets a lower CC score compared to CoHS-CQG.\n\nAblation of Question Type Classifier (QTC)\nWe conduct an ablation study of the Question Type Classifier (QTC) module. We name this experiment SG-CQG + w/o QTC. Table 2 shows the evaluation results of generated question-answer pairs. Compared with SG-CQG, the performance of SG-CQG + w/o QTC drops slightly on nearly all metrics (except Distinct), which consolidates our hypothesis that explicitly modeling the question type improves the overall coherency of the conversation. Furthermore, Table 3 shows that QTC enhances the diversity of the generated questions, while Table 4 illustrates that QTC improves the quality of the 2 ) and questions (Table 3 ) significantly. Notably, without the RF module, the extracted answer spans by SG-CQG + w/o RF can be very different from the true conversational answers, resulting in very low F1 and EM scores (Table 4 ). Although the CC score is perfect, the generated question-answer pairs from this experiment are of bad-quality.\n\nCase Study\nWe present one conversation generated by our proposed SG-CQG in Table 6 . We observe that the rationale of Q2-A2 is the 3-rd sentence in the context, and the rationale of Q3-A3 is the 8-th sentence, which is a forward jump of the model. On the other hand, the rationale of the Q4-A4 is the 7-th sentence, which is a traceback. Such a traceback enhances reasonable coherence between Q3-A3 and Q4-A4. Furthermore, Q5-A5 to Q6-A6 is also a traceback, and especially, Q6 is a boolean question. More case studies are shown in Appendix A.10.\n\nWhy Do Control Signals Work?\nExperimental Settings We design the experiments to verify the helpfulness of our two proposed control_signals: <BOOLEAN> and <NORMAL>.\nIn particular, we train a T5 model (Raffel et al., 2020 ) in the answer-aware setting. Given the input D a n = {C, H n , a n , r n } with C, H n , a n , r n as the context, ground-truth conversational history, ground-truth answer, and round-truth rationale, respectively, we conduct three experiments in Table 9 : original input with Yes/No keyword (With Y/N ), original input without Yes/No keyword (W/o Y/N ), original input without Yes/No and with the ground-truth control_signal (W/o Y/N + control_signal). Note that we train the model with the whole context, and a maximum of three previous history turns, as discussed in Appendix A.3. We measure the performance of the answer-aware CQG model separately on two types of questions: boolean and span-based by ROUGE-L (Lin, 2004) and BERTScore (Zhang et al., 2020) .\nObservations Table 9 shows the experimental results. We derive two main observations. Firstly, without knowing the keyword Yes/No (W/o Y/N ) -this is the case in the answer-unaware setting, the model performs worse. This decrease shows that the Yes/No keyword is indeed helpful in hinting the model towards generating the correct questions. Secondly, by inputting the groundtruth control_signal into the model (W/o Y/N + control_signal), the performance is improved by a large margin compared to (W/o Y/N ). We obtain three implications from the above improvement. Firstly, it consolidates our hypothesis that inputting the ground-truth control_signal is truly helpful. Secondly, by training with the control_signal, the performance of the model is even higher than with Y/N in the span-based cases, which indicates that training the model with control_signal makes it more stable to generate the correct questions. Thirdly, the performance of (W/o Y/N + control_signal) is lower than (With Y/N ) in boolean cases. The reason is <BOOLEAN> only informs the model to generate a boolean question without informing to generate an Yes or No one.\n\nConclusion\nThis paper presents SG-CQG, a two-stage framework for the CQG task in the answer-unaware setting. Firstly, the what-to-ask module aims to select a sentence as the rationale by the proposed semantic graph and extract the answer span from it. The how-to-ask module classifies the type of the question before generating and filtering it. Additionally, we propose a set of automatic evaluation criteria for answer-unaware CQG, especially a novel metric, Conv-Distinct, to evaluate the generated conversation from a context. Extensive automatic evaluation and human evaluation show that our method achieves state-of-the-art performances in the answer-unaware setting on CoQA, with a significant improvement in the conversational alignment property compared to previous frameworks. In the future, we will focus on how to reason over our semantic graph to select the rationale, and further improve the performances of how-to-ask module.\n"}
{"question": "For someone who barely knows about meditation, which one bellow is not helpful to infer that meditation makes people relaxed?", "evidence": "  For someone who barely knows about meditation, while is knowledgeable about singing, he can still infer that meditation makes people relaxed from the existing knowledge that singing makes people relaxed by first conceptualizing singing as a relaxing event and then instantiating that event to meditation.  ", "options": ["A. Realizing the existing knowledge that singing makes people relaxed.", "B. Conceptualizing singing as a relaxing event.", "C. Instantiating relaxing event to meditation.", "D. Connect singing with dancing."], "answer": "D", "content": "\nIntroduction\n\"Concepts are the glue that holds our mental world together.\"- Murphy (2004) Commonsense reasoning is a crucial ability for machines to make situational presumptions and draw inferences from the knowledge that reflects our humans' understanding of situations and common facts (Davis, 1990; Davis and Marcus, 2015) . It has gained increasing popularity in the Natural Language Processing (NLP) community with the emergence of CommonSense Knowledge Bases (CSKB) (Sap et al., 2019a; Speer et al., 2017;  Figure 1 : A demonstration of commonsense reasoning on an unknown situation, PersonX plays with his dog, with the aid of abstract commonsense knowledge. Decontextualized conceptualization, such as observe, may yield wrong abstract commonsense knowledge that cannot be instantiated within the corresponding context. Hwang et al., 2021) and large language models (Bosselut et al., 2019; Rajani et al., 2019; Liu et al., 2022b; Su et al., 2022; Yu et al., 2022b) . However, when encountering situations beyond the data given, more abstract background knowledge must be acquired and generalized to assist the reasoning (Tenenbaum et al., 2011) , and language models trained with an autoregressive language modeling objective do not explicitly leverage such abstract knowledge during inference.\nInstead, humans rely on conceptual induction and deduction (Murphy, 2004) to make inferences on novel situations without the need to memorize all special cases. As shown in Figure 1 , humans can derive conceptualizations based on the assertion that \"PersonX watches a football game, as a result, he feels relaxed\" to infer that \"relaxing events can make someone feel relaxed,\" where the acquired abstract commonsense knowledge can be further used as general knowledge to perform reasoning on similar or associated situations. A new commonsense knowledge \"PersonX plays with his dog, as a result, he feels happy and relaxed\" can be deduced by instantiating relaxing events to playing with his dog.\nAs the cornerstone of generalizable commonsense reasoning, such a process is extremely challenging for machines to replicate due to the absence of contextualized conceptualizations and abstract commonsense knowledge in CSKBs and a lack of relevant methodologies.\nYet, existing works address the process of induction and deduction separately via conceptualization and instantiation. Several methods performing conceptualization are proposed with a specific focus on entity-level (Durme et al., 2009; Song et al., 2011; Gong et al., 2016; He et al., 2020; Peng et al., 2022; Song et al., 2015) and event-level (Chen et al., 2020; He et al., 2022) semantics. Instantiation (Allaway et al., 2023) , as the process that simulates conceptual deduction, is tackled separately and not leveraged by these methods. Though abstract commonsense knowledge can be derived by using existing conceptualization methods to abstract a certain instance from factual commonsense knowledge, several limitations still exist.\nFirst, the plausibility of abstract commonsense knowledge banks on both the correctness of conceptualization and proper contextualization under specific assertions. The latter one, which is an essential step for the deduction of abstract knowledge, is missing from current methodologies. Take Figure 1 as an example, the concept observe will not necessarily lead to the result of \"feeling relaxed\", as observe omits the entertaining property of the original instance as a cost of abstraction. Second, instantiating abstract commonsense knowledge can yield much more and diverse concrete commonsense knowledge that can serve as an augmentation of the training dataset, while current methods undervalue such a process and only focus on conceptualization. Finally, the complex contextualization and conceptualization of commonsense knowledge can easily bring more than two orders of magnitude of data on top of the original dataset. This makes current labeled data scarce and infeasible for practitioners to annotate all of them, leaving a large amount of unlabeled data.\nTo fill in these research gaps, we propose CAT (Contextualized ConceptuAlization and InsTantiation), a semi-supervised learning framework that unites event conceptualization and instantiation in cascade to conceptualize CSKBs and acquire abstract commonsense knowledge to aid commonsense reasoning. Inspired by how humans learn with concepts (Carey, 2004) , we design a novel bootstrapping 1 method to enhance conceptualizations and abstract commonsense knowledge verification with the help of similar conceptualizations and instantiations as a reference. We demonstrate the effectiveness of CAT by using the acquired abstract commonsense knowledge to train COMET (Bosselut et al., 2019) , a commonsense inference language model that generates if-then commonsense knowledge, and showing that our derived abstract commonsense knowledge can significantly improve commonsense inference modeling.\nOur contributions are three-fold: (1) We introduce a semi-supervised learning framework, CAT, to conceptualize CSKBs with the assistance of progressively bootstrapping similar abstract concepts or instantiations in the conceptualization process.\n(2) We use CAT to acquire abstract commonsense knowledge at scale with high quality, which can be used for commonsense inference modeling. (3) We demonstrate the effectiveness of our framework by achieving state-of-the-art performance on two CSKB conceptualization tasks and remarkably improving commonsense inference modeling with our derived abstract commonsense knowledge.\n\nRelated Works\nConceptualization and Instantiation. Many existing works have studied conceptualization and instantiation separately. Durme et al. (2009) first attempted to derive more general knowledge by abstracting over large sets of factoids obtained from WordNet (Miller, 1995) synsets. Song et al. (2011 Song et al. ( , 2015) ) and Gong et al. (2016) proposed to turn instances in a sentence into concepts via weight matching from Probase (Wu et al., 2012) . Recently, Liu et al. (2022c) proposed a taxonomy-guided induction method to mine verb-oriented commonsense knowledge from verb phrases. Peng et al. (2022) constructed a conceptual knowledge benchmark to evaluate language models with three zeroshot probing tasks. While these works focus on the conceptualization of entities, He et al. (2022) constructed an event conceptualization benchmark based on ATOMIC (Sap et al., 2019a ) by combining syntactic parsing, semantically heuristic matching, and human annotation. Besides, the line of works focusing on ultra-fine entity typing (Choi et al., 2018; Dai et al., 2021; Li et al., 2022) similar objectives of typing named entities, nominal nouns, and pronouns into a set of free-form phrases. Instantiation was attempted by Allaway et al. (2023) , who proposed a controllable generative framework to probe valid instantiations for abstract knowledge automatically. Though Porada et al. (2021) and Peng et al. (2022) both proved that existing pretrained language models lack conceptual knowledge, none of existing works explicitly combine both techniques to derive abstract knowledge that is context-sensitive and generalizable.\nCommonsense Reasoning. Endowing NLP systems with the ability to perform commonsense reasoning is an elusive goal of artificial intelligence (Sap et al., 2020) . A diverse collection of commonsense reasoning tasks have been proposed as evaluation benchmarks (Talmor et al., 2019; Omura et al., 2020; Ponti et al., 2020; Fang et al., 2021a) . Among them, Bosselut et al. (2019) proposed a generative model, COMET, to learn to produce if-then commonsense knowledge as an effective approach toward modeling commonsense inference that can be applied in various commonsense reasoning tasks (Talmor et al., 2019) .\nSemi-Supervised Learning. Semi-supervised learning (SSL) aims at taking advantage of unlabeled data to equip models with stronger generalization ability (van Engelen and Hoos, 2020) . The most common approach is using pseudo labels (Iscen et al., 2019; Wang et al., 2022) to expose more unseen data to the student model. It has been applied in various machine learning tasks such as image classification (Liu et al., 2022a; Hu et al., 2021 ), text classification (Li et al., 2021; Meng et al., 2019; Xiao et al., 2019) , commonsense knowledge base population (Fang et al., 2022) , and named entity recognition (Liu et al., 2021; Chen et al., 2021) .\n\nProblem Definition\nDefinition. Conceptualizing an event-centric CSKB to derive abstract commonsense knowledge comprises two steps (He et al., 2022) : event conceptualization and triple conceptualization.\n\nDenote the triples in the original CSKB as\nD o = {(h o , r, t)|h o \u2208 H o , r \u2208 R, t \u2208 T }, where H o , R,\nand T are the set of heads, relations, and tails in the original CSKB. The first step only operates on head events without considering the context in r and t. The goal of event conceptualization is to produce conceptualized head event h a from the original head h o to represent an abstraction of h o . In the second step, the task is to verify whether the conceptualized head h a still makes sense in the context of r and t, as r and t will further restrict the level of abstractness in h a . As shown in Figure 1 , conceptualizing watch football game to observe is wrong within the context of having feel relaxed as a result. Plausible (h a , r, t) triples will be considered as valid abstract commonsense knowledge. Specifically, in the first step, there are two ways of conceptualizing head events alone: a retrievalbased discriminative way and a generative way. The retrieval-based discriminative paradigm identifies and links a component i in h o to a concept c in a concept taxonomy C to form a conceptualization h a by replacing i with c. The model needs to verify whether h a is a valid conceptualization of h o . The generative paradigm aims to generate a h a directly given h o and the designated component i in h o .\nFormally, denote the annotated dataset in the first step, event conceptualization, as\nD l h = {(h o , h a , y)|h o \u2208 H o , h a \u2208 H a , y \u2208 {0, 1}},\nwhere h o is an original head event without conceptualization, h a is a corresponding conceptualization of h o , and y is the human-annotated label indicating whether such a conceptualization is plausible or not. The labeled dataset in the second step, triple conceptualization, is denoted as\nD l t = {(h, r, t, y)|h \u2208 H a , r \u2208 R, t \u2208 T, y \u2208 {0, 1}},\nwhere h is a conceptualized head event from the first step, r and t are a relation and a tail from the original CSKB accompanied with the corresponding original head h o , and y is the human-annotated label indicating whether such abstract commonsense knowledge, in the form of a conceptualized triple, is plausible or not. Besides labeled datasets, unlabeled datasets are defined similarly as D u h and D u t only with the difference that labels y are missing. Thus, the task objective for discriminative event conceptualization is to determine whether a h o can be properly abstracted using h a , where h a is derived by replacing a component i \u2282 h o with its linked concept c from a concept taxonomy C. The task objective for generative event conceptualization is to generate h a directly from h o with text generation models. For the triple conceptualization task, the objective is to distinguish whether a conceptualized triple (h a , r, t), representing abstract commonsense knowledge, is plausible or not.\nDataset. To study conceptualization over CSKBs, we use the AbstractATOMIC dataset provided by He et al. (2022) as the benchmark. In Ab-stractATOMIC, ATOMIC is used as the original CSKB. And the event conceptualization adopts a discriminative way, where a syntactic parsing schema is defined to identify the components i in h o to be heuristically linked to concept taxonomies Probase (Wu et al., 2012) and WordNet (Miller, 1995) to form conceptualized h a . Such a heuristic can produce over 32 times more candidate conceptualized head events and over 10 times more conceptualized triples compared with the original ATOMIC, as the number of retrieved concepts from the concept taxonomy C can be manually controlled to acquire a large number of conceptualizations. Triple conceptualization is defined as predicting the plausibility of the triples whose head is conceptualized. Only 131K (26%) conceptualizations of 7K (45%) ATOMIC head events and 81K (1.3%) conceptualized triples are manually anno-tated as D l h and D l t , while others remain unlabeled D u h and D u t . The trn/dev/tst partition follows the same split as in the original ATOMIC. Statistics and more detailed explanations of AbstractATOMIC are shown in Table 1 and Appendix A.\n\nCAT Framework\nThis section introduces our proposed Contextualized ConceptualizAtion and InsTantiation (CAT) framework for conceptualizing commonsense knowledge bases and acquiring abstract commonsense knowledge. An overview is presented in Figure 2 . Our motivation is two-fold: first, adding instantiation after conceptualization to form a cycle can strongly benefit two conceptualization tasks simultaneously. On the one hand, instantiating conceptualized triple relies on the correctness of event conceptualization. On the other hand, properly conceptualized triples can benefit event conceptualization via instantiation by providing more context brought by (r, t). Second, to address the lack of annotations, we resort to pseudo labeling, a typical semi-supervised learning approach to automatically assign pseudo labels to the vast majority of unlabeled data using a teacher model.\nFollowing He et al. (2022) , we study the retrieval-based discriminative paradigm of event conceptualization and leave the generative paradigm as an intrinsic evaluation. In CAT, we unify event conceptualization and triple conceptualization into one cycle and make them mutually benefit each other through instantiation and conceptualization. Our framework can be summarized into four steps:\n(1) Train teacher models for both event conceptualization and triple conceptualization on the labeled dataset D l h and D l t , respectively. Use the two teachers to assign pseudo labels to unlabeled datasets.\n(2) Conduct alternative conceptualization or instantiation on labeled and pseudo-labeled data.\n(3) Bootstrap (aggregate) the alternative concepts and instances in the second step using natural language prompt templates and train student models on both labeled and pseudo-labeled data. (4) Use the student models to refine the pseudo labels and then re-train the student models.\n\nTeacher Model Training\nTwo teacher models on both event and triple conceptualization tasks are trained separately on the labeled dataset D l h and D l t . As both tasks are inherently text/triple classification, we adopt KG-BERT (Yao et al., 2019) as the skeleton of our models. The event conceptualization model determines whether h a is a valid conceptualization of h o , and the triple conceptualization model determines whether a conceptualized triple (h a , r, t) is plausible or not. The two models \u03b8 are trained on annotated examples x i with a cross-entropy loss (Eq. 1) and used to provide pseudo labels to instances from the unlabeled datasets D u h and D u t . Two thresholds, T + and T \u2212 , are set to determine the pseudo labels of unlabeled examples with high confidence. Examples with a pseudo-labeled score higher than T + will be labeled y i = 1, and those lower than T \u2212 will be labeled y i = 0. The rest will be discarded.\nL(x i , \u03b8) = \u2212 |x| i=1 y i log(\u03b8(x i ))\n(1)\n\nAlternative Conceptualization and Instantiation\nAccording to Murphy (2004) , when humans learn a new concept, we pre-extract similar known concepts in our minds and infer possibly equivalent unknown concepts on the fly. Inspired by this theory, we retrieve additional abstract concepts or instantiated events to help discriminate conceptualizations and abstract commonsense knowledge. For event conceptualization, we retrieve some alternative possible conceptualizations of h o to accompany the learning of h a . Additional conceptualizations of h o from both labeled and pseudo-labeled examples are predicted again by the teacher model and ranked according to their plausibility score prediction. And top m conceptualizations are retrieved with m being a hyperparameter to control the number of retrievals. For triple conceptualization, we perform instantiation in cascade to instantiate c to some concrete instances to assist the learning process.\nPossible instantiations of c are extracted from annotated and pseudo-labeled event conceptualizations by searching for conceptualized events h \u2032 a \u2208 H a other than h a with c as the concept and extracting their corresponding instances i \u2282 h \u2032 a . Similarly, the instances are then scored by the teacher model, and the top n of them are retrieved. Intuitively, alternative event conceptualizations can serve as hints for discriminating the correctness of the target conceptualization, and instantiations can carry additional contextualized information to help verify the plausibility of a conceptualized triple, which meets the objective of deriving abstract commonsense knowledge that is context-sensitive.\n\nPrompt Aggregation\nWe then bootstrap the retrieved alternative conceptualizations/instantiations via natural language prompts. Here bootstrap (Carey, 2004 ) can be understood as binding the alternative retrievals and the target concept/triple together to strengthen the discrimination of the target concept/triple. As shown in Figure 2 step (3), the initially given input and retrieved concepts/instances are concatenated via human-defined prompts for both conceptualization tasks. Alternative concepts/instances are sorted in the order of their plausibility score ranking. Two student models S h and S t for both tasks are trained using the modified text with such prompts as inputs. They are expected to learn the bootstrapping connectionism between the target and the additional retrievals we provided. More detail about the prompt design is in Appendix B.\n\nPseudo-Label Refinement\nAll pseudo labels, initially derived by a teacher model trained on the original labeled dataset, are relabeled according to the plausibility score predicted by our newly enhanced student models S h and S t . Similar to the teacher model, two thresholds, T + and T \u2212 , are applied to distinguish positive and negative examples for both tasks. In addition, negative Table 2 : Performance (%) by our CAT framework on the discriminative event conceptualization and triple conceptualization tasks. We report the average AUC score and standard deviation across experiments with three random seeds. The best performances within each framework are underlined, and the best among all models are bold-faced. labels are assigned to triples whose conceptualized head events are predicted as wrong conceptualizations by S h , as wrong conceptualizations will not yield plausible abstract commonsense knowledge.\n\nApplication and Evaluation of CAT\nThe resulting models of CAT include an event conceptualization model and a triple conceptualization model, both fine-tuned on the refined pseudo labels and the labeled data. These two models can be used to conceptualize ATOMIC to a larger commonsense knowledge base on a more abstract level. We further conduct intrinsic evaluations on the acquired event conceptualization model under a generative event conceptualization paradigm and extrinsic evaluations on the resulting conceptualized CSKB with commonsense inference modeling task (COMET; Bosselut et al. (2019) ) in Section 5. Here we select COMET as the representative because it is a general commonsense model that can be applied to various downstream commonsense reason-ing tasks such as SocialIQA (Sap et al., 2019b) , self-talk (Shwartz et al., 2020) , and CSKB completion (Malaviya et al., 2020) . Meanwhile, generative event conceptualization enables performing automatic conceptualization scalably. Both are important applications and evaluations of CAT.\n\nExperiments\nWe conduct conceptualization experiments using CAT in Section 5.1 and generative experiments as evaluations in Section 5.2. These experiments demonstrate that CAT has a strong capability in conceptualizing CSKBs, and better conceptualization modeling can help populate more novel and diverse commonsense knowledge and thus help commonsense modeling (COMET).\n\nCSKB Conceptualization\nBaselines. We collectively introduce the baselines for both event and triple conceptualization tasks, as they are inherently classification tasks. Table 3 : Performance (%) of GPT2 (XL) on the generative event conceptualization task. D l h stands for annotated labeled data, and D u stands for the data acquired by CAT. The underfoot value indicates the threshold for selecting plausible pseudo labels. The best performances are bold-faced, and the second-best ones are underlined.\nAUC is used as the evaluation metric. Under a supervised learning setting, we apply KG-BERT (Yao et al., 2019) model with BERT (Devlin et al., 2019) , BART (Lewis et al., 2020) , RoBERTa (Liu et al., 2019 ), DeBERTa (He et al., 2021 , 2023 ), and ELECTRA (Clark et al., 2020) as the backbone language models. We also attempt to leverage supervised generative language models as baselines. GPT2 (Radford et al., 2019) models are trained with a text generation objective only on positive examples, and we use perplexity as the prediction scores to calculate AUC. For the semi-supervised learning baselines, we leverage UDA (Xie et al., 2020a) , NoisyStudent (Xie et al., 2020b), and Pseu-doReasoner (Fang et al., 2022) with RoBERTalarge being the backbone model. Additional explanations can be found in Appendix C.1.1.\nDiscriminative Results. The results for both tasks are presented in Table 2 . Under a supervised learning setting, KG-BERT family mostly performs better on both tasks than GPT2 due to the fact that GPT2 is only fine-tuned on positive examples and thus cannot learn from negative examples that contain wrong conceptualizations and implausible abstract commonsense knowledge. As for the semi-supervised learning setting, previous SSL baselines are rather limited in improving the performance against supervised learning. The best Pseu-doReasoner only improves by 0.5% and 0.3% on the test set for both tasks compared with supervised RoBERTa-large models. Instead, models trained with CAT can outperform all other training methodologies. Comparing the test set performance with PseudoReasoner, small backbone models (BERTbase) can improve by 3.4% and 2.2%, and large models (RoBERTa-large) can be improved by 2.1% and 2.2%. This shows pipelining two-step concep-tualizations as a loop and leveraging our proposed bootstrapping-based method can yield a larger performance gain compared with simply applying a semi-supervised learning strategy. Due to limited space, ablation studies on framework components and the semi-supervised learning paradigm of CAT are conducted in Appendix C.1.4. For example, the results indicate that bootstrapping alternative conceptualization and instantiation plays the most important role in assisting learning conceptualization among all components of CAT. Additional results and a computational cost study can be found in Appendix C.1.3 and Appendix D.\n\nApplication and Evaluation of CAT\nAs CAT is a framework for acquiring conceptualized commonsense knowledge, including both conceptualized head events (from h o to h a ) and abstract commonsense triples (h a , r, t), we assess these pseudo-labeled outcomes via two generative tasks with various threshold tuning as evaluations.\nGenerative Event Conceptualization. To intrinsically evaluate the effectiveness of CAT's event conceptualization, we use the acquired conceptualized head events as training data to learn a generative event conceptualizer. Specifically, the models are trained with instance-conceptualizations pairs in the format of \"<instance> is an instance of <concept>\". At the evaluation phase, the model is prompted with \"<instance> is an instance of [GEN]\" where <instance> is the instance to be conceptualized and [GEN] is the generation token. We then retrieve the top-1 generation and compare it against the target set from the evaluation dataset to compute four NLG metrics, as listed in Appendix C.2.1. These scores can be regarded as an approximation of the top-1 generations' recall. 40.0 40.3 27.1 27.8 20.0 20.8 16.5 17.5 16.1 16.3 35.3 35.7 31.6 31 Additionally, we uniformly sample 500 generations from each evaluation split and conduct expert annotations on the plausibility of each conceptualization to ensure that out-of-domain concepts can be properly evaluated. The experts are asked to determine whether each top-1 generation is indeed a plausible conceptualization or not, such that the top-1 generations' precision is reflected. Thus, current evaluation measures jointly evaluate the top-1 generations' precision and recall, which makes it robust and non-easy to be impacted by repetition problems (Li et al., 2020) . Zero-shot GPT2 and GPT2 fine-tuned on the originally labeled event conceptualizations in D l h are used as baselines. We also study the effect of the threshold T + that selects plausible conceptualized heads, where higher thresholds indicate higher plausibility regarded by CAT. The results are presented in Table 3 . With a relatively high threshold, generators trained on a mixture of pseudo-labeled data by CAT and annotated concepts significantly outperform the baselines in every automated metric. A plausible rate of 93.3% is maximally achieved on the test set, which is 11.8% higher than the baseline. Gradually reducing the threshold also decreases the performance, indicating abstract heads with lower plausibility scores can be of poorer quality. Such results indicate that CAT can produce high-quality event conceptualizations for generative models to learn better conceptualizers without the need to annotate a large number of data.\n\nCommonsense Inference Modeling (COMET).\nThe second component of CAT produces triple-level abstract commonsense knowledge. We evaluate these abstract commonsense triples with a commonsense inference task that generates commonsense tails given heads and relations as inputs, as in COMET (Bosselut et al., 2019) . Following He et al. ( 2022), we apply the same training and evaluation process to the models. The base training data we use are a subset of ATOMIC triples corresponding to those annotated abstract triples in D l t , which contains 17K (3.7%) among the original ATOMIC. We derive abstract commonsense knowledge using CAT from a subset of D u t where the heads correspond to those in the ATOMIC subset to ensure no data leakage, denoted as D u CAT . GPT2 is fine-tuned on the ATOMIC subset, the annotated abstract triples D l t , the abstract knowledge verified by CAT, or their combinations. The commonsense generation results are presented in Table 4 . Similar to COMET (Bosselut et al., 2019) , all models are evaluated on the original ATOMIC's full validation and testing sets. The best result is achieved using a mixture of the ATOMIC subset and abstract triples pseudo-labeled by our framework, with 0.95 as the threshold for selecting plausible triples. This indicates high-quality abstract commonsense triples can indeed provide a more general view of the original commonsense knowledge, thus helping commonsense inference. Additionally, training with our pseudo-labeled examples outperforms training with those annotated triples in AbstractATOMIC, which also validates the effectiveness of our model that leverages a large amount of unlabeled data. To further investigate how conceptual knowledge 5HWULHYDO1XPEHU (YHQW&RQFHSWXDOL]DWLRQ$8& improves commonsense inference modeling, we conduct more empirical analysis in Section 5.4. Additional experiment results with other thresholds and case studies can be found in Appendix C.2.3 and Appendix E, respectively.\n\nNumber of Retrieved Alternative\nConceptualizations and Instantiations.\nWe then study the ablation of bootstrapping different numbers of alternative conceptualizations/instantiations (denoted as #retrieval) in our CAT framework. For simplicity, when tuning the #retrieval for one task, the #retrieval of the other task is fixed at the best value we acquired. We plot the test AUC score with #retrieval from 0 to 11 using BERT-base as the backbone model in Figure 3 . #retrieval=0 refers to training with a simple student-teacher framework without bootstrapping alternative conceptualizations and instantiations. For event conceptualization, the performance generally positively correlates with the number of retrievals, while it starts dropping after 9. A reversed trend is observed for triple conceptualization, where using only two instances achieves the best performance. One possible reason is that in triple conceptualization, the retrieved instances are events and much longer than the retrieved concepts in event conceptualization, and aggregating various alternative events for a triple will cause language models to be less sensitive to the semantics of the original triple (Holtzman et al., 2020) .\n\nThe Effect of Abstract Knowledge\nWe finally study the effect of abstract commonsense knowledge acquired by CAT by studying the semantic overlaps between training and testing data. We sort the test set by the BERTScore (Zhang \n\nConclusion\nIn conclusion, this paper proposes CAT, a semisupervised learning framework for commonsense reasoning, by leveraging the power of abstract commonsense knowledge. By achieving state-of-theart performances in CSKB conceptualization tasks, we remarkably improve modeling commonsense inference, as an important cornerstone of many commonsense reasoning tasks. Our analysis also demonstrates that high-quality abstract commonsense knowledge can benefit commonsense inference modeling by providing more generalizability on hard commonsense knowledge. We hope this work can draw insights toward commonsense reasoning from a conceptualization perspective.\n"}
{"question": "What is one of the main reasons why technical analysis has gained popularity among AI practitioners in stock prediction, as mentioned in the text?", "evidence": "  technical analysis has gained more popularity among AI practitioners, many of whom focus on employing long short-term memory networks and other innovative architectures to model stock price history alongside technical analysis indicators (Nelson et al., 2017; Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) This is primarily because processing a single stream of price data is relatively simpler than analyzing and synthesizing a range of diverse data sources with varying frequencies and characteristics.  ", "options": ["A. Technical analysis simplifies the analysis of diverse data sources.", "B. Technical analysis is based on fundamental value assessment.", "C. Technical analysis incorporates natural language processing techniques.", "D. Technical analysis relies on social media posts for predictions."], "answer": "A", "content": "\nIntroduction\nFinancial services, known for their competitiveness, have always been at the forefront of adopting data science techniques to drive investment decisions. Quantitative trading, a specific field within it, has drawn immense interest from both academia and industry over the last few decades. With the rapid advancements in deep learning recently, computer scientists and quantitative researchers have joined forces to apply AI techniques to tackle the challenges within this domain.\nAmong various tasks, one of the most prominent is stock price movement prediction (Bhardwaj, 2021) . The reason for its popularity is selfevident: once a model is able to predict future movement with considerable accuracy, numerous trading strategies can be easily built around it.\nRecent studies have shown that deep neural networks are ideal candidates for such prediction models (Yoo et al., 2021; Gunduz, 2021) . Supporters of the efficient-market hypothesis (EMH), which posits that asset prices reflect all available information, tackle the task with price information alone (Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . However, an alternative perspective suggests that additional insights can be gained from analyzing news articles and social media posts, which may hold valuable clues about the future (Hu et al., 2018; Xu and Cohen, 2018; Wang et al., 2019b; Tang et al., 2020) .\nAnother intriguing approach analyzes the relationships between different stocks. Clearly positive and negative correlations, or even non-correlations can be immensely useful in constructing a diversified stock portfolio (Borodin et al., 2003) . Several studies even empirically demonstrate that exploiting correlations can improve the accuracy of stock price movement prediction (Long et al., 2020; Yoo et al., 2021) . However, their correlations are often realized by acquiring industry sector and calculating correlation matrices or attention scores, which are bidirectional and symmetrical, leading to excessive attention on spurious correlations. Due to the lag problem widely existed between two time series, we are more concerned about the dominance of information flow between stocks, specifically, the direction of causality.\nAdditionally, we have observed that the situation can significantly change when incorporating text information. Let's consider two highly correlated companies (A and B) and there is promising news specifically about company A. In such a scenario, it's fairly easy to infer that the current news might still have a substantial impact on company B, despite there being no direct connection between the two companies on paper. However, it's impossible to reach this conclusion by just examining the news about company A or the correlation between A and B alone, which highlights the limitations of relying solely on individual pieces of textual information or traditional correlations between stocks.\nInspired by observations above, we propose the Causality-guided Multi-memory Interaction Network (CMIN), a novel end-to-end deep neural network which captures both financial news as well as the causality-enhanced correlations between stocks for better stock price movement prediction.\nTo achieve this goal, CMIN incorporates two key components: the Text Memory Network and the Stock Correlation Memory Network. Both networks utilize a recurrent neural network with nonlinear combination of memory attentions to generate a global memory abstraction. And we introduce a global causality matrix according to the transfer entropy between stock price time series to guide the abstraction process, forming a Causal Attention mechanism to capture the asymmetric correlations. By considering causality, CMIN goes beyond traditional symmetric correlations and captures the true inter-dependencies between stocks. Furthermore, we employ an attention-based fusion mechanism between the two networks, introducing multi-directional interactions through which CMIN learns not only the self-influence within each network but also the interactive influence between them. It captures the interrelationship between textual information and correlations, enhancing the overall predictive power of CMIN.\nWe further demonstrate the effectiveness of CMIN with experiments conducted on 3 real-world datasets collected from both the U.S. and Chinese markets, where CMIN achieves state-of-the-art prediction accuracy, surpassing existing models in terms of performance.\nTo summarize, our main contributions are:\n\u2022 Proposal of a causality-guided multi-memory interaction network for stock movement prediction which is to our best knowledge the first attempt to simultaneously consider causalityenhanced correlations and textual information to achieve higher prediction accuracy;\n\u2022 Introduction of the attention-based multidirectional interactions, so that CMIN captures not only the self-influence of temporal movements and textual information but also the interactions between these two types of information flows;\n\u2022 Collection and release of two new datasets: one for the U.S. market and another for the Chinese market.\nBoth datasets include comprehensive financial texts and stock price time series data, which are publicly available at https://github.com/ BigRoddy/CMIN-Dataset, facilitating further research and benchmarking in the field.\n\nStock Movement Prediction\nIn traditional trading practices, two main frameworks are commonly used to make predictions on future stock prices (Ferreira et al., 2021) . The first is fundamental analysis, which aims to assess the intrinsic value of a stock by considering various factors related to it as a whole, such as financial statements, industry trends and economic conditions. The other is technical analysis, which operates under the assumption that the market is efficient (i.e., the Efficient Market Hypothesis holds true) and focuses on analyzing only historical and current price patterns in order to predict future movements.\nAlthough both frameworks have been widely adopted by top hedge funds and investment firms, technical analysis has gained more popularity among AI practitioners, many of whom focus on employing long short-term memory networks and other innovative architectures to model stock price history alongside technical analysis indicators (Nelson et al., 2017; Zhang et al., 2017; Stoean et al., 2019; Sezer and \u00d6zbayoglu, 2020) . This is primarily because processing a single stream of price data is relatively simpler than analyzing and synthesizing a range of diverse data sources with varying frequencies and characteristics.\n\nPredicting with the Help of Text Data\nThe recent advancement of natural language processing (NLP) techniques has opened up new possibilities for analyzing large volumes of text data in the context of stock movement prediction. Many researchers have recognized the potential value of incorporating news articles, analysis, commentaries and even social media posts (Xu and Cohen, 2018) , which are believed to provide valuable insights about the future. Some studies focus solely on textual information. For example, (Hu et al., 2018) leverages attention mechanism at multiple levels within a deep structure to identify the most important news articles and predict price trends. Others adopt a two-step approach. First, they extract features (e.g. investor sentiment) from financial texts. Then they fuse these features with price information to make predictions such as (Li et al., 2017) and (Jin et al., 2020) . This integration of text analysis with quantitative techniques holds promise for enhancing the accuracy and effectiveness of stock movement prediction models.\n\nExploiting the Relations between Stocks\nAnother important trading framework takes advantage of the correlations between different stocks. Portfolio selection, particularly pairs trading, is a well-known and successful trading strategy that exploits the correlated nature of stocks, whether positive or negative. In fact, as early as (Borodin et al., 2003) pointed out that stock correlations based portfolio selection could beat any strategy that relied on predicting trends or specific targets.\nThe incorporation of correlations in stock movement prediction has gained attention in recent years, drawing inspiration from several existing works. For example, (Yoo et al., 2021) utilizes transformer to learn dynamic correlations between stocks in an end-to-end manner. (Long et al., 2020) employs knowledge graphs and graph embedding techniques to model the relationships between stocks. These studies have achieved admirable results, potentially due to effective feature engineering however, because the direct benefit of stock correlations in predicting future prices lacks fundamental logic.\nIn this paper, we propose constructing a single model to handle both textual data and stock correlations simultaneously, aiming to shed light on the success of correlation-based approaches with the help of financial texts. We also introduce a novel causal attention mechanism to interpret the underlying logic behind stock correlations, leveraging transfer entropy to provide insights. We further model the multi-directional interactions between texts and correlations so that we could uncover not only relevant texts for prediction through correlations, but also the hidden stock correlations through texts. By integrating text data and stock correla-tions within a unified model, we aim to provide a comprehensive understanding of the relationship between the two and discover valuable insights for stock movement prediction.\n\nProblem Formulation\nThis paper is dedicated to predict the price movement of a target stock. To this end, we leverage both the correlations between stocks and textual information to make prediction.\nConsider a target stock with numerical features denoted as P target \u2208 R k\u00d7d , where k represents the number of time steps in the monitoring window and d represents the dimension of price features, such as the highest and the closing prices. The prices of n other relevant stocks are denoted as:\nP = {P 1 , P 2 , \u2022 \u2022 \u2022 , P n } \u2208 R n\u00d7k\u00d7d .\nBesides, we have financial documents associated with the target stock, which are represented as\nM = {M 1 , M 2 , \u2022 \u2022 \u2022 , M k } \u2208 R k\u00d7l\u00d7w ,\nwhere l denotes the number of documents in a time step and w is the maximum number of words in a document. In cases where a specific stock has fewer than l documents at a given time step, zero padding values are added to align the lengths. Similarly, if a document contains fewer than w words, zero padding is applied to ensure uniform length across all documents (Ang and Lim, 2022) .\nWe formulate the task as a binary classification problem whose goal is to predict the movement of the target stock at the next time step, denoted as \u0177target . Here, \u0177target = 1 indicates a rise in the price while \u0177target = 0 indicates a fall. (1) The feature embedding module includes two encoders, one for embedding the textual information and another for embedding the price time series. Additionally, a global causality matrix is introduced to capture the asymmetric correlations using transfer entropy, which then guides the calculation of attention weights in the multi-memory networks.\n(2) The multi-memory networks consist of the Text Memory Network and Stock Correlation Memory Network, which are designed to select and re- tain the most relevant and influential information (textual and correlational) for the target stock.\n(3) The multi-directional interaction module facilitates the interaction between the textual and correlational information. This interaction allows the two types of information to reinforce each other and leverage the advantages of different information flows for better prediction performance, enhancing the predictive capabilities of the CMIN.\n\nFeature Embedding\nSelf-attention mechanisms have proven to be effective in capturing long-term dependencies and modeling complex sequential patterns, particularly in the Transformer architecture (Vaswani et al., 2017) . Given the significance of historical information in financial documents and stock prices for stock price movement prediction, we employ attention mechanisms to summarize this information.\n\nText Encoder\nThe Text Encoder focuses on processing the financial documents M to extract meaningful representations for stock movement prediction. We firstly use a popular word representation tool Glove (Li et al., 2018) to generate the word embedding tensor M word \u2208 R k\u00d7l\u00d7w\u00d7dw , where d w is the size of word embeddings. Each word in the financial documents is represented as a d w -dimensional vector.\nThen the word embeddings are passed through a text embedding layer. Here we adopt the bidirectional Gated Recurrent Unit (Bi-GRU) (Li et al., 2022) to capture both preceding and succeeding contexts within each document. The average of the last hidden vectors is taken as the text embeddings M text \u2208 R k\u00d7l\u00d7dm , or equivalently M text \u2208 R s\u00d7dm , where s is the total number of documents in the monitoring window.\nAfter that, the text attention mechanism is applied to summarize all historical documents across time steps. The text embedding of the last time step M text,\u22121 \u2208 R l\u00d7dm , serves as the query matrix, while the entire text embeddings M text \u2208 R s\u00d7dm acts as both the key and value matrices. Soft scaled dot-product attention is used to compute the attention weights, which are then applied to the text embedding to obtain a representation E text \u2208 R l\u00d7dm enhanced by the history state attention:\nE text = softmax( M text,\u22121 M T text \u221a d m )M text . (1)\nThe resulting E text is the textual embedding that contains highly concentrated information from the stock's related texts. This embedding serves as a summary of the historical text data and is used for further processing in the multi-memory networks and multi-directional interaction module of CMIN.\n\nPrice Encoder\nThe Price Encoder is introduced to utilize multivariate features from historical prices and capture their temporal interrelationships. Firstly we employ a feature mapping layer to project them into a latent space of dimension d p , aiming to improve the learning capacity (Yoo et al., 2021) . For target stock price P target \u2208 R k\u00d7d , the historical price embeddings Ptarget \u2208 R k\u00d7dp can be formulated as:\nEQUATION\nwhere\nW t \u2208 R d\u00d7dp , b t \u2208 R dp are parameters.\nMoreover, recognizing that historical patterns can repeat themselves sometimes, we incorporate a multi-head price attention layer to capture each stock's distinctive changing patterns. The price embedding of the target stock at the last time step is donated as P\u22121 target \u2208 R dp . Then we employ the multi-head attention mechanism with the query P\u22121 target and the key/value Ptarget as follows:\nv target = MultiheadAtt( Ptarget , P\u22121 target ) (3)\nv target is a key vector that serves as the initial hidden state for the two memory networks, playing a crucial role in the final prediction. Similarly, we process the remaining stocks and obtain the correlational embedding E corr \u2208 R n\u00d7dp . Notably, the shared parameters across all stocks ensure the stability and generality of the extracted features (Wang et al., 2019a) .\n\nCausality Matrix\nWhen it comes to detecting causal relationships and conducting predictive analysis, transfer entropy, a non-linear generalization of Granger causality (Seth, 2007) , serves as a conceptually neat and mathematically rigorous method. It has been considered as an important tool for causality analysis and successfully applied in diverse domains including financial markets (Sandoval Junior et al., 2015) .\nTransfer entropy is derived from Shannon Entropy: H = \u2212 N i=1 p i log p i . In this context, considering the time series of a stock, we can partition the possible values into different bins and calculate the probabilities at each time step. Transfer entropy from series X to another series Y can be defined as the average amount of information contained in the source X but not contained in Y's past:\nEQUATION\nBased on this principle, for each monitoring window, we calculate the transfer entropy between all stocks using their historical closing prices and generate a transfer entropy matrix, referred to as the Causality Matrix C \u2208 R n\u00d7n , which illustrates the asymmetric flow of information from one stock to another. Specifically, C[i, j] represents the transfer entropy from stock i to stock j, and C[i, j] > C [j, i] indicates that stock i provides more predictive information about the movement of stock j than j to i. This Causality Matrix will next serve as a guide for the memory networks, enabling the identification of causal dependencies between multivariate stocks.\n\nMulti-memory Networks\nWe introduce a Text Memory Network and a Stock Correlation Memory Network (Sukhbaatar et al., 2015) to manage the textual and correlational information separately. They each maintain a continuous representation and update it iteratively using multiple computational steps (hops), ultimately producing a global memory abstraction.\nAs shown in Figure 1 , each layer of the memory network comprises an attention unit and a GRU unit, which receive textual or correlational embeddings as inputs and are supervised by the continuous representation generated in the previous layer. To initialize the continuous representations of each network, we use the target stock vector v target (generated from Eq.3):\nv (0) text = v (0) corr = v target .\n(5)\n\nText Memory Network\nIn each layer h \u2208 [1, H] of the Text Memory Network, we input the textual embeddings E text (Eq.1) and the continuous representation from the previous layer v\n(h\u22121)\ntext . We utilize an attention unit (Eq.3) to identify important information within the textual embeddings. Subsequently, a non-linear GRU cell unit (Xu et al., 2019) acts as an information aggregator, determining the amount of text information to retain:\nv Att(h) text = MultiheadAtt(E text , v (h\u22121) text ), (6)\nwhere v (h\u22121) text is the query matrix and E text represents the raw form of the key and value matrices.\nThen the GRU cell unit updates the current hidden state into the next hidden state and outputs it to the next layer as the new continuous representation:\nv (h) text = GRU (v Att(h) text , v (h\u22121) text ).\n(7)\n\nStock Correlation Memory Network\nThe Stock Correlation Memory Network is employed to dynamically identify stock relationships and update the continuous representation of stock correlations in an intuitive and asymmetric manner. However, the use of unsupervised attention weights in previous models can be problematic as they may be inevitably misled by the dataset bias, resulting in excessive attention on spurious stock correlations. To address this, we introduce extra knowledge in the form of Transfer Entropybased causality to guide the attention weights and mitigate potential confounding effects.\nFor each target stock, we extract a causal vector v causal = C[:, target] from the pre-calculated causality matrix, which quantifies the degree of information flow from other stocks to it. Then we modify the traditional attention mechanism into Causal Attention by incorporating causal guidance:\nS = softmax( QK T \u221a d ), S = f (S, v causal ). (8)\nHere, f is a function that aggregates the attention weight S and the causal vector v causal to produce a causality-guided attention weight S. We use the average aggregation method for simplicity (i.e., f (S, v causal ) = (S + v causal )/2). To better balance them, one can introduce a hyperparameter \u03bb \u2208 [0, 1]. Then f() updates to f (S, v causal ) = \u03bbS + (1 \u2212 \u03bb)v causal . We believe that different degrees of causal attention can impact the model's performance, and leave it for future exploration. The continuous representation is gradually updated through the Causal Attention, indicating the influence of causal relationships on movement prediction and the self-influence on the flow of correlation information:\nEQUATION\nIt is important to note that although we design multiple layers within each memory network to learn deep representations, different layers of the same memory network share the same unit. This enables the network to focus on crucial information that affects the movement of the target stock, thereby enhancing the continuous representation.\n\nMulti-directional Interactions\nIn reality, textual information and correlations have an impact on each other when it comes to stock price movement prediction. For instance, news about a technological breakthrough in the new energy sector may uplift the prices of most stocks in that industry, thereby affecting the correlations among those stocks.\nTo simulate this phenomenon and enhance the synergy between textual and correlational information, we introduce a multi-directional interaction module. This module allows textual and correlational information to reinforce each other and amplify the advantages of different information flows for better prediction performance.\nTake the Text Memory Network as an example, in each layer we firstly calculate the self-influence by using v (h\u22121) text as the query:\nv Att(h) text\u2212>text = MultiheadAtt(E text , v (h\u22121) text ) (11)\nNext we consider the interactive influences from correlations to texts using v (h\u22121) corr as the query:\nv Att(h) corr\u2212>text = MultiheadAtt(E text , v (h\u22121) corr ) (12)\nFinally, we produce a new attentional continuous representation by averaging these two influences:\nEQUATION\nwhich means that we replace Eqs. 6 with Eqs. 11-13 to obtain the new attention-aggregated vector.\nThe workings of Stock Correlation Memory Network are quite similar.\nConsequently, the fusion of different information flows is promoted due to the multi-directional interaction mechanism in which CMIN learns not only the influences from text/correlation to movement prediction within each information flow but also the interactive influences between different information flows, representing the interrelationship between text and correlations.\n\nLearning Objective\nWith the continuous representations v (H) text and v (H) corr from the last layer of each memory network, along with the target stock representation v target , we concatenate them and apply a softmax function to generate the final prediction vector \u0177:\n\u0177 = softmax(W y [v (H) text , v target , v (H) corr ] + b y ). (14)\nThe objective is to minimize the cross entropy loss:\nL(y, \u0177) = \u2212 n i=1 (yi log (\u0177i) + (1\u2212yi) log (1\u2212 \u0177i)) (15)\nwhere n is the size of the training set.\n\nExperiments\nIn this section, we empirically evaluate our CMIN model with three real-world datasets collected from the U.S. and Chinese stock markets.\n\nDatasets\nIn our experiments we have used three datasets, namely ACL18, CMIN-US and CMIN-CN, spanning different time periods to evaluate our proposed model CMIN against other baselines.\nACL18 (Xu and Cohen, 2018 ) is a classic dataset with tweets from Twitter as financial texts in the task of text-enhanced stock movement prediction. As there are few existing high-quality datasets containing both texts and price, we are also making available two new benchmark datasets along with this paper from 2018-01-01 to 2021-12-31 in the U.S. and Chinese market named CMIN-US and CMIN-CN. These two datasets are available at https://github.com/BigRoddy/ CMIN-Dataset to facilitate further research and enable reproducibility. More details and statistics of those three datasets are in Appendix A.\n\nBaselines\nWe compare CMIN against the following four baselines, all of which are high-performing stock movement prediction models proposed by recent studies:\n\u2022ALSTM (Qin et al., 2017 ) is a dual-stage attention-based recurrent neural network, which selects relevant time series across all time steps.\n\u2022Adv-LSTM (Feng et al., 2019) uses adversarial training to improve the generalization of ALSTM.\n\u2022Stocknet (Xu and Cohen, 2018) introduces recurrent continuous latent variables and uses variational inference to address the posterior inference.\n\u2022DTML (Yoo et al., 2021) is a newly published attention-based model that exploits the correlations between stocks to improve the prediction accuracy.\n\nEvaluation metrics\nAs we have formulated stock price movement prediction as a classification problem, we choose two classic metrics: Accuracy (Acc.) and Matthews Correlation Coefficient (MCC), similar to the previous work (Xu and Cohen, 2018; Yoo et al., 2021) . \nEQUATION\n\nImplementation details\nWe set our model for daily price prediction, with a history market window size k = 5 and the number of price features d p = d = 3, namely the highest, the lowest and the closing prices. We limit the maximum number of financial texts in one single day to be l = 20 , and the maximum length of a text document w = 30. Within the Text Encoder, we set the size of word embedding vector d w = 50 and the hidden state of Bi-GRU network d m = 50.\nWe implement the CMIN with Pytorch on a NVIDIA Tesla V100 and train it with an Adam optimizer (Kingma and Ba, 2015) . All parameters of our model are initialized with Xavier Initialization (Glorot and Bengio, 2010) . We search the hyperparameters of CMIN as follows: number of layers of each memory network H in {1, 2, 3, 4, 5}, dropout rate in {0.1, 0.2, 0.3}, number of epochs in {10, 20, 50}, and size of the price hidden state d p in {3, 10, 50}. For baselines, we use their default parameters and fine-tune them to fit our data.\n\nPerformance Analysis\nThe results are summarized in Table 1 .\nAmong all models, ALSTM and Adv-LSTM performed poorly with little improvement over random prediction. This could be attributed to the fact that these models rely solely on stock prices as the basis for decision-making. The Stocknet and DTML incorporate additional information beyond stock prices, demonstrated significant improvements over ALSTM and Adv-LSTM, which highlights the importance of utilizing financial texts and stock correlations for this challenging task. CMIN outperformed all baselines and achieved state-of-the-art performance on both two metrics across all datasets, showing its excellent capabilities to leverage both financial texts and stock correlations, as well as capture their interrelationship. \n\nAblation Studies\nTo evaluate the contribution of CMIN's different components, we compare against several variants:\n\u2022CMIN-TE: CMIN without the Text (TE), which makes decisions just based on stock prices.\n\u2022CMIN-PR: CMIN without the Price (PR), which makes decisions just based on related texts.\n\u2022CMIN-CM: CMIN without the guide of causality matrix (CM).\n\u2022CMIN-MI: CMIN without multi-directional interactions (MI) between memory networks.\nThe results are summarized in Table 2 . CMIN-TE only achieves a level of prediction accuracy on par with ALSTM and Adv-LSTM, and is worst among all the variants, again indicating the importance of text data. Similar to the performance of Stocknet, CMIN-PR has a relatively high Acc. but a low MCC, suggesting texts are particularly helpful to predict on one side of the binary classification. By modeling both text data and stock relationships, CMIN-CM reaches a good result. Finally, better performance achieved when causality matrix and multi-directional interactions are introduced into the network. Overall, the ablation studies show that every component makes an important contribution to CMIN, and as a result the full model with all components achieves the best performance.\n\nAnalysis of Memory Network Depth\nAs introduced before, we propose two memory networks to retain vital features of texts and correlations with multiple computational layers. And we want to understand what would be the ideal number of depths to achieve the best prediction results.\nWe change the number of layers H of each memory network to find out how the performance fluctuates with it. The results are summarized in Figure 2 . When we only have one memory layer, there is no multi-directional information flows between the two memory networks and as a result they only try to identify the vital information in the embeddings related to or having an impact on the movement of the target stock under the supervision of v target . As the number of memory layers increases, the interactions between two memory networks also intensifies. It is intuitive that the performance of CMIN reaches its peak when it has three memory layers. With further increase the number of memory layers, CMIN is prone to overfit.\n\nCase Study\nHere we present an example to illustrate how CMIN considers both financial texts and stock correlations to avoid random noises in time series.\nWe visualized the causality matrix of ACL18 using a heat map as shown in Figure 3 . Stocks are sequenced by their industry sector. The black box on the left shows weak causality, representing weak information flow from Utilities to Materials. On the other hand, the yellow box on the right indicates the relative strong information flow from Materials to Finance and within the Finance industry.\nThe target stock is Bank Of America (BAC) with a monitor window spanning from 13/11/2015 to 19/11/2015. We employ CMIN to predict BAC's next movement direction on the day of 20/11/2015 and then output the attention scores of texts and causality-guided correlation. The most focused stock by CMIN is Berkshire Hathaway Inc. (BRK-A). It's interesting to note that both are in the same industry sector: Finance, and they do appear to follow a very similar movement pattern in the trading days leading to 20/11/2015, which demonstrates the ability of CMIN to find the dynamic stock correlations with the guidance of Causality Matrix.\nThe financial text of BAC that obtains the highest attention score is \"Beer, Credit Card Debt And Other Positives For Bank Of America\", the title of an news article 1 which reports the rapidlyimproving banking landscape in the U.S.. This text is clearly highly relevant to BAC's subsequent stock performance, which demonstrates that CMIN is able to identify highly relevant texts having a impact on the target stock movement.\nFurthermore, it also illustrates the underlying interrelationship between financial texts and stock correlations. Except expressing an optimistic sentiment towards BAC, the news also shows a rapidly improving state of affairs for the wider financial industry. Therefore, through the Multi-directional Interactions mechanism, the text strengthens the model's attention stocks in the same sector. These two aspects mutually reinforce and complement each other to help the model make the best judgment that BAC's stock price will rise on the next day.\n\nConclusions\nIn this paper, we proposed CMIN, a causalityguided multi-memory interaction network that simultaneously models financial documents, causality-enhanced stock correlations and the interactions between the two, and recurrently learns a global memory representation for movement prediction. This multi-modality network was designed to enable the concurrent discovery of texts and stock correlations relevant to future price change and we demonstrated, through experiments on three datasets across two distinct markets, that each component of the proposed architecture made significant contributions to the model, leading CMIN to achieve state-of-the-art accuracy.\n"}
{"question": "How do we confirm our method\u2019s advantage?", "evidence": "  The performance comparison of all methods on CH-SIMS, MOSI, and MOSEI is summarized in Table 1 and Table 2 . The scores of the proposed method and its variations are the averages of 5 runs. The performances of all other baselines, except for MAG-BERT, have been sourced from published papers or official repositories 1 .\n ", "options": ["A. Compare the performance of all methods on CH-SIMS, MOSI, and MOSEI.", "B. Through logical reasoning.", "C. By comparing with previous research.", "D. By imagination."], "answer": "A", "content": "\nIntroduction\nMultimodal deep learning involves interpreting and analyzing multimodal signals together, where each modality refers to a way in which something is experienced and felt, e.g., the visual, audio, or language modality. With the widespread popularity of online social media, such as Instagram, Tik-Tok, Facebook, etc., videos containing multiple modalities have become a major information carrier, which brings new challenges to content recommendation and classification, e.g., video question answering (Lei et al., 2021; Li et al., 2020) , video captioning (Ging et al., 2020; Li et al., 2020) , and video retrieval (Akbari et al., 2021; Lei et al., 2021) .\nWhile traditional sentiment analysis is mainly based on language, multimodal sentiment analysis (MSA) predicts the human emotion by utilizing extra information available in visual and audio modalities of the content to assist with language-based prediction. Here, the text modality contains the semantic meaning of the spoken language. The visual modality extracts the facial characteristics (e.g., head orientation, facial expressions, and pose) of the speaker. The audio modality reflects the emphasis on the utterance (e.g., through pitch, bandwidth and intensity). MSA has recently gained much attention in research for several reasons. On one hand, because of the abundance of social media content, commercial interests are switching from gauging user opinions/emotions from text only to more thorough multimodal analysis based on videos. On the other hand, short video platforms (e.g., TikTok, Instagram) allow users to easily create multimodal content including visual information, audio, and inserted text, while these modalities are sometimes noisy or even contradicting each other in sentiments. Therefore, the presence of multimodal information in addition to the text or language itself is necessary to make a thorough conclusion about the overall sentiment of a video.\nMultimodal fusion has become essential to gaining a deeper understanding of these video scenes (Baltru\u0161aitis et al., 2018) and has proven to be helpful in many downstream tasks. Various multimodal fusion techniques have been proposed for MSA, among which a basic solution is concatenating the extracted feature of each modality before performing downstream regression or classification. Recent work has recognized the importance of identifying modality-invariant information across modalities and fuse them to strengthen sentiment prediction (Hazarika et al., 2020; Zadeh et al., 2018a; Rahman et al., 2020; Sun et al., 2020) .\nAlthough modality-invariant information helps reinforce the understanding of the content, there are also cases where sentiments of different modalities contradict each other. For example, when one thanks someone with phrases like \"Finally I can rest easy tonight\" or \"I can't thank you enough\", it is very hard to conclude whether the sentiment is positive or negative without looking at the nonverbal cues, such as tones, facial expressions, and gestures. In fact, many sarcastic opinions are expressed by non-linguistic markers. In these cases, the overall sentiment cannot simply be judged by a majority vote among all modalities. Thus, multimodal representation learning that respects both consistency and incongruity between modalities have recently shown great promise (Yu et al., 2020; Hazarika et al., 2020) .\nIn this paper, we propose ConFEDE, a Contrastive FEature DEcomposition framework, which integrates both modality decomposition within each sample and supervised contrastive learning across samples in a single unified contrastive learning framework. Our main contributions are summarized as follows: (1) We integrate inter-sample contrastive learning and intra-sample modality decomposition into a simple unified loss function, based on a customized data sampler that allows us to sample positive/negative data pairs to perform both learning tasks. (2) We propose to decompose each modality into a similarity feature and a dissimilarity feature, and use the similarity feature of the text as an anchor to build the contrastive relation among all decomposed features. This is due to the observation that sentiment analysis is still largely centered around text and spoken language, while other modalities can provide extra information to assist with prediction. (3) Based on multimodal representation learning proposed above, we further introduce a multi-task prediction loss that depends on each decomposed modality representation and enables the model to learn from both multimodal prediction and unimodal prediction.\nWe mainly evaluated ConFEDE on CH-SIMS (Yu et al., 2020) benchmark, which contains both unimodal and overall sentiment labels for each sample. The result shows that the proposed method significantly outperforms a wide range of stateof-the-art multimodal sentiment analysis methods. To test the capability when no unimodal labels are provided, we further conduct experiments on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , which contain only an overall sentiment label for each sample, which shows that our proposed method can also achieve better per-formance than state-of-the-art methods on a number of performance metrics without unimodal labels. We provide extensive ablation studies to show the effectiveness and necessity of each design component in ConFEDE. The code is released at https://github.com/XpastaX/ConFEDE/.\n\nRelated Work\nIn this section, we discuss the related work in MSA and contrastive representation learning.\n\nMultimodal Sentiment Analysis\nPrior works on multimodal sentiment analysis mostly focus on predicting sentiments based on text and vision (Zhu et al., 2022; Ji et al., 2019; Liu et al., 2019) .However, there is growing interest in analyzing sentiment using all three modalities: text, audio, and vision (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020) . Zadeh et al. (2016) were among the first to propose a multimodal dictionary that could learn the dynamic interactions between facial gestures and spoken words to model sentiments. They later introduced a Tensor Fusion Network (TFN) to learn the intra-modality and inter-modality dynamics of three modalities in an end-to-end way (Zadeh et al., 2017) . Furthermore, they presented a Memory Fusion Network (MFN) which is composed of Long Short Term Memories (LSTMs) to learn the view-specific and cross-view interactions of three views (text, video, and audio) to improve sentiment analysis performance. Rahman et al. (2020) proposed a Multimodal Adaptation Gate (MAG) to fine-tune BERT (Devlin et al., 2019) on multimodal data to improve sentiment analysis performance. However, these prior works do not consider modality-specific information.\nTo better study the impacts that modalityspecific information can bring to MSA, Yu et al. (2020) construct a new multimodal sentiment analysis dataset CH-SIMS, which contains a unimodal label for each modality of a sample. Experiments show a great improvement in overall sentiment prediction after simply integrating unimodal predictions as subtasks in the learning objective. Hazarika et al. (2020) further decompose each modality into a modality-invariant and a modalityspecific representation, and employ squared Frobenius norm loss as the regularizer. However, they treat all modalities equally while regularizing the prediction result, which ignores the different effectiveness of modalities. In real cases, the text is usually more effective on MSA tasks compared to vision and audio. In other words, it is less \"noisy\" than the other two modalities. Also, they employ Central Moment Discrepancy loss to push the modality-invariant representations close and a Frobenius norm to push modality-specific representations to be orthogonal, while in our method, we integrate the above mechanism into a single loss function. Moreover, they regularize the decomposed features by reconstructing the original features with the generated features. We, instead, avoid using such a method and regularize the decomposed features with unimodal prediction tasks. To improve the decomposition performance, we further aggregate the supervised contrastive learning between samples into our frameworks by a custom-designed sampling method.\nA concurrent work HyCon (Mai et al., 2021) introduces a contrastive learning method for MSA, taking both inter-sample and intra-sample contrasts into consideration. However, they ignore the regularization for each decomposed feature. In contrast, in ConFEDE, within-sample feature contrasts are constructed based on a specific pattern centered around text similarity features. Also, when performing inter-sample contrastive learning, Hy-Con samples positive and negative pairs randomly based on MSA labels. In contrast, we design a data sampler that considers both the labels and similarities between modalities to retrieve positive/negative pairs. Due to these reasons, our method beats Hy-Con on most metrics on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , and is able to utilize unimodal labels to further boost performance, e.g., on CH-SIMS (Yu et al., 2020) .\n\nContrastive Representation Learning\nContrastive learning has achieved great success in representation learning by contrasting positive pairs against negative pairs (Akbari et al., 2021; Hassani and Khasahmadi, 2020; Chen et al., 2020) . Akbari et al. (2021) train a Video-Audio-Text Transformer (VATT) using multimodal contrastive learning for the alignment of video-text and videoaudio pairs, and thus achieve state-of-the-art on various computer vision tasks (e.g., audio classification and video action recognition). Hassani and Khasahmadi (2020) propose to learn node and graph level representations by contrasting encodings obtained from different structural views of graphs and achieve the state-of-the-art on various graph classification benchmarks. Chen et al. (2020) present a self-supervised framework, SimCLR, to learn visual representations through a contrastive loss between augmented views of the same image sample. Khosla et al. (2020) extend self-supervised contrastive learning to the supervised setting, i.e., contrasting samples from different classes. They also claim that the supervised setting is more stable for hyperparameters. We design a novel contrastive learning framework that utilizes the contrasts of modalities both within a sample and between samples to enhance multimodal representation in a unified contrastive loss guided by a specific pairing pattern. Furthermore, we propose a data sampler to retrieve similar samples as positive pairs, which is in contrast to the above prior work that obtains positive pairs by data augmentation.\n\nMethodology\nIn this section, we introduce the overall architecture of ConFEDE followed by a detailed description of the contrastive feature decomposition process for learning multimodal representations.\n\nModel Architecture\nThe overall architecture of ConFEDE is shown in Figure 1 . Given a sample, we first encode each modality with corresponding feature extractors. Specifically, we use the [CLS] tag of BERT to encode text (i.e., T), and two separate transformer encoders to encode vision and audio modalities (i.e., V and A), respectively. After that, we decompose each encoded modality into a similarity feature (i.e., T s /V s /A s in Figure 1 ) and a dissimilarity feature (i.e., T d / V d /A d in Figure 1 ) with different projectors. Each projector is composed of layer normalization, a linear layer with the Tanh activation, and a dropout layer. Finally, we update the six decomposed features and fuse them to train the ConFEDE model with the following multi-task learning objective function:\nL all = L pred + \u00b4uni L uni + \u00b4cl L cl ,\nwhere L pred is the multimodal prediction loss, L uni represents the unimodal prediction loss and L cl represents the contrastive loss. \u00b4cl and \u00b4uni are hyper-parameters that balance the contribution of each regularization component to the overall loss L all . We describe each loss term as follows. L pred -Multimodal Prediction Loss. We use a multilayer perceptron (MLP) with the ReLU activation function as the classifier to get the final predictive result (i.e., \u0177 in Figure 1 ). We concatenate all 6 decomposed modality features to obtain the input to the classifier,\n[T i s ; T i d ; V i s ; V i d ; A i s ; A i d ]\n, where [\u2022; \u2022] denotes the concatenation of two vectors. Denote the set of samples in a batch as B. For a given sample i \u2208 B, let its prediction from the classifier be \u0177i m , we calculate the multimodal prediction loss by mean squared error:\n\u0177i m = MLP([T i s ; V i s ; A i s ; T i d ; V i d ; A i d ]), L pred = 1 n n i=1 (y i m \u2212 \u0177i m ) 2 ,\nwhere n is the number of samples in a batch and y i m is the multimodal label. L uni -Unimodal Prediction Loss. For each sample i, we also feed the 6 decomposed features\n[T i s , V i s , A i s , T i d , V i d , A i d ]\ninto a weight-shared MLP classifier separately to get the 6 predictions denoted by the vector \u00fbi . Specifically, we compute the unimodal prediction loss by:\n\u00fbi = MLP([T i s , V i s , A i s , T i d , V i d , A i d ]), u i = [y i m , y i m , y i m , y i t , y i v , y i a ], L uni = 1 n \u2225u i \u2212 \u00fbi \u2225 2 2 ,\nwhere the vector\nu i = [y i m , y i m , y i m , y i t , y i v , y i a ]\nrepresents the ground-truth labels for unimodal prediction. In other words, each decomposed feature is regularized to perform prediction individually.\nNote that the similarity features T i s , V i s , A i s are mapped through the MLP to predict the multimodal label y i m , whereas the dissimilarity features T i d , V i d , A i d are mapped through the MLP to predict modality-specific labels y i t , y i v , y i a (if available). When modality-specific labels are not available, the dissimilarity features T i d , V i d , A i d will also be used to predict multimodal label y i m . The rationale behind this design is that we let the similarity features capture the consistent information shared across different modalities via the overall multimodal label for the sample, while the dissimilarity features can retain modality-specific information represented by unimodal labels.\nL cl -Contrastive Loss. We further regularize the learning through Contrastive Feature Decomposition in one simple joint contrastive loss that contrasts (1) similar samples against dissimilar samples; (2) similarity features against dissimilarity features within a sample. The contrastive loss is denoted as:\nL cl = 1 n n i=1 \u2113 i cl ,\nwhere \u2113 i cl is the contrastive loss of sample i, the detailed derivation of which will be given in the following subsection. \nT V A T V A T V A Ts k Vs k As k T V A T V A T V A\n\nContrastive Feature Decomposition\nWe unify intra-sample and inter-sample contrastive learning into one simple NT-Xent contrastive loss framework (Chen et al., 2020) to conduct both modality representation learning and modality decomposition simultaneously. The loss for sample i is given by\n\u2113 i cl = (a,p)\u2208P i \u2212 log exp(sim(a, p)/\u00c4 ) (a,k)\u2208N i \u222aP i exp(sim(a, k)/\u00c4 ) ,\nwhere (a, p) and (a, k) denote a pair of decomposed feature vectors either within a sample, e.g., (T i s , V i s ), (T i s , A i d ), or across different samples, e.g., (T i s , T j s ). The sets P i and N i are given by\nP i = P i intra \u222a P i inter , N i = N i intra \u222a N i inter .\nHere P i is the positive pair set that includes both intra-sample positive pairs P i intra and inter-sample positive pairs P i inter , while N i is the negative pair set that consists of both intra-sample negative pairs N i intra and inter-sample negative pairs N i inter . Note that (a, p) is a positive pair in P i , and (a, k) is a pair in P i or N i . Specifically, we use the six decomposed features (T s , V s , A s , T d , V d , A d ) to form intra-sample positive/negative pairs, as shown in Figure 2 (a) , with P i intra and N i intra given by\nP i intra ={(T i s , V i s ), (T i s , A i s )} \u222a {(T j s , V j s ), (T j s , A j s ) |j \u2208 Neighbor i \u222a Outlier i }, N i intra ={(T i s , T i d ), (T i s , V i d ), (T i s , A i d )} \u222a {(T j s , T j d ), (T j s , V j d ), (T j s , A j d ) |j \u2208 Neighbor i \u222a Outlier i },\nwhere Neighbor i and Outlier i represent the similar samples and dissimilar samples for the sample i, respectively, to enlarge the scope of the contrast, the detail of which is given in Algorithm 1 that will be explained subsequently.\nNote that instead of treating all modalities equally as in other contrastive learning schemes, here we choose the text similarity feature T i s as an anchor, such that the visual and audio similarity features V i s and A i s are pushed closer to T i s , while in the meantime, the dissimilarity features in all modalities are pushed away from T i s . This is due to the observation that multimodal sentiment analysis is still largely centered around text information. Although other modalities can provide additional information to assist with sentiment prediction, they may also introduce more noise than text. Therefore, unlike other work, we avoid using visual/audio similarity features as anchors, which may bring noise into contrastive learning and confuse model training.\nWe now describe the data sampler shown in Algorithm 1 that retrieves similar samples for a given sample based on both multimodal features and multimodal labels to perform supervised contrastive learning across samples. Specifically, the sampling procedure can be divided into two steps.\nFirst, given the dataset D that contains |D| samples, for each sample pair (i, j) in D, we calculate the cosine similarity score between them:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]),\nwhere sim(w, v) = w T v/||w|| \u2022 ||v|| denotes the cosine similarity between two vectors w and v. And T, V, and A (in Figure 1 ) are the output of BERT, vision and audio encoders, respectively.\nSecond, we retrieve candidate similar/dissimilar sample sets for each sample. For each sample i, we sort samples that have the same multimodal label y i m according to the similarity scores in ascending order as a candidate similar sample set S i 0 . In contrast, we sort samples that have labels other than y i m as a candidate dissimilar sample set S i 1 . Two similar samples with high cosine similarity scores from S i 0 are randomly selected to form inter-sample positive pairs with sample i, which is denoted as Neighbor i . Four dissimilar samples from S i 1 are selected to form inter-sample negative pairs. We denote them as Outlier i in which two samples Outlier i 1 have low cosine similarity scores and the other two samples Outlier i 2 have high cosine similarity scores.\nUsually, we tend to select the samples in Neighbor i and Outlier i 1 to form positive and negative pairs with sample i, respectively. However, samples in Outlier i 2 have different labels but similar semantic information to sample i, making them hard to distinguish from sample i. Therefore, we additionally add these samples to Outlier i to specifically handle this issue by contrastive learning.\nBased on the samples retrieved by Algorithm 1 and the pairing strategy shown in Figure 2 (b), the inter-sample positive/negative pairs for sample i are given by:\nP i inter ={(T i s , T j s ), (V i s , V j s ), (A i s , A j s ) |j \u2208 Neighbor i } , N i inter ={(T i s , T k s ), (V i s , V k s ), (A i s , A k s ) |k \u2208 Outlier i }.\nNotably, our data sampler enables contrastive learning across samples through decomposed modality features without data augmentation. This contrasts original contrastive learning in image classification, which obtains positive pairs by augmentation applied to images. Moreover, we only use similarity features to obtain inter-sample pairing since the similarity features of similar samples in the same class should be close while the similarity features of samples in different classes should be far apart.\n\nExperiments\nWe mainly evaluate ConFEDE on CH-SIMS (Yu et al., 2020) , since it has unimodal labels, which can best meet the design of ConFEDE. To justify the effectiveness of ConFEDE when unimodal labels are unavailable, we further test ConFEDE on the MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018b) , which are two English MSA datasets. However, they can not best test the performance of ConFEDE.\nWe compare our methods with the state-of-theart baselines in Table 1 and 2: LF-DNN (Yu et al., 2020) , MFN (Zadeh et al., 2018a) , LMF (Liu et al., 2018) , TFN (Zadeh et al., 2017) , MulT (Tsai et al., 2019) , MISA (Hazarika et al., 2020) , MAG-BERT (Rahman et al., 2020) , HyCon (Mai et al., 2021) and Self-MM (Yu et al., 2021) . For a fair comparison, the methods which only report the results of a single run and have no valid official code released Algorithm 1: Data Sampling Algorithm Input: Dataset D with the corresponding features T , V , A and multimodal labels ym. Output: Neighbor i , Outlier i for every i \u2208 D Define: sim(w, v) = w T v/||w|| \u2022 ||v|| for every (i, j) \u2208 D do\nCompute the cosine similarity score:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]), end Define: argsort(X) = indices sort X ascendingly Let |D| = length of D, z = |D| 4 . for every sample i \u2208 D do\nRetrieve the similar sample set S i 0 :\nS i 0 = argsort({C i,j |j : y j m = y i m });\nRetrieve the dissimilar sample set S i 1 :\nS i 1 = argsort({C i,j |j : y j m \u0338 = y i m }),\nRandomly select two samples from the last z elements of S i 0 as Neighbor i ; Randomly select two samples from the first z elements of S i 1 as Outlier i 1 ; Randomly select two samples from the last z elements of S i 1 as Outlier i 2 ;\nOutlier i = Outlier i 1 \u222a Outlier i 2 .\nend for reproduction are not selected. A detailed introduction can be found in the supplementary material. The detailed experimental settings are introduced in Appendix C.\n\nEvaluation Metrics\nFollowing the previous works (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020; Hazarika et al., 2020) , we report our results in (multi-class) classification and regression with the average of 5 runs of different seeds. For classification, we report the multiclass accuracy and weighted F1 score. We calculate the accuracy of 2-class prediction (Acc-2), 3-class prediction (Acc-3), and 5-class (Acc-5) prediction for CH-SIMS and the accuracy of 2-class prediction and 7-class prediction (Acc-7) for MOSI and MOSEI. Besides, Acc-2 and F1-score of MOSI and MOSEI have two forms: negative/non-negative (non-exclude zero) (Zadeh et al., 2017; Yu et al., 2021) and negative/positive (exclude zero) (Tsai et al., 2019; Yu et al., 2021) \n\nResults\nThe performance comparison of all methods on CH-SIMS, MOSI, and MOSEI is summarized in Table 1 and Table 2 . The scores of the proposed method and its variations are the averages of 5 runs. The performances of all other baselines, except for MAG-BERT, have been sourced from published papers or official repositories 1 .\nOn the CH-SIMS dataset, our proposed method outperforms all baselines on all metrics. We achieve superior performance compared to the best baseline model, Self-MM, with an improvement of 2.19% on acc-2 and 1.64% on F1 scores. Additionally, the proposed model demonstrates exceptional ability in multi-class classification, outperforming the best baseline by 4.68% on acc-3 and 4.77% on acc-5.\nAs seen in the results, our proposed method, ConFEDE, consistently outperforms all other baselines on the CH-SIMS dataset. The superior classification performance demonstrates that our designed learning method is more effective than the compared methods. Our method, ConFEDE, effectively distinguishes similarity and dissimilarity information between modalities, providing clearer modality features to the downstream classifier for improved prediction. Additionally, the significant improvement in MAE and Corr further highlights the ability of our model to better understand the CH-SIMS dataset than the other baselines.\nTo further evaluate the effectiveness of our proposed method, ConFEDE, we trained our models on the MOSI and MOSEI datasets without uni-modal labels. Instead, we used their multimodal labels for compatibility. The results are presented in Table 2 . On the MOSI dataset, our method outperforms all other baselines in both the negative/nonnegative (NN) setting and negative/positive (NP) setting for acc-2 and F1 metrics. Additionally, our acc-7 and MAE metrics surpass most of the baselines. For the MOSEI dataset, our ConFEDE method outperforms all baselines in all metrics except for the NN Acc-2 and F1 score. Furthermore, our MAE is significantly lower than all baselines, reaching 0.522.\nIt is worth noting that our models perform much better in NP acc-2 than NN acc-2 for MOSEI, as shown in Table 2 . This is because the NN acc-2 setting is generally more challenging than the NP acc-2 setting, as it places more pressure on a model to classify data samples with a regression label of 0. Specifically, if there are two samples with a regression label of 0, when predicted by a regression model, the results might be -0.01 and 0.01. As the value range of \"Neutral\" is [-0.5,0.5) in MOSI and (-0.1,0.1] in SIMS, these two samples should be classified as \"Neutral\". However, in NN settings, they will be classified into two different classes, resulting in a worse acc-2. In contrast, with the NP setting, all \"Neutral\" samples are abandoned, resulting in a better acc-2.\nIn contrast, our method shows better performance in both NN and NP settings on MOSI when compared to other models. The Acc-7, MAE and Corr are also better or comparable to most baselines.\n\nAblation Study and Analysis\nTo evaluate the impact of our proposed structures, we conducted an ablation study on our proposed method by removing inter-sample contrastive learning and intra-sample contrastive learning. The results are shown in Table 1 . \"Plain\" represents the model without contrastive learning method, \"Inter\" represents the model with inter-sample contrastive learning only, and \"Intra\" represents the model with intra-sample contrastive learning and unimodal prediction as a sub-task.\nThe experiment shows that all three models perform worse than the original model. Among the three models, the plain setting has the lowest performance. Both intra-sample contrastive learning and inter-sample contrastive learning provide positive impacts on performance. Compared with Plain, by using text feature as the anchor, Intra filters out noise (useless information for sentiment analysis) in the vision and audio modality, leading to better prediction. This is also the reason why it reaches better acc-2 accuracy than both the other models. Since the acc-2 metric in CH-SIMS follows the negative/non-negative setting, a feature with lower noise helps the classifier make a more precise prediction value, making it easier to classify the 0-labeled samples. This also explains why we achieve better NN Acc-2 performance than all baselines on MOSI.\nFor the inter-sample contrastive learning method, by learning the common and different information between samples, \"Inter\" performs better on multiclass classification. The result of Inter on CH-SIMS shows great improvements on both acc-5 and MAE compared with the other two models, which proves that Acc-5 and regression performance benefits more from \"Inter\". This can also explain why we have a lower NN Acc-2 performance on MO-SEI. Since MOSEI is much larger than MOSI and CH-SIMS, it introduces more noise in each modality, the contrastive feature decomposition learning needs more epochs and a smaller learning rate to separate the useful information from noise. Meanwhile, inter-sample contrastive learning is more efficient on MOSEI. With the larger amount of samples, it is much easier for the sampler to find the most similar and dissimilar samples with the given sample, from which the model can understand the difference between samples better. Thus, ConFEDE can reach higher Acc-7 and regression performance than all other baselines on MOSEI.\nTo further evaluate the effectiveness of the contrastive feature decomposition method, we conducted an ablation study on Intra using the CH-SIMS dataset. As presented in Table 3 , we created three variations of Intra: 1) Intra with only multimodal labels for unimodal prediction (M-label); 2) Intra without the unimodal prediction component (-uni); 3) Intra without the similarity-dissimilarity learning method (-cl); and 4) Intra that uses all similarity features as anchors (+full), which utilizes T s , V s , and A s as anchors instead of T s only.\nThe results in the table demonstrate that all variations resulted in a decrease in performance compared to the original Intra in classification matrices. Both the intra-sample contrastive learning and the unimodal prediction task can regularize the learned representation, resulting in clearer information that aids the classifier in understanding the sample better. However, the \"+full\" setting introduces more noise by also using V s and A s as anchors, which confuses the model and diminishes the denoising ability of the contrastive feature decomposition learning.\n\nConclusion\nIn this paper, we propose a novel method for multimodal sentiment analysis (MSA) called ConFEDE. The ConFEDE framework is based on contrastive feature decomposition, which utilizes a unified contrastive training loss to capture the consistency and difference across modalities and samples. This approach allows for the simultaneous learning of modality decomposition within each sample and su-pervised contrastive learning across samples. Our proposed method is mainly evaluated on CH-SIMS. The result shows that the proposed method significantly outperforms many state-of-the-art multimodal sentiment analysis methods. We further conduct an extensive experiment on MOSI and MOSEI to test the capability of ConFEDE when no unimodal label is available, where our method achieves better performance than state-of-the-art methods on a number of performance metrics.\n"}
{"question": "What is the computational complexity of parsing in binary LCFRS with fan-out two?", "evidence": "  The complexity of parsing in a LCFRS is O(\u2113^3k |G|)  |G| is the grammar size. While polynomial, this is too high to be practical for unsupervised learning on real-world data. We thus restrict ourselves to LCFRS-2, i.e., binary LCFRS with fan-out two, which has been shown to have high coverage on discontinuous treebanks (Maier et al., 2012) ", "options": ["A. O(\u2113^2)", "B. O(\u2113^3k)", "C. O(\u2113^4)", "D. O(\u2113^5) "], "answer": "B", "content": "\nIntroduction\nUnsupervised parsing aims to induce hierarchical linguistic structures given only the strings in a language. A classic approach to unsupervised parsing is through probabilistic grammar induction (Lari and Young, 1990) , which learns a probabilistic grammar (i.e., a set of rewrite rules and their probabilities) from raw text. Recent work has shown that neural parameterizations of probabilistic contextfree grammars (PCFG), wherein the grammar's rule probabilities are given by a neural network over shared symbol embeddings, can achieve promising results on unsupervised constituency parsing (Kim et al., 2019; Jin et al., 2019 Jin et al., , 2021;; Yang et al., 2021b Yang et al., , 2022)) .\nHowever, context-free rules are not natural for modeling discontinuous language phenomena such as extrapositions, cross-serial dependencies, and Code: https://github.com/sustcsonglin/TN-LCFRS. wh-movements. Mildly context-sensitive grammars (Joshi, 1985) , which sit between context-free and context-sensitive grammars in the classic Chomsky-Sch\u00fctzenberger hierarchy (Chomsky, 1959; Chomsky and Sch\u00fctzenberger, 1963 ), 1 are powerful enough to model richer aspects of natural language including discontinuous and non-local phenomena. And despite their expressivity they enjoy polynomial-time inference algorithms, making them attractive both as cognitively plausible models of human language processing and as targets for unsupervised learning.\nThere are several weakly equivalent formalisms for generating the mildly context-sensitive languages which might serve as potential targets for grammar induction: tree adjoining grammars (Joshi, 1975) , head grammars (Pollard, 1985) , combinatory categorial grammars (Steedman, 1987) , and linear indexed grammars (Gazdar, 1988) . In this paper we work with linear context-free rewriting systems (LCFGS, Vijay-Shanker et al., 1987) , which generalize the above formalisms and are weakly equivalent to multiple context-free grammars (Seki et al., 1991) . Derivation trees in an LCFRS directly correspond to discontinuous constituency trees where each node can dominate a non-contiguous sequence of words in the yield, as shown in Fig. 1 .\nWe focus on the LCFRS formalism as it has previously been successfully employed for supervised discontinuous constituency parsing (Levy, 2005; Maier, 2010; van Cranenburgh et al., 2016) . The complexity of parsing in a LCFRS is O(\u2113 3k |G|), where \u2113 is the sentence length, k is the fan-out (the maximum number of contiguous blocks of text that can be dominated by a nonterminal), and |G| is the grammar size. While polynomial, this is too high to be practical for unsupervised learning on real-world data. We thus restrict ourselves to LCFRS-2, i.e., binary LCFRS with fan-out two, which has been shown to have high coverage on discontinuous treebanks (Maier et al., 2012) . Even with this restriction LCFRS-2 remains difficult to induce from raw text due to the O(\u2113 6 |G|) dynamic program for parsing and marginalization. However Corro (2020) observe that a O(\u2113 5 |G|) variant of the grammar that discards certain rules can still recover 98% of real world treebank constituents. Our approach uses with this restricted variant of LCFRS-2 (see Sec 2.2). Finally, following recent work which finds that that overparameterizing deep latent variable models is beneficial for unsupervised learning (Buhai et al., 2020; Yang et al., 2021b; Chiu and Rush, 2020; Chiu et al., 2021) , we scale LCFRS-2 to a large number of nonterminals by adapting tensor-decomposition-based inference techniques-originally developed for PCFGs (Cohen et al., 2013; Yang et al., 2021b Yang et al., , 2022)) -to the LCFRS case.\nWe conduct experiments German and Dutchboth of which have frequent discontinuous and non-local language phenomena and have available discontinuous treebanks-and observe that our approach is able to induce grammars with nontrivial performance on discontinuous constituents. Rabanser et al., 2017) to decompose the 3D binary rule probability tensor T \u2208 R m\u00d7m\u00d7m as,\n\nApproach\nT = r q=1 u q \u2297 v q \u2297 w q ,\nwhere u q , v q , w q \u2208 R m , r is the tensor rank (a hyperparameter), and \u2297 is the outer product. Letting U, V, W \u2208 R r\u00d7m be the matrices resulting from stacking all u q , v q , w q , Cohen et al. ( 2013) give the following recursive formula for calculating the inside tensor \u03b1 \u2208 R (\u2113+1)\u00d7(\u2113+1)\u00d7m for a sentence of length \u2113:\n\u03b1 L i,j = V \u03b1 i,k , \u03b1 R j,k = W \u03b1 k,j , \u03b1 i,j = U T j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k .\nHere 2021b) and further pre-compute matrices J = V U T , K = W U T to rewrite the above recursive formula as:\n\u03b1 L , \u03b1 R \u2208 R (\u2113+1)\u00d7(\u2113+1\n\u03b1 L i,j = J\u03b1 \u2032 i,j ,\u03b1 R i,j = K\u03b1 \u2032 i,j \u03b1 \u2032 i,j = j\u22121 k=i+1 \u03b1 L i,j \u2022 \u03b1 R j,k\nwhere \u03b1 \u2032 \u2208 R (n+1)\u00d7(n+1)\u00d7r is an auxiliary inside score tensor. The resulting complexity of this approach is O(\u2113 3 r + \u2113 2 r 2 ), which is smaller than O(\u2113 3 r + \u2113 2 mr) when r \u226a m, i.e., in the setting with a large number of nonterminals whose probability tensor is of low rank. In this paper we adapt this low rank neural parameterization to the LCFRS case to scale to a large number of nonterminals.\n\nRestricted LCFRS\nIn an LCFRS, a single nonterminal node can dominate a tuple of strings that need not be adjacent in the yield. The tuple size is referred to as the fanout. We mark the fan-out of each non-leaf node in Fig. 1 . The fan-out of an LCFRS is defined as the maximal fan-out among all its nonterminals, and influences expressiveness and parsing complexity. For a binary LCFRS (i.e., LCFRS with derivation rules that have at most two nonterminals on the right hand side) with fan-out k, the parsing complexity for a sentence of length \u2113 is O(\u2113 3k ). 2 In this paper we work with binary LCFRS with fanout 2 (Stanojevi\u0107 and Steedman, 2020, LCFRS-2) , which is expressive enough to model discontinuous constituents but still efficient enough to enable practical grammar induction from natural language data. This choice is also motivated by Maier et al. (2012) who observe that restricting the fan-out to two suffices for capturing a large proportion of discontinuous constituents in standard treebanks. 3 However, LCFRS-2's inference complexity of O(\u2113 6 |G|) is still too expensive for practical unsupervised learning. We thus follow Corro (2020) and discard all rules that require O(\u2113 6 ) time to parse, which reduces parsing complexity to O(\u2113 5 |G|). 4 Formally, this restricted LCFRS-2 is a 6-tuple G = (S, N 1 , N 2 , P, \u03a3, R) where: S is the start symbol; N 1 , N 2 are a finite set of nonterminal symbols of fan-out one and two, respectively; P is a finite set of preterminal symbols; \u03a3 is a finite set of terminal symbols; and R is a set of rules of the following form (where M \u225c N 1 \u222a P):\nS(x) \u2192 A(x) A \u2208 N 1 A(xy) \u2192 B(x)C(y) A \u2208 N 1 , B, C \u2208 M A(yxz) \u2192 B(x)C(y, z) A \u2208 N 1 , B \u2208 M, C \u2208 N 2 A(x, y) \u2192 B(x)C(y) A \u2208 N 2 , B, C \u2208 M\n2 A binary CFG is thus a special case of a binary LCFRS with fan-out one, and parsing in this case reduces to the classic CKY algorithm.\n3 For instance, Stanojevi\u0107 and Steedman (2020) report that LCFRS-2 can cover up to 87% of the gold discontinuous constituents in the NEGRA treebank. We refer readers to Table 1 of Corro (2020) for more details. 4 These correspond to rules (d), (i), (j), (k), and (l) in Figure 3 of Corro (2020) .\n\nItem form:\n[A, i, j]: fan-out-1 node A spanning [i, j)\n[A, i, j, k, n]: fan-out-2 node A spanning [i, j), [k, n) Axioms: [A, i, i + 1], 0 \u2264 i < \u2113 + 1, A \u2208 N 1 Goals: [S, 0, n] Deductive rules: [B, i, k] [C, k, j] [A, i, j] A(xy) \u2192 B(x)C(y) i < k < j 1a [B, i, j] [C, m, n] [A, i, j, m, n] A(x, y) \u2192 B(x)C(y) i < j < m < n 1b [B, m, n] [C, i, m, n, j] [A, i, j] A(yxz) \u2192 B(x)C(y, z) i < m < n < j 2a [B, i, k] [C, k, j, m, n] [A, i, j, m, n] A(xy, z) \u2192 B(x)C(y, z) i < k < j < m < n 2b [B, k, j] [C, i, k, m, n] [A, i, j, m, n] A(yx, z) \u2192 B(x)C(y, z) i < k < j < m < n 2c [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, xz) \u2192 B(x)C(y, z) i < j < m < k < n 2d [B, m, k] [C, i, j, k, n] [A, i, j, m, n] A(y, zx) \u2192 B(x)C(y, z) i < j < m < k < n 2e\nTable 1 : Chart parsing algorithm described in the parsing-asdeduction framework. Here \u2113 is the sentence length and we use interstice indices (not word indices) as in Corro (2020) .\nA(xy, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(yx, z) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, xz) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M A(y, zx) \u2192 B(x)C(y, z) A, C \u2208 N 2 , B \u2208 M T (w) \u2192 w, T \u2208 P, w \u2208 \u03a3.\nHere A(x) indicates that A has a fan-out 1; A(x, y) indicates that A has a fan-out 2 and x and y are nonadjacent contiguous strings in the yield of A. \n\u2192 B(x)C(y, z) above. B is a fan-out-1 node whose yield is x = w i \u2022 \u2022 \u2022 w k\u22121 and C is a fan-out-2 node whose first span is y = w k \u2022 \u2022 \u2022 w j\u22121 and whose second span is z = w m \u2022 \u2022 \u2022 w n\u22121 .\nA is the parent node of B, C, and inherits the yields of B and C, where x is concatenated with y to form a contiguous span and z is a standalone span.\nParsing. Table 1 gives the parsing-asdeduction (Pereira and Warren, 1983 ) description of the CKY-style chart parsing algorithm of our restricted LCFRS-2.\n\nTensor decomposition-based neural parameterization\nWe now describe a parameterization of LCFRS-2 that combines a neural parameterization with tensor decomposition, which makes it possible to scale LCFRS-2 to thousands of nonterminals.\nLet\nm 1 = |N 1 |, m 2 = |N 2 |, p = |P|, and m = m 1 + p.\nThe rules involving A \u2208 N 1 on the left hand side are 1a and 2a , whose probabilities can be represented by 3D tensors\nC 1 \u2208 R m 1 \u00d7m\u00d7m and D 1 \u2208 R m 1 \u00d7m\u00d7m 2 . For A \u2208 N 2 , the relevant rules are 1b , 2b , 2c , 2d , 2e , whose proba- bilities can be represented by 3D tensors C 2 \u2208 R m 2 \u00d7m\u00d7m and D 3 , D 4 , D 5 , D 6 \u2208 R m 2 \u00d7m\u00d7m 2 . We stack D 3 , D 4 , D 5 , D 6 into a single 4D tensor D 2 \u2208 R m 2 \u00d7m\u00d7m 2 \u00d74\nto leverage the structural similarity of these rules. Since these tensors are probabilities, we must have\nEQUATION\nEQUATION\nTensor decomposition. To scale up the LCFRS-2 to a large number of nonterminals, we first apply CPD on all the binary rule probability tensors,\nC 1 = r 1 \u22121 q=0 U 1 :,q \u2297 V 1 :,q \u2297 W 1 :,q C 2 = r 2 \u22121 q=0 U 2 :,q \u2297 V 2 :,q \u2297 W 2 :,q D 1 = r 3 \u22121 q=0 U 3 :,q \u2297 V 3 :,q \u2297 W 3 :,q D 2 = r 4 \u22121 q=0 U 4 :,q \u2297 V 4 :,q \u2297 W 4 :,q \u2297 P :,q\nwhere U :,q denotes the q-th column of U . The dimensions of these tensors are\nU 1 \u2208 R m 1 \u00d7r 1 , V 1 , W 1 \u2208 R m\u00d7r 1 , U 2 \u2208 R m 1 \u00d7r 2 , V 2 \u2208 R m\u00d7r 2 , W 2 \u2208 R m 2 \u00d7r 2 , U 3 , W 3 \u2208 R m 2 \u00d7r 3 , U 4 , W 4 \u2208 R m 2 \u00d7r 4 , V 3 \u2208 R m\u00d7r 3 , V 4 \u2208 R m\u00d7r 4\n, and P \u2208 R 4\u00d7r 4 . Here r 1 , r 2 , r 3 , r 4 are the ranks of the tensors that control inference complexity. To ensure these factorizations lead to valid probability tensors, 1), we additionally impose the following restrictions: (1) all decomposed matrices are non-negative; (2) P, V i , W i are column-wise normalized where i \u2208 {1, 2, 3, 4};\n(3) \u2200i, j U 1 ij + k U 2 ik = 1; and (4) \u2200i, j U 3 ij + k U 4 ik = 1.\nIt is easy to verify that Eq. 1 and 2 are satisfied if the above requirements are satisfied.\nRank-space dynamic programming. For unsupervised learning, we need to compute the marginal likelihood of a sentence p(w 1 w 2 \u2022 \u2022 \u2022 w \u2113 ). We give the rank-space dynamic program (i.e., the inside algorithm) for computing p(w\n1 w 2 \u2022 \u2022 \u2022 w \u2113 ) in this tensor decomposition-based LCFRS-2 in App. A.\nThe resulting complexity is dominated by O(\u2113 5 r 4 + \u2113 4 (r 3 +r 4 )(r 2 +r 4 )). We thus set r 4 to a very small value, which greatly improves runtime.\nParameterization. Following prior work on neural parameterizations of grammars (Jiang et al., 2016; Kim et al., 2019) , we parameterize the component matrices to be the output of neural networks over shared embeddings.\nThe symbol embeddings are given by: E 1 \u2208 R m\u00d7d where the first m 1 rows correspond to fanout-1 nonterminal embeddings and the last p rows are the preterminal embeddings; E 2 \u2208 R m 2 \u00d7d for the fan-out-2 nonterminal embedding matrix; r \u2208 R d for the start symbol embedding. We also have four sets of \"rank embeddings\"\nR 1 \u2208 R r 1 \u00d7d , R 2 \u2208 R r 2 \u00d7d , R 3 \u2208 R r 3 \u00d7d\n, and R 4 \u2208 R r 4 \u00d7d . Given this, the entries of the U, V, W matrices are given by,\nU o ij \u221d exp{(R o j ) \u22a4 f o U (E 1 i )}, o \u2208 {1, 2} U o ij \u221d exp{(R o j ) \u22a4 f o U (E 2 i )}, o \u2208 {3, 4} V o ij \u221d exp{(R o j ) \u22a4 f o V (E 1 i )}, o \u2208 {1, 2, 3, 4} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 1 i )}, o \u2208 {1, 2} W o ij \u221d exp{(R o j ) \u22a4 f o W (E 2 i )}, o \u2208 {3, 4} where f o U , f o V , f o W are one-layer ReLU MLPs with output size d. U o , V o , W o\nare normalized according to the requirements described in the previous subsection. We share the parameters of the following MLP pairs:\n(f 1 U , f 2 U ), (f 3 U , f 4 U ), (f 1 V , f 3 V ), (f 2 V , f 4 V ), (f 1 W , f 3 W ), (f 2 W , f 4 W )\nas they play similar roles (e.g., f 1 V and f 3 V are both applied to left children). For the D 2 tensor we also require the matrix P \u2208 R 4\u00d7r 4 , and this is given by P \u22a4 = f P (R 4 ), where f P is a one-layer residual network with output size 4 that is normalized via a softmax across the last dimension.\nFinally, for the starting and the terminal distributions we have\ns = f s (r), Q = f Q (E 1 m 1 :\n), which results in s \u2208 R m 1 (i.e., the probability vector for rules of the form S \u2192 A) and Q \u2208 R p\u00d7v (i.e., probability matrix for rules of the form T (w) \u2192 w). Here E 1 m 1 : is the last p rows of E 1 , and f s and f Q are residual MLPs with softmax applied in the last layer to ensure that s and Q are valid probabilities.\nDecoding. While the rank-space inside algorithm enables efficient computation of sentence likelihoods, direct CKY-style argmax decoding in this grammar requires instantiating the full probability tensors and is thus computationally intractable. We follow Yang et al. (2021b) and use Minimal Bayes Risk (MBR) decoding (Goodman, 1996) . This procedure first obtains the posterior probability of each span's being a constituent via the inside-outside algorithm (which has the same complexity as the inside algorithm). Then, these posterior probabilities are used as input into CKY in a grammar that only has a single nonterminal. The complexity of this approach is thus independent of the number of nonterminals in the original grammar, and takes O(\u2113 5 ). This strategy can be seen as finding the tree that has the largest number of expected constituents (Smith and Eisner, 2006) . See App. A for details.\n\nEmpirical Study\nData. We conduct experiments with our Tensor decomposition-based Neural LCFRS (TN-LCFRS) on German and Dutch, where discontinuous phenomena are more common (than in English). For German we concatenate TIGER (Brants et al., 2001) and NEGRA (Skut et al., 1997) as our training set, while for Dutch we use the LASSY Small Corpus treebank (van Noord et al., 2013) . The data split can be found in App. B.1. For processing we use disco-dop 5 (van Cranenburgh et al., 2016) and discard all punctuation marks. We further take the most frequent 10,000 words for each language as the vocabulary, similar to the standard setup in unsupervised constituency parsing on PTB (Shen et al., 2018 (Shen et al., , 2019;; Kim et al., 2019) .\nGrammar size. To investigate the importance of using a large number of latent variables (which has previously been shown to be helpful for structure induction (Buhai et al., 2020; Yang et al., 2021b )), we train TN-LCFRSs of varying sizes. We first choose the number of preterminals |P| \u2208 {45, 450, 4500} and set the number of fan-out one and fan-out two nonterminals to be\n|N 1 | = |N 2 | = 1 3 |P|.\nThe rank of the probability tensors are set to r 1 = r 3 = 400, r 2 = r 4 = 4, and the dimensionality of the Baselines. Our baselines include: the neural PCFG (N-PCFG) and the compound PCFG (C-PCFG) (Kim et al., 2019) , which cannot directly predict discontinuous constituents 6 but still serve as strong baselines for overall F1 since the majority of spans in these treebanks are continuous; and their direct extensions, neural LCFRS (N-LCFRS) and compound LCFRS (C-LCFRS), which do not employ the tensor-based low-rank factorization. These non-low-rank models have high computational complexity and hence we set |P| = 45 for these models. When |P| = 4500, we also compare against the tensor decompositional-based neural PCFG (TN-PCFG) from Yang et al. (2021b) .\nEvaluation. We use unlabeled corpus-level F1 to evaluate unsupervised parsing performance, reporting both overall F1 and discontinuous F1 (DF1). For all experiments, we report the mean results and standard deviations over four runs with different random seeds. See App. B.2 for further details.\n\nMain results\nTable 2 shows the main results. With smaller grammars (|P| = 45), we find that both neural/compound LCFRSs have lower F1 than their PCFG counterparts, despite being able to predict discontinuous constituent spans. On the other hand, TN-LCFRS achieves better F1 than N-LCFRS even though it is a more restricted model (since it assumes that the rule probability tensors are of low rank), showing the benefits of parameter sharing through low rank factorizations. As we scale up TN-LCFRSs with |P| \u2208 {45, 450, 4500} we observe continuous improvements in performance, with TN-LCFRS 4500 achieving the best F1 and DF1 on all three datasets. These results all outperform trivial (left branching, right branching, and random tree) baselines.\nAs an upper bound we also train a supervised model with TN-LCFRS 4500 . 7 We also show the maximum possible performance with oracle binary trees with this optimal binarization. While the discontinuous F1 of our unsupervised parsers are nontrivial, there is still a large gap between the unsupervised and supervised scores (and also between the supervised and the oracle scores), indicating opportunities for further work in this area.\n\nAnalysis\nRecall by constituent label. Table 3 shows the recall by constituent tag for the different models averaged over four independent runs. Overall the unsupervised methods do well on noun phrases (NP), prepositional phrases (PP) and proper nouns (PN), with some of the models approach the supervised baselines. Verb phrases (VP) and adjective dynamic programming to sum out all possible nonterminals for each node, resulting in the joint log probability of unlabeled binarized tree and sentence. This was then maximized during training. As for the oracle bound, we emphasize that the gold trees are nonbinary while our model can only predict binary trees. Approximation error. Approximation error in the context of unsupervised learning arises due to the mismatch between the EM objective (i.e., log marginal likelihood) and structure recovery (i.e., F1), and is related to model misspecification (Liang and Klein, 2008) . Figure 2 (left column) plots the training/dev perplexity as well as the dev F1/DF1 as a function of the number of epochs. We find that larger grammars result in better performance in terms of both perplexity and structure recovery, which ostensibly indicates that the unsupervised objective is positively correlated with structure induction performance. However, when we first perform supervised learning on the log joint likelihood and then switch to unsupervised learning with log marginal likelihood (Figure 2 , right), we find that while perplexity improves when we switch to the unsupervised objective, structure induction performance deteriorates. 8 Still, the difference in F1 before and after switching to the unsupervised objective is less for larger models, confirming the benefits of using larger grammars.\n\nEven more restricted LCFRS formalisms.\nThere are even more restricted versions of LCFRSs which have faster parsing (e.g. O(\u2113 3 ), O(\u2113 4 )) but 8 It is worth noting that the phenomenon of mismatch between log marginal likelihood objective and parsing accuracy is quite common in unsupervised grammar induction (and latent variable modeling approaches to structured induction more generally). Many previous works have observed this phenomenon, e.g., Merialdo (1994) in the context of HMMs, and Johnson et al. (2007) and Liang and Klein (2008) in the context of PCFGs. This is partially attributed to the fact that generative grammars often make some unreasonable independence assumptions to make the training process tractable, which does not fully comply with the true generative process of human languages and their underlying structures. 45.4 0.9 44.5 0.5\n\nModel\nTable 5 : Ablation studies on the German (TIGER) treebank.\ncan still model discontinuous constituents. In the supervised case, these restricted variants have been shown to perform almost as well as the more expressive O(\u2113 5 ) and O(\u2113 6 ) variants (Corro, 2020) .\nIn the unsupervised case however, we observe in Table 5 that disallowing O(\u2113 5 ) rules ( 2b , 2c , 2d , 2e ) significantly degrades discontinuous F1 scores. We posit that this phenomena is again related to empirical benefits of latent variable overparameterization-while in theory it is possible to model most discontinuous phenomena with more restricted rules, making the generative model more expressive via \"overparameterizing\" in rule expressivity space (i.e., using more flexible rules than is necessariy) empirically leads to better performance.\nParameter sharing. As shown in Table 5 , it was important to share the symbol embeddings across the different rules. Sharing the parameters of the MLPs as described in Sec. 2.3 was also found to be helpful. This highlights the benefits of working with neural parameterizations of grammars which enable easy parameter sharing across rules that share symbols and/or have similar shapes.\nQualitative analysis. In Fig. 3 , we show some examples trees in German. For each sentence, we show the gold, TN-LCFRS 4500 , and TN-PCFG 4500 trees. In the first sentence, the crossing dependency occurs due to the initial adverb (\"So\")'s being analyzed as a dependent of the non-finite verb phrase at the end of the sentence which occurs due to German V2 word order. Our parser correctly predicts this dependency, although the subject NP (which itself is correctly identified) has the wrong internal structure. relative clause a part of the non-finite verb complex, which does not conform to the annotation guidelines but resembles an alternative analysis that has been proposed for extraposed relative clauses (Baltin, 1983) . Sentence initial adverbs in the context of auxiliary verb constructions and right-extraposed relative clauses describe two common instances of discontinuous phenomena in German. Wh-questions constitute another potential class of discontinuous phenomena; however, these are not treated as discontinuous in TIGER/NEGRA. See App. D for more examples trees (including on Dutch).\n\nRelated work\nMildly context-sensitive grammars. Given the evidence against the context-freeness of natural language (Shieber, 1985) , mildly context-sensitive grammars such as tree adjoining grammars were thought to be just flexible (but still constrained) enough to model natural language (Joshi, 1985) . Prior work on inducing mildly context-sensitive grammars has generally focused on combinatory categorial grammars (Bisk and Hockenmaier, 2012, 2013) , and we are unaware of any work on in-ducing LCFRSs from observed yields alone. Our work is also related to the rich line of work on supervised discontinuous parsing (Kallmeyer and Maier, 2010; Maier et al., 2012; Maier, 2015; Corro, 2020; Vilares and G\u00f3mez-Rodr\u00edguez, 2020; Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez, 2020 , 2021 , 2023) , though we are unaware of any prior work on unsupervised discontinuous parsing.\nNeural grammars. Early work on probabilistic approaches to grammar induction was largely negative (Lari and Young, 1990; Carroll and Charniak, 1992) . However, recent work has shown that neural parameterizations of classic grammars can greatly improve structure induction. Our work adds to the line of work on neural parameterizations of dependency models (Jiang et al., 2016; Han et al., 2017; He et al., 2018; Yang et al., 2020) , context-free grammars (Kim et al., 2019; Jin et al., 2019; Zhu et al., 2020; Yang et al., 2021a) , and synchronous grammars (Kim, 2021; Wang et al., 2022; Friedman et al., 2022) . Neural parameterizations make it easy to share parameters and condition on additional side information (images/audio/video) which has shown to be particularly useful for multimodal grammar induction (Zhao and Titov, 2020; Jin and Schuler, 2020; Su et al., 2021; Hong et al., 2021; Zhang et al., 2021) .\nScaling latent variable models. Buhai et al. (2020) study the empirical benefits of overparameterization in learning latent variable models. Other works have explored parameterizations of latent variable models that make it especially amenable to scaling (Chiu and Rush, 2020; Chiu et al., 2021; Yang et al., 2021b Yang et al., , 2022)) . Relatedly, Peharz et al. (2020) and Liu et al. (2022) show the benefits of scaling probabilistic circuits (Choi et al., 2020) .\n\nConclusion\nThis work studied unsupervised discontinuous constituency parsing with mildly context-sensitive grammars, focusing on the formalism of linear context-free rewriting systems. By using a tensor decomposition-based neural parameterization of linear context-free rewriting systems, our approach was able to induce grammars that had nontrivial discontinuous parsing performance on German and Dutch. Whether even more expressive grammars will eventually lead to models learn linguistically meaningful structures and are at the same time competitive with pure neural language models (as a language model) remains an open question.\n"}
{"question": "Which is not the paper\u2019s intention?", "evidence": "  In this paper, we propose ConFEDE, a unified learning framework that jointly performs contrastive representation learning and contrastive feature decomposition to enhance representation of multimodal information.   We conducted extensive experiments on CH-SIMS, MOSI and MOSEI to evaluate various state-of-the-art multimodal sentiment analysis methods. Experimental results show that ConFEDE outperforms all baselines on these datasets on a range of metrics.  ", "options": ["A. Build a unified learning framework --ConFEDE", "B. Simultaneously combines contrastive representation learning and contrastive feature decomposition to improve the representation of multimodal information.", "C.Assess the effectiveness of different state-of-the-art methods for multimodal sentiment analysis.", "D. Find a definition for Multimodal Sentiment Analysis."], "answer": "D", "content": "\nIntroduction\nMultimodal deep learning involves interpreting and analyzing multimodal signals together, where each modality refers to a way in which something is experienced and felt, e.g., the visual, audio, or language modality. With the widespread popularity of online social media, such as Instagram, Tik-Tok, Facebook, etc., videos containing multiple modalities have become a major information carrier, which brings new challenges to content recommendation and classification, e.g., video question answering (Lei et al., 2021; Li et al., 2020) , video captioning (Ging et al., 2020; Li et al., 2020) , and video retrieval (Akbari et al., 2021; Lei et al., 2021) .\nWhile traditional sentiment analysis is mainly based on language, multimodal sentiment analysis (MSA) predicts the human emotion by utilizing extra information available in visual and audio modalities of the content to assist with language-based prediction. Here, the text modality contains the semantic meaning of the spoken language. The visual modality extracts the facial characteristics (e.g., head orientation, facial expressions, and pose) of the speaker. The audio modality reflects the emphasis on the utterance (e.g., through pitch, bandwidth and intensity). MSA has recently gained much attention in research for several reasons. On one hand, because of the abundance of social media content, commercial interests are switching from gauging user opinions/emotions from text only to more thorough multimodal analysis based on videos. On the other hand, short video platforms (e.g., TikTok, Instagram) allow users to easily create multimodal content including visual information, audio, and inserted text, while these modalities are sometimes noisy or even contradicting each other in sentiments. Therefore, the presence of multimodal information in addition to the text or language itself is necessary to make a thorough conclusion about the overall sentiment of a video.\nMultimodal fusion has become essential to gaining a deeper understanding of these video scenes (Baltru\u0161aitis et al., 2018) and has proven to be helpful in many downstream tasks. Various multimodal fusion techniques have been proposed for MSA, among which a basic solution is concatenating the extracted feature of each modality before performing downstream regression or classification. Recent work has recognized the importance of identifying modality-invariant information across modalities and fuse them to strengthen sentiment prediction (Hazarika et al., 2020; Zadeh et al., 2018a; Rahman et al., 2020; Sun et al., 2020) .\nAlthough modality-invariant information helps reinforce the understanding of the content, there are also cases where sentiments of different modalities contradict each other. For example, when one thanks someone with phrases like \"Finally I can rest easy tonight\" or \"I can't thank you enough\", it is very hard to conclude whether the sentiment is positive or negative without looking at the nonverbal cues, such as tones, facial expressions, and gestures. In fact, many sarcastic opinions are expressed by non-linguistic markers. In these cases, the overall sentiment cannot simply be judged by a majority vote among all modalities. Thus, multimodal representation learning that respects both consistency and incongruity between modalities have recently shown great promise (Yu et al., 2020; Hazarika et al., 2020) .\nIn this paper, we propose ConFEDE, a Contrastive FEature DEcomposition framework, which integrates both modality decomposition within each sample and supervised contrastive learning across samples in a single unified contrastive learning framework. Our main contributions are summarized as follows: (1) We integrate inter-sample contrastive learning and intra-sample modality decomposition into a simple unified loss function, based on a customized data sampler that allows us to sample positive/negative data pairs to perform both learning tasks. (2) We propose to decompose each modality into a similarity feature and a dissimilarity feature, and use the similarity feature of the text as an anchor to build the contrastive relation among all decomposed features. This is due to the observation that sentiment analysis is still largely centered around text and spoken language, while other modalities can provide extra information to assist with prediction. (3) Based on multimodal representation learning proposed above, we further introduce a multi-task prediction loss that depends on each decomposed modality representation and enables the model to learn from both multimodal prediction and unimodal prediction.\nWe mainly evaluated ConFEDE on CH-SIMS (Yu et al., 2020) benchmark, which contains both unimodal and overall sentiment labels for each sample. The result shows that the proposed method significantly outperforms a wide range of stateof-the-art multimodal sentiment analysis methods. To test the capability when no unimodal labels are provided, we further conduct experiments on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , which contain only an overall sentiment label for each sample, which shows that our proposed method can also achieve better per-formance than state-of-the-art methods on a number of performance metrics without unimodal labels. We provide extensive ablation studies to show the effectiveness and necessity of each design component in ConFEDE. The code is released at https://github.com/XpastaX/ConFEDE/.\n\nRelated Work\nIn this section, we discuss the related work in MSA and contrastive representation learning.\n\nMultimodal Sentiment Analysis\nPrior works on multimodal sentiment analysis mostly focus on predicting sentiments based on text and vision (Zhu et al., 2022; Ji et al., 2019; Liu et al., 2019) .However, there is growing interest in analyzing sentiment using all three modalities: text, audio, and vision (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020) . Zadeh et al. (2016) were among the first to propose a multimodal dictionary that could learn the dynamic interactions between facial gestures and spoken words to model sentiments. They later introduced a Tensor Fusion Network (TFN) to learn the intra-modality and inter-modality dynamics of three modalities in an end-to-end way (Zadeh et al., 2017) . Furthermore, they presented a Memory Fusion Network (MFN) which is composed of Long Short Term Memories (LSTMs) to learn the view-specific and cross-view interactions of three views (text, video, and audio) to improve sentiment analysis performance. Rahman et al. (2020) proposed a Multimodal Adaptation Gate (MAG) to fine-tune BERT (Devlin et al., 2019) on multimodal data to improve sentiment analysis performance. However, these prior works do not consider modality-specific information.\nTo better study the impacts that modalityspecific information can bring to MSA, Yu et al. (2020) construct a new multimodal sentiment analysis dataset CH-SIMS, which contains a unimodal label for each modality of a sample. Experiments show a great improvement in overall sentiment prediction after simply integrating unimodal predictions as subtasks in the learning objective. Hazarika et al. (2020) further decompose each modality into a modality-invariant and a modalityspecific representation, and employ squared Frobenius norm loss as the regularizer. However, they treat all modalities equally while regularizing the prediction result, which ignores the different effectiveness of modalities. In real cases, the text is usually more effective on MSA tasks compared to vision and audio. In other words, it is less \"noisy\" than the other two modalities. Also, they employ Central Moment Discrepancy loss to push the modality-invariant representations close and a Frobenius norm to push modality-specific representations to be orthogonal, while in our method, we integrate the above mechanism into a single loss function. Moreover, they regularize the decomposed features by reconstructing the original features with the generated features. We, instead, avoid using such a method and regularize the decomposed features with unimodal prediction tasks. To improve the decomposition performance, we further aggregate the supervised contrastive learning between samples into our frameworks by a custom-designed sampling method.\nA concurrent work HyCon (Mai et al., 2021) introduces a contrastive learning method for MSA, taking both inter-sample and intra-sample contrasts into consideration. However, they ignore the regularization for each decomposed feature. In contrast, in ConFEDE, within-sample feature contrasts are constructed based on a specific pattern centered around text similarity features. Also, when performing inter-sample contrastive learning, Hy-Con samples positive and negative pairs randomly based on MSA labels. In contrast, we design a data sampler that considers both the labels and similarities between modalities to retrieve positive/negative pairs. Due to these reasons, our method beats Hy-Con on most metrics on MOSI (Zadeh et al., 2018a) and MOSEI (Zadeh et al., 2018b) , and is able to utilize unimodal labels to further boost performance, e.g., on CH-SIMS (Yu et al., 2020) .\n\nContrastive Representation Learning\nContrastive learning has achieved great success in representation learning by contrasting positive pairs against negative pairs (Akbari et al., 2021; Hassani and Khasahmadi, 2020; Chen et al., 2020) . Akbari et al. (2021) train a Video-Audio-Text Transformer (VATT) using multimodal contrastive learning for the alignment of video-text and videoaudio pairs, and thus achieve state-of-the-art on various computer vision tasks (e.g., audio classification and video action recognition). Hassani and Khasahmadi (2020) propose to learn node and graph level representations by contrasting encodings obtained from different structural views of graphs and achieve the state-of-the-art on various graph classification benchmarks. Chen et al. (2020) present a self-supervised framework, SimCLR, to learn visual representations through a contrastive loss between augmented views of the same image sample. Khosla et al. (2020) extend self-supervised contrastive learning to the supervised setting, i.e., contrasting samples from different classes. They also claim that the supervised setting is more stable for hyperparameters. We design a novel contrastive learning framework that utilizes the contrasts of modalities both within a sample and between samples to enhance multimodal representation in a unified contrastive loss guided by a specific pairing pattern. Furthermore, we propose a data sampler to retrieve similar samples as positive pairs, which is in contrast to the above prior work that obtains positive pairs by data augmentation.\n\nMethodology\nIn this section, we introduce the overall architecture of ConFEDE followed by a detailed description of the contrastive feature decomposition process for learning multimodal representations.\n\nModel Architecture\nThe overall architecture of ConFEDE is shown in Figure 1 . Given a sample, we first encode each modality with corresponding feature extractors. Specifically, we use the [CLS] tag of BERT to encode text (i.e., T), and two separate transformer encoders to encode vision and audio modalities (i.e., V and A), respectively. After that, we decompose each encoded modality into a similarity feature (i.e., T s /V s /A s in Figure 1 ) and a dissimilarity feature (i.e., T d / V d /A d in Figure 1 ) with different projectors. Each projector is composed of layer normalization, a linear layer with the Tanh activation, and a dropout layer. Finally, we update the six decomposed features and fuse them to train the ConFEDE model with the following multi-task learning objective function:\nL all = L pred + \u00b4uni L uni + \u00b4cl L cl ,\nwhere L pred is the multimodal prediction loss, L uni represents the unimodal prediction loss and L cl represents the contrastive loss. \u00b4cl and \u00b4uni are hyper-parameters that balance the contribution of each regularization component to the overall loss L all . We describe each loss term as follows. L pred -Multimodal Prediction Loss. We use a multilayer perceptron (MLP) with the ReLU activation function as the classifier to get the final predictive result (i.e., \u0177 in Figure 1 ). We concatenate all 6 decomposed modality features to obtain the input to the classifier,\n[T i s ; T i d ; V i s ; V i d ; A i s ; A i d ]\n, where [\u2022; \u2022] denotes the concatenation of two vectors. Denote the set of samples in a batch as B. For a given sample i \u2208 B, let its prediction from the classifier be \u0177i m , we calculate the multimodal prediction loss by mean squared error:\n\u0177i m = MLP([T i s ; V i s ; A i s ; T i d ; V i d ; A i d ]), L pred = 1 n n i=1 (y i m \u2212 \u0177i m ) 2 ,\nwhere n is the number of samples in a batch and y i m is the multimodal label. L uni -Unimodal Prediction Loss. For each sample i, we also feed the 6 decomposed features\n[T i s , V i s , A i s , T i d , V i d , A i d ]\ninto a weight-shared MLP classifier separately to get the 6 predictions denoted by the vector \u00fbi . Specifically, we compute the unimodal prediction loss by:\n\u00fbi = MLP([T i s , V i s , A i s , T i d , V i d , A i d ]), u i = [y i m , y i m , y i m , y i t , y i v , y i a ], L uni = 1 n \u2225u i \u2212 \u00fbi \u2225 2 2 ,\nwhere the vector\nu i = [y i m , y i m , y i m , y i t , y i v , y i a ]\nrepresents the ground-truth labels for unimodal prediction. In other words, each decomposed feature is regularized to perform prediction individually.\nNote that the similarity features T i s , V i s , A i s are mapped through the MLP to predict the multimodal label y i m , whereas the dissimilarity features T i d , V i d , A i d are mapped through the MLP to predict modality-specific labels y i t , y i v , y i a (if available). When modality-specific labels are not available, the dissimilarity features T i d , V i d , A i d will also be used to predict multimodal label y i m . The rationale behind this design is that we let the similarity features capture the consistent information shared across different modalities via the overall multimodal label for the sample, while the dissimilarity features can retain modality-specific information represented by unimodal labels.\nL cl -Contrastive Loss. We further regularize the learning through Contrastive Feature Decomposition in one simple joint contrastive loss that contrasts (1) similar samples against dissimilar samples; (2) similarity features against dissimilarity features within a sample. The contrastive loss is denoted as:\nL cl = 1 n n i=1 \u2113 i cl ,\nwhere \u2113 i cl is the contrastive loss of sample i, the detailed derivation of which will be given in the following subsection. \nT V A T V A T V A Ts k Vs k As k T V A T V A T V A\n\nContrastive Feature Decomposition\nWe unify intra-sample and inter-sample contrastive learning into one simple NT-Xent contrastive loss framework (Chen et al., 2020) to conduct both modality representation learning and modality decomposition simultaneously. The loss for sample i is given by\n\u2113 i cl = (a,p)\u2208P i \u2212 log exp(sim(a, p)/\u00c4 ) (a,k)\u2208N i \u222aP i exp(sim(a, k)/\u00c4 ) ,\nwhere (a, p) and (a, k) denote a pair of decomposed feature vectors either within a sample, e.g., (T i s , V i s ), (T i s , A i d ), or across different samples, e.g., (T i s , T j s ). The sets P i and N i are given by\nP i = P i intra \u222a P i inter , N i = N i intra \u222a N i inter .\nHere P i is the positive pair set that includes both intra-sample positive pairs P i intra and inter-sample positive pairs P i inter , while N i is the negative pair set that consists of both intra-sample negative pairs N i intra and inter-sample negative pairs N i inter . Note that (a, p) is a positive pair in P i , and (a, k) is a pair in P i or N i . Specifically, we use the six decomposed features (T s , V s , A s , T d , V d , A d ) to form intra-sample positive/negative pairs, as shown in Figure 2 (a) , with P i intra and N i intra given by\nP i intra ={(T i s , V i s ), (T i s , A i s )} \u222a {(T j s , V j s ), (T j s , A j s ) |j \u2208 Neighbor i \u222a Outlier i }, N i intra ={(T i s , T i d ), (T i s , V i d ), (T i s , A i d )} \u222a {(T j s , T j d ), (T j s , V j d ), (T j s , A j d ) |j \u2208 Neighbor i \u222a Outlier i },\nwhere Neighbor i and Outlier i represent the similar samples and dissimilar samples for the sample i, respectively, to enlarge the scope of the contrast, the detail of which is given in Algorithm 1 that will be explained subsequently.\nNote that instead of treating all modalities equally as in other contrastive learning schemes, here we choose the text similarity feature T i s as an anchor, such that the visual and audio similarity features V i s and A i s are pushed closer to T i s , while in the meantime, the dissimilarity features in all modalities are pushed away from T i s . This is due to the observation that multimodal sentiment analysis is still largely centered around text information. Although other modalities can provide additional information to assist with sentiment prediction, they may also introduce more noise than text. Therefore, unlike other work, we avoid using visual/audio similarity features as anchors, which may bring noise into contrastive learning and confuse model training.\nWe now describe the data sampler shown in Algorithm 1 that retrieves similar samples for a given sample based on both multimodal features and multimodal labels to perform supervised contrastive learning across samples. Specifically, the sampling procedure can be divided into two steps.\nFirst, given the dataset D that contains |D| samples, for each sample pair (i, j) in D, we calculate the cosine similarity score between them:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]),\nwhere sim(w, v) = w T v/||w|| \u2022 ||v|| denotes the cosine similarity between two vectors w and v. And T, V, and A (in Figure 1 ) are the output of BERT, vision and audio encoders, respectively.\nSecond, we retrieve candidate similar/dissimilar sample sets for each sample. For each sample i, we sort samples that have the same multimodal label y i m according to the similarity scores in ascending order as a candidate similar sample set S i 0 . In contrast, we sort samples that have labels other than y i m as a candidate dissimilar sample set S i 1 . Two similar samples with high cosine similarity scores from S i 0 are randomly selected to form inter-sample positive pairs with sample i, which is denoted as Neighbor i . Four dissimilar samples from S i 1 are selected to form inter-sample negative pairs. We denote them as Outlier i in which two samples Outlier i 1 have low cosine similarity scores and the other two samples Outlier i 2 have high cosine similarity scores.\nUsually, we tend to select the samples in Neighbor i and Outlier i 1 to form positive and negative pairs with sample i, respectively. However, samples in Outlier i 2 have different labels but similar semantic information to sample i, making them hard to distinguish from sample i. Therefore, we additionally add these samples to Outlier i to specifically handle this issue by contrastive learning.\nBased on the samples retrieved by Algorithm 1 and the pairing strategy shown in Figure 2 (b), the inter-sample positive/negative pairs for sample i are given by:\nP i inter ={(T i s , T j s ), (V i s , V j s ), (A i s , A j s ) |j \u2208 Neighbor i } , N i inter ={(T i s , T k s ), (V i s , V k s ), (A i s , A k s ) |k \u2208 Outlier i }.\nNotably, our data sampler enables contrastive learning across samples through decomposed modality features without data augmentation. This contrasts original contrastive learning in image classification, which obtains positive pairs by augmentation applied to images. Moreover, we only use similarity features to obtain inter-sample pairing since the similarity features of similar samples in the same class should be close while the similarity features of samples in different classes should be far apart.\n\nExperiments\nWe mainly evaluate ConFEDE on CH-SIMS (Yu et al., 2020) , since it has unimodal labels, which can best meet the design of ConFEDE. To justify the effectiveness of ConFEDE when unimodal labels are unavailable, we further test ConFEDE on the MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018b) , which are two English MSA datasets. However, they can not best test the performance of ConFEDE.\nWe compare our methods with the state-of-theart baselines in Table 1 and 2: LF-DNN (Yu et al., 2020) , MFN (Zadeh et al., 2018a) , LMF (Liu et al., 2018) , TFN (Zadeh et al., 2017) , MulT (Tsai et al., 2019) , MISA (Hazarika et al., 2020) , MAG-BERT (Rahman et al., 2020) , HyCon (Mai et al., 2021) and Self-MM (Yu et al., 2021) . For a fair comparison, the methods which only report the results of a single run and have no valid official code released Algorithm 1: Data Sampling Algorithm Input: Dataset D with the corresponding features T , V , A and multimodal labels ym. Output: Neighbor i , Outlier i for every i \u2208 D Define: sim(w, v) = w T v/||w|| \u2022 ||v|| for every (i, j) \u2208 D do\nCompute the cosine similarity score:\nC i,j = sim([T i ; V i ; A i ], [T j ; V j ; A j ]), end Define: argsort(X) = indices sort X ascendingly Let |D| = length of D, z = |D| 4 . for every sample i \u2208 D do\nRetrieve the similar sample set S i 0 :\nS i 0 = argsort({C i,j |j : y j m = y i m });\nRetrieve the dissimilar sample set S i 1 :\nS i 1 = argsort({C i,j |j : y j m \u0338 = y i m }),\nRandomly select two samples from the last z elements of S i 0 as Neighbor i ; Randomly select two samples from the first z elements of S i 1 as Outlier i 1 ; Randomly select two samples from the last z elements of S i 1 as Outlier i 2 ;\nOutlier i = Outlier i 1 \u222a Outlier i 2 .\nend for reproduction are not selected. A detailed introduction can be found in the supplementary material. The detailed experimental settings are introduced in Appendix C.\n\nEvaluation Metrics\nFollowing the previous works (Yu et al., 2020 (Yu et al., , 2021;; Rahman et al., 2020; Hazarika et al., 2020) , we report our results in (multi-class) classification and regression with the average of 5 runs of different seeds. For classification, we report the multiclass accuracy and weighted F1 score. We calculate the accuracy of 2-class prediction (Acc-2), 3-class prediction (Acc-3), and 5-class (Acc-5) prediction for CH-SIMS and the accuracy of 2-class prediction and 7-class prediction (Acc-7) for MOSI and MOSEI. Besides, Acc-2 and F1-score of MOSI and MOSEI have two forms: negative/non-negative (non-exclude zero) (Zadeh et al., 2017; Yu et al., 2021) and negative/positive (exclude zero) (Tsai et al., 2019; Yu et al., 2021) \n\nResults\nThe performance comparison of all methods on CH-SIMS, MOSI, and MOSEI is summarized in Table 1 and Table 2 . The scores of the proposed method and its variations are the averages of 5 runs. The performances of all other baselines, except for MAG-BERT, have been sourced from published papers or official repositories 1 .\nOn the CH-SIMS dataset, our proposed method outperforms all baselines on all metrics. We achieve superior performance compared to the best baseline model, Self-MM, with an improvement of 2.19% on acc-2 and 1.64% on F1 scores. Additionally, the proposed model demonstrates exceptional ability in multi-class classification, outperforming the best baseline by 4.68% on acc-3 and 4.77% on acc-5.\nAs seen in the results, our proposed method, ConFEDE, consistently outperforms all other baselines on the CH-SIMS dataset. The superior classification performance demonstrates that our designed learning method is more effective than the compared methods. Our method, ConFEDE, effectively distinguishes similarity and dissimilarity information between modalities, providing clearer modality features to the downstream classifier for improved prediction. Additionally, the significant improvement in MAE and Corr further highlights the ability of our model to better understand the CH-SIMS dataset than the other baselines.\nTo further evaluate the effectiveness of our proposed method, ConFEDE, we trained our models on the MOSI and MOSEI datasets without uni-modal labels. Instead, we used their multimodal labels for compatibility. The results are presented in Table 2 . On the MOSI dataset, our method outperforms all other baselines in both the negative/nonnegative (NN) setting and negative/positive (NP) setting for acc-2 and F1 metrics. Additionally, our acc-7 and MAE metrics surpass most of the baselines. For the MOSEI dataset, our ConFEDE method outperforms all baselines in all metrics except for the NN Acc-2 and F1 score. Furthermore, our MAE is significantly lower than all baselines, reaching 0.522.\nIt is worth noting that our models perform much better in NP acc-2 than NN acc-2 for MOSEI, as shown in Table 2 . This is because the NN acc-2 setting is generally more challenging than the NP acc-2 setting, as it places more pressure on a model to classify data samples with a regression label of 0. Specifically, if there are two samples with a regression label of 0, when predicted by a regression model, the results might be -0.01 and 0.01. As the value range of \"Neutral\" is [-0.5,0.5) in MOSI and (-0.1,0.1] in SIMS, these two samples should be classified as \"Neutral\". However, in NN settings, they will be classified into two different classes, resulting in a worse acc-2. In contrast, with the NP setting, all \"Neutral\" samples are abandoned, resulting in a better acc-2.\nIn contrast, our method shows better performance in both NN and NP settings on MOSI when compared to other models. The Acc-7, MAE and Corr are also better or comparable to most baselines.\n\nAblation Study and Analysis\nTo evaluate the impact of our proposed structures, we conducted an ablation study on our proposed method by removing inter-sample contrastive learning and intra-sample contrastive learning. The results are shown in Table 1 . \"Plain\" represents the model without contrastive learning method, \"Inter\" represents the model with inter-sample contrastive learning only, and \"Intra\" represents the model with intra-sample contrastive learning and unimodal prediction as a sub-task.\nThe experiment shows that all three models perform worse than the original model. Among the three models, the plain setting has the lowest performance. Both intra-sample contrastive learning and inter-sample contrastive learning provide positive impacts on performance. Compared with Plain, by using text feature as the anchor, Intra filters out noise (useless information for sentiment analysis) in the vision and audio modality, leading to better prediction. This is also the reason why it reaches better acc-2 accuracy than both the other models. Since the acc-2 metric in CH-SIMS follows the negative/non-negative setting, a feature with lower noise helps the classifier make a more precise prediction value, making it easier to classify the 0-labeled samples. This also explains why we achieve better NN Acc-2 performance than all baselines on MOSI.\nFor the inter-sample contrastive learning method, by learning the common and different information between samples, \"Inter\" performs better on multiclass classification. The result of Inter on CH-SIMS shows great improvements on both acc-5 and MAE compared with the other two models, which proves that Acc-5 and regression performance benefits more from \"Inter\". This can also explain why we have a lower NN Acc-2 performance on MO-SEI. Since MOSEI is much larger than MOSI and CH-SIMS, it introduces more noise in each modality, the contrastive feature decomposition learning needs more epochs and a smaller learning rate to separate the useful information from noise. Meanwhile, inter-sample contrastive learning is more efficient on MOSEI. With the larger amount of samples, it is much easier for the sampler to find the most similar and dissimilar samples with the given sample, from which the model can understand the difference between samples better. Thus, ConFEDE can reach higher Acc-7 and regression performance than all other baselines on MOSEI.\nTo further evaluate the effectiveness of the contrastive feature decomposition method, we conducted an ablation study on Intra using the CH-SIMS dataset. As presented in Table 3 , we created three variations of Intra: 1) Intra with only multimodal labels for unimodal prediction (M-label); 2) Intra without the unimodal prediction component (-uni); 3) Intra without the similarity-dissimilarity learning method (-cl); and 4) Intra that uses all similarity features as anchors (+full), which utilizes T s , V s , and A s as anchors instead of T s only.\nThe results in the table demonstrate that all variations resulted in a decrease in performance compared to the original Intra in classification matrices. Both the intra-sample contrastive learning and the unimodal prediction task can regularize the learned representation, resulting in clearer information that aids the classifier in understanding the sample better. However, the \"+full\" setting introduces more noise by also using V s and A s as anchors, which confuses the model and diminishes the denoising ability of the contrastive feature decomposition learning.\n\nConclusion\nIn this paper, we propose a novel method for multimodal sentiment analysis (MSA) called ConFEDE. The ConFEDE framework is based on contrastive feature decomposition, which utilizes a unified contrastive training loss to capture the consistency and difference across modalities and samples. This approach allows for the simultaneous learning of modality decomposition within each sample and su-pervised contrastive learning across samples. Our proposed method is mainly evaluated on CH-SIMS. The result shows that the proposed method significantly outperforms many state-of-the-art multimodal sentiment analysis methods. We further conduct an extensive experiment on MOSI and MOSEI to test the capability of ConFEDE when no unimodal label is available, where our method achieves better performance than state-of-the-art methods on a number of performance metrics.\n"}
{"question": "Which of the following models corresponds incorrectly to the evaluation index in ablation studies", "evidence": "  For HopotQA-distractor we report the Exact Match (EM) score, and for the summarization tasks we report the ROUGE-1 (R-1) score. For QA we used the EM score, and for MDS and QMDS we used the ROUGE-1 score.  ", "options": ["A. MDS corresponds EM score", "B. HopotQA-distractor corresponds EM score", "C. QMDS corresponds ROUGE-1 score", "D. QA corresponds EM score"], "answer": "A", "content": "\nIntroduction\nAmong recent NLP research, multi-document processing is gaining increasing attention, due to the need to handle and process an increasing amount of textual data and available documents online. A * Work partly done as an intern at AI2. 1 Our code is available at https://github.com/ aviclu/peekacross. which we split into context documents (2) and a held-out document (3), we select the most salient sentence (4) that is used for generating a question-answer pair (5).\nThen, we pre-train a model by generating the proper answer and the salient sentence, given the question and the context documents (6).\nnumber of prominent applications that are concerned with aggregating information from multiple texts are multi-document summarization (Fabbri et al., 2019; Zhao et al., 2020) , query-focused multidocument summarization (Xu and Lapata, 2020; Pasunuru et al., 2021a) , and multi-hop question answering (Yang et al., 2018; Welbl et al., 2018) . These tasks remain challenging mostly since existing NLP models are designed to handle single texts, rather than processing multiple documents at once (Caciularu et al., 2021) .\nEarly solutions for multi-text processing were task-specific and used complex architectures that were difficult to generalize across different multidocument tasks (Liu and Lapata, 2019; Wang et al., 2020; Ginzburg et al., 2021) . Efficient LMs (Tay et al., 2021; Beltagy et al., 2020) recently demonstrated that by simply concatenating multiple documents into a single sequence, the transformer can offload the goal of identifying and connecting relevant information between the documents. Recently, it was suggested that these long-context LMs can be equipped with new pre-training objectives to enable them to process multiple documents more effectively (Caciularu et al., 2021; Xiao et al., 2022; Yasunaga et al., 2022) .\nThese pre-trained models demonstrated state-ofthe-art performance on a variety of multi-document downstream tasks, and outperformed underlying LMs and task-specific architectures. Such models are often pre-trained using a dataset where each instance is a set of related documents (e.g., news articles all discussing a specific event), which facilitates modeling of cross-text relationships. Existing multi-document pre-training objectives involve unmasking tokens in a document (Caciularu et al., 2021) , or generating a salient masked sentence (Zhang et al., 2020; Xiao et al., 2022) , encouraging the model to recover missing information using other documents. While successful, these models are either limited to classification tasks (Caciularu et al., 2021) or primarily designed for summarization (Zhang et al., 2020; Xiao et al., 2022) .\nIn this work, we propose a novel pre-training objective that supports both short and long text generation, resulting in a versatile and general multidocument language model. In particular, we hypothesize that using questions and answers involving multiple documents can encourage the model to better learn and incorporate both fine-grained information (by asking questions about core information units in a specific sentence) as well as coarsegrained cross-document relationships required to generate a long text such as a summary. We show that this approach holds not only for summarization, but for other multi-document downstream tasks as well.\nDuring the pre-training of existing multidocument language models, the goal is to unmask spans (for encoder-only models) or generate masked textual spans (for encoder-decoder models) under a multi-document context. To that end, multiple concatenated sequences of related documents are fed during pre-training, thus requiring a large number of sets of related documents for an effective pre-training phase (Hoffmann et al., 2022) . In a variety of existing multi-document benchmarks, such as multi-document summarization, only small to medium-scale document clusters are readily available. These are acquired either automatically with lexical similarity and retrieval (Fabbri et al., 2019) or semi-automatically (Gu et al., 2020) , but generally, this process requires a substantial amount of human effort for filtering instances and generating high quality corpora.\nBy employing a novel multi-document question-answer generation procedure, we propose an effective method for expanding the multi-document pre-training corpora. Our approach allows us to provide multiple views for every single cluster of documents, thereby artificially increasing the pretraining data size (in terms of number of instances) via augmentation. To expose the model to a variety of contexts and diversify the pre-training data, we propose to generate multiple pairs of questions and answers and condition them on a subset of the documents' cluster. We select a salient sentence in one held-out document and then employ a recent parser to generate a high-quality question-answer pair about one predicate in the selected sentence, using a systematic semantically-oriented approach (Klein et al., 2022) . This new multi-document pre-training objective challenges the model to generate both the answer to the question as well as the salient sentence, while discarding the held-out document or parts of it (see Figures 1, 2 for illustration). This procedure exposes the model to a variety of contexts -a question and a different subset of the documents in the cluster per instance, in contrast to prior methods that provide only a single view of the cluster. Our contributions are summarized below:\n\u2022 A new pre-training approach for multidocument modeling, formulated as a crossdocument question answering task, further directing the LM to model cross-text relationships, focusing on both fine-and coarsegrained information. \n\nRelated Work\nLong-context efficient text generation transformers (Tay et al., 2021 (Tay et al., , 2022) ) extend earlier transformer models (Vaswani et al., 2017) for processing long sequences, often using a sparse self-attention architecture. Examples include the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) , and LongT5 (Guo et al., 2022) . These models demonstrated that single-text approaches be can adapted to multi-document tasks by concatenat-ing multiple documents into a single sequence and processing them using their sparse attention patterns. They sparsify the full self-attention matrix of transformers by using a combination of a localized sliding window (called local attention), as well as a global attention pattern on a few specific input locations. LED is build upon the BART model (Lewis et al., 2020) by using additional positional embeddings and global attention weights, and introduces the global attention mode that operates over pre-selected tokens. LongT5 extends the T5 model (Raffel et al., 2020 ) by using a similar technique introduced in the ETC and BIGBIRD models (Ainslie et al., 2020; Zaheer et al., 2020) , relieving the requirement to manually select global tokens by automatically globalizing the aggregated representations of groups of tokens.\nFurther strategies have been proposed for increasing these models' abilities in multi-document tasks. The Cross-Document Language Model (CDLM) (Caciularu et al., 2021) suggested pretraining a Longformer-encoder (Beltagy et al., 2020) over sets of related documents, and showed superior performance results over several multidocument tasks. Following this methodology, the authors of LinkBERT (Yasunaga et al., 2022 ) used a similar approach, but utilized Wikipedia's hyperlinks in order to curate informative pairs of linked documents for LM pre-training.\nIn order to adopt the multi-document pretraining approach for sequence-to-sequence tasks, PRIMERA (Xiao et al., 2022) , which is built on top of the Longformer encoder-decoder model (LED), selected salient sentences within clusters of related documents using a pyramid estimation approach, resembling the method presented for pre-training the single-document PEGASUS model (Zhang et al., 2020) . While this work is the closest to ours, it was pre-trained to generate masked salient sentences without any control, which makes the model potentially hallucinate while generating text, while our model uses a controlled QA-based objective. Furthermore, unlike these works, our method generates significantly more data then used to pre-train PRIMERA, which is possible to obtain by the singledocument QA generation approach. Our QA pretraining formulation allows us to generate multiple contexts per document cluster.\nAnother related line of work includes methods that incorporate large-scale QA-generated data for pre-training LMs (He et al., 2020; Jia et al., 2022 ;\n\n(a) The held-out document is discarded from the context (c) The held-out document is included in the context, but the answer in the anchor sentence is masked (b) The held-out document is included in the context, but the anchor sentence is masked\nFigure 2 : A schematic of our pretraining data modes. The salient sentence which is used for QA generation is colored in yellow. (a) The context does not include the held-out document, therefore this mode is the most challenging. (b) The held-out document is present in the context, but the salient sentence used for the QA generation is masked (red). (c) The held-out document is present in the context, but the answer span within the salient sentence is masked (red). Huber et al., 2022) . These works hypothesize and show that pre-training by utilizing generated QA data can encourage contextual representations to encode useful semantic information for other non-QA downstream tasks. Inspired by that, we conjecture that LMs can strongly benefit from infusing QA during pre-training in the multi-document setup, for adding an additional signal for modelling cross-text relationships.\n\nAugmenting the Multi-Document Pre-training objective\nIn this section, we provide the required steps for compiling the pre-training dataset for QAMDEN.\nWe next elaborate on the details of the data creation and provide analysis of the resulted corpus.\nRecent works have shown that for text summarization, pre-training LMs to generate a \"summarylike\" sequence, termed pseudo summary, inherently provides gains over general-purpose pre-trained LMs (PEGASUS, PRIMERA; Zhang et al., 2020; Xiao et al., 2022) . The data in which the PEGASUS and PRIMERA models were pre-trained on was constructed using the Gap Sentence Generation (GSG) method, which suggests masking highly-ranked salient sentences, where salience is pre-determined by a sentence-scoring method of interest. Particularly, in PEGASUS, GSG has been adopted as its pre-training objective, where some sentences in a single document are masked in the input and the model is tasked to generate them.\nFormally, for each sentence s i in a given input document D, PEGASUS computes its salience score based on its ROUGE score (Lin, 2004) w.r.t the rest of the sentences within the document (D/{s i }), i.e. Score(s i ) = ROUGE(s i , D/{s i }). Intuitively, \u2026Pokemon Sword and Shield might have already been announced, but we now know there's another new Pokemon game on the way from DeNA\u2026 QASem QA generation (Klein et al., 2022) Q1: What might been announced? A1: Pokemon Sword and Shield. (Answer length: 4) Q3: Who knows something? A: We. (Answer length: 1) Q2: Where does someone know something? A: On the way from DeNA. (Answer length: 5) Contextualization (Pyatkin et al., 2022) Q: Where did we know there's another new Pokemon game? A: On the way from DeNA.\n\nSelected\nFigure 3 : A schematic of the process of QA generation using QASEM (Klein et al., 2022) and the contextualization model from Pyatkin et al. (2021) . This is an actual sample that was created and used for pre-training QAMDEN, where the document is taken from New-SHead (Gu et al., 2020) .\nthis metric assigns a high score to the sentences that have a high overlap and share more lexical information with the rest of the sentences in the document, thus assigning high scores to prominent sentences. PRIMERA has generalized this notion to support the multi-document setup, by applying a GSG variant over a cluster of related documents.\nCross-Document GSG. We propose augmenting the GSG technique to formulate a cross-document question answering pre-training objective for multidocument tasks, instead of the existing pseudo summary generation methods. Our approach supports identification of both fine-and coarse-grained information as we describe below, and results in a substantially larger amount of pre-training examples compared to the preceding methods.\nFormally, we are given a cluster of related documents S = D 1 , D 2 , . . . , D |S| in a corpus C. Our cross-document (CD) GSG salience score for the i th sentence within the k th document in the set (s i k ), is defined by its ROUGE score w.r.t the rest of the sentences within the document (D k /{s i k }) as well as the other documents (S/D k ), i.e. CD-GSG-Score(s i k ) = ROUGE(s i k , S/{s i k }). Then, for every document k, following Zhang et al. (2020) ; Xiao et al. (2022) we select the top-scored sentence s * k , and then we use this sentence to generate a pair of a question and an answer.\nGenerating Cross-Document QAs. For generating the cross-document questions and their answers, we employ QASEM, a recent semantic parsing framework for question generation (Klein et \nfor k \u2190 1 to |Sn| do 4 s * k \u2190 arg max i CD-GSG-Score(s i k ); 5 (q * k , a * k ) \u2190 QASEM(s * k ); 6 t * k = [a * k , s * k ] # target text; 7 D \u2190 D \u222a {([Sn/D k , q * k ] , t * k )} # (a); 8 D \u2190 D \u222a {([Sn/ {s * k } , q * k ] , t * k )} # (b); 9 D \u2190 D \u222a {([Sn/ {a * k } , q * k ] , t * k )} # (c); 10 Return D;\n2022). 2 QASEM intended soliciting a manageable, discrete account of information in a text for the sake of building natural language semantic representations. It automatically labels each verbal predicate-argument relation with a questionanswer pair, where a natural language question represents a semantic role, while the answers correspond to the arguments that appear in the input text. QASEM is thus an appealing approach since it is capable of generating multiple high-quality questions given a sentence. We apply QASEM over the sentences withing the pre-training data in order to generate question-answer pairs, and then apply the model from Pyatkin et al. (2021) which transforms the question into a more natural and clear form, with contextualized arguments (see example in Figure 3 ). In order to resemble a summarization task where the generated text is typically long, we select the question-answer pair with the longest argument produced by QASEM. Formally, QASEM(\u2022) receives a sentence s * k as an input, and produces question-answer pair (q * k , a * k ), where a * k is the longest among the generated answers. See a detailed example and full description in App. A.1.\nConsidering the question-answer pair, our goal is to encourage the LM to generate the correct answer as well as the salient sentence in a multi-document context in order to learn cross-text relationships.\nData Generation Process. In order to facilitate the construction of a multi-document context, we propose three different modes, each one is responsible for uncovering information by using different contexts. For all the modes, we first generate a QA pair out of the most salient sentence in the held-out document.\n(a) Excluding the source document. In this mode we disregard the held-out document D k from the context S n given to the model, i.e, S n /D k . Hence, the model is tasked to predict the answer without having access to the source document at all, and is restricted to observe only the other documents in the set. Thus, this mode is considered as the most challenging one.\n(b) Masking the salient sentence. In this mode, the source salient sentence is masked, i.e, S n / {s * k }. The model has access to the surrounding context of the masked sentence in the held-out document, as well as the other documents in the set.\n(c) Masking the answer. In this mode, only the answer span within the salient sentence is masked, i.e, S n / {a * k }. The model has access to the surrounding salient sentence, as well as all the documents in the set.\nAs part of the new pre-training process of our novel multi-document model, we append the question after the context and instruct the model to generate an answer followed by its salient sentence, i.e., output = \u27e8answer\u27e9, \u27e8sentence\u27e9, inspired by Bohnet et al. (2022) . Generating the salient sentence introduces a copying mechanism (allows the model to also learn to copy information from the source directly) as well as allowing longtext generation, which is crucial for summarization downstream tasks (Zhang et al., 2020) , as well as outperforming a model which was pre-trained for generating the answer solely -according to the ablations study, this setup yields the best performance results ( \u00a74.4). In the pre-training evaluation phase, the held-out set was split and the loss was measured separately for each mode of the data. As expected, we observed that the loss for (a) was significantly higher than those for the other modes, with (a)\u227b(b)\u227b(c) ranking highest. The procedure for generating the pre-training data is summarized in Algorithm 1 and Figure 2 .\nThe resulted pre-training corpus. We applied our procedure over the NewSHead corpus (Gu et al., 2020) , which consists of a set of related documents per instance. This is the exact same pre-training corpus used also by our main baseline PRIMERA (Xiao et al., 2022) \n\nExperimental Setup and Results\nThis section presents experiments conducted to evaluate QAMDEN, as well as the the ablations and baselines we used. For the intrinsic evaluation we evaluated the models over multi-document QA tasks. For extrinsic evaluations we considered the multi-document abstractive summarization task.\nModel Implementation Details Following Xiao et al. ( 2022), we use the large-sized Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) for our model initialization. The length limits of input and output are 4096 and 1024, respectively. 3 Following the Huggingface implementation (Wolf et al., 2020) , we set the sliding window size to 1024 for local attention in the encoder part.\nSimilar to the PRIMERA model (Xiao et al., 2022) , when concatenating the documents and the question, we add a special document separator token (<doc-sep>) between the documents to signal to the model to be aware of the document boundaries. We also assign the global attention mode to these tokens which enables the model to share information across documents (Caciularu et al., 2021) . For further hyperparameter and pre-training execution details, see App. B.\n\nMulti-Document Question Answering\nMulti-document QA is the task of generating the correct answer, given a set of related multiple documents. For several multi-document QA benchmarks, models are often tasked to implicitly solve multiple sub-tasks or follow intermediate steps, such as comprehending the question, filtering out distracting documents in the context, and stitching pieces of information across the relevant documents (Geva et al., 2021; Caciularu et al., 2022) . Recall that QAMDEN was pre-trained over a automatically generated multi-document QA dataset. Hence, as a preliminary assessment, we first investigate QAMDEN's performance over two multi-document QA benchmarks, HopotQAdistractor (Yang et al., 2018) and WikiHop (Welbl et al., 2018) (see more details of the datasets in App. C.1), and compare to other models that were pre-trained using underling un-masking objectives.\nFine-Tuning Format. To follow our pre-training scheme, we append the question to the context and fine-tune the model to generate the correct answer. We use the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) and PRIMERA (Xiao et al., 2022) as the baselines, for assesing the contribution of our pre-trainig format. Confirmed by Beltagy et al. (2020) , we found out that appending the question: and context: prefixes before the question and the context tokens, respectively, resulted in better performance.\nBaselines. We compare QAMDEN (447M parameters) against a set of strong long-context transformer baselines, including LED (447M parameters) (Beltagy et al., 2020) , PRIMERA (447M parameters) (Xiao et al., 2022) , 4 and LongT5-xl (3B parameters) 5 (Guo et al., 2022 ) (see \u00a72). 6 Results. The results on multi-document QA are shown in Table 2 . We adopted the F1 and Exact Match (EM) evaluation metrics corresponding to the original works. Our QAMDEN outperforms both PRIMERA, LED, and LongT5, confirming that our pre-training data and input format are beneficial for both capturing cross-document relationships (QAMDEN\u227bLED) as well as exploiting both context and question (QAMDEN\u227bPRIMERA).\n\nMulti-Document Summarization (MDS)\nThis task aims at generating a summary for a given set of topically-related documents. Inherently, end-Model F1 EM HotpotQA LED (Beltagy et al., 2020) 65.8 50.6 LongT5-xl (Guo et al., 2022) 66.1 50.9 PRIMERA (Xiao et al., 2022) 65 Results. Tables 3 and 4 present the evaluation results over the Multi-News and Multi-XScience datasets, respectively. Following previous MDS works, we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.2 for details). For a fair comparison, we include the results of PRIMERA as well as the results of the previous state-of-the-art methods (Pasunuru et al. (2021b) and Lu et al. (2020) , for Multi-News and for Multi-XScience, respectively), and LED (Beltagy et al., 2020) . As shown in the results tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, especially on the Multi-News dataset, clearly demonstrating its consistent advan- (Beltagy et al., 2020) 47.4 20.7 23.7 LongT5-xl (Guo et al., 2022) 47.4 20.7 23.7 PRIMERA (Xiao et al., 2022) 49.9 21.1 25.9 QAMDEN 50.9 23.1 27.2 tage. This excludes the results for Multi-XScience where QAMDEN slightly underperforms the prior work and LongT5. An explanation which Xiao et al. (2022) points refers to the fact that the clusters in Multi-XScience have less overlapping information compared to the corpus we used, attributed to the use of abstracts as the input documents in Multi-XScience. In addition, LongT5 advantage over QAMDEN is attributed to significantly larger number of parameters of LongT5-xl.\n\nQuery-Focused Multi-Document Abstractive Summarization\nThe task of Query-focused Multi-Document Summarization (QMDS) aims at generating a summary from a set of documents, that answers a specific given query. Unlike MDS, QMDS tries to solve more realistic query-based scenarios, since it suggests summarizing only predefined salient information of interest that best answers the query. Since we proposed pre-trainng under the multi-document question answering setup, we posit that QAMDEN might be effective for QMDS.\nWe consider the datasets constructed by Pasunuru et al. (2021a), QMDSCNN and QMDSIR (see more details of the datasets in App. C.3) as well as their strong baseline, and include also the results of PRIMERA and LED.\nBaselines. Similar to the previous experiments, we compare QAMDEN against LED, PRIMERA, LongT5-xl. In addition, we consider also the baseline from Pasunuru et al. (2021a) . 37.9 16.4 35.2 LED (Beltagy et al., 2020) 32.3 14.3 30.9 LongT5-xl (Guo et al., 2022) 35.5 15.9 34.3 PRIMERA (Xiao et al., 2022) 36 Results. Tables 5 and 6 present the evaluation results over the QMDSCNN and QMDSIR datasets, respectively. Following MDS tasks and Pasunuru et al. (2021a) , we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.3 for details). As shown in the tables, QAMDEN exhibits the best performance across most of the examined models and benchmarks, clearly demonstrating its consistent advantage over the baselines.\n\nAblation Study\nData Generation. We next turn to a broad ablation study, for assessing our configuration and design choices across our suggested pipeline. First, we show the advantage of combining the three proposed data modes, rather than using a subset of them. We evaluate all the resulted models by fine-tuning them over HopotQA-distractor ( \u00a74.1), Multi-XScience ( \u00a74.2), and QMDSIR ( \u00a74.3). For HopotQA-distractor we report the Exact Match (EM) score, and for the summarization tasks we report the ROUGE-1 (R-1) score.\nBaselines. We pre-train QAMDEN for 100k steps, for using every subset of the set of the set (superset) of modes {(a), (b), (c)} (all its possible combinations) of the generated pre-training data modes presented in \u00a73. Note that our QAMDEN model is referred to as using all the modes, i.e., For QA we used the EM score, and for MDS and QMDS we used the ROUGE-1 score.\nResults. Figure 4 shows the ablation results. In all tasks, pre-training using all modes yields the best results. Among all modes, mode (c) appears to be the most effective for QA, since this is an extractive QA task, and mode (c) provides data in this format. Mode (a) excels at the summarization tasks, attributed to their abstractive nature as well as the requirement of all the documents for generating appropriate summaries.\nInput Format We repeat the previous experiment and ablate the pre-training input format according to the multiple different formats, and compare to the model pre-training format described in \u00a73 (with the same pre-training data): without questions, with random question, with random context document, with prefixes, placing the question before the context, with question filtering, and without generating the salient sentence. Additionally, we assess the choice of QASEM as our questionanswer generation module by using the generators from Jia et al. ( 2022) and Khashabi et al. (2022) . Finally, we also include the results of PRIMERA, which was further pre-trained for additional 300k steps (fine-tuning LED for 400k steps in total), for a fair comparison to QAMDEN ablated models. See full details regarding all the ablations in App. D.\nResults. Overall, our QAMDEN model outperforms the ablation models on most of the tasks, which a significant margin.\nPre-training the model without any questions during or using random questions, negatively impacts the results of downstream tasks. An impor- tant function of the question is to facilitate the model's ability to generate the appropriate answer and the source sentence. This aligns with the findings from Caciularu et al. (2021) , who showed that pre-training with random documents rather than related ones is sub-optimal. The use of question and context prefixes for positioning input appears to be helpful for QA, but is inferior when applied to summarization tasks due to its unique format, which is well suited for QA but seems to generalize harder for other setups. When the question is placed before the context, performance slightly decreases over query-based tasks, while maintaining the same results for summarization (where the question location is irrelevant).\nUsing question filtering is found to harm the downstream results of QAMDEN, in accordance to other QA-based pre-training prior works (Jia et al., 2022) .\nPre-training without generating the attributed source sentence introduces a significant flow to the model, particularly for the summarization downstream tasks. As mentioned before, generating longer sequences, as well as teaching the model to copy text, is beneficial for summarization tasks.\nApplying a different question generator rather then QASEM yields inferior results overall, since the other generators produce open-ended questions and answers which are more prone to errors, while QASEM utilizes an existing span in the context as the answer. In addition, QASEM generated local questions, which allows QAMDEN to focus on the fine-grained details, and not only the coarsegrained information in the multi-document context.\nWhen PRIMERA is pre-trained with 400k steps (to match QAMDEN's number of further pretraining steps), it underperforms QAMDEN and even fails to add any significant improvements over its 100K checkpoint, possibly due to the small amount of pre-training data it contains. \n\nComparison with Large Language Models\nIn order to get insights into how QAMDEN compares with state-of-the-art Generalist Large Language Models (LLMs), we provide a small comparison with two capable models, GPT-3.5 turbo (Ouyang et al., 2022) and GPT-4 8 (OpenAI, 2023) (including the 8k input length version) evaluated on the zero-shot setting.\nFor a fair comparison, we used the same context window size of 4K tokens for all models (and up to 8k for GPT-4 8k). Due to the fact that multidocument tasks involve processing long sequences, the cost of API calls is significant for a comprehensive evaluation across all datasets. Therefore, we only evaluate on a sample of 200 instances from the multi-news dataset (see prompting details in App. E). Table 8 depicts the results. We observe that QAMDEN significantly outperforms both GPT-3.5 and GPT-4 models, though the performance of GPT-4 and GPT-3.5 is comparable. We leave more comprehensive comparisons with LLMs to future work.\nWe further assessed QAMDEN through manual comparison against PRIMERA, GPT-3.5, and GPT-4 8k. NLP graduate students were shown summaries for a given topic from the three systems and QAMDEN in arbitrary order, along with a corresponding reference summary. Following (Ernst et al., 2022) , participants were asked to rank the systems based on Content (overlap with the reference), Readability (the readability of a summary), Grammaticality (avoiding grammar errors), and Non-Redundancy (avoiding repetitions), and we extract the pairwise results out of the rankings (see (Ernst et al., 2022) for further details). In App. F, we provide several examples to system summaries and their corresponding reference summaries.\nThe results of this study are presented in Table 9 . Under each evaluation criterion, it indicates the percentage of cases where QAMDEN was preferred over both baselines. QAMDEN was favored in all cases except for grammatical errors and readability (which corresponds to the Reinforcement Learning from Human Feedback phase of the GPT models).\n\nConclusions\nIn this work, we present a novel pre-training scheme for multi-document tasks. First, our approach suggests to augment the existing multidocument pre-training objectives into a crossdocument question answering task. Second, we generate high-quality large-scale QA pre-training data using a controlled generation approach, in which each QA pair originates from a salient sentence in one of the documents in the set.\nDuring pre-training, we task the the Longformer Encoder-Decoder (LED) model to generate the answer and the salient sentence on the basis of the remaining context. This objective encourages the LED model to elicit cross-document relationships, and stitch pieces of information across the input documents, which are relevant for performing multi-document tasks. The resulted model QAMDEN shows significant performance improvements compared to prior models under extensive experimentation over multiple challenging multidocument summarization and QA datasets.\nFuture work can extend the ideas in this work for equipping decoder-only large LMs with crossdocument modeling using our proposed method, also in the setup of in-context learning and prompt tuning. We foresee that our method should be significant specifically for retrieval-augmented language modeling setups (Izacard et al., 2022) , where there is a use of related documents as an outsourced external non-parametric knowledge source. Finally, the use of a single document in order to trigger cross-document relationships, as firstly introduced in this work, might be further investigated.\n"}
{"question": "What is the purpose of the Atomic Content Units (ACUs) in the paper?", "evidence": "  The ACU protocol is designed to reduce the subjectivity of reference-based human evaluation by simplifying the basic annotation unit... We propose the ACU protocol for high-agreement human evaluation of summary salience. ", "options": ["A. To increase the length of reference summaries", "B. To reduce subjectivity in reference-based human evaluation", "C. To evaluate the performance of large language models", "D. To measure the quality of machine-generated summaries "], "answer": "B", "content": "\nIntroduction\nHuman evaluation plays an essential role in both assessing the rapid development of summarization systems in recent years (Lewis et al., 2020a; Zhang et al., 2020a; Brown et al., 2020; Sanh et al., 2022; He et al., 2022) and in assessing the ability of automatic metrics to evaluate such systems as a proxy * Equal contribution for manual evaluation (Bhandari et al., 2020; Fabbri et al., 2022a; Gao and Wan, 2022) . However, while human evaluation is regarded as the gold standard for evaluating both summarization systems and automatic metrics, as suggested by Clark et al. (2021) an evaluation study does not become \"gold\" automatically without proper practices. For example, achieving a high inter-annotator agreement among annotators can be difficult (Goyal et al., 2022) , and there can be a near-zero correlation between the annotations of crowd-workers and expert annotators (Fabbri et al., 2022a) . Also, a human evaluation study without a large enough sample size can fail to find statistically significant results due to insufficient statistical power (Card et al., 2020) .\nTherefore, we believe it is important to ensure that human evaluation can indeed serve as a solid foundation for evaluating summarization systems and automatic metrics. For this, we propose using a robust human evaluation protocol for evaluating the salience of summaries that is more objective by dissecting the summaries into finegrained content units and defining the annotation task based on those units. Specifically, we introduce the Atomic Content Unit (ACU) protocol for summary salience evaluation ( \u00a73), which is modified from the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. We demonstrate that with the ACU protocol, a high inter-annotator agreement can be established among crowd-workers, which leads to more stable system evaluation results and better reproducibility.\nWe then collect, through both in-house annotation and crowdsourcing, RoSE, a large human evaluation benchmark of human-annotated summaries with the ACU evaluation protocol on recent state-of-the-art summarization systems, which yields higher statistical power ( \u00a74). To support evaluation across datasets and domains, our benchmark consists of test sets over three summarization datasets, CNN/DailyMail (CNNDM) (Nalla-\n\nStatistical Power\n\u2212 High statistical power is difficult to reach for human evaluation of similar-performing systems. \u00a74.1 \u2212 Increasing the sample size of human evaluation effectively raises statistical power.\n\nSummary Length\n\u2212 Summaries from different summarization systems show a large difference in average length. \u00a74.2 \u2212 Difference in summary length is not well-reflected by automatic evaluation metrics.\n\u2212 Reference-free and reference-based human evaluation results have a near-zero correlation. Evaluation \u2212 Reference-free human evaluation strongly correlates with input-agnostic, annotator preference. Protocol Comparison \u2212 Annotator's input-agnostic preference has a strong positive correlation with summary lengths.\n\u00a75.2 \u2212 Annotator's input-agnostic preference does not favor reference summaries. \u2212 Compared to smaller, fine-tuned models, zero-shot large language models (e.g. GPT-3) perform better under reference-free evaluation, but worse under reference-based evaluation.\nEvaluating \u2212 A higher-powered human evaluation dataset can lead to a more robust automatic metric evaluation, as shown by a tighter confidence interval and higher statistical power of metric evaluation. Automatic Metrics \u2212 Automatic metric performance differs greatly under different human evaluation protocols.\n\u00a76.1 & \u00a76.2 \u2212 Automatic metrics show relatively strong system-level correlation and moderate summary-level correlation with our robust human evaluation protocol.\nTable 1 : Summary of the key findings in our work. pati et al., 2016) , XSum (Narayan et al., 2018), and SamSum (Gliwa et al., 2019) , and annotations on the validation set of CNNDM to facilitate automatic metric training. To gain further insights into the characteristics of different evaluation protocols, we conduct human evaluation with three other protocols ( \u00a75). Specifically, we analyze protocol differences in the context of both fine-tuned models and large language models (LLMs) in a zero-shot setting such as GPT-3 (Brown et al., 2020) . We find that different protocols can lead to drastically different results, which can be affected by annotators' prior preferences, highlighting the importance of aligning the protocol with the summary quality intended to be evaluated. We note that our benchmark enables a more trustworthy evaluation of automatic metrics ( \u00a76), as shown by statistical characteristics such as tighter confidence intervals and more statistically significant comparisons ( \u00a76.2).\nOur evaluation includes recent methods based on LLMs (Fu et al., 2023; Liu et al., 2023) , and we found that they cannot outperform traditional metrics despite their successes on related benchmarks such as SummEval (Fabbri et al., 2022a) . We summarize our key findings in Tab. 1. Our contributions are the following: (1) We propose the ACU protocol for high-agreement human evaluation of summary salience. (2) We curate the RoSE benchmark, consisting of 22000 summary-level annotations and requiring over 150 hours of in-house annotation, across three summarization datasets, which can lay a solid foundation for training and evaluating automatic metrics. 1 (3) We compare four human evaluation protocols for summarization and show how they can lead to drastically different model preferences. (4) We evaluate automatic metrics across different human evaluation protocols and call for human evaluation to be conducted with a clear evaluation target aligned with the evaluated systems or metrics, such that task-specific qualities can be evaluated without the impact of general, input-agnostic preferences of annotators. We note that the implications of our findings can become even more critical with the progress of LLMs trained with human preference feedback (Ouyang et al., 2022) and call for a more rigorous human evaluation of LLM performance.\n\nRelated Work\nHuman Evaluation Benchmarks Human annotations are essential to the analysis of summarization research progress. Thus, recent efforts have focused on aggregating model outputs and annotating them according to specific quality dimensions (Huang et al., 2020; Bhandari et al., 2020; Stiennon et al., 2020; Zhang and Bansal, 2021; Fabbri et al., 2022a; Gao and Wan, 2022) . The most relevant work to ours is Bhandari et al. (2020) , which annotates summaries according to semantic content units, motivated by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. However, this benchmark only covers a single dataset (CNNDM) without a focus on similarly-performing state-of-the-art systems, which may skew metric analysis (Tang et al., 2022a) and not fully reflect realistic scenarios (Deutsch et al., 2022) . In contrast, our benchmark consists only of outputs from recently-introduced models over three datasets.\nSummarization Meta-Evaluation With a human evaluation dataset, there exist many directions of meta-evaluation, or re-evaluation of the current state of evaluation, such as metric performance analyses, understanding model strengths, and human evaluation protocol comparisons.\nWithin metric meta-analysis, several studies have focused on the analysis of ROUGE (Lin, 2004b) , and its variations (Rankel et al., 2013; Graham, 2015) , across domains such as news (Lin, 2004a) , meeting summarization (Liu and Liu, 2008) , and scientific articles (Cohan and Goharian, 2016) . Other studies analyze a broader set of metrics (Peyrard, 2019; Bhandari et al., 2020; Deutsch and Roth, 2020; Fabbri et al., 2022a; Gabriel et al., 2021; Kasai et al., 2022b) , including those specific to factual consistency evaluation (Kryscinski et al., 2020; Durmus et al., 2020; Wang et al., 2020; Maynez et al., 2020; Laban et al., 20d; Fabbri et al., 2022b; Honovich et al., 2022; Tam et al., 2022) .\nRegarding re-evaluating model performance, a recent line of work has focused on evaluating zeroshot large language models (Goyal et al., 2022; Liang et al., 2022; Tam et al., 2022) , noting their high performance compared to smaller models.\nAs for the further understanding of human evaluation, prior work has compared approaches to human evaluation (Hardy et al., 2019) , studied annotation protocols for quality dimensions such as linguistic quality (Steen and Markert, 2021) and factual consistency (Tang et al., 2022b) , and noted the effects of human annotation inconsistencies on system rankings (Owczarzak et al., 2012) . The unreliability and cost of human evaluation in certain settings have been emphasized (Chaganty et al., 2018; Clark et al., 2021) , with some work noting that thousands of costly data points may need to be collected in order to draw statistically significant conclusions (Wei and Jia, 2021). Our meta-analysis focuses on this latter aspect, and we further analyze potential confounding factors in evaluation such as length and protocol design, with respect to both small and large zero-shot language models.\n\nAtomic Content Units for Summarization Evaluation\nWe now describe our Atomic Content Unit (ACU) annotation protocol for reference-based summary salience evaluation, including the procedure of writing ACUs based on reference summaries and matching the written ACUs with system outputs.\n\nPreliminaries\nIn this work, we focus on a specific summarization meta-evaluation study on summary salience. Salience is a desired summary quality that requires the summary to include all and only important information of the input article. The human evaluation of summary salience can be conducted in either reference-free or reference-based manners. The former asks the annotators to assess the summary directly based on the input article (Fabbri et al., 2022a) , while the latter requires the annotators to assess the information overlap between the system output and reference summary (Bhandari et al., 2020) , under the assumption that the reference summary is the gold standard of summary salience. 2 Given that reference-based protocols are more constrained, we focus on reference-based evaluation for our human judgment dataset collection, and we conduct a comparison of protocols in \u00a75.\n\nACU Annotation Protocol\nInspired by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols and subsequent annotation collection efforts (Bhandari et al., 2020; Zhang and Bansal, 2021) , the ACU protocol is designed to reduce the subjectivity of reference-based human evaluation by simplifying the basic annotation unit -the annotators only need to decide on the presence of a single fact, extracted from one text sequence, in another text sequence, to which a binary label can be assigned with more objectivity. Specifically, the evaluation process is decomposed into two steps: (1) ACU Writing -extracting facts from one text sequence, and (2) ACU Matching -checking for the presence of the extracted facts in another sequence. We formulate the ACU protocol as a recall-based protocol, such that the first step only needs to be performed once for the reference summary, allowing for reproducibility and reuse of these units when performing matching on new system outputs. ACU Writing While the LitePyramid approach defines its basic content unit as a sentence containing a brief fact, we follow Bhandari et al. (2020) to emphasize a shorter, more fine-grained information unit. Specifically, we define the ACU protocol with the concept of atomic facts -elementary information units in the reference summaries, which no\nThe clash occurred inside the box.\nOscar is Brazilian.\nOscar was taken off at half time.\nDidier Drogba replaced Oscar. longer need to be further split for the purpose of reducing ambiguity in human evaluation. 3 Then, ACUs are constructed based on one atomic fact and other minimal, necessary information. Fig. 1 shows an example of the written ACUs. To ensure annotation quality, we (the authors) write all the ACUs used in this work. We define guidelines to standardize the annotation process; for each summary sentence the annotator creates an ACU constituting the main information from the subject of the main clause (e.g., root), followed by additional ACUs for other facts while including the minimal necessary information from the root. We provide rules for dealing with quotations, extraneous adjectives, noisy summaries, and additional cases. We note that there can still be inherent subjectivity in the written ACUs among different annotators even with the provided guidelines. However, such subjectivity should be unbiased in summary comparison because all the candidate summaries are evaluated by the same set of written ACUs. ACU Matching Given ACUs written for a set of reference summaries, our protocol evaluates summarization system performance by checking the presence of the ACUs in the system-generated summaries as illustrated in Fig. 1 . For this step, we recruit annotators on Amazon Mechanical Turk 4 (MTurk). The annotators must pass a qualification test, and additional requirements are specified in Appendix A. Besides displaying the ACUs and the system outputs, we also provide the reference summaries to be used as context for the ACUs. Scoring Summaries with ACU ACU matching annotations can be aggregated into summary scores. We first define an un-normalized ACU score f of a candidate summary s given a set of ACUs A as: where A s is a subset of A that is matched with s.\n\nOscar collided with\nEQUATION\nWe note that f by default is a recall based score with respect to the reference summary r. Therefore, we also define a normalized ACU score f as:\nf\u03b1(s, A, r) = e min (0,\nEQUATION\nwhere |s|, |r| are the length (i.e., number of words) of the candidate summary s and the reference summary r respectively, and \u03b1 is a positive number controlling the strength of the normalization. This normalization is in effect a redundancy penalty, which penalizes the summaries longer than the reference and resembles the brevity penalty in BLEU (Papineni et al., 2002) . In practice, we set the value of \u03b1 by de-correlating f with the summary length using the collected ACU annotations.\n\nACU Annotation Collection\nWe collect ACU annotations on three summarization datasets: CNNDM (Nallapati et al., 2016 ), XSum (Narayan et al., 2018 ), and SamSum (Gliwa et al., 2019) . To reflect the latest progress in text summarization, we collect and annotate the generated summaries of pre-trained summarization systems proposed in recent years. 5 Detailed informa-tion about the summarization systems we used can be found in Appendix A.2. Table 2 shows the statistics of the collected annotations. The annotations are collected from the test set of the above datasets, and additionally from the validation set of CNNDM to facilitate the training of automatic evaluation metrics. In total, we collect around 21.8k ACU-level annotations and around 22k summary-level annotations, aggregated over around 50k individual summary-level judgments.\nTo calculate inter-annotator agreement, we use Krippendorff's alpha (Krippendorff, 2011) . The aggregated summary-level agreement score of ACU matching is 0.7571, and the ACU-level agreement score is 0.7528. These agreement scores are higher than prior collections, such as RealSumm (Bhandari et al., 2020) and SummEval (Fabbri et al., 2022a) , which have an average agreement score of crowd-workers 0.66 and 0.49, respectively.\n\nRoSE Benchmark Analysis\nWe first analyze the robustness of our collected annotations and a case study on the system outputs.\n\nPower Analysis\nWe analyze the statistical power of our collected human annotations to study whether it can yield stable and trustworthy results (Card et al., 2020) . Statistical power is the probability that the null hypothesis of a statistical significance test is rejected given there is a real effect. For example, for a human evaluation study that compares the performance of two genuinely different systems, a statistical power of 0.80 means there is an 80% chance that a significant difference will be observed. Further details can be found in Appendix B.1.\nWe conduct the power analysis for pair-wise system comparisons with ACU scores (Eq. 1) focusing on two factors, the number of test examples and the observed system difference. Specifically, we run the power analysis with varying sample sizes, and group the system pairs into buckets according to their performance difference, as determined by ROUGE1 recall scores (Fig. 2 ). 6 We observe the following: (1) A high statistical power 7 is difficult to reach when the system performance is similar. 6 We note that these scores are proxies of the true system differences, and the power analysis is based on the assumption that the systems have significantly different performance. 7 An experiment is usually considered sufficiently powered if the statistical power is over 0.80. Notably, while the sample size of the human evaluation performed in recent work is typically around 50-100, 8 such sample size can only reach a power of 0.80 when the ROUGE1 recall score difference is above 5. (2) Increasing the sample size can effectively raise the statistical power. For example, when the system performance difference is within the range of 1-2 points, the power of a 500-sample set is around 0.50 while a 100-sample set only has a power of around 0.20. The results of power analysis on three datasets with both ROUGE and ACU score differences are provided in Appendix B.2 with the same patterns, which indicates that our dataset can provide more stable summarization system evaluation thanks to its higher statistical power.\n\nSummarization System Analysis\nAs a case study, in Tab. summaries. Meanwhile, the systems that generate longer summaries may be favored by users who prefer more informative summaries. Therefore, we join the previous work (Sun et al., 2019; Song et al., 2021; Gehrmann et al., 2022; Goyal et al., 2022) in advocating treating summary lengths as a separate aspect of summary quality in evaluation, as in earlier work in summarization research. 9\n\nEvaluating Annotation Protocols\nApart from ACU annotations, we collect human annotations with three different protocols to better understand their characteristics. Specifically, two reference-free protocols are investigated: Prior protocol evaluates the annotators' preferences of summaries without the input document, while Ref-free protocol evaluates if summaries cover the salient information of the input document. We also consider one reference-based protocol, Ref-based, which evaluates the content similarity between the generated and reference summaries. Appendix D.1 provides detailed instructions for each protocol.\n\nAnnotation Collection\nWe collected three annotations per summary on a 100-example subset of the above CNNDM test set using the same pool of workers from our ACU qualification. Except for ACU, all of the summaries from different systems are evaluated within a single task with a score from 1 (worst) to 5 (best), similar (2) annotations for summaries from GPT-3 (Brown et al., 2020), 10 T0 (Sanh et al., 2022), BRIO, and BART to better understand annotation protocols with respect to recently introduced large language models applied to zero-shot summarization.\n\nResults Analysis\nWe investigate both the summary-level and systemlevel correlations of evaluation results of different protocols to study their inherent similarity. Details of correlation calculation are in Appendix C.\nResults on Fine-tuned Models We show the system-level protocol correlation when evaluating the fine-tuned models in Tab. 4, and the summarylevel correlation can be found in Appendix D.2. We use the normalized ACU score (Eq. 2) because the other evaluation protocols are supposed to resemble an F1 score, while the ACU score is by definition recall-based. We have the following observations:\n(1) The Ref-free protocol has a strong correlation with the Prior protocol, suggesting that the latter may have a large impact on the annotator's document-based judgments.\n(2) Both the Prior and Ref-free protocols have a strong correlation with summary length, showing that annotators may favor longer summaries.\n(3) The Ref-free protocol and the Ref-based protocol have a negative correlation while ideally they are supposed to measure similar quality aspects.\nWe perform power analysis on the results following the procedure in \u00a74.1 and found that ACU protocol can yield higher statistical power than the Ref-based protocol, suggesting that the ACU protocol leads to more robust evaluation results. We also found that the reference-free Prior and Ref-free Table 6 : The Kendall's correlation between the automatic metric scores and ACU scores of system outputs on CNNDM, XSum, and SamSum datasets. The correlation is calculated at both the system level and the summary level. We use the recall score of the automatic metrics when available to align with the ACU scores.\nautomatic metric variants. We focus the metric evaluation on ACU annotations because of two insights from \u00a75: (1) Reference-based metrics should be evaluated with reference-based human evaluation.\n(2) ACU protocol provides higher statistical power than the summary-level Ref-based protocol.\n\nMetric Evaluation with ACU Annotations\nWe use the correlations between automatic metric scores and ACU annotation scores of system outputs to analyze and compare automatic metric performance. The following metrics are evaluated:\n(1) lexical overlap based metrics, ROUGE (Lin, 2004b), METEOR (Lavie and Agarwal, 2007) 5) evaluation methods based on large language models, GPTScore (Fu et al., 2023) and G-Eval (Liu et al., 2023) , with two variants that are based on GPT-3.5 11 (G-Eval-3.5) and GPT-4 12 (OpenAI, 2023) (G-Eval-4) respectively. We note that for LLM-based evaluation we require the metric to calculate the recall score. For G- -3.5 -.091 -.273 -.091 .818 1.00 1.00 G-Eval-3.5-S -.091 -.273 -.273 .818 1.00 1.00 G-Eval-4\n.091 .818 .636 1.00 1.00 1.00\nTable 7 : The system-level Kendall's correlation between the automatic metric and ACU scores on different system pairs grouped by their ACU score differences on the CNNDM dataset, into six equal-sized buckets. We use the recall score of the automatic metrics when available.\nEval-3.5 we report two variants that are based on greedy decoding (G-Eval-3.5) and sampling (G-Eval-3.5-S) respectively, Details of the LLM-based evaluation are in Appendix E.2. Tab. 6 shows the results, with additional results of more metrics in Appendix E.3. We note:\n(1) Several automatic metrics from the different families of methods (e.g., ROUGE, BARTScore) are all able to achieve a relatively high correlation with the ACU scores, especially at the system level.\n(2) Metric performance varies across different datasets. In particular, metrics tend to have stronger correlations on the SamSum dataset and weaker correlations on the XSum dataset. We hypothesize that one reason is that the reference summaries of the XSum dataset contain more complex structures.\n(3) Despite their successes (Fu et al., 2023; Liu et al., 2023) in other human evaluation benchmarks such as SummEval, LLM-based automatic evaluation cannot outperform traditional methods such as ROUGE on RoSE. Moreover, their low summarylevel correlation with ACU scores suggests that their predicted scores may not be well-calibrated.\nFollowing Deutsch et al. ( 2022), we further investigate metric performance when evaluating system pairs with varying performance differences. Specifically, we group the system pairs based on the difference of their ACU scores into different buckets and calculate the modified Kendall's correlation (Deutsch et al., 2022) on each bucket. The system pairs in each bucket are provided in Appendix E.4. Tab. 7 shows that the automatic metrics generally perform worse when they are used to evaluate similar-performing systems. \n\nAnalysis of Metric Evaluation\nWe analyze the metric evaluation with respect to the statistical characteristics and the impact of different human evaluation protocols on metric evaluation.\nConfidence Interval We select several representative automatic metrics and calculate the confidence intervals of their system-level correlations with the ACU scores using bootstrapping. Similar to Deutsch et al. ( 2021b), we find that the confidence intervals are large. However, we found that having a larger sample size can effectively reduce the confidence interval, which further shows the importance of increasing the statistical power of the human evaluation dataset as discussed in \u00a74.1. We provide further details in Appendix E.5.\n\nPower Analysis of Metric Comparison\nWe conduct a power analysis of pair-wise metric comparison with around 200 pairs, which corresponds to the chance of a statistical significance result being found. More details can be found in Appendix E.6. The results are in Fig. 3 , showing similar patterns as in the power analysis of summarization system comparison ( \u00a74.1):\n(1) Significant results are difficult to find when the metric performance is similar;\n(2) Increasing the sample size can effectively increase the chance of finding significant results. automatic metrics generally perform better under reference-based evaluation protocols, but can have negative correlations with reference-free protocols.\n\nConclusion and Implications\nWe introduce RoSE, a benchmark whose underlying protocol and scale allow for more robust summarization evaluation across three datasets. With our benchmark, we re-evaluate the current state of human evaluation and its implications for both summarization system and automatic metric development, and we suggest the following:\n(1) Alignment in metric evaluation. To evaluate automatic metrics, it is important to use an appropriate human evaluation protocol that captures the intended quality dimension to be measured. For example, reference-based automatic metrics should be evaluated by reference-based human evaluation, which disentangles metric performance from the impact of reference summaries.\n(2) Alignment in system evaluation. We advocate for targeted evaluation, which clearly defines the intended evaluation quality. Specifically, text summarization, as a conditional generation task, should be defined by both the source and target texts along with pre-specified, desired characteristics. Clearly specifying characteristics to be measured can lead to more reliable and objective evaluation results. This will be even more important for LLMs pretrained with human preference feedback for disentangling annotators' prior preferences for LLMs with the task-specific summary quality.\n(3) Alignment between NLP datasets and tasks.\nHuman judgments for summary quality can be diverse and affected by various factors such as summary lengths, and reference summaries are not al-ways favored. Therefore, existing summarization datasets (e.g. CNNDM) should only be used for the appropriate tasks. For example, they can be used to define a summarization task with specific requirements (e.g. maximum summary lengths), and be important for studying reference-based metrics.\n"}
{"question": "What are the two main categories of Greek papyri mentioned in the text?", "evidence": "  Greek papyri, which mostly survive in fragments, are divided into two broad categories: books (literary and sub-literary papyri) and documents of all kinds (documentary papyri). The former ones never carry a date, whereas the latter often do, albeit not always unambiguously convertible by modern scholars. ", "options": ["A. Literary and sub-literary papyri", "B. Documentary and inscriptions papyri", "C. Ancient and modern papyri", "D. Official and personal papyri"], "answer": "B", "content": "\nIntroduction\nAncient textual artefacts are arguably the richest source of information on the ancient world. In the Graeco-Roman world and particularly in its Greekspeaking part, the most extensive coeval texts come from inscriptions and papyri. The latter is a collective term used for all ancient manuscripts, regardless of their writing material which, apart from papyrus, may be parchment, pottery, wood, and others. To correctly evaluate and make good use of these texts, we need to determine their date, provenance and historical context of their production and use. As far as dating is concerned, the value of the relevant evidence provided by the artefacts themselves varies considerably, ranging from a direct date in the text (following, of course, the calendar and dating system of the respective historical period) to no evidence at all. In between, there are texts containing references to known historical figures and events of a certain period, papyri which have been found next to other objects that can be dated, or other indirect evidence. The presence or absence of a date depends on the type of text preserved on the papyrus and its use through time, as well as on its state of conservation. Just like in modern times, it is much more likely to include a date in an official letter than in a page torn from a novel book. At the same time, it is more probable to find a date in a fully surviving letter than in a damaged one missing, for instance, the upper part of the first page.\nGreek papyri, which mostly survive in fragments, are divided into two broad categories: books (literary and sub-literary papyri) and documents of all kinds (documentary papyri). The former ones never carry a date, whereas the latter often do, albeit not always unambiguously convertible by modern scholars. Most importantly for our study, literary papyri contain copies of works authored many years (often centuries) before the production of the actual manuscripts. On the other hand, documentary texts were usually written down as they were composed or shortly after that, making the content of their texts contemporary to their writing style or script. Therefore, any temporal indication in the text is also dating evidence regarding the production of the document. Even when there is no direct date in the text (e.g. Figure 1 ), documentary papyri can be dated securely sometimes within a short time-frame, because they may refer to known historical events or concern people known through other sources to have lived at a particular time.\nWhen neither direct or indirect dating is possible, papyrologists resort to palaeography, the study of the script. In palaeography, particular writing styles are associated with certain chronological periods. Therefore, similar writing styles point to similar dates (Mazza, 2019) . Securely dated specimens are used as a guide to chronologically place the undated ones. Growing criticism on the subjectivity of palaeographical dating (Mazza, 2019; Choat, 2019; Nongbri, 2019 Nongbri, , 2014) ) highlights the need for more reliable methods. Recent efforts for computational dating of historical manuscripts are based on the script rather than the text and, although they consider various languages, they disregard Greek (Omayio et al., 2022) .\nIn this study we focus on computational dating of Greek documentary papyri based on their transcriptions, contributing in the following three ways:\n1. We present and publicly release a machineactionable dataset of 389 documentary Greek papyri, containing texts of various aspects of daily life (e.g. contracts, receipts, letters).\n2. We draw the baseline in text regression for the tasks of dating experimenting with Monte Carlo and leave one out cross validation.\n3. We apply a committee of regressors to three papyri, which present different types of dating challenges, and on 159 manuscripts for which only the upper date limit is known.\nThis approach does not apply to literary papyri and our research involves solely documents. Apart from their texts being contemporary with the actual manuscripts (by dating the text, we date the papyrus), nonliterary papyri also include vastly more numerous objectively dated specimens than literary ones. Specific dates on our training set also allow for more accurate (narrower date-spans) predictions by our models.\n\nRelated Work\nDating historical documents with computational means has been studied for many languages (Baledent et al., 2020; Dhali et al., 2020; Li et al., 2015; Hamid et al., 2019; Adam et al., 2018) . However, very limited work has been done for Greek and no published work at all has focused on Greek papyri. The only work to our knowledge is Ithaca, a Transformer trained on ancient Greek inscriptions performing text restoration, geographical attribution, and dating (Assael et al., 2022) . Ithaca has achieved an error of 0.29 centuries in dating epigraphs. This result is by far better than an onomastic baseline using the known distribution of Greek personal names to infer the date, which scored 1.44. Inscriptions differ from papyri in many aspects (such as the genre, the length, and their geographical distribution), but in principle, this system is applicable to our data and was therefore used as a baseline. Below, given the absence of dating studies for Greek, we summarise work for other languages.\nThe studied languages are Latin (Baledent et al., 2020; Wahlberg et al., 2016 Wahlberg et al., , 2015)) , Hebrew (Dhali et al., 2020) , Dutch (Hamid et al., 2019 (Hamid et al., , 2018;; He et al., 2014 He et al., , 2016b,a),a) , Arabic (Adam et al., 2018) , Swedish (Wahlberg et al., 2016 (Wahlberg et al., , 2015)) , French (Baledent et al., 2020) and English (Li et al., 2015; Rastas et al., 2022) . A collection of 595 Dead Sea Scrolls, in Aramaic script, was the dataset with the oldest manuscripts, dated from 250 to 135 BCE, and the only one so far concerning texts written on papyri (Dhali et al., 2020) . The rest of the datasets comprised more data, ranging from less than five (Adam et al., 2018) to more than ten thousand manuscripts (Wahlberg et al., 2015) or more (Rastas et al., 2022) , while the one with the most recent manuscripts comprises historical English-language documents (Li et al., 2015) , printed between the 15th and 19th CE.\nThe employed methods usually were standard machine learning methods, such as KNN (Adam et al., 2018) , decision trees (Baledent et al., 2020) , random forests (Baledent et al., 2020) and support vector machines (Hamid et al., 2019; Dhali et al., 2020; He et al., 2014 He et al., , 2016b,a),a) . Textural features, such as Gabor filters, Uniform Local Binary Patterns and Histogram of Local Binary Patterns are extracted and then fed to the classifiers (Hamid et al., 2018) . The writing style evolution, however, has also been used as an intermediate step (Dhali et al., 2020; Adam et al., 2018) . In this case, the periods are first aligned with specific writing styles. Then, any new manuscript is dated based on the detected style.\nPre-trained convolutional neural networks have been used to extract features, which are passed to a classifier or regressor (Hamid et al., 2019; Wahlberg et al., 2016) , or used in combination with text features extracted with optical character recognition methods (Li et al., 2015) . Transfer learning has been reported to lead to human performance (Wahlberg et al., 2016) . This was deemed to be the most promising direction for the present study on Greek manuscripts, and was, hence, employed.\n\nData\nOur dataset, which we release publicly, 1 comprises the transcriptions of 389 manuscripts, dated from the 3rd century BCE to the 7th century CE, originating from Greco-Roman Egypt (with a few exceptions from the Near-East).\n\nThe source\nThe dataset was compiled mainly from PA-PYRI. INFO. 2 The documents in its collections set a reliable point of reference for scholars who aspire to study the evolution of ancient manuscripts in time. These collections incorporate full transcriptions and references to scholarly editions of the papyri, as well as a set of metadata that can also assist in dating (e.g. provenance).\n\nThe scripts and the language\nNonliterary papyri in Greek from the 3rd c. BCE to the 7th c. CE are written in a great variety of cursive hands (Harrauer, 2010) , posing an extra challenge for image classification methods and calling for other approaches. The language of the papyri, Greek of the Ptolemaic, Roman and early Byzantine periods, reflects the diversity and the diachronic changes of the Greek-speaking communities in Egypt, which is the provenance of most of our specimens.\n\nThe ground truth\nThe date of a manuscript may be found in different forms. It can be an exact date, a range of years, a starting date (not before that date), or an ending date (not after that date), or two-three alternative dates. Our dataset has been curated so that dating applies at the level of the quarter of the century, by considering manuscripts dated exactly or with a period ranging within that quarter. We did not consider manuscripts that were dated only before or after a specific date.\n\nData collection\nOur first dataset comprised 400 manuscripts, 40 samples per century. Our initial pool consisted of 77,040 items and we opted for ones that satisfy the following conditions:\n\u2022 The transcriptions must be available in machine actionable form.\n\u2022 The papyri must contain documents (not works of literature) to ensure that text and papyrus are contemporary. 3\n\u2022 The papyri must be securely and accurately dated. Many papyri do not carry a date and are, therefore, dated with subjective criteria or with a large date span (e.g. 1st-2ndCE).\n\u2022 The image is available, to allow image-based dating and potentially jointly from different modalities: text and image.\nGiven these limitations, it was the 7thCE that dictated the size per century of a balanced dataset, since there are not more than 40 securely dated papyri from 7thCE. For each of these records, the text was retrieved afterwards from PAPYRI.INFO by parsing the respective XML files. We discarded records whose extracted text was less than ten characters, which resulted in our final 389 records. From these records, we extracted the entire text from one side of the papyrus (the side that had more text than the other). In the few cases of papyri with more than one fragment, we only included the first one. This decision was based on weighing the benefit of avoiding a considerable amount of noise during automatic parsing against eliminating a portion of text, in a dataset whose nature is by definition fragmentary.\n\nNormalisation\nThe transcribed text comprises a variety of characters and symbols. We preprocessed the data by lowercasing and normalising the text (see Table 1 ). We (o) '\u1f45', '\u1f43', '\u03cc', '\u03cc', '\u1f44', '\u1f41', '\u1f40', '\u1f78' (\u03b1) '\u1f02', '\u1fb4', '\u1f03', '\u1f85', '\u03ac', '\u1f01', '\u1f04', '\u1fb6', '\u1f00', '\u1fb7', '\u1f70', '\u03ac', '\u1f05' (\u03b7) '\u1f24', '\u1f23', '\u1fc3', '\u1f22', '\u1f20', '\u03ae', '\u1f26', '\u1f25', '\u1fc6', '\u1f27', '\u1f97', '\u1f94', '\u1fc7', '\u1f21', '\u1fc4', '\u1f91', '\u1f74', '\u03ae' (\u03b9) '\u03af', '\u1fd6', '\u1f37', '\u1f31', '\u1f36', '\u1fd2', '\u03af', '\u1f30', '\u1f76', '\u0390', '\u1f34', '\u03b9', '\u03ca', '\u1f33', '\u1f35' (\u03b5) '\u03ad', '\u1f72', '\u03ad', '\u1f14', '\u1f10', '\u1f15', '\u1f11', '\u03b5' (\u03c5) '\u1f57', '\u03cd', '\u1fe6', '\u03cd', '\u1f55', '\u1f53', '\u1f7a', '\u1f51', '\u1f54', '\u1f50', '\u1f56' (\u03c1) '\u1fe5', '\u1fe4' (\u03c9) '\u03ce', '\u1ff6', '\u1f66', '\u1fa4', '\u1f60', '\u1f67', '\u1ff3', '\u1ff7', '\u1f62', '\u1f65', '\u03ce', '\u1fa7', '\u03ce', '\u1fa6', '\u1f64', '\u1fa0', '\u1f7c', '\u1f61', '\u1ff4' (\u03c3) '\u03c3', '\u03c2' Table 1 : Normalisation rules of characters in the dataset, all characters on the right have been replaced by the character on the left. also discarded any character besides the 24 Greek letters, also removing white space and all punctuation marks. We did not eliminate the editors' corrections and supplements nor edit otherwise the data, which often led to duplicate words with alternative orthography (original and normalisation).\nThe transcriptions available are not diplomatic (reflecting exactly what is written) but normalised according to modern conventions, for example as far as punctuation and word separation (or sometimes spelling) are concerned. Therefore, we chose to disregard these conventions, because they do not represent data present in our sources, but normalisation on the papyrologists' part for the purpose of scholarly editions.\nTo provide some more concrete examples, there is no capitalization of proper names or initial words in sentences in papyri. Punctuation is very scarce and sometimes completely absent. Diacritics are not meaningless, but they are extremely rare in documentary papyri (i.e., except diaresis which is used in a different way than modern conventions, to mark iota and upsilon as the first letter of a word). Breathings and accents are marked inconsistently (if at all) by different scribes. Hence, removing diacritics leads to inclusion and can help avoid multiple variations of what is in fact the same word. Regarding spelling, we kept both the original and the corrected form (if provided by the editors), because spelling mistakes reflect language evolution.\n\nExploratory analysis\nThe overall text length per quarter of century varies over time, as can be seen in Figure 2 . Although we have selected an equal number of manuscripts per century ( \u00a73.4), the number of lines within each manuscript varies, and so does the line length. Furthermore, within a century, manuscripts of a spe- cific quarter of a century may be more frequent due to random discoveries, as is the case of 7thCE, where the first quarter holds most of the support, a discrepancy deriving from the reduced number of dated papyri in this century overall.\nThe most frequent character in our dataset is '\u03b1' (35,101 occurrences), followed by '\u03bf' (33,176), '\u03b9' (30,151), and '\u03b5' (25,116) . On the other hand, the least common are '\u03b2' (2520), '\u03be' (1210), '\u03b6' (379), and '\u03c8' (334). These figures are coherent with general frequencies of letters in Ancient and Modern Greek (Mikros et al., 2005) .\nIn order to assess the quality of the ground truth, we employed the Callimachus' Conservation number (CCN), 4 which provides an educated estimation of the preservation and legibility of a papyrus. The lowest score is 0 and the highest score (i.e., 1) indicates readability and 'perfect' conservation of the text. The status of the conservation of a papyrus affects the quality of the transcription, indicating the amount of text that has not been recorded in the transcriptions (or recorded with some level of uncertainty) because of the material state of preservation of the manuscripts. Damage in papyri could affect as little as one or two letters (or even none), to as much as several lines and whole parts of the \n\nMethodology\nTo estimate the date of production of manuscripts, we opted for text regression, taking advantage of the continuous target objective. Statistical validity was established with 5-fold Monte Carlo crossvalidation. The best regression method was used to form a committee of models, which were applied on unseen data in order to analyse the predictions.\n\nBenchmarking\nWe performed Monte Carlo cross-validation, by sampling 90% for training, 10% for validation, and then re-sampling with replacement five times. We report the mean absolute error (MAE), the mean squared error (MSE), and the explained variance (R 2 ). Besides the average results across folds, we also report the best score achieved per metric.\n\nRegression methods\nFern\u00e1ndez-Delgado et al. ( 2019) surveyed 77 regression methods and undertook an experimental analysis on 83 datasets. Regression with extremely randomised trees achieved the best R 2 in many datasets but gradient boosting and random forests were also found to have a promising performance. Following these findings, we opted for extremely randomised trees, random forests, gradient boosting, and linear regression for our experiments. 5 Extremely randomised trees (XTrees) is a treebased ensemble, created with the Extra-Trees algorithm (Geurts et al., 2006) . Although simple in nature, it is both accurate and efficient Fern\u00e1ndez-Delgado et al. (2019) . Compared to other ensembles that use decision trees, XTrees splits the nodes of the tree by choosing randomly cut-off points and the trees grow by using the whole sample to learn instead of bootstrapping.\n\nThe Committee\nUsing the best-performing regression method out of the ones examined, we performed leave one out cross-validation, which allowed an evaluation using the whole dataset. Furthermore, it yielded as many regressors as the data points, which in our case is 389. We used these models to form a committee and date unseen papyri (further discussed in \u00a76).\n\nEmpirical analysis\nThis section presents our experimental results using regression on textual features to date Greek manuscripts. First, we present preliminary experiments and then we analyse the experimental findings from our regression analysis.\n\nPreliminary experiments\nPreliminary experiments comprised image classification (Hamid et al., 2018 ), text classification with Transformers trained on another domain (Assael et al., 2022) , and transferring learning from large language models (Koutsikakis et al., 2020) . Image classification was used prior to using transcribed text as our input, experimenting with using the documents' images (Hamid et al., 2018; Wahlberg et al., 2016; Paparigopoulou et al., 2022) . Vanilla convolutional neural networks were outperformed by a pre-trained one (Tan and Le, 2019), fine-tuned for our dating task. Our estimated MAE, however, was consistently more than a hundred years (Table 2 ), hence we opted for textual input. Ithaca was presented by Assael et al. (2022) , consisting of a Transformer that is trained not only in dating but also in text restoration and geographical attribution. Ithaca has achieved an error of 0.29 centuries in dating inscriptions, which is by far better than an onomastics baseline (error of 144 years).\nBy using the open-access web interface, 6 we scored all our preprocessed texts, 7 registering a MAE of approx. one century by using the maximum decade predicted or the average of the distribution (Table 2). The difference from the published result possibly stems from the fact that this is a model trained and focused on inscriptions, not papyri. Transfer learning was used with GreekBERT, a Transformer that is pre-trained in masked language modelling, among other tasks, in modern Greek (Koutsikakis et al., 2020) . GreekBERT has been further pre-trained in ancient Greek (Singh et al., 2021) . We experimented with fine-tuning both variants in predicting the date, 8 but MAE was approx. one century (Table 2 ).\n\nRegression analysis\nExperiments were undertaken with Google Colaboratory, using a 12GB NVIDIA Tesla K80 GPU. We extracted term-frequency-inverse-documentfrequency features using lower-cased text and character n-grams (from 1-to 5-grams). 9 All other parameters were set to default values. 10\n\nMonte Carlo cross validation\nLinear regression achieved a MAE of 86 years on average (Table 2 ) and a MSE of 1.33. R 2 was similar across folds, around 83. A random forest had an even better MAE of 73 years on average but a worse MSE (1.58). Its average R 2 was lower than that of linear regression, but the maximum one achieved across folds was much better. Random forest also outperformed both gradient boosting methods in MAE but GBoost achieved a better MSE and R 2 on average. XTrees achieved the best results in all metrics, with a MAE of 54 years and the best R 2 climbing up to 95.43.\n\nLeave one out cross validation\nUsing the best performing XTrees, we performed leave one out cross validation, by hiding one instance, training the algorithm on the remaining instances, and then using the model to predict the hidden record. 11 The MAE was found to be 55 years, MSE was 1.11, and R 2 was 85.89, close to the Monte Carlo evaluation scores. In order to better understand the errors, we rounded the predictions and the ground truth, evaluating as if we would in a classification setting. Predictions most often fall on or close to the diagonal (Figure 4 ), which explains the low error. The best result is 8 We used white space, to allow subword computation. 9 Preliminary experiments with centroid or trainable word embeddings before recurrent or convolutional neural networks deteriorated performance.\n10 Manual hyper-parameter tuning per regressor yielded insignificant improvements.\n11 The experiment lasted 15 hours. 3 .\nachieved for the 1st and 2nd CE, followed by the 7th CE (see Table 3 ). The overall accuracy is 60%.\n\nError analysis\nIn very few cases, our leave-one-out regression fell considerably out of its predictions (Figure 4 ). Our analysis showed that these texts happen to contain specific words typical of another period, which confused the prediction. For instance among the highest prediction error were two late texts (6-7thCE) that exceptionally contain \u03a3\u03b5\u03c1\u03b1\u03c0\u03af\u03bf\u03c5 and \u0392\u03b1\u03c3\u03b9\u03bb\u03b5\u03af\u03bf\u03c5, usually found in Ptolemaic time (3rd-1stBCE). In another case, we provided experimentally the longer version of the text, initially parsed only partially ( \u00a73.4). Using the full text led to an accurate prediction, influenced by the word 'indiction' in the additional text ( \u00a77.1).\n\nUse cases\nWe applied our 389 regressors, produced upon leave-one-out cross-validation, to three use cases, which present different types of dating challenges.\n\nPSI 8 934\nThis document 12 preserves the ca. 15 last lines of a land lease. texts from the 6th and early 7thCE c., the Dioscorus archive (Fournet, 2008) , because, among other concordant elements, it contains microtoponyms from the respective village countryside. The notary who signed the contract, Abraam, is known from other documents, which is crucial evidence for the dating of the papyrus. This notary's period of activity has been proven to span at least between 524 and 545 (Fournet, 2003) . This papyrus, therefore, is securely dated by indirect evidence, but no date is explicitly mentioned in the text (Fournet, 2008) . Our average prediction is 310 CE, dated between 260 CE (min) and 352 CE (maximum prediction).\n\nP. Basel 2 15\nThis papyrus, also shown in Figure 1 , is a private letter dated indirectly from the 1st CE. The letter is almost complete, except for a damaged word at the end of line 5. Private letters usually do not bear a date. The dating, therefore, by the editor is done on palaeographical grounds as well as on the basis of scribal habits: \"the hand [...] is more at home in the first century CE than the second, a dating that is supported by the writer's use of iota adscript...\" (Huebner et al., 2020) . Iota adscript is an expected feature in the 3rd BCE, starting to be irregularly written between the 2nd BCE and the first CE to almost completely disappear from the 2nd CE onwards (Clarysse, 1976) . Onomastics strengthen the editor's dating hypothesis: of the three personal names mentioned in the letter (Pasis, Orsenouphis, and Tithoes), the first two are attested from ca. 250 BCE to 250 CE while the last one starts appearing in the papyri only in the 1st c. CE. 13 Our models date this to 140 BCE, from 165 BCE to 112 BCE.\n\nP. Petra 1 5\nThe last manuscript 14 contains a request for transfer of taxation from 538 CE. It is a geographical outsider since it does not come from Egypt but from Petra (Jordan). We tested this manuscript since many of the words found in the text are infrequent in Egyptian manuscripts, on which our models are trained. The date mentioned in the papyrus is \"second indiction\". This refers to the second year of a repeated fifteen-year cycle (indiction) and the year 538 is relative, since it could be the second year of the previous or the next indiction (523 or 553). 538 is logically deduced by the editors in view of the whole dossier of papyri from Petra. Our models date this manuscript to 555 CE (521-575 CE), overcoming the geographical variation.\n\nDiscussion\nThe computational, quantitative method suggested in this work is intended to complement human expertise. Its main contribution lies in providing an additional dating criterion for ancient Greek documents, in addition to the ones usually employed by papyrologists (palaeography, onomastics, prosopography, toponymy, archaeological evidence, etc.). It can predict a date for those papyri that do not include one, narrow down the possible time-span of doubtful dating, or contribute to deciding on one particular date when several alternatives seem possible. Despite the fact that limitations exist (discussed in \u00a77.3), compared to traditional approaches the models trained in this study are expected to reduce biases. Their value is not limited to predicting dates for individual manuscripts, but they can be applied to any attribute of a group of papyri, e.g. the place of provenance or the text's type. At the same time, easily accessible open-source metadata exist for most published papyri ( \u00a73.1).\n\nRationale generation\nThe use of supervised learning, such as the work of Assael et al. (2022) or ours, can yield accurate estimations, which can at least help the human expert. The assistance is greater, however, when explanations are provided for the models' decisions. In our case, we used a committee of hundreds of regressors in order to estimate the date of three use cases. Therefore, we sampled models per case and generated rationales regarding their predictions, by using their Shapley values (Lundberg and Lee, 2017).\nIn the case of PSI 8 934 ( \u00a76.1), our investigation showed that the mention of the name 'Aurelios Victor' ('\u0391\u1f50\u03c1\u03ae\u03bb\u03b9\u03bf\u03c2 \u0392\u03af\u03ba\u03c4\u03c9\u03c1') influenced the decision, resulting to a more recent date than what would have been predicted otherwise. Similarly, in the case of P. Petra 1 5 ( \u00a76.3), the decision was influenced by a reference to 'indiction' ('\u1f30\u03bd\u03b4\u03b9\u03ba\u03c4\u03af\u03c9\u03bd\u03bf\u03c2'), a word that refers to a periodic reassessment of taxation in the Late Roman Empire.\n\nIn the wild\nComputational dating can facilitate a macroscopic analysis of vaguely dated or undated manuscripts. By generating estimated dates for hundreds of such manuscripts, the expert can view from distance the collection, potentially drawing useful conclu-sions or making significant remarks. To test this hypothesis, we collected 220 manuscripts dated with an upper CE date limit (i.e., not after that date). We formed a committee of regressors, 15 and we estimated the minimum, the maximum, and the average chronology of each manuscript. In 28% of them, the maximum prediction exceeded the upper threshold and was discarded to avoid doubting the expert. This process led to the date estimation for 159 manuscripts, which we release publicly in our repository to assist other researchers. As can be seen in Figure 5 , some of our estimations fall far away from the upper limit (in red) while others fall close. The estimated date from our regressors' committee should be read along with other information, which is kept in the shared corpus, such as the place settlement (Figure 6 shows frequent places). We observe, for example, that in some places the estimated dates fall closer to the upper limit (e.g. in Oxyrhynchos and Tebtynis the distance is 132 years) compared to others (e.g. in Antinoopolis and Hermopolis the distance is 283 and 384 years).\n\nChallenges and limitations\nOur experimental analysis proved that text regression is a considerably reliable and accurate tool in dating nonliterary papyri. Limitations and challenges stem mainly from the composition of our dataset, which is balanced as far as the dates of the papyri included are concerned, both at the level of the century (approx. 40 records per century) and at the level of the quarter of the century (albeit less strictly and with the exception of the 7th CE). Furthermore, although we retained a substantial text sample of each papyrus, in approximately 1/4 of the records some text was eliminated.\n\nBiases\nDespite our effort to balance the dataset in terms of dates, biases are present. Since our main concern in collecting the data was for the date distribution, no deliberate selection was made on the basis of the document types. Some types are thus over or underrepresented (e.g. private letters that do not usually bear a date; \u00a76.2). Each type of document has however distinctive linguistic characteristics, such as the level of formality or unusual constructions (e.g. accounts). This uneven typological representation probably affects the performance of the models. Other possible biases in the dataset concern the provenance of papyri, the length of their text, and the state of conservation (sizeable portions of missing text or entirely missing parts of the documents).\n\nChronological analysis of words\nChronological analysis of word occurrence is possible if we detect and collect terms only attested in the papyrological material during a limited period. The word 'denarius' only appears after the 2nd CE and before the 5th CE, its presence in a text thus means that the text must have been written during this timespan. Likewise a text containing the word 'indiction' cannot have been written before the 4th CE. The investigation should also regard the possibility that the models make a prediction for a papyrus based on typical dating formulas present in the text like the name of the ruling emperor. Although our investigation of explanations did not yield any major concerns, a bigger sample of test cases should be created and more explainability methods should be employed (Ribeiro et al., 2016) to make conclusive remarks on this front.\n\nTranscription of papyri is not optional\nTranscription of the papyri is required (at least partial, but substantial) to reach this high degree of accuracy with our method. Thus, while there are transcriptions available for most already published papyri, it is less practical for dating unpublished papyri that have not been yet transcribed to a relatively high standard. In that case, image classification on the scripts can provide a less accurate prediction of the date as starting point.\n\nConclusion\nWe presented a machine-actionable dataset of 389 Greek documentary papyri of (mostly) Egyptian provenance, dated and balanced in terms of chronological quarter-century distribution. We trained extremely randomised trees on top of character n-gram-based features, reaching a mean absolute error of 54 years and 60% in century-level classification accuracy. We then formed a committee of regressors, which we applied to three use cases: a land lease, a private letter, and a geographical outsider (not from Egypt). To assist future research, our committee dated 159 manuscripts, for which only the upper limit is known. Future endeavours for this research extend far beyond the dating of individual manuscripts. It can produce valuable data for the study of the Greek language and its evolution through a millennium, help identify and trace linguistic habits and trends, as well as the history of document production, circulation, and use (e.g. which period produces what kind of texts, which administration relied on what type of documents, etc.). It can also produce further data and resources towards the typology of ancient Greek documents, completing with computational methods the work already underway and well-advanced of the grammateus project. Last, it can in the future fruitfully be combined with computational paleography to analyse the script and content of a given text.\n"}
