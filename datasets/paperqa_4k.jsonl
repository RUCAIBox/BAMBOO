{"question": "How to get additional knowledge for entities?", "evidence": "  Our evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is lost when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data. ", "options": ["A. Making V&L model pre-trained.", "B. Supplementing image information.", "C. Build a bigger dataset.", "D. Supplementing textual information."], "answer": "B", "content": "\nIntroduction\nVision & Language (V&L), which is the fusion of vision and language tasks, has achieved great success in tasks such as caption generation from images (Xu et al., 2015) and image generation from texts (Reed et al., 2016) . This progress has been driven by pre-trained V&L models that are trained on large-scale V&L datasets (Du et al., 2022) . To generate appropriate captions and images for input, pre-trained V&L models need to have prior knowledge of the features of the objects they are generating (Cao et al., 2020; Yun et al., 2021) . These models retain knowledge about entities in particular by inheriting parameters from pre-trained language models used in natural language processing to indirectly utilize data resources such as Wikipedia.\nIn this way, V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang This learning process raises a number of questions, such as whether the knowledge about entities acquired from natural language is adequately retained in the pre-trained V&L model, or whether it is enhanced by combining it with image features. These are important in understanding the limits of what can be generated by the pre-trained V&L model.\nTo answer these questions, we propose a task of generating tables and images of infoboxes in English Wikipedia. Figure 1 shows an example of the target infobox, in which either tables or images are generated by the proposed task. In both cases, the model must know the entities to generate them properly.\nWe collected about 200,000 infoboxes to construct the Wikipedia posed task. In addition, we used OFA (Wang et al., 2022) , a pre-trained V&L model that has achieved state-of-the-art performance in various V&L tasks.\nOur evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is lost when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.\nIn image generation, we found that OFA can generate more accurate images by using the knowledge expressed in the table. We also found that the models trained only on natural language can infer table knowledge, which increases the diversity of generated images. Our code and dataset will be released at https://github.com/kamigaito/WikiTIG.\n\nVision & Language Models\nMany pre-trained V&L models have achieved stateof-the-art performance on various tasks by inheriting the weights of the conventional pre-trained models for natural language and images (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) before learning V&L datasets. Our study examines how the knowledge represented in the pre-trained model for natural language is transformed through such a learning process. We select OFA, which has achieved state-of-the-art performance in multiple V&L tasks, as our target model.\nFigure 2 shows the network structure of OFA and its relation to each dataset 2 . OFA uses VQGAN (Esser et al., 2020) on the decoder to transform images into discrete sequences so that the same Transformer (Vaswani et al., 2017) is used for image and natural language generation. Because OFA inherits 2 Appendix A describes the data for the pre-training.\n\nTask\nInput Output parameters from BART (Lewis et al., 2020) , which shares a similar Transformer structure, OFA should include knowledge acquired from natural language such as Wikipedia articles. Unlike the decoder, the encoder handles images directly; thus, OFA uses the output of ResNet (He et al., 2016) to embed images in addition to the embedding layer inherited from BART.\n\nTable and Image Generation\nIn this section, we describe two tasks for verifying knowledge behavior in the V&L model: table generation and image generation. Both tasks are based on infoboxes in Wikipedia articles, which correspond to summary information of the Wikipedia articles comprising tables and images 3 . Thus, it is suitable for verifying the knowledge about entities in Wikipedia kept in the pre-trained V&L model.\nIn the following subsections, we explain the details of each task.\n\nTable Generation\nIn the table generation task, the target V&L model generates a table from a title and/or image of the infobox. To do this, the model generates linearized tables, similarly to table generation by descriptions (Wu et al., 2022b) . In our setting, we linearize tables as shown in Figure 3 using the column separator \"|\" and the row separator \"<>\" to reuse pretrained token embeddings. The separator symbols are accompanied by spaces before and after for use in BPE tokenization. We investigate the target model by directly generating such linearized text. We use the following settings for the investigation.\nGeneration from titles We investigate the knowledge about entities held by V&L models by comparing tables generated from titles by pre-trained V&L models and by pre-trained models trained only on natural language.\nGeneration from title and images We generate tables from titles with images and compare the results with those generated from only titles. This enables us to investigate the new knowledge in pretrained V&L models transferred from images.\nMetrics For comparison, we use the following evaluation metrics to measure how close the generated tables are to the actual ones.\n-ROUGE: Since the linearized tables are text data and the infobox plays the role of summarizing the article, we use ROUGE (Lin, 2004) , the most widely used evaluation method for automatic summarization. In our evaluation with ROUGE, we convert the column separator \"|\" and the row separator \"<>\" to spaces so that the sequence of strings is not restricted to rows and columns.\n-Table - F 1 : To evaluate the tables with respect to their structure, we divide the cells by their types and then evaluate the matches with the reference table in terms of the F 1 measure for each case and average them. When calculating the matches, we apply clipping used in ROUGE to prevent the score from increasing due to the repetition of the same cell in the output 4 . We treat cells of each type separately 5 as follows:\n\u2022 Group: The infobox sometimes divides the table into groups, with the first row of each group serving as a header for the group name. The prediction performance for the group names is important for verifying what aspects of knowledge the model has about the entities. Since these rows consist of a single column, we target rows consisting of a single column in this type of cell.\n\u2022 Header: The head of each row in the table consisting of more than one column is usually the header of a subsequent cell in the same row. Therefore, the prediction performance for headers is important for the same reason as for group names. to the headers. Therefore, the prediction performance of the values is important for knowing whether the model has detailed knowledge about the entity. To examine the correspondence between headers and their values, we treat a header and its corresponding value as a pair.\n-Corpus-F 1 : Because the above \n\nImage Generation\nIn the image generation task, the model receives a title, caption, and table to generate the corresponding image:\nGeneration from a title and caption By using the minimum input required to generate images, we investigate the difficulty of generating them compared to other datasets.\nGeneration from a title, caption, and table We investigate the impact of knowledge about entities on image generation by generating images from input, including tables, and compare the results to the setting without tables.\nMetrics We use the following three widely used measures for evaluating image generation.\n-CLIP: The relevance of the input text to the generated images inferred by the pre-trained V&L model CLIP (Radford et al., 2021) .\n-Inception Score (IS): How easily a model can distinguish the differences between each image and the variety of generated images (Salimans et al., 2016) . It is inferred by the pre-trained image classification model Inception-v3 (Szegedy et al., 2016) .\n-Frechet Inception Distance (FID): How close the generated image is to the reference image, es- \nModel Input ROUGE \u2191 Table-F 1 \u2191 Corpus-F 1 \u2191 1 2 L\n\nDataset Creation\nWe created the Wikipedia Table and Image Generation (WikiTIG) dataset by extracting infoboxes from the HTML dump data of the English Wikipedia 8 . To ensure consistency in the format of infoboxes, we limited the extraction target to those containing a title in the first row and an image in the second row, as shown in Figure 1 .\nIn order to use only entities with sufficient information, we targeted entities for which the table was not empty. In addition, to ensure reliable correspondence, only rows one column wide, which often describe groups, and rows two columns wide, which often consist of a header and its value, were targeted for extraction.\nThe target images are limited to those in jpeg, png, and gif formats. Since some captions do not include a title, we used a hyphen to join the title at the beginning of the caption in such cases.\nTable 2 shows the size of each dataset. The dataset size diverges between two tasks because some infoboxes do not include captions 9 .\n\nTable Generation\nSettings We chose OFA (Wang et al., 2022) , a pre-trained V&L model, and BART (Lewis et al., 2020) , pre-trained only in natural language, as models for comparison. For both models, we used the base settings with the hyperparameters reported in Wang et al. (2022) . We performed the training three times with different seeds and reported their average scores with their standard deviations 10 .\nResults Table 3 shows the results for each setting in the table generation 11 . When only the title is used as input, the result of BART is more accurate than that of OFA, indicating that part of the knowledge acquired from natural language is lost due to additional learning in the V&L model. The use of image information improves Table-F 1 for headers, indicating that images reinforce the knowledge of what kind of features an entity has.\nIn contrast, F 1 for cell values did not improve, indicating that information obtained from images does not complement detailed knowledge, such as the values corresponding to each header obtained from natural language.\nThe results of BART in Corpus-F 1 also suggest that BART contains more diverse knowledge internally than in other settings. This result reinforces that the V&L model forgot part of the knowledge from natural language through additional learning, and images could not fully complement them.\n\nImage Generation\nSettings Similarly to the table generation, we chose OFA for the comparison. We additionally join the reference tables (Gold) and those generated by models in \u00a75.1 (OFA, BART) as the input in order to investigate the impact of the ability to infer table knowledge. We also used the base settings with the hyperparameters reported in Wang et al. (2022) . We also performed the training three times with different seeds and reported their average scores with their standard deviations 12 .\nResults Table 4 shows the results for each setting in the image generation 13 in OFA is close to the result (Wang et al., 2022) in MS COCO (Chen et al., 2015) for image generation, the use of our created dataset is reasonable for training models. In addition, the input of Table (Gold) improves all metrics, indicating that the model produces higher quality images when provided with complementary knowledge about the entities. This result also indicates that OFA does not retain sufficient knowledge of the entities in English Wikipedia.\nIn addition, we did not observe any performance improvement in CLIP and FID when fed with automatically generated tables from BART and OFA. However, tables generated by BART improves IS with the lower performance degradation of FID than that by OFA, indicating that automatically generated tables can improve the diversity of the output images and accurate tables are more important for improving performance in image generation.\n\nRelated Work\nFollowing the advancements in V&L models (Du et al., 2022) , there have been various studies that investigate V&L models. Cao et al. (2020) conducted a comprehensive analysis of V&L models including the difference between model structures. Through their analysis, they revealed the importance of text information in V&L tasks over image information.\nSeveral studies focused on the performance differences between V&L models and text-only models. Yun et al. (2021) investigated the improvement of linguistic representations by pre-training V&L models on PhysicalQA (PIQA) (Bisk et al., 2020) and the probing framework of (Tenney et al., 2019) . They concluded that the benefit of pretrained V&L models for text-only tasks is marginal. Iki and Aizawa (2021) ; Hagstr\u00f6m and Johansson (2022) compared the performance of V&L models and text-only models on the text-only benchmark, GLUE (Wang et al., 2018) and determined that the text-only model achieved higher scores than the V&L models.\nHowever, even though various kinds of V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) inherit language-related knowledge from pre-trained language-only models, how the knowledge is inherited has yet to be investigated. Our work clarifies this by using our created dataset, Wikipedia Table and Image Generation (WikiTIG).\n\nConclusion\nThis paper investigates how knowledge about entities are preserved in a pre-trained V&L model which is originally transferred from a pre-trained natural language model.\nWe analyzed a pre-trained V&L model by creating the Wikipedia Table and Image Generation (WikiTIG) dataset for generating images and tables of the infoboxes in Wikipedia. WikiTIG consists of 200,000 infoboxes and their corresponding images from English Wikipedia.\nExperimental results on a pre-trained V&L model OFA (Wang et al., 2022) showed that the model forgot part of the knowledge about entities during pre-training, and the image information did not fully compensate for the forgotten knowledge.\n"}
{"question": "What is not the strengths of our work compared with Ho\u2019s work?", "evidence": "This work is inspired by the seminal work of Wei et al. (2022) on CoT prompting. They demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021) . Ho et al. (2022) and Li et al. (2022) also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. We also achieve a higher accuracy on common datasets, such as GSM8K, than Ho et al. (2022) .", "options": ["A:This work is inspired by the seminal work of Wei et al. (2022) on CoT prompting. They demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021) . ", "B\\C:Ho et al. (2022) and Li et al. (2022) also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. ", "D:We also achieve a higher accuracy on common datasets, such as GSM8K, than Ho et al. (2022) ."], "answer": "A", "content": "\nIntroduction\nChain of thought (CoT) prompting encourages language models (LMs) to break down a reasoning task into a series of intermediate steps (Wei et al., 2022) . They demonstrate that this prompting significantly increases the task accuracy of large language models (LLMs) across commonsense, symbolic and mathematical reasoning datasets. Here, LLMs are models with at least tens of billions of parameters, such as PaLM 540B (Chowdhery et al., 2022) , GPT-3 175B (Brown et al., 2020) , or UL2 20B (Tay et al., 2022) . However, the reasoning capabilities of smaller LMs do not improve with CoT prompting, mostly producing illogical CoT. Notably, CoT prompting even reduces the accuracy of models with less than 10 billion parameters. Wei et al. (2022) attribute this to abilities, such as semantic understanding and symbolic mapping, only emerging at larger scales. This leads us to our re- * Research conducted during an internship at Google. search question: can the reasoning capabilities of LLMs be transferred to smaller LMs via finetuning?\nThis work explores CoT knowledge distillation (Hinton et al., 2015) from PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) to different sizes of the smaller language model T5 (Raffel et al., 2020) , such as T5 XXL, XL and base, which have 11 billion, 3 billion and 220 million parameters, respectively. As a result of our work, we make two recommendations: (1) perform knowledge distillation by finetuning the student model on the CoT generated by a large teacher model; and (2) generate the CoT from an LLM, as proposed by Wei et al. (2022) , but crucially provide the solution to the task in the few-shot prompt. We demonstrate that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets irrespective of the teacher model used. For example, we show an accuracy increase from 8.11% to 21.99% and 18.42% on the GSM8K (Cobbe et al., 2021) dataset when finetuning T5 XXL on PaLM 540B and GPT-3 175B generated CoT data, respectively.\n\nRelated Work\nThis work is inspired by the seminal work of Wei et al. (2022) on CoT prompting. They demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021) . Wang et al. (2022) show that task accuracy can be further improved by using self-consistency in CoT prompting. Selfconsistency samples CoT reasoning paths from a model's decoder and returns the most consistent path by taking the majority vote. Subsequently, Chung et al. (2022) explore finetuning a FLANbased (Wei et al., 2021) version of PaLM on manually generated CoT data.\nConcurrent to our work, a small number of other works propose methods focused on CoT student-teacher knowledge distillation. Ho et al. (2022) and Li et al. (2022) also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. We also achieve a higher accuracy on common datasets, such as GSM8K, than Ho et al. (2022) . In contrast to our work, Shridhar et al. (2022) focus on training two models, one for problem decomposition and one for solving. Yet differently, the focus of Eisenstein et al. (2022) relies on producing markupand-mask explanations for open-book question answering. Lastly, Huang et al. (2022) present one related experiment, however, we present a more indepth exploration on more datasets. To the best of our knowledge, our work is the first to extensively explore the improvement of the reasoning ability of small LMs via knowledge distillation across multiple model architectures, and observing the effects of student model size and dataset size on accuracy.\n\nMethod\nWe propose a two-step pipeline for CoT knowledge distillation. The first step comprises annotating an existing supervised dataset with CoT reasoning generated by a teacher model. To generate high quality data, we propose using LLMs, such as PaLM 540B or GPT-3 175B, as teachers, based on the finding that CoT reasoning improves with model scale (Wei et al., 2022) . Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by Wei et al. (2022) . We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT. This is based on the observation that providing this guidance allows LLMs to correct small mistakes in the CoT. Lastly, we remove all incorrect CoT based on the target answer to prevent the student to learn from bad examples. The second step comprises finetuning a student model via teacher forcing (Williams and Zipser, 1989) . The student is provided with the question as input, and the CoT and answer as the target. As the model is trained on producing a CoT during finetuning, prompting is not required. \n\nExperimental Setup\nWe follow a similar experimental setup to Wei et al. (2022) , focusing on tasks covering arithmetic, commonsense and symbolic reasoning.\n\nArithmetic Reasoning\nWe benchmark the proposed method on the following math word problem datasets: (1) GSM8K (Cobbe et al., 2021) , ( 2) MAWPS (Koncel-Kedziorski et al., 2016) and (3) ASDiv (Miao et al., 2021) . We use the official training and testing split for GSM8K, taking the last 10% of the training split for validation, and the 5-fold cross validation splits available for MAWPS and ASDiv. We evaluate task accuracy by checking for the target answer as the final answer in the CoT. In addition, we compute the task accuracy given an external calculator, to account for arithmetic mistakes made by the model, despite the CoT being correct. The external calculator moves through the generated output, recalculating the left hand-side of equations. It then replaces the right-hand side with the calculated output, to avoid arithmetic mistakes being carried forward. For example, if a model outputted '5 + 5 = 11. 11 * 2 = 22', then the external calculator would first calculate '5+5' and replace the '11' with a '10'. In the subsequent equation, it would also replace the '11' with a '10' and arrive at the final result of '20'.\n\nCommonsense Reasoning\nWe benchmark the model's ability to perform commonsense reasoning on the StrategyQA dataset (Geva et al., 2021a) . As a testing split is not available, we do not shuffle the dataset to allow reproducing our split of taking the first 80% as training data, the following 10% as validation data, and the final 10% as testing data. We compute task accuracy in the same manner as previously mentioned.\n\nSymbolic Reasoning\nLastly, we benchmark the model on two synthetic tasks for symbolic reasoning: (1) last letter concatenation and (2) coinflip (Wei et al., 2022) . Last letter concatenation prompts the model to concatenate the last letter of each word in a string. Coinflip prompts the model to perform state tracking of the coin being flipped. We evaluate task accuracy in the same manner as before. Due to the rigid structure of the datasets, we focus on evaluating the model's generalizability to out-of-distribution (OOD) examples. We finetune the models on examples of length two and evaluate on sequences of length three and four. We initially infer the CoT using PaLM 540B, however, find that the LLM is able to perfectly replicate the desired CoT bar one example due to the rigidness of the template. We therefore decide to use the template generated CoT in our experiments.\n\nBaselines and setup\nWe select PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) as teacher models. We select PaLM 540B based on the stateof-the-art results on the benchmarking datasets reported by Wei et al. (2022) , and confirm the observed trends with GPT-3 175B. The publicly accessible teacher models are prompted as described in Section 3.\nWe select different sizes of T5 (Raffel et al., 2020) as student models, as T5 is publicly available in many sizes. The student models are trained on the PaLM 540B or GPT-3 175B generated CoT data as described in Section 3. We establish T5 XXL model finetuned on the original target as the baseline. We refrain from shuffling the datasets to allow for reproducibility.For the MAWPS and ASDiv dataset, we perform 5-fold cross validation. For all remaining datasets, we take 10% of the Input: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nOutput:\nRoger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. We refer the reader to Appendix A for an overview of the dataset licenses. We also refer the reader to Appendix B for an overview of the computatinal resources.\n\nArithmetic reasoning\nTable 1 details the task accuracy with and without an external calculator for the arithmetic reasoning benchmarks. Our results show that the proposed method improves task accuracy across all datasets. Most notably, the task accuracy of MAWPS is significantly improved. The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. 1 : Task accuracy across arithmetic reasoning datasets for T5 XXL without finetuning (baseline) and finetuned on PaLM 540B generated chain-of-thought (CoT). We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on generating chain-of-thought data\nWe perform an ablation study to confirm that providing a LLM with the target during CoT generation is beneficial. We found that for the GSM8K dataset, PaLM 540B only achieves a 59.98% accuracy if prompted without the target. In comparison, when including the target in the prompt the accuracy is 79.37%. A superficial explanation would be that when the model is conditioned on the expected answer, it produces the same CoT but copies the answer. However, an analysis of a subset of the differences between CoT produced with and without this conditioning shows that most of the benefits actually come from the model correcting CoT that had a single step missing or was wrong.\n\nCommonsense reasoning\nFor the StrategyQA dataset (Table 3 ), we found that using CoT finetuning improves accuracy from 68.12% to 71.98%, using only 1319 of the original 1648 examples. Compared to the arithmetic reasoning datasets, the improvement is not as significant. This can be explained by the model lacking factual knowledge that the dataset requires. The task is heavily focused on the model reasoning on such knowledge, however, a smaller LM is most likely not in possession of this knowledge compared to a larger model with higher memorisation capacity.\n\nSymbolic reasoning\nTable 2 shows the results obtained for the synthetic symbolic reasoning datasets, focusing on OOD generalization. Focusing on Last Letter Concatenation, it can be stated that both traditional finetuning and the suggested method fail at generalizing to a longer sequence length. In comparison, the proposed method significantly increases accuracy for the Coinflip dataset with regard to generalizing to three coinflips. In contrast, generalisation to four coinflips is slightly weaker than the baseline, which performs very strongly. This may be related to the task length being twice that of the training task.\n\nReplicating Results using different Teacher Models\nWe demonstrate the robustness of our method using a different teacher model, namely GPT-3 175B. Table 3 shows the results for GSM8K and Strat-egyQA when T5 XXL is finetuned on CoT data generated by GPT-3. The results show that the proposed method elicits improvements also with other Table 3 : Task accuracy for T5 XXL finetuned on chainof-thought (CoT) data generated by PaLM 540B and GPT-3 175B. We also finetune on the reasoning steps provided by the datasets. We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on model size\nWe investigate the performance gain achieved via finetuning student models of different sizes. Figure 3 shows the performance gain achieved when finetuning T5 of different sizes on the GSM8K dataset.\nOur results show that T5 base, with 44 times fewer parameters than T5 XXL, matches the performance of the baseline T5 XXL when trained on CoT data. Moreover, given an external calculator, even T5 small outperforms the baseline T5 XXL.\n\nAblation study on dataset size\nWe also investigate the trade-off between the performance gain from CoT finetuning and dataset size. \n\nDiscussion\nWe demonstrate that finetuning larger LMs on the CoT data generated by LLMs of over 100 billion parameters can significantly improve task accuracy. Even a small number of CoT examples appear to suffice for this. However, such improvements appear to be task dependent. For example, the effects are limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller LMs may not have memorised due to their limited capacity. Nevertheless, there is some performance improvement, which may be attributed to the model learning how to approach such tasks. Moreover, the CoT knowledge distillation pipeline presented allows to trade-off model and dataset size with accuracy. Future work could explore improving the reasoning of small models in multi-task settings, as well as the generation of new training data using LLMs, rather than annotating existing datasets.\n\nConclusion\nThis work explores CoT knowledge distillation from LLMs of over 100 billion parameters to smaller LMs. We propose a knowledge distillation pipeline consisting of two keys steps: (1) generate CoT for existing datasets using LLMs and\n(2) finetune smaller LMs on the CoT. Our results demonstrate that finetuning on CoT improves task accuracy across a range of benchmarking datasets.\n"}
{"question": "What is the primary purpose of introducing the ranking loss in the BalSum model?", "evidence": "  We introduce a ranking loss to f (\u2022) that identifies the correct summary from the outputs of the generation model g. our loss directly reflects the quality measure during training ", "options": ["A. To maximize ROUGE scores", "B. To minimize the number of candidate summaries", "C. To reflect the quality measure during training", "D. To increase the instance weighting threshold "], "answer": "C", "content": "\nIntroduction\nThe performance of sequence-to-sequence (Seq2Seq) neural models for abstractive summarization (Lewis et al., 2020; Nallapati et al., 2016; See et al., 2017; Zhang et al., 2020) has improved significantly. The dominant training paradigm of Seq2Seq models is that of Maximum Likelihood Estimation (MLE), maximizing the likelihood of each output given the gold history of target sequences during training. However, since the models generate the sequence in an auto-regressive manner at inference, the errors made in the previous steps accumulate in the next step thereby affecting the entire sequence. This phenomenon is known as exposure bias (Bengio et al., 2015; Ranzato et al., 2016) . To mitigate this * Corresponding author problem, re-ranking systems (Liu et al., 2021; Liu and Liu, 2021; Liu et al., 2022; Ravaut et al., 2022) have recently been introduced to generate a more appropriate summary.\nThere are two training objectives for applying reranking to abstractive summarization: contrastive learning and multi-task learning. The contrastive learning-based approaches deploy margin-based losses. SimCLS (Liu and Liu, 2021) and BRIO-Ctr (Liu et al., 2022) train a large pre-trained model, such as RoBERTa (Liu et al., 2019) and BART (Lewis et al., 2020) , to align the candidate summaries according to the quality. The authors use the ROUGE (Lin, 2004) score as a quality measurement. The multi-task learning-based approaches combine at least two losses that perform different roles. SummaReranker (Ravaut et al., 2022) minimizes the average over the binary cross-entropy losses optimized for each evaluation metric. In addition, BRIO-Mul (Liu et al., 2022) demonstrates that the combination of the contrastive and cross-entropy loss works complementarily and has better performance.\nIn this paper, we analyze the three main drawbacks of existing re-ranking approaches. First, we argue that current methods focus excessively on ranking summaries in terms of lexical overlap. Inspired by Zhong et al. (2020) , we conduct a preliminary study, by sorting candidate summaries in descending order based on the ROUGE score and then defining z as the rank index of the highest BERTScore summary. As demonstrated in Fig. 1 , we can observe that there is a large gap between lexical overlap and semantic similarity. In a majority (52%) of cases z > 1. Second, despite more than half of the candidates with the same ROUGE score, previous studies do not accurately reflect quality measurements as they are trained with different ranks even if they have equal scores (Appendix F). Lastly, for the first time, we find summaries with high lexical overlap but low semantic similarity as false positives (Appendix G). They can be noises during training phrase, which are not considered substantially in the prior works.\nTo address these issues, we propose a novel training method in which a re-ranker balances lexical and semantic quality. Based on a two-stage framework, our model, named BalSum, is trained on multi-task learning. We directly reflect the ROUGE score difference on a ranking loss to preserve the lexical quality as much as possible. Then, we use a contrastive loss with instance weighting to identify summaries whose meanings are close to the document. Specifically, we define novel false positives (semantic mistakes) and present a strategy to reduce their influence in ranking. Experiments on CNN/DM and XSum datasets demonstrate the effectiveness of our method. Notably, BalSum achieves an 89.67 BERTScore on CNN/DM, reaching a new state-of-the-art performance.\n\nMethod\nOur method follows the two-stage framework. Given a source document D, a function g is to generate a pool of candidate summaries C = {C 1 , C 2 , ..., C m } at the first stage:\nEQUATION\nThen, a function f is to assign scores to each candidate and select the best summary C * with the highest score at the second stage: Our goal is to train the ranking model f that identifies the correct summary from the outputs of the generation model g.\nC * = argmax C i \u2208C {f (C i , D)} (2)\n\nModel Architecture\nWe start with a bi-encoder using RoBERTa-base (Liu et al., 2019) as a back-bone neural network.\nInspired by Khattab and Zaharia (2020) , we aim to capture rich semantic units at the sentence level.\nAs shown in Fig. 2 , we insert the [CLS] tokens in front of K sentences in the document D to let them encode into multi-vector representations. Then, we compute the individual score Score k which is modeled as an inner-product:\nEQUATION\nwhere E 1 (C i ) and E k (D)(k = 1, 2, ..., K) mean the representations of [CLS] tokens for candidate summary C i and document D, respectively. We calculate the similarity score f (C i , D):\nEQUATION\nIn Appendix E, we show that our model can capture more information from documents at the sentence level.\n\nTraining objective\nRanking Loss The core idea is that the higher the quality of the candidate summary, the closer to the document. We introduce a ranking loss to f (\u2022):\nL rank = i j>i max(0, f (Cj, D) \u2212 f (Ci, D) +(\u2212cost(Ci, S) + cost(Cj, S)) * \u03bb) (5)\nwhere S is the reference summary and \u03bb is the hyper-parameter. 1 Here, cost(C i , S) = 1 \u2212 M (C i , S) is the margin, and M is the automatic evaluation metric. We define it as ROUGE. We use the same metric in previous work (Liu and Liu, 2021; Liu et al., 2022) , but the difference is that our loss directly reflects the quality measure during training. In other words, the quality was not properly reflected before because different margin ((j \u2212 i) * \u03bb) was assigned even if the candidate summaries had the same ROUGE score.\nContrastive Loss with Instance Weighting The construction of positive and negative pairs is the critical point in constrative learning. Therefore, we consider generated summaries from the same document as positive samples and irrelevant summaries from other documents as negative samples. Thus, we design a set of candidate summaries C in Eq. 1 as positive and a set of randomly sampled summaries N as negative. 2 To identify summaries whose meanings are close to the document, we introduce a contrastive learning objective with instance weighting: D) (6) We newly define summaries that have a high lexical matching but a low semantic similarity as false positives. Inspired by Zhou et al. (2022) , we design an instance weighting method to reduce the influence of false positives. We produce the weights for positives using the SimCSE (Gao et al., 2021) which is the state-of-the-art model for the sentence representation task:\nL ctr = 1 |C| C i \u2208C \u2212log \u03b1 C i \u00d7 e f (C i ,D) e f (C i ,D) + s i \u2208N e f (s i ,\n\u03b1 C i = 0, sim(C i , S) < \u03d5 1, sim(C i , S) \u2265 \u03d5 (7)\nwhere \u03d5 is a hyper-parameter of the instance weighting threshold, and sim(\u2022) is the cosine similarity score evaluated by the SimCSE model. Finally, as shown in Fig. 3 , we combine the ranking (Eq. 5) and contrastive (Eq. 6) losses:\nEQUATION\n)\nwhere \u03b3 is the scale factor of each loss and we find the optimal values (\u03b3 1 = 10, \u03b3 2 = 0.1) in Appendix H.\n3 Experiments\n\nDatasets\nWe experiment on two datasets, whose statistics are shown in Appendix C. CNN/DailyMail (Hermann et al., 2015) is the most commonly used summarization dataset which contains articles from the CNN and DailyMail newspapers.\nXSum (Narayan et al., 2018 ) is a one-sentence summary dataset from the British Broadcasting Corporation (BBC) for the years 2010 -2017.\n\nTraining Details\nWe use diverse beam search (Vijayakumar et al., 2016) to generate 16 candidate summaries. We start from pre-trained checkpoints of RoBERTabase (Liu et al., 2019) . We train BalSum for five epochs. It takes 33 hours on CNN/DM and 22 hours on XSum on a single RTX 3090 GPU. More details are described in Appendix D.\n\nMain Results\nIn terms of the two-stage framework, we compare our results with SimCLS (Liu and Liu, 2021) , Sum-maReranker (Ravaut et al., 2022) , and BRIO (Liu et al., 2022) . We apply BalSum on top of each base model which is BART or PEGASUS.\nThe results on CNN/DM are described in Table 1. BalSum outperforms a base BART model, according to gains of 2.54/1.27/2.63 R-1/2/L. Notably, while it has comparable performances on ROUGE to previous models, it achieves an 89.67 BERTScore, reaching a new state-of-the-art performance. When ranking the candidate summaries, our model can estimate the meaning of summaries without seriously degrading the lexical aspect. We argue that this is because BalSum decreases more false positives than other ranking models. We provide fine-grained analyses for this result and present a case study in Sec.3.4.\nIn addition, we apply our method on XSum, as shown in Table 2 . Though we use a different strategy to generate the validation and test data 3 , our method improves a base PEGASUS with a small margin. We believe the one of reasons is that XSum is restricted to capturing diverse semantic units because it consists of much shorter summaries (onesentence) than CNN/DM. Model BS@1 BS@3 BS@5 R@1 R@3 R@5\nOracle \n\nAnalysis\nWeighting Threshold \u03d5 Intuitively, the larger the weighting threshold, the lower false positives. We train our model with different instance weighting thresholds from 0.7 to 0.9. In Table 3 , the highest threshold (\u03d5 = 0.9) shows the best performance and it rises largely to 0.3 BERTScore compared to when not applied. We also find that increasing the threshold leads to performance improvement. Therefore, we demonstrate that false positives can be considered noise in training.\nRanking Evaluation Regardless of the number of candidates, an ideal ranking model should yield oracle results considering diverse aspects of summarization. We conduct an experiment to measure the qualities by selecting the top-k summaries after aligning the candidates through different models. As shown in Table 4 , we can see that our model shows consistent performance in both evaluation metrics depending on the k (about \u00b10.06 BERTScore, \u00b10.34 ROUGE average score). Compared to SimCLS and BRIO-Ctr, the second block in Table 4 demonstrates that BalSum captures semantic similarity best while maintaining the intermediate level from the perspective of lexical overlap quality. Moreover, we find that BalSum has the lowest drop ratio of BERTScore (\u22121.52%) from the perfect ranking \"oracle\" scores. We also investigate whether all ranked summaries by models satisfy both lexical and semantic quality. We evaluate models using F 1 which measures the cases where the higher-ranked summary \n\nConclusion\nIn this work, we propose BalSum which aims to evaluate summaries by considering the balance between lexical and semantic quality. To achieve this, we perform a multi-task learning, which aligns summaries according to their lexical overlap qualities and identifies whether they are similar to the document. In addition, to our best knowledge, our method is the first attempt to present a new perspective of false positives (semantic mistakes) in ranking and creating the model to reduce their in-fluence. Our experimental results and fine-grained analyses validate that our model achieves consistent improvements over competitive baselines.\n"}
{"question": "What is the primary focus of the error detection problem discussed in the paper?", "evidence": "  In the error detection problem, we have to detect data points with wrong labels. Given a (potentially noisy) dataset Z, we have to rank data points in Z by how likely they are erroneous.  ", "options": ["A. Identifying outliers in data.", "B. Detecting adversarial attacks on neural networks.", "C. Detecting data points with incorrect labels.", "D. Finding the optimal parameters for deep networks. "], "answer": "C", "content": "\nIntroduction\nDeep learning models are data hungry. Large models such as transformers (Vaswani et al., 2017) , BERT (Devlin et al., 2019) , and GPT-3 (Brown et al., 2020) require millions to billions of training data points. However, data labeling is an expensive, time consuming, and error prone process. Popular datasets such as the ImageNet (Deng et al., 2009) contain a significant amount of errors -data points with incorrect or ambiguous labels (Beyer et al., 2020) . The need for automatic error detection tools is increasing as the sizes of modern datasets grow.\nInfluence function (IF) (Koh and Liang, 2017) and its variants (Charpiat et al., 2019; Khanna et al., 2019; Barshan et al., 2020; Pruthi et al., 2020) are a powerful tool for estimating the influence of a data point on another data point. Researchers leveraged this capability of IFs to design or detect adversarial (Cohen et al., 2020) , poisonous (Koh et al., 2022; Koh and Liang, 2017) , and erroneous (Dau et al., 2022) examples in large scale datasets. The intuition is that these harmful data points usually have a negative influence on other data points and this influence can be estimated with IFs. Basu et al. (2021) empirically observed that IFs are unstable when they are applied to deep neu- * Joint first authors ral networks (DNNs). The quality of influence estimation deteriorates as networks become more complex. In this paper, we provide empirical and theoretical explanations for the instability of IFs. We show that IFs scores are very noisy when the two data points belong to two different classes but IFs scores are much more stable when the two data points are in the same class (Sec. 3). Based on that finding, we propose IFs-class, variants of IFs that use class information to improve the stability while introducing no additional computational cost. IFs-class can replace IFs in anomalous data detection algorithms. In Sec. 4, we compare IFs-class and IFs on the error detection problem. Experiments on various NLP tasks and datasets confirm the advantages of IFs-class over IFs.\n\nBackground and Related work\nWe define the notations used in this paper. Let z = (x, y) be a data point, where x \u2208 X is the input, y \u2208 Y is the target output; Z = z (i) n i=1 be a dataset of n data points; Z \u2212i = Z\\z (i) be the dataset Z with z (i) removed; f \u03b8 : X \u2192 Y be a model with parameter \u03b8; L Z,\u03b8 = 1 n n i=1 \u2113(f \u03b8 (x (i) ), y (i) ) = 1 n n i=1 \u2113(z (i) ; \u03b8) be the empirical risk of f \u03b8 measured on Z, where \u2113 : Y \u00d7 Y \u2192 R + is the loss function; \u03b8 = arg min \u03b8 L Z,\u03b8 and \u03b8\u2212i = arg min \u03b8 L Z \u2212i ,\u03b8 be the optimal parameters of the model f \u03b8 trained on Z and Z \u2212i . In this paper, f \u03b8 is a deep network and \u03b8 is found by training f \u03b8 with gradient descent on the training set Z.\n\nInfluence function and variants\nThe influence of a data point z (i) on another data point z (j) is defined as the change in loss at z (j) when z (i) is removed from the training set\nEQUATION\nThe absolute value of s (ij) measures the strength of the influence of z (i) on z (j) . The sign of s (ij) show the direction of influence. A negative s (ij) means that removing z (i) decreases the loss at z (j) , i.e. z (i) is harmful to z (j) . s (ij) has high variance because it depends on a single (arbitrary) data point z (j) .\nTo better estimate the influence of z (i) on the entire data distribution, researchers average the influence scores of z (i) over a reference set Z \u2032\ns (i) = 1 |Z \u2032 | z (j) \u2208Z \u2032 s (ij) = L Z \u2032 , \u03b8\u2212i \u2212 L Z \u2032 , \u03b8 (2)\ns (i) is the influence of z (i) on the reference set Z \u2032 . Z \u2032 can be a random subset of the training set or a held-out dataset. Naive computation of s (ij) requires retraining f \u03b8 on Z \u2212i . Koh and Liang (2017) proposed the influence function (IF) to quickly estimate s (ij) without retraining\ns (ij) \u2248 IF (z (i) , z (j) ) \u2248 1 n \u2207 \u03b8\u2113(z (i) ; \u03b8) \u22a4 H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) (3)\nwhere\nH \u03b8 = \u2202 2 L Z, \u03b8/\u2202\u03b8 2\nis the Hessian at \u03b8. Exact computation of H \u22121 \u03b8 is intractable for modern networks. Koh and Liang (2017) developed a fast algorithm for estimating H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) and used only the derivatives w.r.t. the last layer's parameters to improve the algorithm's speed. Charpiat et al. (2019) proposed gradient dot product (GD) and gradient cosine similarity (GC) as faster alternatives to IF. Pruthi et al. (2020) argued that the influence can be better approximated by accumulating it through out the training process (TracIn). The formula for IFs are summarized in Tab. 3 in Appx. A.\nIFs can be viewed as measures of the similarity between the gradients of two data points. Intuitively, gradients of harmful examples are dissimilar from that of normal examples (Fig. 1 ).\n\nInfluence functions for error detection\nIn the error detection problem, we have to detect data points with wrong labels. Given a (potentially noisy) dataset Z, we have to rank data points in Z by how likely they are erroneous. Removing or correcting errors improves the performance and robustness of models trained on that dataset.\nTraditional error detection algorithms that use hand designed rules (Chu et al., 2013) or simple statistics (Huang and He, 2018) , do not scale well to deep learning datasets. Cohen et al. (2020) 2021) empirically showed that IFs with last layer gradient perform as well as or better than IFs with all layers' gradient and variants of IF behave similarly. Therefore, we analyze the behavior of GD with last layer's gradient and generalize our results to other IFs. Fig. 1 shows the last layer's gradient of an MLP on a 3-class classification problem. In the figure, gradients of mislabeled data points have large magnitudes and are opposite to gradients of correct data points in the true class. However, gradients of mislabeled data points are not necessarily opposite to that of correct data points from other classes. Furthermore, gradients of two data points from two different classes are almost perpendicular. We make the following observation. A mislabeled/correct data point often has a very negative/positive influence on data points of the same (true) class, but its influence on other classes is noisy and small.\nWe verify the observation on real-world datasets. (Fig. 2 ). We compute GD scores of pairs of clean data points from 2 different classes and plot the score's distribution. We repeat the procedure for pairs of data points from each class. In the 2-class case, GD scores are almost normally distributed with a very sharp peak at 0. That means, in many cases, a clean data point from one class has no significant influence on data points from the other class. And when it has a significant effect, the effect could be positive or negative with equal probability. In contrast, GD scores of pairs of data points from the same class are almost always positive. A clean data point almost certainly has a positive influence on clean data points of the same class.\nOur theoretical analysis shows that when the two data points have different labels, then the sign of GD depends on two random variables, the sign of inner product of the features and the sign of inner product of gradients of the losses w.r.t. the logits. And as the model becomes more confident about the labels of the two data points, the magnitude of GD becomes smaller very quickly. Small perturbations to the logits or the features can flip the sign of GD. In contrast, if the two data points have the same label, then the sign of GD depends on only one random variable, the sign of the inner product of the feature, and the GD's magnitude remains large when the model becomes more confident. Mathematical details are deferred to Appx. D.\n\nClass based IFs for error detection\nOur class based IFs for error detection is shown in Alg. 1. In Sec. 3.1, we see that an error has a very Algorithm 1 Class based influence function for error detection.\n\nRequire:\n1: Z = z (i) n i=1 : a big noisy dataset 2: C: number of classes 3: for k = 1, ..., C do 9:\nZ \u2032 k = z \u2032(j k ) m k j k =1 : clean data from class k 4: Z \u2032 = C k=1 Z \u2032 k : a clean\ns (i) k = m k j=1 sim(\u2207 \u03b8 \u2113(z (i) ),\u2207 \u03b8 \u2113(z \u2032(j k ) )) m k 10:\nend for 11:\ns (i) = min k (s (i)\nk ) 12: end for 13: \u1e90 = sort(Z, key = s, ascending = True) 14: return \u1e90 strong negative influence on correct data points in the true class, and a correct data point has a positive influence on correct data points in the true class. Influence score on the true class is a stronger indicator of the harmfulness of a data point and is better at differentiating erroneous and correct data points. Because we do not know the true class of z (i) in advance, we compute its influence score on each class in the reference set Z \u2032 and take the minimum of these influence scores as the indicator of the harmfulness of z (i) (line 8-11 create benchmark datasets Z's, we inject random noise into the above datasets. For text classification datasets, we randomly select p% of the data points and randomly change their labels to other classes.\nFor the CoNLL-NER dataset, we randomly select p% of the sentences and change the labels of r% of the phrases in the selected sentences. All tokens in a selected phrase are changed to the same class. The reference set Z \u2032 is created by randomly selecting m k clean data points from each class in Z. To ensure a fair comparison, we use the same reference set Z \u2032 for both IFs and IFs-class algorithms. Models are trained on the noisy dataset Z. To evaluate an error detection algorithm, we select top q% most harmful data points from the sorted dataset \u1e90 and check how many percent of the selected data points are really erroneous. Intuitively, increasing q allows the algorithm to find more errors (increase recall) but may decrease the detection accuracy (decrease precision). Our code is available at https://github.com/Fsoft-AIC/ Class-Based-Influence-Functions.\nResult and Analysis Because results on all datasets share the same patterns, we report representative results here and defer the full results to Appx. C.\nFig. 3(a) shows the error detection accuracy on the SNLI dataset and how the accuracy changes with q. Except for the GC algorithm, our classbased algorithms have higher accuracy and lower variance than the non-class-based versions. When q increases, the performance of IFs-class does not decrease as much as that of IFs. This confirms that IFs-class are less noisy than IFs. Class information fails to improve the performance of GC. To understand this, let's reconsider the similarity measure sim(\u2022, \u2022). Let's assume that there exist some clean data points z \u2032(j) \u2208 Z \u2032 with a very large gradient \u2207 \u03b8\u2113(z \u2032(j) ). If the similarity measure does not normalize the norm of \u2207 \u03b8\u2113(z \u2032(j) ), then z \u2032(j) will have the dominant effect on the influence score. The noise in the influence score is mostly caused by these data points. GC normalizes both gradients, \u2207 \u03b8\u2113(z (i) ) and \u2207 \u03b8\u2113(z \u2032(j) ), and effectively removes such noise. However, gradients of errors tend to be larger than that of normal data points (Fig. 1 ). By normalizing both gradients, GC removes the valuable information about magnitudes of gradients of errors \u2207 \u03b8\u2113(z (i) ). That lowers the detection performance. In Fig. 3 (a), we see that the performance of GC when q \u2265 15% is lower than that of other classbased algorithms. Similar trends are observed on other datasets (Fig. 6 , 7, 8 in Appx. C). Fig. 3(b) shows the change in detection accuracy as the level of noise p goes from 5% to 20%. For each value of p, we set q to be equal to p. Our class-based influence score significantly improves the performance and reduces the variance. We note that when p increases, the error detection problem becomes easier as there are more errors. The detection accuracy, therefore, tends to increase with p as shown in Fig. 3 (b), 9, 10. Fig. 3(c ) shows that GD-class outperforms GD on all entity types in CoNLL2003-NER. The performance difference between GD-class and GD is greater on the MISC and ORG categories. Intuitively, a person's name can likely be an organization's name but the reverse is less likely. Therefore, it is harder to detect that a PER or LOC tag has been changed to ORG or MISC tag than the reverse. The result shows that IFs-class is more effective than IFs in detecting hard erroneous examples.\n\nThe effect of data on error detection algorithms\nWe study the effect of the size and the cleanliness of the reference set on the performance of error detection algorithms.\nThe size of the reference set. We changed the size of classes in the reference set from 10 to 1000 to study the effect of the reference set's size on the detection performance. We report the mean performance of GD and GC algorithms in Tab. 1. We observe no clear trend in the performance as the size of the reference set increases. Our conjecture is that gradients of clean data points from the same class have almost the same direction. Averaging the gradient direction over a small set of data points already gives a very stable gradient direction. Therefore, increasing the size of the reference set does not have much impact on detection performance. \n\nConclusion\nIn this paper, we study influence functions and identify the source of their instability. We give a theoretical explanation for our observations. We introduce a stable variant of IFs and use that to develop a high performance error detection algorithm. Our findings shed light of the development of new influence estimators and on the application of IFs in downstream tasks.\n"}
{"question": "Regarding the description of the statistical indicators of the STT4SG-350 dataset, which of the following is accurate\uff1f", "evidence": "  The provided by 316 different speakers, of which 51% identified as female and 49% as male.  The age groups from the thirties to the sixties are well represented, while the twenties are overrepresented and the teens as well as seventies are underrepresented. The age groups eighties and above are not represented at all.   ", "options": ["A. The age groups eighties and above are represented well.", "B. 51% identified as female and 49% as male in 316 different speakers.", "C. The age groups of twenties are well represented.", "D. The groups from the thirties to the sixties are overrepresented and the teens as well as seventies are underrepresented."], "answer": "B", "content": "\nIntroduction\nWe present STT4SG-350, a corpus of Swiss German speech, annotated with Standard German text at the sentence level. The corpus represents all Swiss German dialect regions and contains 343 hours of speech.\nSwiss German is a family of German dialects spoken by around 5 million people in Switzerland. It differs from Standard German regarding phonology, vocabulary, morphology, and syntax. There are significant differences among the Swiss German dialects as well, particularly regarding phonology and vocabulary. Swiss German is primarily a spoken language. It is also used in writing, but mainly in informal text messages. In most other contexts, including formal letters, laws, and newspapers, Standard German is used instead. One important reason for this is Swiss German's lack of a standardized orthography.\nThe diversity among dialects, exacerbated by the lack of a standardized orthography, leads to a large number of written variants for each word. This, together with the small amount of text resources compared to Standard German, makes automated processing of Swiss German text challenging.\nSTT4SG-350 is, to the best of our knowledge, the largest public speech corpus for Swiss German. While the primary use case is automatic speech recognition (ASR), it is also a useful resource for text-to-speech (TTS), dialect identification, and speaker recognition. By providing roughly the same amount of data per dialect region, irrespective of its population size, the corpus contributes to improving speech technology for underrepresented dialects. In addition, the test set, which contains the same spoken sentences in each dialect, allows a fair evaluation of the quality of speech technologies in different dialects. Furthermore, it contributes to more inclusive speech technology by keeping a balanced gender ratio and featuring speakers of all ages.\n\nRelated Work\nThe SDS-200 corpus (Pl\u00fcss et al., 2022) contains 200 hours of speech by around 4,000 speakers with Standard German transcripts. The recordings cover a large part of the Swiss German dialect landscape. The number of recordings per speaker follows a long-tail distribution. For example, the top 3 speak-ers account for 23% of recordings. The Swiss Parliaments Corpus or SPC (Pl\u00fcss et al., 2021a) contains 299 hours of speech in the Bernese dialect. The text is Standard German, taken from parliament minutes, and is not a fully accurate transcription. Text and audio are automatically aligned. The SwissDial corpus (Dogan-Sch\u00f6nberger et al., 2021) contains 26 hours of studio-quality recordings by 8 speakers, each speaking a different dialect, with both Standard German and Swiss German transcripts. The Radio Rottu Oberwallis corpus (Garner et al., 2014) contains 8 hours of speech transcribed in Swiss German, of which 2 are also transcribed in Standard German. The ArchiMob corpus (Samard\u017ei\u0107 et al., 2016) contains 69 hours of speech with Swiss German transcripts.\nFor Swiss German ASR, the desired output text language is Standard German for the vast majority of use cases. Tackling speech-to-text translation with an end-to-end approach is feasible as shown by Weiss et al. (2017) . Applying a similar approach to Swiss German ASR and therefore avoiding Swiss German text and its challenges altogether lead to promising results in recent years, see (Pl\u00fcss et al., 2023; Khosravani et al., 2021; Pl\u00fcss et al., 2022 Pl\u00fcss et al., , 2021a)) . Dogan-Sch\u00f6nberger et al. (2021) experiment with TTS for Swiss German. Their models achieve a 5-scale mean opinion score of 2.9 to 4.1. Importantly, their approach requires Swiss German input text.\n\nData Collection\nData for STT4SG-350 was collected in two phases: 1) the test set with 76 participants from December 2021 until March 2022, and 2) the train and validation sets with 240 participants from May until November 2022.\n\nRecording\nSpeech was recorded using a web app based on the code 1 by Pl\u00fcss et al. (2022) . Recordings are made sentence by sentence. The app displays a Standard German sentence, which the participant is asked to translate to Swiss German and speak aloud. A screenshot of the recording functionality can be found in Appendix A. The goal of the translation step is to get a correct, natural-sounding Swiss German sentence in the participant's dialect. We display a popup with examples before the first 1 MPL-2.0 license recording to explain this to participants. We also display a short explanation below the sentence to be recorded. We manually validated the correctness of at least 10 randomly sampled recordings per participant at collection time. In contrast to Pl\u00fcss et al. (2022) , for phase 2, we recorded 44.1 kHz lossless FLAC audio rather than 32 kHz lossy MP3 audio. The recording quality depends on the microphones used by participants, which range from studio microphones to headsets and laptop microphones. Depending on the microphone, mouse clicks can be audible in recordings.\n\nDialect Regions\nFor this work, we divided the Swiss German dialect continuum into 7 dialect regions, listed in Table 1 , based on the clustering method by Scherrer and Stoeckle (2016) 2 . The cluster analysis was carried out on 350 phonological, lexical, morphological, and syntactic phenomena. We slightly adjusted the resulting clusters to match the dialect regions commonly used in public discourse more closely. The goal of these adjustments was to make it more intuitive for participants to choose their dialect region. The borders are intentionally fuzzy to give participants the freedom to choose the region that fits their dialect best.\n\nSentence Selection\nSentences were randomly selected from Swiss newspapers and from parliament minutes of 2 Swiss parliaments. Sentence filtering for newspapers follows Pl\u00fcss et al. (2022) . The goal of the filtering is to limit sentence complexity to reduce errors in the translation task. For example, only sentences of 5 to 12 words are kept. The newspaper sentences cover a broad range of topics, including culture, finance, science, sports, and technology. They also cover content and named entities particularly relevant for Switzerland. Parliament sentences are not filtered. They bring additional diversity to the corpus with longer sentences on average and a distinct vocabulary. For the test set, 3,515 sentences were selected (67% newspapers, and 33% parliaments). To allow a fair comparison among the dialects, each sentence was recorded in each of the 7 dialects. For the training and validation data, 94% news and 6% parliament sentences were selected, and we dropped the requirement to record each sentence in all dialect regions to in-crease vocabulary and phrase diversity.\n\nMetadata\nParticipants self-reported the following metadata:\n\u2022 The dialect region that best fits the participant's dialect. \u2022 The zip code of the place where the participant grew up or went to school. \u2022 Age group (< 19, 19-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89 , > 89) \u2022 Gender (female, male, non-binary) We manually checked the correspondence of reported metadata and recordings for each participant. Collecting the dialect provenance as a zip code allows us to investigate dialects and the performance of speech technologies for them at different granularity levels. Collecting age group and gender helps to make sure that speech technology is inclusive and works across different demographic groups.\n\nRecruitment\nFor the test set, all participants were recruited via the crowdsourcing platform TestingTime 3 . For the train set, half the participants were recruited via TestingTime, whereas the other half were recruited via universities, high schools, newspaper ads, personal contacts, and the crowdsourcing platform seniors@work 4 (for details refer to Appendix F and 6). Only native Swiss German speakers able to correctly translate Standard German to Swiss German were recruited. The goal was to collect the same amount of recordings in each dialect region and we recruited accordingly. The number of recordings per participant was limited to 368 for the test set 5 and 1,112 for the train data. Recruiting the 316 participants required a considerable effort, especially in the low-population regions GR and VS.\n\nCorpus\nThe corpus is publicly available 6 \n\nData Cleaning\nFiltering. Recordings with a duration of less than 2 seconds were removed. Silent recordings were also removed. For the test set, we applied heuristics to flag incomplete sentences, which were removed after double-checking them. We only kept sentences with a recording in all dialect regions in the test set. In total, we filtered out 1.5% of recordings.\nValidation. We validated each speaker manually.\nFor this, we randomly sampled 10 recordings from each speaker, and checked whether the dialect is correct, the recording is in Swiss German, the translation is correct, and whether the sound quality is high enough. All of the participants passed the manual check.\n\nStatistics\nThe provided by 316 different speakers, of which 51% identified as female and 49% as male. No speaker identified as non-binary. Figure 1 shows the distribution of the recordings over the age groups, as well as the gender distributions per age group. The age groups from the thirties to the sixties are well represented, while the twenties are overrepresented and the teens as well as seventies are underrepresented. The age groups eighties and above are not represented at all. Table 1 shows the corpus statistics per dialect region. While the German-speaking population differs by a factor of up to 16 between regions, the number of recordings per region is a lot more balanced, differing by a factor of not more than 1.2.\n\nSplits\nTable 2 shows the different corpus splits. We provide training, validation, and test splits. There is no speaker overlap between training, validation, and test. There are no common sentences between test and either training or validation. There is, however, an intersection of 835 sentences between training and validation. There are 2 different training splits. train_all contains all training data, 276 hours of speech. train_balanced is a subset of train_all with 239 hours of speech that is balanced in the number of recordings per dialect region. For GR, the region with the fewest recordings, the recordings of all speakers are included in train_balanced. For the other regions, we randomly chose speakers and added their recordings until the number of GR recordings was reached. train_balanced includes 33-35 hours of speech, 24,088-25,183 recordings, and 25-32 speakers per region.\nLike train_balanced, the validation split, with 34 hours of speech, is balanced in the number of recordings per dialect region. We randomly chose 3 speakers per region with at least 1,000 recordings. The test set comprises 34 hours of speech. Importantly, the same 3,515 sentences were recorded in all 7 dialect regions to allow a fair comparison between different dialects. equate speaker diversity in each region. For this reason, the mean number of recordings per speaker is markedly lower than in the other splits.\n\nAutomatic Speech Recognition Baseline\nWe train a baseline model to demonstrate the use of the STT4SG-350 corpus for Swiss German ASR. We fine-tune XLS-R (1B) 8 (Babu et al., 2021) on the train_balanced split. XLS-R is a model based on wav2vec 2.0 (Baevski et al., 2020) with 965 million parameters pretrained on 436K hours of unlabeled speech data covering more than 128 languages. Swiss German was not part of the training data. We provide the fine-tuning details and experimental setup in appendix C. We report the results of our fine-tuned model on three publicly available Swiss German datasets and the STT4SG-350 validation and test sets in Table 3 . The model achieves state-of-the-art results on the All Swiss German Dialects Test Set (ASGDTS) (Pl\u00fcss et al., 2021b) and SDS-200 (Pl\u00fcss et al., 2022) , and improves the best reported BLEU scores on the test sets by 43% and 9%, respectively. Our model is 6% behind the best reported BLEU score on the SPC test set (Pl\u00fcss et al., 2021a) . These results highlight the benefit of the STT4SG-350 dataset on test data from different domains.\n\nConclusion\nWe have described STT4SG-350, which is, to the best of our knowledge, the largest public speech corpus for Swiss German with 343 hours of speech. Our ASR baseline model trained on the corpus achieves a BLEU score of 74.7 on the test set. In addition, it beats the best published BLEU scores on 2 other test sets, demonstrating the quality of the corpus. STT4SG-350 is balanced across the 7 dialect regions, and the test set allows a fair comparison of ASR performance on different dialects. We intend to take advantage of these properties in future work and conduct in-depth experiments to explore differences in ASR quality between dialects. Subsequently, we want to find ways to improve performance for underrepresented dialects.\n"}
{"question": "What is the foils to descriptive findings?", "evidence": "  The foils to descriptive findings are integrative findings, which offer causal explanations that enable future predictions-a theory, or as a 'model' in the sense of the Standard Model, rather than of a statistical model. Integrative findings can either offer new theories or couch their explanations in existing theories-but the theory is essential either way. ", "options": ["A.  Integrative findings", "B. A statistical model.", "C. Causal explanations ", "D. New theories."], "answer": "A", "content": "\nIntroduction\nComputational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period (Lazer and Radford, 2017; van Atteveldt and Peng, 2018) . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" (Boydstun et al., 2014) . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.\nThe conclusion of this reasoning, and the dominant stance in computational social science, is a reliance on machines alone to answer questions in the field, surrendering to their supposed objectivity * Equal contribution.\nor impartiality. Can a machine's output go beyond descriptive catalogs of evidence, accelerating understanding of processes and motivations? From our experience, computers are nowhere near supplanting humans in interpreting social science results.\n1 An interdisciplinary inquiry must go farther than matching computational techniques to social science questions (O'Connor et al., 2011; Nguyen et al., 2020) . It embraces synergistic methodology and connects the norms and standards of evidence from both. This means partnering computer science's preference for the structured, generalizable, and objective with the unstructured, critical, and contextual which the social sciences champion. This level of interdisciplinarity addresses the question raised by descriptive findings: So what?\nWe see theory as the solution, empowering rather than shackling investigations. What this paper advocates is not one particular theory-certainly these are myriad, and \"even subject matter which has been under intensive and prolonged study remains at the unsettled periphery of research\" (Nagel, 1963) . Instead, we expand on our prior work (Dore and McCarthy, 2022) to clarify calls echoed for decades by computational and social science (McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . Underlying each, we find, is the urge to return to theory, which we espouse herein.\n\nDescription vs. Integration\nWe contrast descriptive findings and theoretical analysis. An example of a descriptive finding is that an apple falls, or that it falls faster when pushed than dropped, or even that it falls at a particular rate estimated with some standard error by a complex interpolation. A theoretical analysis of the same phenomenon, credited to Newton, is that a fundamental force acts upon the apple, and that this same force governs the motion of the heavens. The theoretical analysis links the finding about the world critically to a broader body of knowledge and context.\nDespite advances in causal inference in NLP, the descriptive is all that a machine can provide to the social sciences (Feder et al., 2021) . Certainly the methods of computational text analysis have advanced since the General Inquirer (Stone and Hunt, 1963) and Mosteller and Wallace's statistical inference of text authorship (1963) . But methods are means, not ends. They uncover more descriptive findings in data: the rate of an apple's fall, the topics of refugees' tweets (Walk et al., 2022) , the space given to marginalized groups in textbooks (Lucy et al., 2020) , or patterns of state censorship (Bamman et al., 2012; King et al., 2013) .\nThe foils to descriptive findings are integrative findings (Hofman et al., 2021) , which offer causal explanations that enable future predictions-a theory, or as a 'model' in the sense of the Standard Model, rather than of a statistical model. Integrative findings can either offer new theories or couch their explanations in existing theories-but the theory is essential either way.\n\nWe Don't Integrate\nTo contrast descriptive and integrative findings, we reviewed approximately 60 papers in computational text analysis published in *ACL venues. In Table 1 , we describe several of these in terms of their descriptive or theory-grounded contributions.\n2 Descriptive papers may refer to social science theories or make generalizable claims, as when Demszky et al. (2019) write, \"The shooter's race appears to play a role in topic preference: if the shooter is white, Democrats become more likely to focus on shooter's identity,\" but they do not link to the two to each other. An excellent theory-grounded quantitative work is Nelson (2021) ; she confirms some of the most compelling features of identity theory, specifically that identities based on race were most distinguished by cultural discourse, whereas those based on gender by the domestic and the economic discourse. Similarly, we conducted theory-grounded quantitative work to investigate the application of the protest paradigm and thematic framing in how westernand Hong Kong based newspapers portray protests in Hong Kong (McCarthy et al., 2021; McCarthy and Dore, 2022) . Generally, it remains challenging to find computational social science papers in *ACL venues that go beyond description and prediction, advancing theory. Why is this? We believe it stemmed from the field's \"empirical turn\".\n3 Few remember when the meetings of ACL offered a few dozen papers, all entrenched in formalisms and linguistic theories. Arguably, 1996 was a turning point when the founders of SIGDAT held the first EMNLP at Penn under the auspices of the ACL. 4 This gave a spotlight to the few but growing empiricists in the field and drew in more.\nEMNLP began a half-decade of measurable reorganization the field (Anderson et al., 2012) . That EMNLP remains affiliated with ACL keeps the language-focused machine learning practitioners in our tent. The slow blurring of boundaries between each *ACL conference's expectations (Church, 2020) increases this unity. Both groups belong under this tent. But without a doubt, one group's voice is becoming less heard.\nPublication venues within the ACL focus on methods over theory. 5 Techniques are taken off the shelf without critical examination because these are \"the best\" (often \"state of the art\") for their purposes (Ethayarajh and Jurafsky, 2020) . This widens the gap between theoretical and empirical work. 6 Hopkins and King (2010) claim, \"computer scientists may be interested in finding the needle in the haystack. . . social scientists are more commonly interested in characterizing the haystack\"-evincing the value of broader context. 7 Wallach (2018), quoting Hopkins and King, explains that the two groups 3 A lesser reason is the challenge of serving two masters: adequately covering both the theoretical and methodological components within 8 pages. We recently received two reviews for an *ACL submission: one advocating for more of the social science context in the main text by eschewing methods to the appendix, and the other instructing us to do the opposite. 4 And its predecessor the Workshop on Very Large Corpora. 5 This is due to the outsized influence of computer science, often seen as the science of method (Hoare and Jones, 1989; Shapiro, 2001) , when not instead seen as an engineering discipline (Rapaport, 2005).\n6 A related criticism is that empirical research has narrowed to focus on 'easy' questions that its tools can address (Coleman, 1986; Baden et al., 2021) , especially when research questions are baked into the design of the task. 7 As evidence, see Siegel (2018) : \"We usually don't know about causation, and we often don't necessarily care. . . the objective is more to predict than it is to understand the world. . . It just needs to work; prediction trumps explanation.\" Descriptive Chang et al. (2009) The article presents new quantitative methods to measure semantic meaning in inferred topics. The authors emphasize the qualitative relevance of their findings as it validates the use of topics for corpus exploration and information retrieval. However, their working hypothesis and empirical findings are not connected to the extremely relevant field of communication theory. Bamman et al. (2012) The article presents the first large-scale analysis of political content censorship in social media. The authors miss the opportunity to relate their hypothesis and findings to censorship theory, a natural theoretical context for the research, which would strengthen the relevance and generalizability of the findings. Field et al. (2018) The article discusses media manipulation in Russia in the context of agenda-setting and framing, the tools that Russian state-owned (or heavily influenced) media outlets use to distract public attention from domestic economic politics. The authors implicitly refer to propaganda theory and autocratic theory throughout the article even though their findings are not discussed in relation to these theories. Demszky et al. (2019) The article applies \"a more comprhensive NLP framework to study linguistic aspects of polarization in social media\". While the article implicitly refer to theories of social conformity and social conflict, the findings are not linked or discussed (either explicitly or implicitly) to the theoretical frameworks that the authors touch on in their \u00a71.\n\nIntegrative\nDiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.are interested in very different research  questions, and that computational social science must be more than computer science with social data; it must strive for valid explanatory models. In the same vein, at ACL 2022, ACL fellow Eduard Hovy remarked that NLP must be more than \"just machine learning on corpora\". Social scientists are also coming to terms with the meaning of computational techniques applied more often in social science (Bail, 2014; Biernacki, 2015; Lee and Martin, 2015; Spillman, 2015) . The focus of the debates, however, is on which methods are best suited to extract meaning from text, without addressing any theoretical considerations related to the methods or whether a theoretical framework for those methods even exists. The discussions on whether computational methods make social science research more efficient, reliable, and reproducible overtake attempts at theory-building.\n\nMoving Forward\nWe are not denying the value of computational approaches to analyzing text. Certainly, comput-ing can be an instrumental approach for modeling and understanding social complexity. This does not mean that other approaches, such as historical, ethnographic, or mathematical, become irrelevant. On the contrary, computational methods necessarily (whether awarely or not) rely on these earlier approaches to add value, in terms of improving our explanations and understanding (Radford and Joseph, 2020) .\nAs we are a field that prioritizes methods, consider the seminal book on methods in science: Abbott ( 2004) taxonomizes scientific ways of knowing. Its five broad categories are ethnography, historical narration, standard causal analysis, small-N comparison, and formal modeling. We in NLP myopically choose the third and fifth of these, ignoring the value of the others. But the broader point of Methods of Discovery is not methods. It is the research question. Any methodology should be grounded in the question, not incremental tweaks and reviewers' comfort (Church, 2020) . This admits even qualitative or mixed-method approaches to text analysis.\nThe role of humans in scientific inquiry is nothing new. Using qualitative analysis to complement quantitative techniques has its roots in Achen and Snidal (1989)'s recommendation to use historical case studies as a complement to statistical research.\n8\nTheir plea was strengthened by Verba's work in the early 1990s (Verba et al., 1993 (Verba et al., , 1995;; Verba, 1996) and Tarrow (1995) , who openly called for bridging qualitative and quantitative modes of research in social science. In doing so, they have enriched the field with critical methodological innovations (Gerring, 2004) , benefiting from the recognition that \"quantitative methods must augment humans, not replace them\" (Grimmer and Stewart, 2013, 4) .\nThe field can draw more from social science's rich tradition of inductive theory-building and interpretation to develop its theoretical approach-to prize either induction or deduction alone is a myth of scientific procedure (Thagard, 1988) , but the melding of the two opens new doors. Rather than eschewing the complexity (a criticism leveled by Baden et al., 2021) , it should put complexity at the center of its ontology on the basis that there are no immutable laws in social life or optimal solutions to social problems.\nSkepticism can linger toward findings not drawn from the standard practices of one's own field; indeed, social science was long skeptical of computational contributions (Armstrong, 1967) . We believe that this drives the hyperfocus on improving a few accepted methods instead of exploring more broadly. If the doorway between disciplines is only narrowly open, this reflects a lack of appreciation for each field's ways of knowing. The disciplinary divide keeps computational researchers from embracing methods beyond standard causal analysis or formal modeling, so the interpreter-centric richness allowed by histories, ethnographies, and small-N exploration are precluded.\n\nConclusion\nWe have explained the distinction between descriptive and theoretical findings as it pertains to computational text analysis. The bulk of work we found provided vast descriptive findings, often of high quality, but not giving back to questions of theory. We offer several suggestions on how to 'push the pendulum back' by prioritizing theory-building or theory-affirming research questions and accepting whichever methods are best suited toward answering it-not only the familiar and entrenched ones.\nWe are not the first to advocate for a shift in the patterns of applying computational techniques to real-world problems. There is a steady drumbeat from voices in the field advocating careful approaches (Nagel, 1963; McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . What we see underlying all of thesethose writing against 'mathiness' and speculation, advocating for clear evaluation over anecdotes, criticizing textual researchers' dilution of conceptual standards, highlighting work that ties linguistic information into complex models-is an unspoken, perhaps unrealized, call for a return to theory.\nNot only do we aver that incorporating theory is essential; but also, other fields have strengthened themselves when espousing organizing principles beyond those of their progenitors. Behavioral economics is a success story here. It transcended the neat (but psychosocially stripped) mathematics it draws from to acknowledge deviations from rationality and blend economics with cognitive science (Kahneman and Tversky, 1979; Thaler, 1980; Thaler and Sunstein, 2009) .\nFor scientific-not simply engineeringadvances to arise from the *ACL community, authors and reviewers alike must resist the temptation toward incremental, 'safe' research and follow Church (2005) : \"Controversial papers are great; boring unobjectionable incremental papers are not.\" In reviewing new research, we should privilege not only work that presents new and unusual computational methods, but also interactions between computational and humanistic approaches to answering research questions. EMNLP was founded because of reviewing biases at ACL against groundbreaking methodological advances, and since then the two have homogenized; \"EMNLP reviewing is no longer much of a differentiator\" (Church, 2020) . We found that theoretically grounded findings in text analysis are often published in non-*ACL venues (Table 1 ), but ACL sets the standard for work involving computational text analysis and NLP. Is there no home for groundbreaking integrative or interdisciplinary work in *ACL, such that a new venue is required? Or can we adapt our standards to invite deeper connections to theory and new ways of knowing?\n"}
{"question": "What does the background mainly show?", "evidence": "  t )} T 1 with U t and M t being current user and preceding system utterance whether any of the slots in S = {S n } N 1 is present, to (2) predict values for each S n and to (3) track the dialogue state\nDS t \u2200t \u2208 [1, T ]. The DS is cumulative, i.e., DS t = update(DS t\u22121 , DS t ) is updated given the predictions of slot-value updates DS t . (OpenAI, 2022) is a dialogue agent (Leike et al., 2018) , and in its core a GPT-3.5 LLM fine-tuned on human-written promptresponse pairs followed by reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020) . RLHF utilizes a reward model trained on human feedback to improve generation quality and adequacy via proximal policy optimization (Schulman et al., 2017) \n ", "options": ["A. The process of dialogue state tracking.", "B. Chat GPT\u2019s meaning.", "C. Investigative approach to zero-shot DST with ChatGPT ", "D. Core concepts in the experimental process.", "Dialogue state tracking is tasked to (1) determine for every turn t in a dialogue {(U t , M ", "ChatGPT", "C.Zero-shot DST with ChatGPT\nOur investigative approach to zero-shot DST with ChatGPT differs considerably from related works. "], "answer": "D", "content": "\nIntroduction\nDialogue state tracking (DST) is a critical component for task-oriented dialogue systems. Its purpose is to extract and track user's goals throughout a conversation (Young et al., 2010) . DST is challenging due to the infinite possibilities of user/agent conversations, and because services and schemas/APIs that dialogue systems interface are subject to constant change (Ren et al., 2018) . Although traditional approaches achieve high accuracy when operating on a pre-defined set of concepts called an ontology (Mrk\u0161i\u0107 et al., 2017; Liu and Lane, 2017; Zhong et al., 2018) , ongoing research explores transfer to new domains with little to no additional learning (Rastogi et al., 2020) using ontology independent architectures to allow seamless adaptation to out-of-ontology concepts.\nMany strategies for zero-shot transfer to unseen domains have been proposed. Li et al. (2021) treat DST as a question answering (QA) task by leveraging data augmentation. Zhao et al. (2022) propose DST by relying on schema descriptions while Heck et al. (2022) utilize natural language descriptions to facilitate zero-shot transfer. Gao et al. (2020) and Lin et al. (2021) suggest learning from non-dialogue QA data which are available in large amounts to improve generalization. Campagna et al. (2020) harness large synthesized data based on abstract dialogue models. However, none of these techniques are ideal solutions. Fine-tuning is challenging due to computational costs, risk of over-fitting and the need for expensive (Budzianowski et al., 2018) task-specific data. Cross-task transfer still requires curated data and careful consideration of suitable learning tasks. Data augmentation requires high level task knowledge and an adequate synthesizing strategy.\nA new generation of large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Glaese et al., 2022) comes with the promise to be equipped to solve any task without task-specific fine-tuning, but solely with world knowledge they acquired during self-training on massive amounts of data. Such LLMs have been shown to perform remarkably well on in-context learning (ICL) , where only a natural language prompt and examples are provided to condition the generation process, achieving significant improvements over fine-tuned approaches in few-shot setups (Brown et al., 2020; Wang et al., 2022) . ChatGPT (Ope-nAI, 2022) -trained using human feedback and reinforcement learning -is the most recent of such models and single-handedly solves an array of challenging natural language processing (NLP) tasks with super-human capabilities, all through a natural language dialogue interface.\nIn this work, we aim to answer the question: does ChatGPT solve the problem of zero-shot DST? We show that crafting intuitive natural language prompts is sufficient to achieve state-of-the-art performance with ChatGPT, exceeding conventional, engineering-heavy approaches to zero-shot DST by a large margin. However, despite our findings, we argue that properties inherent to general purpose models inhibit their ability to simply replace specialized systems. We speculate that while in the foreseeable future general purpose models may not become holistic solutions to complex problems, they will provide ample opportunities to empower specialized systems to go beyond their pre-defined scopes, enable on-the-fly extensibility and generation of high quality training data by zero-shot synthesizing or automatic labeling.\n\nBackground\nDialogue state tracking is tasked to (1) determine for every turn t in a dialogue {(U t , M t )} T 1 with U t and M t being current user and preceding system utterance whether any of the slots in S = {S n } N 1 is present, to (2) predict values for each S n and to (3) track the dialogue state\nDS t \u2200t \u2208 [1, T ]. The DS is cumulative, i.e., DS t = update(DS t\u22121 , DS t ) is updated given the predictions of slot-value updates DS t .\n\nChatGPT\n(OpenAI, 2022) is a dialogue agent (Leike et al., 2018) , and in its core a GPT-3.5 LLM fine-tuned on human-written promptresponse pairs followed by reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020) . RLHF utilizes a reward model trained on human feedback to improve generation quality and adequacy via proximal policy optimization (Schulman et al., 2017) \n\nZero-shot DST with ChatGPT\nOur investigative approach to zero-shot DST with ChatGPT differs considerably from related works. We decode dialogue state updates with a general purpose model, without undergoing any parameter updates. Consequently, we neither employ data augmentation nor cross-task transfer learning. Instead, we solely rely on the general capacities of ChatGPT as an aligned dialogue agent. We take a most rigorous approach to zero-shot transfer where we do not allow the provision of any examples, nor of a formal task definition. Instead, we only permit natural language explanations of what the model is supposed to do. This sets our investigation apart from the closely related IC-DST (Hu et al., 2022) .\nIn zero-shot DST, the set of slots S relevant during inference and the set of slots S \u2032 seen during training of the model X \u03b8 with parameters \u03b8 are disjoint, i.e., S \u2229 S \u2032 = \u2205. Further, it may be S \u2032 = \u2205, in which case \u03b8 is not specifically tuned towards solving DST. This is precisely the case for Chat-GPT in our setup. Our approach to zero-shot DST with ChatGPT is formalized as follows. Let\nA 1 =P \u2295 \"system\":M 1 \u2295 \"user\":U 1 , A t =\"system\":M t \u2295 \"user\":U t , \u2200t \u2208 [2, T ],\nwhere P is the task description which provides the model with instructions for how to process a dialogue between a system M and a user U . A 1 is the initial prompt to ChatGPT. A t\u22652 are the follow-up prompts, only containing a single turn-pair of the dialogue of interest. ChatGPT is particularly suitable for this strategy due to its chat based interface. ChatGPT generates its next output B t conditioned on the current prompt A t\u22121 , as well as all preceding user queries and system responses of the same chat. The dialogue state update DS t can be found in B t , but may not be directly interpretable as such due to the diversity in the output surface forms. Thus, we require a normalization operation DS t = normalize(B t ). In contrast to (Hu et al., 2022) , we do not condition B t on DS t . This renders the task even more challenging, as Chat-GPT is forced to solve complex subtasks such as coreference resolution -the case where a newly encountered slot refers to the value of another slot -solely given the initial prompt and its own latent dialogue state given the dialogue history.\n\nExperiments\nAt the time of conducting our experiments, Chat-GPT is a proprietary research preview accessible for free via a web interface 1 . We used the Jan 9 version of the model. We use a regular expression term to extract all parts that are JSON formatted. We form DS t by accumulating all predicted updates up to turn t.\nEvaluation. We evaluate on the 1000 dialogues of the MultiWOZ 2.1 (Eric et al., 2020) test split and use joint goal accuracy (JGA) to compare methods. For a fair judgement of the ChatGPT predictions, we follow the evaluation procedure of Heck et al. ( 2020). We process each dialogue once and refrain from using ChatGPT's regeneration feature.\nPrompt. We imposed restrictions that the taskdefining prompt P be intuitive natural language and provides no formal schema. The crafting process involves simple trial-and-error on fewer than 10 held-out dialogues from the MultiWOZ training set. The design process was guided by the intention to imitate the behavior of a triple copy strategy (TripPy) DST (Heck et al., 2020) . P consists of three parts. First, a list of names for detectable informable slots along with natural language descriptions. The slot names help us extract a DS t that is compatible with the dataset's labels. Second, a sparse list of slots that are categorical, along with their value candidates for (1) aiding normalization of values that are expected to show high variability in expression, and (2) modeling Boolean slots.\nThird, an informal task description. 2\n4.1 ChatGPT vs. Supervised SOTA Comparing ChatGPT's performance to state-of-theart supervised approaches that achieve close to 60% JGA is not a fair fight 3 , and yet we observe an impressive 31.5% zero-shot JGA. This result is double-edged; on the one hand it is evidence that ChatGPT is capable of DST 4 , and on the other hand is no match for specialized systems. The comparison to TripPy, a SOTA supervised model, allows us a more fine-grained analysis. In Figure 1 , slot filling performance is broken down into value types. We observed that ChatGPT underperforms in non-trivial cases, namely refer, where a newly encountered slot refers to the value of another slot, and inform, where a slot-value was mentioned by the system and confirmed by the user. ChatGPT shows slight underperformance for Boolean slots. Remarkably, performance for values that are extracted directly from user utterances -the most relevant category in terms of frequency - (Hu et al., 2022) was the first successful attempt at pseudo 5 zero-shot DST via ICL. Our preliminary results with ChatGPT are on par, which is remarkable for the following reasons.\n(1) Our prompt is non-schematic and without examples, (2) our task-defining prompt is stated only once at the beginning of the chat, and (3) we do not maintain a DS to serve as additional input at each turn. The heightened zero-shot performance of IC-DST can be mainly attributed to these points.\n\nError Analysis\nWe identified a set of recurring errors that are likely caused by either the content of P or by the model's inherent properties. See ChatGPT is a sophisticated dialogue agent that, via alignment with human judgements, is capable of understanding context and intent of a multi-turn conversation far beyond the capacities of the previous generation of LLMs. This makes it well-suited for DST. Our results demonstrate that even with intuitive natural language prompts, a complex task such as DST can be solved exceedingly well without any form of additional learning. While specialized systems can exert control over its input-processing and output-generation to arbitrary degrees, this is not the case for Chat-GPT. Even with the most rigorous and schematic prompts, there can be no guarantee that the model interprets the input as intended or generates the output as required, which may lead to unexpected behavior. Furthermore, there is no guarantee that behavior is consistent across a series of similar inferences, such as in our experimental evaluation. In terms of deployment, the cost factor of building and running massive models may hinder their utilization as a plug-and-play module.\nDespite impressive zero-shot and ICL results for general purpose models, specialist models still perform best on most tasks thanks to task-specific solutions via adequate engineering (Heck et al., 2020; Ye et al., 2021; Kim et al., 2020) and taskrelated data. However, the opportunities to improve dedicated systems with the help of general purpose models are plenty. Their predictive powers could be used for developing smaller, specialized, low inference cost models. Automatic labeling and data a) PMUL4050 system: \"I'd recommend the Autumn House. Would you like to make a booking?\" user: \"Yes please. I need the reservation to be for 8 people and 2 nights starting on Tuesday.\" Prediction: ... hotel-name: none Label: ..., hotel-name: autumn house b) PMUL0117 user: \"Yes I also need a taxi that will get me to the restaurant by the booked time please.\" Prediction: taxi-destination: hotel, taxi-departure: restaurant Label: taxi-destination: the gonville hotel, taxi-departure: la mimosa c) SNG01873 user: \"I need to be picked up from pizza hut city centre after 04:30\" Prediction: ..., hotel-name: dontcare, ..., attraction-type: dontcare, ... Label: ... d) PMUL0599 user: \"[...] Can you just help me find a high-end Mexican restaurant?\" Prediction: ..., restaurant-pricerange: high-end Label: ..., restaurant-pricerange: expensive e) MUL2051 user: \"Can I get address and postcode for the hotel?\" Prediction: hotel-address: ?, hotel-postcode: ? Label:system: \"The address is 74 chesterton road, the postal code is cb41er, can I assist with anything else?\" user: \"That is all for now, goodbye.\" Prediction: hotel-address: 74 chesterton road, hotel-postcode: cb41er Label:f) MUL0524 user: \"I'm going to Cambridge on saturday and want to arrive by 14:15 please.\" Prediction: ..., train-day: Saturday Label: ..., train-day: saturday g) PMUL4246 user: \"i need a place to go and should be a museum\" Prediction: attraction-type: museum Label: attraction-type: museum system: \"Okay! There are several museums in Cambridge. What part of town would you like to visit?\" user: \"How about ones in the centre, what's available?\" Prediction: attraction-type: museum, attraction-area: centre Label: attraction-area: centre augmentation are natural use cases for ChatGPT, as is evident from our experimental results; a perdomain JGA of 70% (see Section 4.2) is surely sufficient to generate additional mid-to high-quality training data for dedicated systems. Automatic labeling may be conducted on-line for on-the-fly adaptation of production systems or off-line for iterative learning. Another way of harnessing general purpose models is the integration into dedicated systems as fallback options in case of out-of-domain or out-ofontology requests. An integration via knowledgeseeking term detection (Gunasekara et al., 2020) could facilitate the ability to provide context-aware responses that go beyond the original scope of the specialized system. General purpose models may handle unseen domains in place of the main model.\nWhile hallucinations may be an issue if not handled adequately, they also pose an opportunity to enable zero-shot concept detection. We observed that many slot hallucinations were sensible and pointed at elements that were meaningful to conversations. Zero-shot slot detection may be utilized to annotate and prepare unstructured data for model training, and to expand a system's capacities on-the-fly. Dialogue state trackers with dynamic dialogue states have the potential to expand a taskoriented dialogue system's conversational range seamlessly (Geishauser et al., 2022) . A general purpose model that has the capacity to identify new concepts may be utilized to generate API calls and database queries that are unknown to the specialized system (OpenAI, 2023; Chase, 2023) .\nGeneral purpose models may replace some components in a modular dialogue system (Zhu et al., 2022) . It might still be beneficial to rely on specialized DST and a dedicated policy for particular tasks in order to maintain interpretability and a desired level of control over information flow. However, natural language understanding (NLU) and natural language generation (NLG) modules may be powered by generative large language model based systems such as ChatGPT in order to benefit from a heightened ability of semantic modeling and to facilitate more natural and diverse output, thus promoting more natural conversations with modular task-oriented dialogue systems.\n\nConclusion\nThis work is the first to investigate ChatGPT's capacities for zero-shot DST. Despite remarkable preliminary results that we achieved, we identified limitations rooted in inherent properties of general purpose models, preventing these to become holistic solutions to complex NLP problems without further research. We discussed opportunities provided by ChatGPT and similar models to advance the development of specialized systems. With our insights and discussion, we hope to stimulate research in similar directions.\n"}
{"question": "What is the proposed solution to improve the stability of IFs in this paper?", "evidence": "  Our solution leverages class information to improve the stability of IFs. We propose IFs-class, variants of IFs that use class information to improve the stability while introducing no additional computational cost.  ", "options": ["A. Using a larger reference set.", "B. Leveraging class information.", "C. Increasing the number of layers in the neural network.", "D. Removing IFs from the error detection process. "], "answer": "B", "content": "\nIntroduction\nDeep learning models are data hungry. Large models such as transformers (Vaswani et al., 2017) , BERT (Devlin et al., 2019) , and GPT-3 (Brown et al., 2020) require millions to billions of training data points. However, data labeling is an expensive, time consuming, and error prone process. Popular datasets such as the ImageNet (Deng et al., 2009) contain a significant amount of errors -data points with incorrect or ambiguous labels (Beyer et al., 2020) . The need for automatic error detection tools is increasing as the sizes of modern datasets grow.\nInfluence function (IF) (Koh and Liang, 2017) and its variants (Charpiat et al., 2019; Khanna et al., 2019; Barshan et al., 2020; Pruthi et al., 2020) are a powerful tool for estimating the influence of a data point on another data point. Researchers leveraged this capability of IFs to design or detect adversarial (Cohen et al., 2020) , poisonous (Koh et al., 2022; Koh and Liang, 2017) , and erroneous (Dau et al., 2022) examples in large scale datasets. The intuition is that these harmful data points usually have a negative influence on other data points and this influence can be estimated with IFs. Basu et al. (2021) empirically observed that IFs are unstable when they are applied to deep neu- * Joint first authors ral networks (DNNs). The quality of influence estimation deteriorates as networks become more complex. In this paper, we provide empirical and theoretical explanations for the instability of IFs. We show that IFs scores are very noisy when the two data points belong to two different classes but IFs scores are much more stable when the two data points are in the same class (Sec. 3). Based on that finding, we propose IFs-class, variants of IFs that use class information to improve the stability while introducing no additional computational cost. IFs-class can replace IFs in anomalous data detection algorithms. In Sec. 4, we compare IFs-class and IFs on the error detection problem. Experiments on various NLP tasks and datasets confirm the advantages of IFs-class over IFs.\n\nBackground and Related work\nWe define the notations used in this paper. Let z = (x, y) be a data point, where x \u2208 X is the input, y \u2208 Y is the target output; Z = z (i) n i=1 be a dataset of n data points; Z \u2212i = Z\\z (i) be the dataset Z with z (i) removed; f \u03b8 : X \u2192 Y be a model with parameter \u03b8; L Z,\u03b8 = 1 n n i=1 \u2113(f \u03b8 (x (i) ), y (i) ) = 1 n n i=1 \u2113(z (i) ; \u03b8) be the empirical risk of f \u03b8 measured on Z, where \u2113 : Y \u00d7 Y \u2192 R + is the loss function; \u03b8 = arg min \u03b8 L Z,\u03b8 and \u03b8\u2212i = arg min \u03b8 L Z \u2212i ,\u03b8 be the optimal parameters of the model f \u03b8 trained on Z and Z \u2212i . In this paper, f \u03b8 is a deep network and \u03b8 is found by training f \u03b8 with gradient descent on the training set Z.\n\nInfluence function and variants\nThe influence of a data point z (i) on another data point z (j) is defined as the change in loss at z (j) when z (i) is removed from the training set\nEQUATION\nThe absolute value of s (ij) measures the strength of the influence of z (i) on z (j) . The sign of s (ij) show the direction of influence. A negative s (ij) means that removing z (i) decreases the loss at z (j) , i.e. z (i) is harmful to z (j) . s (ij) has high variance because it depends on a single (arbitrary) data point z (j) .\nTo better estimate the influence of z (i) on the entire data distribution, researchers average the influence scores of z (i) over a reference set Z \u2032\ns (i) = 1 |Z \u2032 | z (j) \u2208Z \u2032 s (ij) = L Z \u2032 , \u03b8\u2212i \u2212 L Z \u2032 , \u03b8 (2)\ns (i) is the influence of z (i) on the reference set Z \u2032 . Z \u2032 can be a random subset of the training set or a held-out dataset. Naive computation of s (ij) requires retraining f \u03b8 on Z \u2212i . Koh and Liang (2017) proposed the influence function (IF) to quickly estimate s (ij) without retraining\ns (ij) \u2248 IF (z (i) , z (j) ) \u2248 1 n \u2207 \u03b8\u2113(z (i) ; \u03b8) \u22a4 H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) (3)\nwhere\nH \u03b8 = \u2202 2 L Z, \u03b8/\u2202\u03b8 2\nis the Hessian at \u03b8. Exact computation of H \u22121 \u03b8 is intractable for modern networks. Koh and Liang (2017) developed a fast algorithm for estimating H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) and used only the derivatives w.r.t. the last layer's parameters to improve the algorithm's speed. Charpiat et al. (2019) proposed gradient dot product (GD) and gradient cosine similarity (GC) as faster alternatives to IF. Pruthi et al. (2020) argued that the influence can be better approximated by accumulating it through out the training process (TracIn). The formula for IFs are summarized in Tab. 3 in Appx. A.\nIFs can be viewed as measures of the similarity between the gradients of two data points. Intuitively, gradients of harmful examples are dissimilar from that of normal examples (Fig. 1 ).\n\nInfluence functions for error detection\nIn the error detection problem, we have to detect data points with wrong labels. Given a (potentially noisy) dataset Z, we have to rank data points in Z by how likely they are erroneous. Removing or correcting errors improves the performance and robustness of models trained on that dataset.\nTraditional error detection algorithms that use hand designed rules (Chu et al., 2013) or simple statistics (Huang and He, 2018) , do not scale well to deep learning datasets. Cohen et al. (2020) 2021) empirically showed that IFs with last layer gradient perform as well as or better than IFs with all layers' gradient and variants of IF behave similarly. Therefore, we analyze the behavior of GD with last layer's gradient and generalize our results to other IFs. Fig. 1 shows the last layer's gradient of an MLP on a 3-class classification problem. In the figure, gradients of mislabeled data points have large magnitudes and are opposite to gradients of correct data points in the true class. However, gradients of mislabeled data points are not necessarily opposite to that of correct data points from other classes. Furthermore, gradients of two data points from two different classes are almost perpendicular. We make the following observation. A mislabeled/correct data point often has a very negative/positive influence on data points of the same (true) class, but its influence on other classes is noisy and small.\nWe verify the observation on real-world datasets. (Fig. 2 ). We compute GD scores of pairs of clean data points from 2 different classes and plot the score's distribution. We repeat the procedure for pairs of data points from each class. In the 2-class case, GD scores are almost normally distributed with a very sharp peak at 0. That means, in many cases, a clean data point from one class has no significant influence on data points from the other class. And when it has a significant effect, the effect could be positive or negative with equal probability. In contrast, GD scores of pairs of data points from the same class are almost always positive. A clean data point almost certainly has a positive influence on clean data points of the same class.\nOur theoretical analysis shows that when the two data points have different labels, then the sign of GD depends on two random variables, the sign of inner product of the features and the sign of inner product of gradients of the losses w.r.t. the logits. And as the model becomes more confident about the labels of the two data points, the magnitude of GD becomes smaller very quickly. Small perturbations to the logits or the features can flip the sign of GD. In contrast, if the two data points have the same label, then the sign of GD depends on only one random variable, the sign of the inner product of the feature, and the GD's magnitude remains large when the model becomes more confident. Mathematical details are deferred to Appx. D.\n\nClass based IFs for error detection\nOur class based IFs for error detection is shown in Alg. 1. In Sec. 3.1, we see that an error has a very Algorithm 1 Class based influence function for error detection.\n\nRequire:\n1: Z = z (i) n i=1 : a big noisy dataset 2: C: number of classes 3: for k = 1, ..., C do 9:\nZ \u2032 k = z \u2032(j k ) m k j k =1 : clean data from class k 4: Z \u2032 = C k=1 Z \u2032 k : a clean\ns (i) k = m k j=1 sim(\u2207 \u03b8 \u2113(z (i) ),\u2207 \u03b8 \u2113(z \u2032(j k ) )) m k 10:\nend for 11:\ns (i) = min k (s (i)\nk ) 12: end for 13: \u1e90 = sort(Z, key = s, ascending = True) 14: return \u1e90 strong negative influence on correct data points in the true class, and a correct data point has a positive influence on correct data points in the true class. Influence score on the true class is a stronger indicator of the harmfulness of a data point and is better at differentiating erroneous and correct data points. Because we do not know the true class of z (i) in advance, we compute its influence score on each class in the reference set Z \u2032 and take the minimum of these influence scores as the indicator of the harmfulness of z (i) (line 8-11 create benchmark datasets Z's, we inject random noise into the above datasets. For text classification datasets, we randomly select p% of the data points and randomly change their labels to other classes.\nFor the CoNLL-NER dataset, we randomly select p% of the sentences and change the labels of r% of the phrases in the selected sentences. All tokens in a selected phrase are changed to the same class. The reference set Z \u2032 is created by randomly selecting m k clean data points from each class in Z. To ensure a fair comparison, we use the same reference set Z \u2032 for both IFs and IFs-class algorithms. Models are trained on the noisy dataset Z. To evaluate an error detection algorithm, we select top q% most harmful data points from the sorted dataset \u1e90 and check how many percent of the selected data points are really erroneous. Intuitively, increasing q allows the algorithm to find more errors (increase recall) but may decrease the detection accuracy (decrease precision). Our code is available at https://github.com/Fsoft-AIC/ Class-Based-Influence-Functions.\nResult and Analysis Because results on all datasets share the same patterns, we report representative results here and defer the full results to Appx. C.\nFig. 3(a) shows the error detection accuracy on the SNLI dataset and how the accuracy changes with q. Except for the GC algorithm, our classbased algorithms have higher accuracy and lower variance than the non-class-based versions. When q increases, the performance of IFs-class does not decrease as much as that of IFs. This confirms that IFs-class are less noisy than IFs. Class information fails to improve the performance of GC. To understand this, let's reconsider the similarity measure sim(\u2022, \u2022). Let's assume that there exist some clean data points z \u2032(j) \u2208 Z \u2032 with a very large gradient \u2207 \u03b8\u2113(z \u2032(j) ). If the similarity measure does not normalize the norm of \u2207 \u03b8\u2113(z \u2032(j) ), then z \u2032(j) will have the dominant effect on the influence score. The noise in the influence score is mostly caused by these data points. GC normalizes both gradients, \u2207 \u03b8\u2113(z (i) ) and \u2207 \u03b8\u2113(z \u2032(j) ), and effectively removes such noise. However, gradients of errors tend to be larger than that of normal data points (Fig. 1 ). By normalizing both gradients, GC removes the valuable information about magnitudes of gradients of errors \u2207 \u03b8\u2113(z (i) ). That lowers the detection performance. In Fig. 3 (a), we see that the performance of GC when q \u2265 15% is lower than that of other classbased algorithms. Similar trends are observed on other datasets (Fig. 6 , 7, 8 in Appx. C). Fig. 3(b) shows the change in detection accuracy as the level of noise p goes from 5% to 20%. For each value of p, we set q to be equal to p. Our class-based influence score significantly improves the performance and reduces the variance. We note that when p increases, the error detection problem becomes easier as there are more errors. The detection accuracy, therefore, tends to increase with p as shown in Fig. 3 (b), 9, 10. Fig. 3(c ) shows that GD-class outperforms GD on all entity types in CoNLL2003-NER. The performance difference between GD-class and GD is greater on the MISC and ORG categories. Intuitively, a person's name can likely be an organization's name but the reverse is less likely. Therefore, it is harder to detect that a PER or LOC tag has been changed to ORG or MISC tag than the reverse. The result shows that IFs-class is more effective than IFs in detecting hard erroneous examples.\n\nThe effect of data on error detection algorithms\nWe study the effect of the size and the cleanliness of the reference set on the performance of error detection algorithms.\nThe size of the reference set. We changed the size of classes in the reference set from 10 to 1000 to study the effect of the reference set's size on the detection performance. We report the mean performance of GD and GC algorithms in Tab. 1. We observe no clear trend in the performance as the size of the reference set increases. Our conjecture is that gradients of clean data points from the same class have almost the same direction. Averaging the gradient direction over a small set of data points already gives a very stable gradient direction. Therefore, increasing the size of the reference set does not have much impact on detection performance. \n\nConclusion\nIn this paper, we study influence functions and identify the source of their instability. We give a theoretical explanation for our observations. We introduce a stable variant of IFs and use that to develop a high performance error detection algorithm. Our findings shed light of the development of new influence estimators and on the application of IFs in downstream tasks.\n"}
{"question": "What is the main focus of the study conducted in this article regarding NLP models' performance?", "evidence": "  Supporting this intuition, recent work in image classification (Radford et al., 2021) and extractive question answering (Awadalla et al., 2022) show that zero-shot inference and fewshot fine-tuning improve average robustness across a range of OOD test sets. We conduct a broad empirical study over 14 datasets across three tasks to investigate the relationship between exposure to ID training examples (sample efficiency) and robustness.  ", "options": ["A. Investigating the impact of zero-shot inference on robustness", "B. Analyzing the relationship between model size and sample efficiency", "C. Exploring the correlation between sample efficiency and OOD robustness", "D. Comparing the performance of models trained with natural language prompts", "Comparing models with equivalent ID performance controls for its effect on OOD performance, since improving ID performance usually yields commensurate improvements on OOD performance-in this study, we focus on OOD performance improvements beyond what is expected from ID gains."], "answer": "C", "content": "\nIntroduction\nNLP models perform well when evaluated on data drawn from their training distribution (indistribution / ID), but they typically suffer large drops in performance when evaluated on data distributions unseen during training (out-of-distribution / OOD; Blitzer, 2008) .\nHow does exposure to ID training examples affect the ID-OOD gap? If two models have the same ID performance, will models trained on fewer ID examples (higher sample efficiency) also have higher OOD performance (higher robustness)? At one extreme, zero-shot models will not learn IDspecific patterns because they are not exposed to any labeled ID examples. Similarly, few-shot models trained on very few ID examples may also rely less on ID-specific patterns; if a model never sees the token \"cat\" while training on SNLI, then it will not learn that its presence is spuriously predictive of the contradiction label (Gururangan et al., 2018; Utama et al., 2021) . Supporting this intuition, recent work in image classification (Radford et al., 2021) and extractive question answering (Awadalla et al., 2022) show that zero-shot inference and fewshot fine-tuning improve average robustness across a range of OOD test sets. However, it is unclear how universal these trends are across various tasks and methods for reducing exposure to ID examples, or how predictive they are for any individual test set of interest. Figure 1 illustrates this central question.\nWe conduct a broad empirical study over 14 datasets across three tasks to investigate the relationship between exposure to ID training examples (sample efficiency) and robustness. We experiment with three modeling interventions that improve sample efficiency: (1) using natural language prompts for zero-shot prediction and during finetuning (Brown et al., 2020; Schick and Sch\u00fctze, 2021; Gao et al., 2021) ; (2) fine-tuning models of increasing size; (3) fine-tuning models pre-trained on increasing amounts of data.\nWe find that higher sample efficiency is only sometimes correlated with better robustness, and the effect of specific modeling interventions varies by task. For example, increasing pre-trained model size substantially improves sample efficiency and results in higher average robustness in sentiment experiments, but these sample efficiency gains do not translate to higher average robustness in NLI and extractive QA experiments. On individual datasets, models with better sample efficiency can even be less robust (e.g., increasing model size when training on SST-2 and evaluating OOD on IMDb).\nOverall, these results indicate that general- purpose methods for improving sample efficiency are far from guaranteed to yield significant OOD robustness improvements-their success is highly dataset-and task-dependent. Furthermore, even in this era of large, multi-purpose pre-trained language models, task-specific decisions are often necessary to achieve OOD generalization.\n2 Measuring Sample Efficiency and Robustness. (3) M 's performance on examples from D ood (i.e., the OOD performance).\nLet M 1 and M 2 be two models with equivalent performance on held-out ID data. If M 1 was trained on fewer ID examples than M 2 , then it has higher sample efficiency. If M 1 has higher OOD performance than M 2 , it has higher effective robustness (henceforth \"robustness\"; Taori et al., 2020) . Comparing models with equivalent ID performance controls for its effect on OOD performance, since improving ID performance usually yields commensurate improvements on OOD performance-in this study, we focus on OOD performance improvements beyond what is expected from ID gains.\nSatisfying this equivalent-ID constraint is often difficult in practice; given an arbitrary model M 1 and its corresponding ID performance, it is difficult to produce a different model M 2 with identical ID performance. Rather than explicitly training models to identical ID performance, we train models on varying-size subsamples of a given ID dataset and interpolate between the results to estimate (1) the number of labeled ID training examples necessary to achieve a particular ID performance (sample efficiency) and (2) OOD performance, given ID performance (robustness). These interpolated curves approximate the ideal setting of training a model for every possible ID value. Figure 1 provides a schematized example, with model B having better sample efficiency and robustness than model A.\n\nExperimental Setup\nWe study three modeling interventions-using natural language prompts, increasing pre-trained model size, and pre-training on more data-on 14 total datasets spanning natural language inference (NLI), sentiment analysis, and extractive question answering (QA). See Appendix A for further details about experimental settings.\nTasks and Datasets. In our natural language inference (NLI) experiments, we use MultiNLI (Williams et al., 2018) , SNLI (Bowman et al., 2015), and MedNLI (Romanov and Shivade, 2018) . For sentiment analysis, we use IMDb reviews Maas et al. (2011) , SST-2 (Socher et al., 2013) , and reviews from the \"Movies and TV\" subsection of the Amazon Reviews corpus (Ni et al., 2019) . Lastly, for extractive question answering, we use SQuAD (Rajpurkar et al., 2016 ), NaturalQuestions (Kwiatkowski et al., 2019) , TriviaQA, BioASQ (Tsatsaronis et al., 2015) , and the four SQuAD-Shifts test sets (Miller et al., 2020) .\nModeling Interventions. To understand the effect of a particular modeling intervention on sample efficiency and robustness, we evaluate pre-trained models that differ only along the axis of interest (e.g., model size or fine-tuning method). Since the optimal fine-tuning hyperparameters depend on the ID training dataset size, we separately tune hyperparameters for each model on each training dataset subsample size, taking the models that achieve the best held-out ID performance for each setting. See \n\nResults and Discussion\nOur results show that models with higher sample efficiency may not necessarily have higher average OOD robustness-different tasks and modeling interventions affect robustness in different ways . For example, prompt-based fine-tuning consistently improves both sample efficiency and average robustness, but only in low-data settings (Figure 2 ). In contrast, increasing model size improves sample efficiency across the range of training dataset sizes and tasks, but only improves average robustness on sentiment analysis (Figure 3 ). On individual datasets, we even observe cases where models with lower sample efficiency have higher robustness (Figure 3d ). See Appendix C for full results on every ID-OOD setting.\nNatural Language Prompting. We compare BERT BASE models using (1) standard fine-tuning, (2) prompt-based fine-tuning, and (3) zero-shot prompting. We also compare these results with zero-shot prompting of text-davinci-001, a much larger model trained on substantially more data. We run experiments on NLI and sentiment analysis, since extractive QA is not amenable to prompt-based fine-tuning with masked language models.\nFigures 2a and 2b plot the average performance on all OOD datasets as a function of ID performance and the ID performance as a function of the number of labeled training examples. Sample efficiency improvements from prompt-based finetuning also translate to higher average robustness. However these improvements only apply in the few-shot setting. As the size of the training dataset increases, the improvements in sample efficiency and average robustness steadily diminish. When using sufficiently large training datasets, models trained with prompt-based fine-tuning yield essentially the same sample efficiency and robustness results as standard fine-tuning (\u223c1K examples for NLI, \u223c130 examples for sentiment).\nHowever, results on individual OOD test sets can significantly differ from averaged-OOD trends. For example, Figure 2c shows that prompt-based fine-tuning on MNLI and evaluating on SNLI improves sample efficiency in the few-shot setting but without any robustness improvements.\nSurprisingly, we also find that zero-shot inference does not necessarily improve average robustness over prompt-based fine-tuning-zero-shot performance lies on or below the trend line formed by prompt-based fine-tuning, despite not using any ID-specific data at all. See Appendix C.1 for full results of increasing pre-trained model size for every ID-OOD setting.\nIncreasing Pre-Trained Model Size. We run experiments with the checkpoints of Turc et al. (2019) , who pre-train BERT models with various numbers of transformer layers (L) and hidden embedding sizes (H). We run experiments on NLI, sentiment analysis, and extractive QA to compare pre-trained models of five sizes: (1) Large (L=24, H=1024), ( 2) Base (L=12, H=768), (3) Medium (L=8, H=512), (4) Small (L=4, H=512), and\n(5) Tiny (L=2, H=128). Although increasing the pre-trained model size improves sample efficiency on every task, it does not always improve average robustness (Figure 3 ). In particular, increasing model size minimally affects average robustness in NLI and extractive QA (Figure 3a ,3c), but substantially improves average robustness on sentiment analysis (Figure 3b ). 4a,b ). In extractive QA experiments, varying the amount of pre-training data does not significantly change average robustness (Figure 4c ). Again, we find that results on average OOD performance are not predictive of results on individual test sets-despite unchanged average OOD robustness when pre-training on more data, OOD performance can be higher on individual extractive QA test sets (e.g., SQuAD \u2192 BioASQ; Figure 4d ). See Appendix C.3 for full results of pre-training on Figure 4 : Pre-training on more data is an effective method for improving sample efficiency, but these sample efficiency improvements are not always accompanied by robustness improvements. In NLI and sentiment analysis experiments, these sample efficiency gains correlate with improved average robustness (a,b). However, there are no average robustness gains in extractive QA (c). Despite no average robustness improvement in extractive QA, pre-training on more data can still improve robustness on particular test sets (e.g., BioASQ; d). more data for every ID-OOD setting.\n\nConclusion\nWe study the relationship between sample efficiency and robustness across three tasks and three modeling interventions, finding that sample efficiency improvements often fail to translate to improved robustness. As larger models quickly become more sample efficient, our results caution that sample efficiency and robustness are different axes of improvement and that optimizing for sample efficiency will not necessarily always yield robustness gains.\n"}
{"question": "Which criteria were used to select the representative parallel French-English sentence pairs in the corpus design process?", "evidence": "  we cleaned the source corpus, including deleting sentences without counterparts, English sentences in the French In the construction process. we filtered 6,779 parallel French-English sentence triples with different tense labels for English originals and predictions. On the basis of the automatic selection, we manually screened out the representative parallel French-English sentence pairs with a certain degree of translation difficulty and a complex grammatical structure.  ", "options": ["A. The length of the sentences and their grammatical complexity.", "B. The number of automatic annotations in each sentence.", "C. The degree of translation difficulty and complex grammatical structure.", "D. The linguistic proficiency of the author and reviewers."], "answer": "C", "content": "\nIntroduction\nTranslation tools are often found in a variety of social situations to enable cross-linguistic communication. Tenses are used to express time relative to the moment of speaking. Human translators frequently pay close attention to tense correspondence (Gagne and Wilton-Godberfforde, 2020) . Similarly, machine translation (MT) systems are supposed to maintain temporal consistency between the original text and the predicted text to avoid misunderstandings by users. However, accurately keeping the tense consistency is undoubtedly difficult. Taking French-English (one of the most classic language pairs for MT) as an example in Table 1 , the original text is in plus-que-parfait de l'indicatif of French, corresponding to the past perfect tense in English, while the English prediction provided by Google Translator is in the past simple tense.\nIn fact, this is not an isolated case. You can also find several examples in Appendix B. Besides. the translation mechanics may not the only reason leading to tense inconsistency. The corpora matter as well. For example, we have extracted 20,000 pairs English-French parellel sentences from the widely used dataset Europarl (Koehn, 2005) , and\n\nSentence\nTense FR: Mais on les avait vot\u00e9s lors de la derni\u00e8re p\u00e9riode de session.\n\nPlus-queparfait\nEN: But we voted on them during the last part-session.\n\nPast simple\nCorrection: But we had voted on them during the last part-session. we have observed all groups of parallel utterances where the original French texts are in the plus-queparfait de l'indicatif tense, examining the tenses of their English counterparts. As a sentence may include several tenses, there are 195 occurences of plus-que-parfait tense in total. Among them, only 35.28% English sentences are in the correct past perfect tense, as shown in Table 2 . Although, compared to other tense correspondences, the pair of plus-que-parfait and past-perfect is prone to error in datasets and there are only 0.94% of sentences in Europarl are in plus-que-parfait, we cannot easily ignore this issue. Like Europarl, tense correspondences are generally credible but unreasonable for certain tenses in several common datasets. In addition to the train set, the difficulty of remaining tense consistency also stems from the lack of metrics on measuring the model's mastery of tense information. The research of Marie et al. (2021) shows that 98.8% of *ACL papers 2 in the field of MT from 2010 to 2020 used BLEU (Papineni et al., 2002) scores to evaluate their models. However, the reliability of BLEU has been questioned in the era of neural machine translation (NMT) as its variants only assess surface linguistic features (Shterionov et al., 2018) , and many studies have shown that BLEU has difficulty in portraying the degree of semantic information mastered by the model, i.e. its score does not necessarily improve when more semantic information is mastered (Mathur et al., 2020; He et al., 2023) , not to mention specific tense information. We have also applied BLEU to measure various baselines on our tense test set in Section 4, and the results explicitly support the above statement. In addition, reviewing the evaluation criteria related to MT tasks over the past ten years, we are surprised to find that there are no criteria to assess the model's mastery of tense prediction from a linguistic perspective.\n\nPast perfect\nTherefore, our paper is devoted to the study of NMT based on semantic understanding in terms of tense. We construct a tense parallel corpus test set consisting of 552 pairs of tense-rich, error-prone parallel utterances for NMT systems, and then propose a new task for evaluating the effectiveness of model translations from the perspective of tense consistency. This paper makes three contributions:\n(1) the presentation of the construction of the tense test set, including its tense labels; (2) the proposal of a feasible and reproducible benchmark for measuring the tense consistency performance of NMT systems; and (3) the various experiments for different baselines with the above test set and corresponding benchmark.\n\nAnnotation Rules and Tools\nAs the first work of the MT tense study, we choose English-French, one of the most classic language pairs of MT, to construct the dataset 3 . TENSE, the dominant topic of our research, is a combination of tense and aspect. In the modern grammar system of English, \"a tense system is a system associated with the verb where the basic contrasts in meaning have to do with the location in time of the situation, or the part of it under consideration\" (Huddleston et al., 2021) . The modern grammatical system divides tense into present and preterit based on the inflections added to the end of verbs, and the aspect into perfective and progressive on the state where an action is (Kamp, 1991) . While this tense classification system is too crude for daily life, we therefore apply the following classification methods. On the one hand, we classify the tenses according to the macro-temporal interval of the action into three major time intervals, namely present, past and future tenses; on the other hand, we classify the tenses according to the state of the action into general, progressive and perfect aspects. Hence, 9 kinds of tenses are born through combining the three tenses and the three aspects.\nFrench and English belong to the same Indo-European language family and share many similarities in various respects. The main difference is that in French there is another grammatical point called mode, part of which is like the aspect in English. In terms of tenses, we will generally discuss the tenses in the indicative mode of French and will describe the others later in this section. In the following, if there is no mode qualifier before a tense, it is by default in the indicative mode. Careful identification and comparison of the subdivided tenses in the three main tense intervals, English and French, reveals a very similar usage of the tenses, as sum-marised in Table 3 . As there is no progressive tense in French, we do not distinguish the progressive tense in English, but rather merge the progressive tense into its corresponding base tense, e.g. the present perfect progressive tense into the category of the present perfect tense.\nWhen discussing tenses from a semantic point of view, the modes also need to be taken into account. The grammatical correlations between French and English modes are quite complicated. Considering the corresponding grammatical expressions of 2 modes strongly related to tense, conditionnel and subjonctif, in French rely on the usage of modal verbs, we introduce modal verbs to simplify the distinguishment of the modes.\nBased on these grammatical rules, we merge the nine common tenses in English into seven categories that correspond reasonably and rigorously to French, namely the 6 tense categories of past/present/future + simple/perfect and statements containing modal verbs that correspond to the French subjonctif and conditionnel tenses. We construct an automatic annotation method based on the spaCy package (Honnibal et al., 2020) . First, we label the grammatical components of each word in the sentence based on the spaCy package, and then we define and compare the grammatical structures of the verb phrases with the structures of each tense classification to derive the sentence tense labels. During this process, to simplify the annotation process and better correspond with French futur proche tense, we classify the expression 'be going to do', grammatically in Future tense, into the Present tense, just like expressions 'be about to do' and 'be + verb progressive', whose stucture are in Present tense but the real meaning is about the close future. Also, a sentence may have several tense structures, in this case, the tense label consists several tenses. For example, the label of the sentence 'So it is in that spirit that we have made this change.' is 'Present+PrePerfect'.\n\nCorpus Design\nWe choose the tense-rich Europarl, namely Eu-roparlPV, processed by Lo\u00e1iciga et al. (2014) as the source corpus, for it contains all the sentences with predicate verb structures in the original Europarl dataset (Koehn, 2005) . First, we cleaned the source corpus, including deleting sentences without counterparts, English sentences in the French In the construction process, with the code mentioned in Section 2, we first automatically annotated the original English text and English prediction in the 20,000 pairs of parallel utterances, given the corresponding tense labels. Then, we filtered 6,779 parallel French-English sentence triples with different tense labels for English originals and predictions. On the basis of the automatic selection, we manually screened out the representative parallel French-English sentence pairs with a certain degree of translation difficulty and a complex grammatical structure. We also corrected the reference translations that did not justify the tense or semantics. It is worth noting that the author has a level of English and French that meets the C1 standard of The Common European Framework of Reference for Languages (CEFR), representing the ability to express herself effectively and flexibly in English and French in social, academic and work situations. A total of 570 parallel pairs of statements were selected at this stage.\nFollowing this, two other reviewers at CEFR C1 level, reviewed the tense test set for semantic and tense correspondence, and the tense labels marked by the automatic annotation code. \n\nCorpus Characteristics\nIn the following paragraphs, we describe the statistical features of our corpus and the elimination of gender coordination influence.\nTense distribution. The corpus consists of 780 tense structures in 552 sentences, and the distribution of tense classifications is shown in Table 4 . In the test set, sentences in present tense are the most, corresponding the situation of the reality: we use present tense most frequently and future perfect sense least frequently.\nElimination of gender effect. Unlike English, gender coordination exists in French. For example, the French sentences 'Nous nous sommes donc abstenus.' and 'Nous nous sommes donc abstenues.' both correspond to the English 'We therefore abstained.'. That is, the MT system's ability to learn gender coordination affects its ability to recognize tense structures, which in consequence affects the maintenance of tense consistency between original French text and predicted English sentence. Therefore, to better measure the tense-predicting capability of different MT systems, rather than their ability to recognize pronominal gender, we controlled for the gender variable by defaulting all pronouns, which do not indicate explicitly their genders, as masculine. These pronouns consists of 167 je (I), 114 nous (we, us) and 28 vous (you).\n\nExperimental Results\nTo measure the tense consistency performance of different systems, we introduce a benchmark called tense (prediction) accuracy, as shown in Eq. ( 1).\nEQUATION\nwhere N c is the number of predicted utterances with the same tense as its reference and N t is the total number of utterances in the tense set.\nTo verify the validity of our tense corpus, the following approach was adopted: To begin with, 100, 000 parallel utterance pairs from the Eu-roparlTR (containing 201, 374 pairs) mentioned in Section 3.1 were extracted as the tense-rich train set, and 100, 000 parallel utterance pairs from the Europarl corpus (Koehn, 2005) were extracted as the tense-poor train set. There were no overlapping utterances between the latter and the former. We performed the same preprocessing procedure, including data cleaning, tokenization and BPE coding. We then trained four pairs of French-English NMT systems with different architectures based on fairseq (Ott et al., 2019) , where two systems in each pair differed only in the train set. After this, we summarized the scores evaluated by Sacre-BLEU (Post, 2018) and COMET (Rei et al., 2020) and tense prediction accuracies of the eight systems on different test sets. We have applied three types of test sets: our tense set, the Europarl test set and the WMT15 test set. The Europarl test set contains 3,000 parallel utterance pairs drawn from the Europarl corpus, the exact same field of train set, while the WMT15 is a test set for the WMT15 (Bojar et al., 2015) , deriving from data in the different field of train set. Besides, we also apply our approach to mesure the tense consistency performance of several business translators, includ-ing Bing Translator, DeepL Translator and Google Translator. The results are listed in Table 5: 1) The BLEU and COMET scores based on the Europarl set and the WMT15 set are quite similar for each system pair, which indicates that the translation capabilities of the two systems are similar in the general evaluation dimension. This suggests that by relying solely on the difference in BLEU scores on traditional test sets, we are unable to measure the tense prediction ability of the systems.\n2) However, there are large differences in our tense set. The tense consistency performance of systems trained on the tense-rich train set was significantly better than that of systems trained on the tense-poor train set. This indicates that our tense set can capture the tense consistency performance.\n3) Further investigation of the BLEU or COMET) scores and tense prediction accuracy for each system reveals their positive correlation for the same architecture, but not across architectures. To measure the tense consistency performance across different architectures, we should focus more on tense accuracy rather than BLEU scores only.\n\nConclusion\nWe presented the French-English parallel tense test set and introduced the corresponding benchmark tense prediction accuracy, providing a brand-new approach to measure the tense consistency performance of machine translation systems. This test set firstly focuses on the tense prediction ability, posing a new dimension to improve the MT quality.\nIn the future, we will endeavour to generalize the test set to other languages. Considering there are statements like \"the use of tense A in language X is equivalent or similar to the use of tense B in English\" in grammar books of other languages (Durrell et al., 2015) , even across language families(Gadalla, 2017) and human translators also apply such rules(Santos, 2016), we are confident in taking this forward.\n"}
{"question": "Why automated processing of Swiss German text is challenging\uff1f", "evidence": "  In most other contexts, including formal letters, laws, and newspapers, Standard German is used instead. One important reason for this is Swiss German's lack of a standardized orthography. The diversity among dialects, exacerbated by the lack of a standardized orthography, leads to a large number of written variants for each word. This, together with the small amount of text resources compared to Standard German, makes automated processing of Swiss German text challenging. ", "options": ["A. . In most other contexts, including formal letters, laws, and newspapers, Standard German is used instead. ", "B. Because Swiss German is lack of a standardized orthography.", "C. Because Swiss German is diverse among dialects, which has a large number of written variants for each word.", "D. Because Swiss German\u2019s lack of a standardized orthography, diversity among dialects, and the small amount of text resources compared to Standard German."], "answer": "D", "content": "\nIntroduction\nWe present STT4SG-350, a corpus of Swiss German speech, annotated with Standard German text at the sentence level. The corpus represents all Swiss German dialect regions and contains 343 hours of speech.\nSwiss German is a family of German dialects spoken by around 5 million people in Switzerland. It differs from Standard German regarding phonology, vocabulary, morphology, and syntax. There are significant differences among the Swiss German dialects as well, particularly regarding phonology and vocabulary. Swiss German is primarily a spoken language. It is also used in writing, but mainly in informal text messages. In most other contexts, including formal letters, laws, and newspapers, Standard German is used instead. One important reason for this is Swiss German's lack of a standardized orthography.\nThe diversity among dialects, exacerbated by the lack of a standardized orthography, leads to a large number of written variants for each word. This, together with the small amount of text resources compared to Standard German, makes automated processing of Swiss German text challenging.\nSTT4SG-350 is, to the best of our knowledge, the largest public speech corpus for Swiss German. While the primary use case is automatic speech recognition (ASR), it is also a useful resource for text-to-speech (TTS), dialect identification, and speaker recognition. By providing roughly the same amount of data per dialect region, irrespective of its population size, the corpus contributes to improving speech technology for underrepresented dialects. In addition, the test set, which contains the same spoken sentences in each dialect, allows a fair evaluation of the quality of speech technologies in different dialects. Furthermore, it contributes to more inclusive speech technology by keeping a balanced gender ratio and featuring speakers of all ages.\n\nRelated Work\nThe SDS-200 corpus (Pl\u00fcss et al., 2022) contains 200 hours of speech by around 4,000 speakers with Standard German transcripts. The recordings cover a large part of the Swiss German dialect landscape. The number of recordings per speaker follows a long-tail distribution. For example, the top 3 speak-ers account for 23% of recordings. The Swiss Parliaments Corpus or SPC (Pl\u00fcss et al., 2021a) contains 299 hours of speech in the Bernese dialect. The text is Standard German, taken from parliament minutes, and is not a fully accurate transcription. Text and audio are automatically aligned. The SwissDial corpus (Dogan-Sch\u00f6nberger et al., 2021) contains 26 hours of studio-quality recordings by 8 speakers, each speaking a different dialect, with both Standard German and Swiss German transcripts. The Radio Rottu Oberwallis corpus (Garner et al., 2014) contains 8 hours of speech transcribed in Swiss German, of which 2 are also transcribed in Standard German. The ArchiMob corpus (Samard\u017ei\u0107 et al., 2016) contains 69 hours of speech with Swiss German transcripts.\nFor Swiss German ASR, the desired output text language is Standard German for the vast majority of use cases. Tackling speech-to-text translation with an end-to-end approach is feasible as shown by Weiss et al. (2017) . Applying a similar approach to Swiss German ASR and therefore avoiding Swiss German text and its challenges altogether lead to promising results in recent years, see (Pl\u00fcss et al., 2023; Khosravani et al., 2021; Pl\u00fcss et al., 2022 Pl\u00fcss et al., , 2021a)) . Dogan-Sch\u00f6nberger et al. (2021) experiment with TTS for Swiss German. Their models achieve a 5-scale mean opinion score of 2.9 to 4.1. Importantly, their approach requires Swiss German input text.\n\nData Collection\nData for STT4SG-350 was collected in two phases: 1) the test set with 76 participants from December 2021 until March 2022, and 2) the train and validation sets with 240 participants from May until November 2022.\n\nRecording\nSpeech was recorded using a web app based on the code 1 by Pl\u00fcss et al. (2022) . Recordings are made sentence by sentence. The app displays a Standard German sentence, which the participant is asked to translate to Swiss German and speak aloud. A screenshot of the recording functionality can be found in Appendix A. The goal of the translation step is to get a correct, natural-sounding Swiss German sentence in the participant's dialect. We display a popup with examples before the first 1 MPL-2.0 license recording to explain this to participants. We also display a short explanation below the sentence to be recorded. We manually validated the correctness of at least 10 randomly sampled recordings per participant at collection time. In contrast to Pl\u00fcss et al. (2022) , for phase 2, we recorded 44.1 kHz lossless FLAC audio rather than 32 kHz lossy MP3 audio. The recording quality depends on the microphones used by participants, which range from studio microphones to headsets and laptop microphones. Depending on the microphone, mouse clicks can be audible in recordings.\n\nDialect Regions\nFor this work, we divided the Swiss German dialect continuum into 7 dialect regions, listed in Table 1 , based on the clustering method by Scherrer and Stoeckle (2016) 2 . The cluster analysis was carried out on 350 phonological, lexical, morphological, and syntactic phenomena. We slightly adjusted the resulting clusters to match the dialect regions commonly used in public discourse more closely. The goal of these adjustments was to make it more intuitive for participants to choose their dialect region. The borders are intentionally fuzzy to give participants the freedom to choose the region that fits their dialect best.\n\nSentence Selection\nSentences were randomly selected from Swiss newspapers and from parliament minutes of 2 Swiss parliaments. Sentence filtering for newspapers follows Pl\u00fcss et al. (2022) . The goal of the filtering is to limit sentence complexity to reduce errors in the translation task. For example, only sentences of 5 to 12 words are kept. The newspaper sentences cover a broad range of topics, including culture, finance, science, sports, and technology. They also cover content and named entities particularly relevant for Switzerland. Parliament sentences are not filtered. They bring additional diversity to the corpus with longer sentences on average and a distinct vocabulary. For the test set, 3,515 sentences were selected (67% newspapers, and 33% parliaments). To allow a fair comparison among the dialects, each sentence was recorded in each of the 7 dialects. For the training and validation data, 94% news and 6% parliament sentences were selected, and we dropped the requirement to record each sentence in all dialect regions to in-crease vocabulary and phrase diversity.\n\nMetadata\nParticipants self-reported the following metadata:\n\u2022 The dialect region that best fits the participant's dialect. \u2022 The zip code of the place where the participant grew up or went to school. \u2022 Age group (< 19, 19-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89 , > 89) \u2022 Gender (female, male, non-binary) We manually checked the correspondence of reported metadata and recordings for each participant. Collecting the dialect provenance as a zip code allows us to investigate dialects and the performance of speech technologies for them at different granularity levels. Collecting age group and gender helps to make sure that speech technology is inclusive and works across different demographic groups.\n\nRecruitment\nFor the test set, all participants were recruited via the crowdsourcing platform TestingTime 3 . For the train set, half the participants were recruited via TestingTime, whereas the other half were recruited via universities, high schools, newspaper ads, personal contacts, and the crowdsourcing platform seniors@work 4 (for details refer to Appendix F and 6). Only native Swiss German speakers able to correctly translate Standard German to Swiss German were recruited. The goal was to collect the same amount of recordings in each dialect region and we recruited accordingly. The number of recordings per participant was limited to 368 for the test set 5 and 1,112 for the train data. Recruiting the 316 participants required a considerable effort, especially in the low-population regions GR and VS.\n\nCorpus\nThe corpus is publicly available 6 \n\nData Cleaning\nFiltering. Recordings with a duration of less than 2 seconds were removed. Silent recordings were also removed. For the test set, we applied heuristics to flag incomplete sentences, which were removed after double-checking them. We only kept sentences with a recording in all dialect regions in the test set. In total, we filtered out 1.5% of recordings.\nValidation. We validated each speaker manually.\nFor this, we randomly sampled 10 recordings from each speaker, and checked whether the dialect is correct, the recording is in Swiss German, the translation is correct, and whether the sound quality is high enough. All of the participants passed the manual check.\n\nStatistics\nThe provided by 316 different speakers, of which 51% identified as female and 49% as male. No speaker identified as non-binary. Figure 1 shows the distribution of the recordings over the age groups, as well as the gender distributions per age group. The age groups from the thirties to the sixties are well represented, while the twenties are overrepresented and the teens as well as seventies are underrepresented. The age groups eighties and above are not represented at all. Table 1 shows the corpus statistics per dialect region. While the German-speaking population differs by a factor of up to 16 between regions, the number of recordings per region is a lot more balanced, differing by a factor of not more than 1.2.\n\nSplits\nTable 2 shows the different corpus splits. We provide training, validation, and test splits. There is no speaker overlap between training, validation, and test. There are no common sentences between test and either training or validation. There is, however, an intersection of 835 sentences between training and validation. There are 2 different training splits. train_all contains all training data, 276 hours of speech. train_balanced is a subset of train_all with 239 hours of speech that is balanced in the number of recordings per dialect region. For GR, the region with the fewest recordings, the recordings of all speakers are included in train_balanced. For the other regions, we randomly chose speakers and added their recordings until the number of GR recordings was reached. train_balanced includes 33-35 hours of speech, 24,088-25,183 recordings, and 25-32 speakers per region.\nLike train_balanced, the validation split, with 34 hours of speech, is balanced in the number of recordings per dialect region. We randomly chose 3 speakers per region with at least 1,000 recordings. The test set comprises 34 hours of speech. Importantly, the same 3,515 sentences were recorded in all 7 dialect regions to allow a fair comparison between different dialects. equate speaker diversity in each region. For this reason, the mean number of recordings per speaker is markedly lower than in the other splits.\n\nAutomatic Speech Recognition Baseline\nWe train a baseline model to demonstrate the use of the STT4SG-350 corpus for Swiss German ASR. We fine-tune XLS-R (1B) 8 (Babu et al., 2021) on the train_balanced split. XLS-R is a model based on wav2vec 2.0 (Baevski et al., 2020) with 965 million parameters pretrained on 436K hours of unlabeled speech data covering more than 128 languages. Swiss German was not part of the training data. We provide the fine-tuning details and experimental setup in appendix C. We report the results of our fine-tuned model on three publicly available Swiss German datasets and the STT4SG-350 validation and test sets in Table 3 . The model achieves state-of-the-art results on the All Swiss German Dialects Test Set (ASGDTS) (Pl\u00fcss et al., 2021b) and SDS-200 (Pl\u00fcss et al., 2022) , and improves the best reported BLEU scores on the test sets by 43% and 9%, respectively. Our model is 6% behind the best reported BLEU score on the SPC test set (Pl\u00fcss et al., 2021a) . These results highlight the benefit of the STT4SG-350 dataset on test data from different domains.\n\nConclusion\nWe have described STT4SG-350, which is, to the best of our knowledge, the largest public speech corpus for Swiss German with 343 hours of speech. Our ASR baseline model trained on the corpus achieves a BLEU score of 74.7 on the test set. In addition, it beats the best published BLEU scores on 2 other test sets, demonstrating the quality of the corpus. STT4SG-350 is balanced across the 7 dialect regions, and the test set allows a fair comparison of ASR performance on different dialects. We intend to take advantage of these properties in future work and conduct in-depth experiments to explore differences in ASR quality between dialects. Subsequently, we want to find ways to improve performance for underrepresented dialects.\n"}
{"question": "What does the article of Bamman et al. in 2014 contain?", "evidence": "DiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that  by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.", "options": ["A.  The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper.", "B. The article empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.  ", "C. The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. ", "D. The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" ", "DiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.  "], "answer": "C", "content": "\nIntroduction\nComputational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period (Lazer and Radford, 2017; van Atteveldt and Peng, 2018) . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" (Boydstun et al., 2014) . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.\nThe conclusion of this reasoning, and the dominant stance in computational social science, is a reliance on machines alone to answer questions in the field, surrendering to their supposed objectivity * Equal contribution.\nor impartiality. Can a machine's output go beyond descriptive catalogs of evidence, accelerating understanding of processes and motivations? From our experience, computers are nowhere near supplanting humans in interpreting social science results.\n1 An interdisciplinary inquiry must go farther than matching computational techniques to social science questions (O'Connor et al., 2011; Nguyen et al., 2020) . It embraces synergistic methodology and connects the norms and standards of evidence from both. This means partnering computer science's preference for the structured, generalizable, and objective with the unstructured, critical, and contextual which the social sciences champion. This level of interdisciplinarity addresses the question raised by descriptive findings: So what?\nWe see theory as the solution, empowering rather than shackling investigations. What this paper advocates is not one particular theory-certainly these are myriad, and \"even subject matter which has been under intensive and prolonged study remains at the unsettled periphery of research\" (Nagel, 1963) . Instead, we expand on our prior work (Dore and McCarthy, 2022) to clarify calls echoed for decades by computational and social science (McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . Underlying each, we find, is the urge to return to theory, which we espouse herein.\n\nDescription vs. Integration\nWe contrast descriptive findings and theoretical analysis. An example of a descriptive finding is that an apple falls, or that it falls faster when pushed than dropped, or even that it falls at a particular rate estimated with some standard error by a complex interpolation. A theoretical analysis of the same phenomenon, credited to Newton, is that a fundamental force acts upon the apple, and that this same force governs the motion of the heavens. The theoretical analysis links the finding about the world critically to a broader body of knowledge and context.\nDespite advances in causal inference in NLP, the descriptive is all that a machine can provide to the social sciences (Feder et al., 2021) . Certainly the methods of computational text analysis have advanced since the General Inquirer (Stone and Hunt, 1963) and Mosteller and Wallace's statistical inference of text authorship (1963) . But methods are means, not ends. They uncover more descriptive findings in data: the rate of an apple's fall, the topics of refugees' tweets (Walk et al., 2022) , the space given to marginalized groups in textbooks (Lucy et al., 2020) , or patterns of state censorship (Bamman et al., 2012; King et al., 2013) .\nThe foils to descriptive findings are integrative findings (Hofman et al., 2021) , which offer causal explanations that enable future predictions-a theory, or as a 'model' in the sense of the Standard Model, rather than of a statistical model. Integrative findings can either offer new theories or couch their explanations in existing theories-but the theory is essential either way.\n\nWe Don't Integrate\nTo contrast descriptive and integrative findings, we reviewed approximately 60 papers in computational text analysis published in *ACL venues. In Table 1 , we describe several of these in terms of their descriptive or theory-grounded contributions.\n2 Descriptive papers may refer to social science theories or make generalizable claims, as when Demszky et al. (2019) write, \"The shooter's race appears to play a role in topic preference: if the shooter is white, Democrats become more likely to focus on shooter's identity,\" but they do not link to the two to each other. An excellent theory-grounded quantitative work is Nelson (2021) ; she confirms some of the most compelling features of identity theory, specifically that identities based on race were most distinguished by cultural discourse, whereas those based on gender by the domestic and the economic discourse. Similarly, we conducted theory-grounded quantitative work to investigate the application of the protest paradigm and thematic framing in how westernand Hong Kong based newspapers portray protests in Hong Kong (McCarthy et al., 2021; McCarthy and Dore, 2022) . Generally, it remains challenging to find computational social science papers in *ACL venues that go beyond description and prediction, advancing theory. Why is this? We believe it stemmed from the field's \"empirical turn\".\n3 Few remember when the meetings of ACL offered a few dozen papers, all entrenched in formalisms and linguistic theories. Arguably, 1996 was a turning point when the founders of SIGDAT held the first EMNLP at Penn under the auspices of the ACL. 4 This gave a spotlight to the few but growing empiricists in the field and drew in more.\nEMNLP began a half-decade of measurable reorganization the field (Anderson et al., 2012) . That EMNLP remains affiliated with ACL keeps the language-focused machine learning practitioners in our tent. The slow blurring of boundaries between each *ACL conference's expectations (Church, 2020) increases this unity. Both groups belong under this tent. But without a doubt, one group's voice is becoming less heard.\nPublication venues within the ACL focus on methods over theory. 5 Techniques are taken off the shelf without critical examination because these are \"the best\" (often \"state of the art\") for their purposes (Ethayarajh and Jurafsky, 2020) . This widens the gap between theoretical and empirical work. 6 Hopkins and King (2010) claim, \"computer scientists may be interested in finding the needle in the haystack. . . social scientists are more commonly interested in characterizing the haystack\"-evincing the value of broader context. 7 Wallach (2018), quoting Hopkins and King, explains that the two groups 3 A lesser reason is the challenge of serving two masters: adequately covering both the theoretical and methodological components within 8 pages. We recently received two reviews for an *ACL submission: one advocating for more of the social science context in the main text by eschewing methods to the appendix, and the other instructing us to do the opposite. 4 And its predecessor the Workshop on Very Large Corpora. 5 This is due to the outsized influence of computer science, often seen as the science of method (Hoare and Jones, 1989; Shapiro, 2001) , when not instead seen as an engineering discipline (Rapaport, 2005).\n6 A related criticism is that empirical research has narrowed to focus on 'easy' questions that its tools can address (Coleman, 1986; Baden et al., 2021) , especially when research questions are baked into the design of the task. 7 As evidence, see Siegel (2018) : \"We usually don't know about causation, and we often don't necessarily care. . . the objective is more to predict than it is to understand the world. . . It just needs to work; prediction trumps explanation.\" Descriptive Chang et al. (2009) The article presents new quantitative methods to measure semantic meaning in inferred topics. The authors emphasize the qualitative relevance of their findings as it validates the use of topics for corpus exploration and information retrieval. However, their working hypothesis and empirical findings are not connected to the extremely relevant field of communication theory. Bamman et al. (2012) The article presents the first large-scale analysis of political content censorship in social media. The authors miss the opportunity to relate their hypothesis and findings to censorship theory, a natural theoretical context for the research, which would strengthen the relevance and generalizability of the findings. Field et al. (2018) The article discusses media manipulation in Russia in the context of agenda-setting and framing, the tools that Russian state-owned (or heavily influenced) media outlets use to distract public attention from domestic economic politics. The authors implicitly refer to propaganda theory and autocratic theory throughout the article even though their findings are not discussed in relation to these theories. Demszky et al. (2019) The article applies \"a more comprhensive NLP framework to study linguistic aspects of polarization in social media\". While the article implicitly refer to theories of social conformity and social conflict, the findings are not linked or discussed (either explicitly or implicitly) to the theoretical frameworks that the authors touch on in their \u00a71.\n\nIntegrative\nDiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.are interested in very different research  questions, and that computational social science must be more than computer science with social data; it must strive for valid explanatory models. In the same vein, at ACL 2022, ACL fellow Eduard Hovy remarked that NLP must be more than \"just machine learning on corpora\". Social scientists are also coming to terms with the meaning of computational techniques applied more often in social science (Bail, 2014; Biernacki, 2015; Lee and Martin, 2015; Spillman, 2015) . The focus of the debates, however, is on which methods are best suited to extract meaning from text, without addressing any theoretical considerations related to the methods or whether a theoretical framework for those methods even exists. The discussions on whether computational methods make social science research more efficient, reliable, and reproducible overtake attempts at theory-building.\n\nMoving Forward\nWe are not denying the value of computational approaches to analyzing text. Certainly, comput-ing can be an instrumental approach for modeling and understanding social complexity. This does not mean that other approaches, such as historical, ethnographic, or mathematical, become irrelevant. On the contrary, computational methods necessarily (whether awarely or not) rely on these earlier approaches to add value, in terms of improving our explanations and understanding (Radford and Joseph, 2020) .\nAs we are a field that prioritizes methods, consider the seminal book on methods in science: Abbott ( 2004) taxonomizes scientific ways of knowing. Its five broad categories are ethnography, historical narration, standard causal analysis, small-N comparison, and formal modeling. We in NLP myopically choose the third and fifth of these, ignoring the value of the others. But the broader point of Methods of Discovery is not methods. It is the research question. Any methodology should be grounded in the question, not incremental tweaks and reviewers' comfort (Church, 2020) . This admits even qualitative or mixed-method approaches to text analysis.\nThe role of humans in scientific inquiry is nothing new. Using qualitative analysis to complement quantitative techniques has its roots in Achen and Snidal (1989)'s recommendation to use historical case studies as a complement to statistical research.\n8\nTheir plea was strengthened by Verba's work in the early 1990s (Verba et al., 1993 (Verba et al., , 1995;; Verba, 1996) and Tarrow (1995) , who openly called for bridging qualitative and quantitative modes of research in social science. In doing so, they have enriched the field with critical methodological innovations (Gerring, 2004) , benefiting from the recognition that \"quantitative methods must augment humans, not replace them\" (Grimmer and Stewart, 2013, 4) .\nThe field can draw more from social science's rich tradition of inductive theory-building and interpretation to develop its theoretical approach-to prize either induction or deduction alone is a myth of scientific procedure (Thagard, 1988) , but the melding of the two opens new doors. Rather than eschewing the complexity (a criticism leveled by Baden et al., 2021) , it should put complexity at the center of its ontology on the basis that there are no immutable laws in social life or optimal solutions to social problems.\nSkepticism can linger toward findings not drawn from the standard practices of one's own field; indeed, social science was long skeptical of computational contributions (Armstrong, 1967) . We believe that this drives the hyperfocus on improving a few accepted methods instead of exploring more broadly. If the doorway between disciplines is only narrowly open, this reflects a lack of appreciation for each field's ways of knowing. The disciplinary divide keeps computational researchers from embracing methods beyond standard causal analysis or formal modeling, so the interpreter-centric richness allowed by histories, ethnographies, and small-N exploration are precluded.\n\nConclusion\nWe have explained the distinction between descriptive and theoretical findings as it pertains to computational text analysis. The bulk of work we found provided vast descriptive findings, often of high quality, but not giving back to questions of theory. We offer several suggestions on how to 'push the pendulum back' by prioritizing theory-building or theory-affirming research questions and accepting whichever methods are best suited toward answering it-not only the familiar and entrenched ones.\nWe are not the first to advocate for a shift in the patterns of applying computational techniques to real-world problems. There is a steady drumbeat from voices in the field advocating careful approaches (Nagel, 1963; McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . What we see underlying all of thesethose writing against 'mathiness' and speculation, advocating for clear evaluation over anecdotes, criticizing textual researchers' dilution of conceptual standards, highlighting work that ties linguistic information into complex models-is an unspoken, perhaps unrealized, call for a return to theory.\nNot only do we aver that incorporating theory is essential; but also, other fields have strengthened themselves when espousing organizing principles beyond those of their progenitors. Behavioral economics is a success story here. It transcended the neat (but psychosocially stripped) mathematics it draws from to acknowledge deviations from rationality and blend economics with cognitive science (Kahneman and Tversky, 1979; Thaler, 1980; Thaler and Sunstein, 2009) .\nFor scientific-not simply engineeringadvances to arise from the *ACL community, authors and reviewers alike must resist the temptation toward incremental, 'safe' research and follow Church (2005) : \"Controversial papers are great; boring unobjectionable incremental papers are not.\" In reviewing new research, we should privilege not only work that presents new and unusual computational methods, but also interactions between computational and humanistic approaches to answering research questions. EMNLP was founded because of reviewing biases at ACL against groundbreaking methodological advances, and since then the two have homogenized; \"EMNLP reviewing is no longer much of a differentiator\" (Church, 2020) . We found that theoretically grounded findings in text analysis are often published in non-*ACL venues (Table 1 ), but ACL sets the standard for work involving computational text analysis and NLP. Is there no home for groundbreaking integrative or interdisciplinary work in *ACL, such that a new venue is required? Or can we adapt our standards to invite deeper connections to theory and new ways of knowing?\n"}
{"question": "What are the three modeling interventions studied in the experiment?", "evidence": "  We study three modeling interventions-using natural language prompts, increasing pre-trained model size, and pre-training on more data-on 14 total datasets spanning natural language inference (NLI), sentiment analysis, and extractive question answering (QA). In our natural language inference (NLI) experiments, we use MultiNLI (Williams et al., 2018) , SNLI (Bowman et al., 2015), and MedNLI (Romanov and Shivade, 2018) .   ", "options": ["A. Increasing model size, fine-tuning hyperparameters, and using different training datasets.", "B. Using natural language prompts, increasing model size, and pre-training on more data.", "C. Fine-tuning hyperparameters, using natural language prompts, and adding more layers to the models.", "D. Using MultiNLI,SNLI, and MedNLI."], "answer": "B", "content": "\nIntroduction\nNLP models perform well when evaluated on data drawn from their training distribution (indistribution / ID), but they typically suffer large drops in performance when evaluated on data distributions unseen during training (out-of-distribution / OOD; Blitzer, 2008) .\nHow does exposure to ID training examples affect the ID-OOD gap? If two models have the same ID performance, will models trained on fewer ID examples (higher sample efficiency) also have higher OOD performance (higher robustness)? At one extreme, zero-shot models will not learn IDspecific patterns because they are not exposed to any labeled ID examples. Similarly, few-shot models trained on very few ID examples may also rely less on ID-specific patterns; if a model never sees the token \"cat\" while training on SNLI, then it will not learn that its presence is spuriously predictive of the contradiction label (Gururangan et al., 2018; Utama et al., 2021) . Supporting this intuition, recent work in image classification (Radford et al., 2021) and extractive question answering (Awadalla et al., 2022) show that zero-shot inference and fewshot fine-tuning improve average robustness across a range of OOD test sets. However, it is unclear how universal these trends are across various tasks and methods for reducing exposure to ID examples, or how predictive they are for any individual test set of interest. Figure 1 illustrates this central question.\nWe conduct a broad empirical study over 14 datasets across three tasks to investigate the relationship between exposure to ID training examples (sample efficiency) and robustness. We experiment with three modeling interventions that improve sample efficiency: (1) using natural language prompts for zero-shot prediction and during finetuning (Brown et al., 2020; Schick and Sch\u00fctze, 2021; Gao et al., 2021) ; (2) fine-tuning models of increasing size; (3) fine-tuning models pre-trained on increasing amounts of data.\nWe find that higher sample efficiency is only sometimes correlated with better robustness, and the effect of specific modeling interventions varies by task. For example, increasing pre-trained model size substantially improves sample efficiency and results in higher average robustness in sentiment experiments, but these sample efficiency gains do not translate to higher average robustness in NLI and extractive QA experiments. On individual datasets, models with better sample efficiency can even be less robust (e.g., increasing model size when training on SST-2 and evaluating OOD on IMDb).\nOverall, these results indicate that general- purpose methods for improving sample efficiency are far from guaranteed to yield significant OOD robustness improvements-their success is highly dataset-and task-dependent. Furthermore, even in this era of large, multi-purpose pre-trained language models, task-specific decisions are often necessary to achieve OOD generalization.\n2 Measuring Sample Efficiency and Robustness. (3) M 's performance on examples from D ood (i.e., the OOD performance).\nLet M 1 and M 2 be two models with equivalent performance on held-out ID data. If M 1 was trained on fewer ID examples than M 2 , then it has higher sample efficiency. If M 1 has higher OOD performance than M 2 , it has higher effective robustness (henceforth \"robustness\"; Taori et al., 2020) . Comparing models with equivalent ID performance controls for its effect on OOD performance, since improving ID performance usually yields commensurate improvements on OOD performance-in this study, we focus on OOD performance improvements beyond what is expected from ID gains.\nSatisfying this equivalent-ID constraint is often difficult in practice; given an arbitrary model M 1 and its corresponding ID performance, it is difficult to produce a different model M 2 with identical ID performance. Rather than explicitly training models to identical ID performance, we train models on varying-size subsamples of a given ID dataset and interpolate between the results to estimate (1) the number of labeled ID training examples necessary to achieve a particular ID performance (sample efficiency) and (2) OOD performance, given ID performance (robustness). These interpolated curves approximate the ideal setting of training a model for every possible ID value. Figure 1 provides a schematized example, with model B having better sample efficiency and robustness than model A.\n\nExperimental Setup\nWe study three modeling interventions-using natural language prompts, increasing pre-trained model size, and pre-training on more data-on 14 total datasets spanning natural language inference (NLI), sentiment analysis, and extractive question answering (QA). See Appendix A for further details about experimental settings.\nTasks and Datasets. In our natural language inference (NLI) experiments, we use MultiNLI (Williams et al., 2018) , SNLI (Bowman et al., 2015), and MedNLI (Romanov and Shivade, 2018) . For sentiment analysis, we use IMDb reviews Maas et al. (2011) , SST-2 (Socher et al., 2013) , and reviews from the \"Movies and TV\" subsection of the Amazon Reviews corpus (Ni et al., 2019) . Lastly, for extractive question answering, we use SQuAD (Rajpurkar et al., 2016 ), NaturalQuestions (Kwiatkowski et al., 2019) , TriviaQA, BioASQ (Tsatsaronis et al., 2015) , and the four SQuAD-Shifts test sets (Miller et al., 2020) .\nModeling Interventions. To understand the effect of a particular modeling intervention on sample efficiency and robustness, we evaluate pre-trained models that differ only along the axis of interest (e.g., model size or fine-tuning method). Since the optimal fine-tuning hyperparameters depend on the ID training dataset size, we separately tune hyperparameters for each model on each training dataset subsample size, taking the models that achieve the best held-out ID performance for each setting. See \n\nResults and Discussion\nOur results show that models with higher sample efficiency may not necessarily have higher average OOD robustness-different tasks and modeling interventions affect robustness in different ways . For example, prompt-based fine-tuning consistently improves both sample efficiency and average robustness, but only in low-data settings (Figure 2 ). In contrast, increasing model size improves sample efficiency across the range of training dataset sizes and tasks, but only improves average robustness on sentiment analysis (Figure 3 ). On individual datasets, we even observe cases where models with lower sample efficiency have higher robustness (Figure 3d ). See Appendix C for full results on every ID-OOD setting.\nNatural Language Prompting. We compare BERT BASE models using (1) standard fine-tuning, (2) prompt-based fine-tuning, and (3) zero-shot prompting. We also compare these results with zero-shot prompting of text-davinci-001, a much larger model trained on substantially more data. We run experiments on NLI and sentiment analysis, since extractive QA is not amenable to prompt-based fine-tuning with masked language models.\nFigures 2a and 2b plot the average performance on all OOD datasets as a function of ID performance and the ID performance as a function of the number of labeled training examples. Sample efficiency improvements from prompt-based finetuning also translate to higher average robustness. However these improvements only apply in the few-shot setting. As the size of the training dataset increases, the improvements in sample efficiency and average robustness steadily diminish. When using sufficiently large training datasets, models trained with prompt-based fine-tuning yield essentially the same sample efficiency and robustness results as standard fine-tuning (\u223c1K examples for NLI, \u223c130 examples for sentiment).\nHowever, results on individual OOD test sets can significantly differ from averaged-OOD trends. For example, Figure 2c shows that prompt-based fine-tuning on MNLI and evaluating on SNLI improves sample efficiency in the few-shot setting but without any robustness improvements.\nSurprisingly, we also find that zero-shot inference does not necessarily improve average robustness over prompt-based fine-tuning-zero-shot performance lies on or below the trend line formed by prompt-based fine-tuning, despite not using any ID-specific data at all. See Appendix C.1 for full results of increasing pre-trained model size for every ID-OOD setting.\nIncreasing Pre-Trained Model Size. We run experiments with the checkpoints of Turc et al. (2019) , who pre-train BERT models with various numbers of transformer layers (L) and hidden embedding sizes (H). We run experiments on NLI, sentiment analysis, and extractive QA to compare pre-trained models of five sizes: (1) Large (L=24, H=1024), ( 2) Base (L=12, H=768), (3) Medium (L=8, H=512), (4) Small (L=4, H=512), and\n(5) Tiny (L=2, H=128). Although increasing the pre-trained model size improves sample efficiency on every task, it does not always improve average robustness (Figure 3 ). In particular, increasing model size minimally affects average robustness in NLI and extractive QA (Figure 3a ,3c), but substantially improves average robustness on sentiment analysis (Figure 3b ). 4a,b ). In extractive QA experiments, varying the amount of pre-training data does not significantly change average robustness (Figure 4c ). Again, we find that results on average OOD performance are not predictive of results on individual test sets-despite unchanged average OOD robustness when pre-training on more data, OOD performance can be higher on individual extractive QA test sets (e.g., SQuAD \u2192 BioASQ; Figure 4d ). See Appendix C.3 for full results of pre-training on Figure 4 : Pre-training on more data is an effective method for improving sample efficiency, but these sample efficiency improvements are not always accompanied by robustness improvements. In NLI and sentiment analysis experiments, these sample efficiency gains correlate with improved average robustness (a,b). However, there are no average robustness gains in extractive QA (c). Despite no average robustness improvement in extractive QA, pre-training on more data can still improve robustness on particular test sets (e.g., BioASQ; d). more data for every ID-OOD setting.\n\nConclusion\nWe study the relationship between sample efficiency and robustness across three tasks and three modeling interventions, finding that sample efficiency improvements often fail to translate to improved robustness. As larger models quickly become more sample efficient, our results caution that sample efficiency and robustness are different axes of improvement and that optimizing for sample efficiency will not necessarily always yield robustness gains.\n"}
{"question": "What do we wanna find in this research?", "evidence": "In this work, we aim to answer the question: does ChatGPT solve the problem of zero-shot DST? We show that crafting intuitive natural language prompts is sufficient to achieve state-of-the-art performance with ChatGPT, exceeding conventional, engineering-heavy approaches to zero-shot DST by a large margin. However, despite our findings, we argue that properties inherent to general purpose models inhibit their ability to simply replace specialized systems. We speculate that while in the foreseeable future general purpose models may not become holistic solutions to complex problems, they will provide ample opportunities to empower specialized systems to go beyond their pre-defined scopes, enable on-the-fly extensibility and generation of high quality training data by zero-shot synthesizing or automatic labeling.", "options": ["A.In this work, we aim to answer the question: does ChatGPT solve the problem of zero-shot DST? ", "B.We show that crafting intuitive natural language prompts is sufficient to achieve state-of-the-art performance with ChatGPT, exceeding conventional, engineering-heavy approaches to zero-shot DST by a large margin. ", "C.However, despite our findings, we argue that properties inherent to general purpose models inhibit their ability to simply replace specialized systems. ", "D.We speculate that while in the foreseeable future general purpose models may not become holistic solutions to complex problems, they will provide ample opportunities to empower specialized systems to go beyond their pre-defined scopes, enable on-the-fly extensibility and generation of high quality training data by zero-shot synthesizing or automatic labeling.\n"], "answer": "A", "content": "\nIntroduction\nDialogue state tracking (DST) is a critical component for task-oriented dialogue systems. Its purpose is to extract and track user's goals throughout a conversation (Young et al., 2010) . DST is challenging due to the infinite possibilities of user/agent conversations, and because services and schemas/APIs that dialogue systems interface are subject to constant change (Ren et al., 2018) . Although traditional approaches achieve high accuracy when operating on a pre-defined set of concepts called an ontology (Mrk\u0161i\u0107 et al., 2017; Liu and Lane, 2017; Zhong et al., 2018) , ongoing research explores transfer to new domains with little to no additional learning (Rastogi et al., 2020) using ontology independent architectures to allow seamless adaptation to out-of-ontology concepts.\nMany strategies for zero-shot transfer to unseen domains have been proposed. Li et al. (2021) treat DST as a question answering (QA) task by leveraging data augmentation. Zhao et al. (2022) propose DST by relying on schema descriptions while Heck et al. (2022) utilize natural language descriptions to facilitate zero-shot transfer. Gao et al. (2020) and Lin et al. (2021) suggest learning from non-dialogue QA data which are available in large amounts to improve generalization. Campagna et al. (2020) harness large synthesized data based on abstract dialogue models. However, none of these techniques are ideal solutions. Fine-tuning is challenging due to computational costs, risk of over-fitting and the need for expensive (Budzianowski et al., 2018) task-specific data. Cross-task transfer still requires curated data and careful consideration of suitable learning tasks. Data augmentation requires high level task knowledge and an adequate synthesizing strategy.\nA new generation of large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Glaese et al., 2022) comes with the promise to be equipped to solve any task without task-specific fine-tuning, but solely with world knowledge they acquired during self-training on massive amounts of data. Such LLMs have been shown to perform remarkably well on in-context learning (ICL) , where only a natural language prompt and examples are provided to condition the generation process, achieving significant improvements over fine-tuned approaches in few-shot setups (Brown et al., 2020; Wang et al., 2022) . ChatGPT (Ope-nAI, 2022) -trained using human feedback and reinforcement learning -is the most recent of such models and single-handedly solves an array of challenging natural language processing (NLP) tasks with super-human capabilities, all through a natural language dialogue interface.\nIn this work, we aim to answer the question: does ChatGPT solve the problem of zero-shot DST? We show that crafting intuitive natural language prompts is sufficient to achieve state-of-the-art performance with ChatGPT, exceeding conventional, engineering-heavy approaches to zero-shot DST by a large margin. However, despite our findings, we argue that properties inherent to general purpose models inhibit their ability to simply replace specialized systems. We speculate that while in the foreseeable future general purpose models may not become holistic solutions to complex problems, they will provide ample opportunities to empower specialized systems to go beyond their pre-defined scopes, enable on-the-fly extensibility and generation of high quality training data by zero-shot synthesizing or automatic labeling.\n\nBackground\nDialogue state tracking is tasked to (1) determine for every turn t in a dialogue {(U t , M t )} T 1 with U t and M t being current user and preceding system utterance whether any of the slots in S = {S n } N 1 is present, to (2) predict values for each S n and to (3) track the dialogue state\nDS t \u2200t \u2208 [1, T ]. The DS is cumulative, i.e., DS t = update(DS t\u22121 , DS t ) is updated given the predictions of slot-value updates DS t .\n\nChatGPT\n(OpenAI, 2022) is a dialogue agent (Leike et al., 2018) , and in its core a GPT-3.5 LLM fine-tuned on human-written promptresponse pairs followed by reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020) . RLHF utilizes a reward model trained on human feedback to improve generation quality and adequacy via proximal policy optimization (Schulman et al., 2017) \n\nZero-shot DST with ChatGPT\nOur investigative approach to zero-shot DST with ChatGPT differs considerably from related works. We decode dialogue state updates with a general purpose model, without undergoing any parameter updates. Consequently, we neither employ data augmentation nor cross-task transfer learning. Instead, we solely rely on the general capacities of ChatGPT as an aligned dialogue agent. We take a most rigorous approach to zero-shot transfer where we do not allow the provision of any examples, nor of a formal task definition. Instead, we only permit natural language explanations of what the model is supposed to do. This sets our investigation apart from the closely related IC-DST (Hu et al., 2022) .\nIn zero-shot DST, the set of slots S relevant during inference and the set of slots S \u2032 seen during training of the model X \u03b8 with parameters \u03b8 are disjoint, i.e., S \u2229 S \u2032 = \u2205. Further, it may be S \u2032 = \u2205, in which case \u03b8 is not specifically tuned towards solving DST. This is precisely the case for Chat-GPT in our setup. Our approach to zero-shot DST with ChatGPT is formalized as follows. Let\nA 1 =P \u2295 \"system\":M 1 \u2295 \"user\":U 1 , A t =\"system\":M t \u2295 \"user\":U t , \u2200t \u2208 [2, T ],\nwhere P is the task description which provides the model with instructions for how to process a dialogue between a system M and a user U . A 1 is the initial prompt to ChatGPT. A t\u22652 are the follow-up prompts, only containing a single turn-pair of the dialogue of interest. ChatGPT is particularly suitable for this strategy due to its chat based interface. ChatGPT generates its next output B t conditioned on the current prompt A t\u22121 , as well as all preceding user queries and system responses of the same chat. The dialogue state update DS t can be found in B t , but may not be directly interpretable as such due to the diversity in the output surface forms. Thus, we require a normalization operation DS t = normalize(B t ). In contrast to (Hu et al., 2022) , we do not condition B t on DS t . This renders the task even more challenging, as Chat-GPT is forced to solve complex subtasks such as coreference resolution -the case where a newly encountered slot refers to the value of another slot -solely given the initial prompt and its own latent dialogue state given the dialogue history.\n\nExperiments\nAt the time of conducting our experiments, Chat-GPT is a proprietary research preview accessible for free via a web interface 1 . We used the Jan 9 version of the model. We use a regular expression term to extract all parts that are JSON formatted. We form DS t by accumulating all predicted updates up to turn t.\nEvaluation. We evaluate on the 1000 dialogues of the MultiWOZ 2.1 (Eric et al., 2020) test split and use joint goal accuracy (JGA) to compare methods. For a fair judgement of the ChatGPT predictions, we follow the evaluation procedure of Heck et al. ( 2020). We process each dialogue once and refrain from using ChatGPT's regeneration feature.\nPrompt. We imposed restrictions that the taskdefining prompt P be intuitive natural language and provides no formal schema. The crafting process involves simple trial-and-error on fewer than 10 held-out dialogues from the MultiWOZ training set. The design process was guided by the intention to imitate the behavior of a triple copy strategy (TripPy) DST (Heck et al., 2020) . P consists of three parts. First, a list of names for detectable informable slots along with natural language descriptions. The slot names help us extract a DS t that is compatible with the dataset's labels. Second, a sparse list of slots that are categorical, along with their value candidates for (1) aiding normalization of values that are expected to show high variability in expression, and (2) modeling Boolean slots.\nThird, an informal task description. 2\n4.1 ChatGPT vs. Supervised SOTA Comparing ChatGPT's performance to state-of-theart supervised approaches that achieve close to 60% JGA is not a fair fight 3 , and yet we observe an impressive 31.5% zero-shot JGA. This result is double-edged; on the one hand it is evidence that ChatGPT is capable of DST 4 , and on the other hand is no match for specialized systems. The comparison to TripPy, a SOTA supervised model, allows us a more fine-grained analysis. In Figure 1 , slot filling performance is broken down into value types. We observed that ChatGPT underperforms in non-trivial cases, namely refer, where a newly encountered slot refers to the value of another slot, and inform, where a slot-value was mentioned by the system and confirmed by the user. ChatGPT shows slight underperformance for Boolean slots. Remarkably, performance for values that are extracted directly from user utterances -the most relevant category in terms of frequency - (Hu et al., 2022) was the first successful attempt at pseudo 5 zero-shot DST via ICL. Our preliminary results with ChatGPT are on par, which is remarkable for the following reasons.\n(1) Our prompt is non-schematic and without examples, (2) our task-defining prompt is stated only once at the beginning of the chat, and (3) we do not maintain a DS to serve as additional input at each turn. The heightened zero-shot performance of IC-DST can be mainly attributed to these points.\n\nError Analysis\nWe identified a set of recurring errors that are likely caused by either the content of P or by the model's inherent properties. See ChatGPT is a sophisticated dialogue agent that, via alignment with human judgements, is capable of understanding context and intent of a multi-turn conversation far beyond the capacities of the previous generation of LLMs. This makes it well-suited for DST. Our results demonstrate that even with intuitive natural language prompts, a complex task such as DST can be solved exceedingly well without any form of additional learning. While specialized systems can exert control over its input-processing and output-generation to arbitrary degrees, this is not the case for Chat-GPT. Even with the most rigorous and schematic prompts, there can be no guarantee that the model interprets the input as intended or generates the output as required, which may lead to unexpected behavior. Furthermore, there is no guarantee that behavior is consistent across a series of similar inferences, such as in our experimental evaluation. In terms of deployment, the cost factor of building and running massive models may hinder their utilization as a plug-and-play module.\nDespite impressive zero-shot and ICL results for general purpose models, specialist models still perform best on most tasks thanks to task-specific solutions via adequate engineering (Heck et al., 2020; Ye et al., 2021; Kim et al., 2020) and taskrelated data. However, the opportunities to improve dedicated systems with the help of general purpose models are plenty. Their predictive powers could be used for developing smaller, specialized, low inference cost models. Automatic labeling and data a) PMUL4050 system: \"I'd recommend the Autumn House. Would you like to make a booking?\" user: \"Yes please. I need the reservation to be for 8 people and 2 nights starting on Tuesday.\" Prediction: ... hotel-name: none Label: ..., hotel-name: autumn house b) PMUL0117 user: \"Yes I also need a taxi that will get me to the restaurant by the booked time please.\" Prediction: taxi-destination: hotel, taxi-departure: restaurant Label: taxi-destination: the gonville hotel, taxi-departure: la mimosa c) SNG01873 user: \"I need to be picked up from pizza hut city centre after 04:30\" Prediction: ..., hotel-name: dontcare, ..., attraction-type: dontcare, ... Label: ... d) PMUL0599 user: \"[...] Can you just help me find a high-end Mexican restaurant?\" Prediction: ..., restaurant-pricerange: high-end Label: ..., restaurant-pricerange: expensive e) MUL2051 user: \"Can I get address and postcode for the hotel?\" Prediction: hotel-address: ?, hotel-postcode: ? Label:system: \"The address is 74 chesterton road, the postal code is cb41er, can I assist with anything else?\" user: \"That is all for now, goodbye.\" Prediction: hotel-address: 74 chesterton road, hotel-postcode: cb41er Label:f) MUL0524 user: \"I'm going to Cambridge on saturday and want to arrive by 14:15 please.\" Prediction: ..., train-day: Saturday Label: ..., train-day: saturday g) PMUL4246 user: \"i need a place to go and should be a museum\" Prediction: attraction-type: museum Label: attraction-type: museum system: \"Okay! There are several museums in Cambridge. What part of town would you like to visit?\" user: \"How about ones in the centre, what's available?\" Prediction: attraction-type: museum, attraction-area: centre Label: attraction-area: centre augmentation are natural use cases for ChatGPT, as is evident from our experimental results; a perdomain JGA of 70% (see Section 4.2) is surely sufficient to generate additional mid-to high-quality training data for dedicated systems. Automatic labeling may be conducted on-line for on-the-fly adaptation of production systems or off-line for iterative learning. Another way of harnessing general purpose models is the integration into dedicated systems as fallback options in case of out-of-domain or out-ofontology requests. An integration via knowledgeseeking term detection (Gunasekara et al., 2020) could facilitate the ability to provide context-aware responses that go beyond the original scope of the specialized system. General purpose models may handle unseen domains in place of the main model.\nWhile hallucinations may be an issue if not handled adequately, they also pose an opportunity to enable zero-shot concept detection. We observed that many slot hallucinations were sensible and pointed at elements that were meaningful to conversations. Zero-shot slot detection may be utilized to annotate and prepare unstructured data for model training, and to expand a system's capacities on-the-fly. Dialogue state trackers with dynamic dialogue states have the potential to expand a taskoriented dialogue system's conversational range seamlessly (Geishauser et al., 2022) . A general purpose model that has the capacity to identify new concepts may be utilized to generate API calls and database queries that are unknown to the specialized system (OpenAI, 2023; Chase, 2023) .\nGeneral purpose models may replace some components in a modular dialogue system (Zhu et al., 2022) . It might still be beneficial to rely on specialized DST and a dedicated policy for particular tasks in order to maintain interpretability and a desired level of control over information flow. However, natural language understanding (NLU) and natural language generation (NLG) modules may be powered by generative large language model based systems such as ChatGPT in order to benefit from a heightened ability of semantic modeling and to facilitate more natural and diverse output, thus promoting more natural conversations with modular task-oriented dialogue systems.\n\nConclusion\nThis work is the first to investigate ChatGPT's capacities for zero-shot DST. Despite remarkable preliminary results that we achieved, we identified limitations rooted in inherent properties of general purpose models, preventing these to become holistic solutions to complex NLP problems without further research. We discussed opportunities provided by ChatGPT and similar models to advance the development of specialized systems. With our insights and discussion, we hope to stimulate research in similar directions.\n"}
{"question": "To verify knowledge behavior in the V&L model, what tasks are designed?", "evidence": "  In this section, we describe two tasks for verifying knowledge behavior in the V&L model: table generation and image generation. Both tasks are based on infoboxes in Wikipedia articles, which correspond to summary information of the Wikipedia articles comprising tables and images 3 . Thus, it is suitable for verifying the knowledge about entities in Wikipedia kept in the pre-trained V&L model. ", "options": ["A. Table generation.", "B. Image generation.", "C. Text generation and image generation.", "D. Table generation and image generation."], "answer": "D", "content": "\nIntroduction\nVision & Language (V&L), which is the fusion of vision and language tasks, has achieved great success in tasks such as caption generation from images (Xu et al., 2015) and image generation from texts (Reed et al., 2016) . This progress has been driven by pre-trained V&L models that are trained on large-scale V&L datasets (Du et al., 2022) . To generate appropriate captions and images for input, pre-trained V&L models need to have prior knowledge of the features of the objects they are generating (Cao et al., 2020; Yun et al., 2021) . These models retain knowledge about entities in particular by inheriting parameters from pre-trained language models used in natural language processing to indirectly utilize data resources such as Wikipedia.\nIn this way, V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang This learning process raises a number of questions, such as whether the knowledge about entities acquired from natural language is adequately retained in the pre-trained V&L model, or whether it is enhanced by combining it with image features. These are important in understanding the limits of what can be generated by the pre-trained V&L model.\nTo answer these questions, we propose a task of generating tables and images of infoboxes in English Wikipedia. Figure 1 shows an example of the target infobox, in which either tables or images are generated by the proposed task. In both cases, the model must know the entities to generate them properly.\nWe collected about 200,000 infoboxes to construct the Wikipedia posed task. In addition, we used OFA (Wang et al., 2022) , a pre-trained V&L model that has achieved state-of-the-art performance in various V&L tasks.\nOur evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is lost when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.\nIn image generation, we found that OFA can generate more accurate images by using the knowledge expressed in the table. We also found that the models trained only on natural language can infer table knowledge, which increases the diversity of generated images. Our code and dataset will be released at https://github.com/kamigaito/WikiTIG.\n\nVision & Language Models\nMany pre-trained V&L models have achieved stateof-the-art performance on various tasks by inheriting the weights of the conventional pre-trained models for natural language and images (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) before learning V&L datasets. Our study examines how the knowledge represented in the pre-trained model for natural language is transformed through such a learning process. We select OFA, which has achieved state-of-the-art performance in multiple V&L tasks, as our target model.\nFigure 2 shows the network structure of OFA and its relation to each dataset 2 . OFA uses VQGAN (Esser et al., 2020) on the decoder to transform images into discrete sequences so that the same Transformer (Vaswani et al., 2017) is used for image and natural language generation. Because OFA inherits 2 Appendix A describes the data for the pre-training.\n\nTask\nInput Output parameters from BART (Lewis et al., 2020) , which shares a similar Transformer structure, OFA should include knowledge acquired from natural language such as Wikipedia articles. Unlike the decoder, the encoder handles images directly; thus, OFA uses the output of ResNet (He et al., 2016) to embed images in addition to the embedding layer inherited from BART.\n\nTable and Image Generation\nIn this section, we describe two tasks for verifying knowledge behavior in the V&L model: table generation and image generation. Both tasks are based on infoboxes in Wikipedia articles, which correspond to summary information of the Wikipedia articles comprising tables and images 3 . Thus, it is suitable for verifying the knowledge about entities in Wikipedia kept in the pre-trained V&L model.\nIn the following subsections, we explain the details of each task.\n\nTable Generation\nIn the table generation task, the target V&L model generates a table from a title and/or image of the infobox. To do this, the model generates linearized tables, similarly to table generation by descriptions (Wu et al., 2022b) . In our setting, we linearize tables as shown in Figure 3 using the column separator \"|\" and the row separator \"<>\" to reuse pretrained token embeddings. The separator symbols are accompanied by spaces before and after for use in BPE tokenization. We investigate the target model by directly generating such linearized text. We use the following settings for the investigation.\nGeneration from titles We investigate the knowledge about entities held by V&L models by comparing tables generated from titles by pre-trained V&L models and by pre-trained models trained only on natural language.\nGeneration from title and images We generate tables from titles with images and compare the results with those generated from only titles. This enables us to investigate the new knowledge in pretrained V&L models transferred from images.\nMetrics For comparison, we use the following evaluation metrics to measure how close the generated tables are to the actual ones.\n-ROUGE: Since the linearized tables are text data and the infobox plays the role of summarizing the article, we use ROUGE (Lin, 2004) , the most widely used evaluation method for automatic summarization. In our evaluation with ROUGE, we convert the column separator \"|\" and the row separator \"<>\" to spaces so that the sequence of strings is not restricted to rows and columns.\n-Table - F 1 : To evaluate the tables with respect to their structure, we divide the cells by their types and then evaluate the matches with the reference table in terms of the F 1 measure for each case and average them. When calculating the matches, we apply clipping used in ROUGE to prevent the score from increasing due to the repetition of the same cell in the output 4 . We treat cells of each type separately 5 as follows:\n\u2022 Group: The infobox sometimes divides the table into groups, with the first row of each group serving as a header for the group name. The prediction performance for the group names is important for verifying what aspects of knowledge the model has about the entities. Since these rows consist of a single column, we target rows consisting of a single column in this type of cell.\n\u2022 Header: The head of each row in the table consisting of more than one column is usually the header of a subsequent cell in the same row. Therefore, the prediction performance for headers is important for the same reason as for group names. to the headers. Therefore, the prediction performance of the values is important for knowing whether the model has detailed knowledge about the entity. To examine the correspondence between headers and their values, we treat a header and its corresponding value as a pair.\n-Corpus-F 1 : Because the above \n\nImage Generation\nIn the image generation task, the model receives a title, caption, and table to generate the corresponding image:\nGeneration from a title and caption By using the minimum input required to generate images, we investigate the difficulty of generating them compared to other datasets.\nGeneration from a title, caption, and table We investigate the impact of knowledge about entities on image generation by generating images from input, including tables, and compare the results to the setting without tables.\nMetrics We use the following three widely used measures for evaluating image generation.\n-CLIP: The relevance of the input text to the generated images inferred by the pre-trained V&L model CLIP (Radford et al., 2021) .\n-Inception Score (IS): How easily a model can distinguish the differences between each image and the variety of generated images (Salimans et al., 2016) . It is inferred by the pre-trained image classification model Inception-v3 (Szegedy et al., 2016) .\n-Frechet Inception Distance (FID): How close the generated image is to the reference image, es- \nModel Input ROUGE \u2191 Table-F 1 \u2191 Corpus-F 1 \u2191 1 2 L\n\nDataset Creation\nWe created the Wikipedia Table and Image Generation (WikiTIG) dataset by extracting infoboxes from the HTML dump data of the English Wikipedia 8 . To ensure consistency in the format of infoboxes, we limited the extraction target to those containing a title in the first row and an image in the second row, as shown in Figure 1 .\nIn order to use only entities with sufficient information, we targeted entities for which the table was not empty. In addition, to ensure reliable correspondence, only rows one column wide, which often describe groups, and rows two columns wide, which often consist of a header and its value, were targeted for extraction.\nThe target images are limited to those in jpeg, png, and gif formats. Since some captions do not include a title, we used a hyphen to join the title at the beginning of the caption in such cases.\nTable 2 shows the size of each dataset. The dataset size diverges between two tasks because some infoboxes do not include captions 9 .\n\nTable Generation\nSettings We chose OFA (Wang et al., 2022) , a pre-trained V&L model, and BART (Lewis et al., 2020) , pre-trained only in natural language, as models for comparison. For both models, we used the base settings with the hyperparameters reported in Wang et al. (2022) . We performed the training three times with different seeds and reported their average scores with their standard deviations 10 .\nResults Table 3 shows the results for each setting in the table generation 11 . When only the title is used as input, the result of BART is more accurate than that of OFA, indicating that part of the knowledge acquired from natural language is lost due to additional learning in the V&L model. The use of image information improves Table-F 1 for headers, indicating that images reinforce the knowledge of what kind of features an entity has.\nIn contrast, F 1 for cell values did not improve, indicating that information obtained from images does not complement detailed knowledge, such as the values corresponding to each header obtained from natural language.\nThe results of BART in Corpus-F 1 also suggest that BART contains more diverse knowledge internally than in other settings. This result reinforces that the V&L model forgot part of the knowledge from natural language through additional learning, and images could not fully complement them.\n\nImage Generation\nSettings Similarly to the table generation, we chose OFA for the comparison. We additionally join the reference tables (Gold) and those generated by models in \u00a75.1 (OFA, BART) as the input in order to investigate the impact of the ability to infer table knowledge. We also used the base settings with the hyperparameters reported in Wang et al. (2022) . We also performed the training three times with different seeds and reported their average scores with their standard deviations 12 .\nResults Table 4 shows the results for each setting in the image generation 13 in OFA is close to the result (Wang et al., 2022) in MS COCO (Chen et al., 2015) for image generation, the use of our created dataset is reasonable for training models. In addition, the input of Table (Gold) improves all metrics, indicating that the model produces higher quality images when provided with complementary knowledge about the entities. This result also indicates that OFA does not retain sufficient knowledge of the entities in English Wikipedia.\nIn addition, we did not observe any performance improvement in CLIP and FID when fed with automatically generated tables from BART and OFA. However, tables generated by BART improves IS with the lower performance degradation of FID than that by OFA, indicating that automatically generated tables can improve the diversity of the output images and accurate tables are more important for improving performance in image generation.\n\nRelated Work\nFollowing the advancements in V&L models (Du et al., 2022) , there have been various studies that investigate V&L models. Cao et al. (2020) conducted a comprehensive analysis of V&L models including the difference between model structures. Through their analysis, they revealed the importance of text information in V&L tasks over image information.\nSeveral studies focused on the performance differences between V&L models and text-only models. Yun et al. (2021) investigated the improvement of linguistic representations by pre-training V&L models on PhysicalQA (PIQA) (Bisk et al., 2020) and the probing framework of (Tenney et al., 2019) . They concluded that the benefit of pretrained V&L models for text-only tasks is marginal. Iki and Aizawa (2021) ; Hagstr\u00f6m and Johansson (2022) compared the performance of V&L models and text-only models on the text-only benchmark, GLUE (Wang et al., 2018) and determined that the text-only model achieved higher scores than the V&L models.\nHowever, even though various kinds of V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) inherit language-related knowledge from pre-trained language-only models, how the knowledge is inherited has yet to be investigated. Our work clarifies this by using our created dataset, Wikipedia Table and Image Generation (WikiTIG).\n\nConclusion\nThis paper investigates how knowledge about entities are preserved in a pre-trained V&L model which is originally transferred from a pre-trained natural language model.\nWe analyzed a pre-trained V&L model by creating the Wikipedia Table and Image Generation (WikiTIG) dataset for generating images and tables of the infoboxes in Wikipedia. WikiTIG consists of 200,000 infoboxes and their corresponding images from English Wikipedia.\nExperimental results on a pre-trained V&L model OFA (Wang et al., 2022) showed that the model forgot part of the knowledge about entities during pre-training, and the image information did not fully compensate for the forgotten knowledge.\n"}
{"question": "What is one of the primary benefits of using Monte-Carlo dropout during inference, as mentioned in the paper?", "evidence": "  To make NLI model scores more robust under this shift, we propose to use Monte-Carlo dropout during inference. This essentially creates a cheap ensemble and has been shown to deal better with noisy labels. This approach leads to consistent score improvements in our tasks.  ", "options": ["A. It reduces model parameters.", "B. It increases computational cost.", "C. It creates a cheap ensemble and deals better with noisy labels.", "D. It improves training efficiency. "], "answer": "C", "content": "\nIntroduction\nConditional language models suffer from a tendency to hallucinate information (Maynez et al., 2020) , resulting in generations that are not faithful to their input documents, which limits the trustworthiness of such models. This raises a need for automatic faithfulness metrics. In this context, models trained on natural language inference (NLI) (Bowman et al., 2015) are attractive since, intuitively, a generation being faithful implies it must be entailed by the source (Falke et al., 2019) . However, pure NLI models have seen mixed success in faithfulness evaluation (Falke et al., 2019; Kryscinski et al., 2020; Wang et al., 2020; Maynez et al., 2020) . While in recent evaluation on the TRUE benchmark (Honovich et al., 2022) , which contains datasets from knowledge-grounded dialogue, summarization and paraphrasing, NLIderived metrics perform best overall, they require impractically large models, or costly additional machinery such as question generation and answering models at inference, while still showing robustness issues. Thus we ask: What is still needed for pure NLI models to perform robustly across faithfulness datasets -while remaining cheap enough to serve as a lean and practical evaluation tool?\nWe enhance a relatively small NLI model to make it work robustly across tasks in three ways:\nTask-Adaptive Data Augmentation. In NLI, a hypothesis must be fully entailed by its supporting premise. However, in faithfulness, not all parts of the generation always need to be grounded. We identify an instance of this phenomenon in dialogue where parts of a turn can fulfill communicative functions such as hedging or establishing emotional connection and are often disregarded in faithfulness annotation. Hence, when applying NLI models to complete dialogue turns that may include statements irrelevant for grounding, we run a risk of producing incorrect unfaithfulness predictions.\nTo alleviate this issue, we propose a simple data augmentation method to adapt NLI models to genres where they need to be aware of statements that must be exempt from NLI-based faithfulness evaluation. Our approach is computationally attractive, as it avoids an increase of cost at inference time.\nIntegration of NLI Contradiction Scores. Existing NLI faithfulness metrics typically use the entailment score for their predictions (Honovich et al., 2022; Falke et al., 2019; Kryscinski et al., 2020) . However, Chen and Eger (2022) show that subtracting the contradiction score from the entail-ment score (referred to as e-c ) can improve NLI performance in certain evaluation tasks. We show that there also is a strong positive effect of e-c for faithfulness prediction, and demonstrate that this is due to a high contradiction probability being a more reliable predictor of unfaithfulness than low entailment probability.\nMonte-Carlo Dropout Inference. Applying NLI models to faithfulness prediction involves a domain shift from largely human-written data to automatically generated text. To make NLI model scores more robust under this shift, we propose to use Monte-Carlo dropout during inference (Srivastava et al., 2014) . This essentially creates a cheap ensemble and has been shown to deal better with noisy labels (Goel and Chen, 2021) . This approach leads to consistent score improvements in our tasks.\nThe combination of all modifications not only strongly improves over a baseline NLI model, but also outperforms all other metrics on TRUE, on average, while being cheaper and smaller. 1 2 Method Details\n\nTask-adaptive Data Augmentation\nTo illustrate that task requirements can be incompatible between faithfulness and NLI, consider the following instance from the Q2 dialogue corpus (Honovich et al., 2021) that is labelled as faithful:\nGrounding: American pancakes are similar to Scotch pancakes or drop scones. Generation: yes , i love american pancakes , they are like scotch pancakes From an NLI perspective, the generation is clearly not entailed, since the statement \"I love american pancakes\" is not supported by the input.\nTo better prepare an NLI system for such genre or task-specific cases, we manually curate a small list of statements that should not influence the faithfulness prediction. We augment NLI data from the ANLI corpus (Nie et al., 2020) by adding a randomly chosen phrase from this set to each instance, while preserving the label. We then train an already fine-tuned NLI model on a concatenation of these augmented samples and original ANLI data. For training details see Appendix A.\n1 All code is available at https://github.com/julmaxi/ with_a_little_push\n\nMonte-Carlo Dropout\nTo compute scores under Monte-Carlo dropout, we randomly sample k dropout masks and compute the average of the model predictions. We set k = 15, since preliminary experiments showed that performance did not profit from additional samples.\n\nExperimental Setup\nWe run experiments on TRUE (Honovich et al., 2022) , a benchmark that compiles a wide variety of faithfulness tasks in a standardized format. It contains summarization (Pagnoni et al., 2021; Maynez et al., 2020; Wang et al., 2020; Fabbri et al., 2021) , knowledge-grounded dialog (Honovich et al., 2021; Gupta et al., 2022; Dziri et al., 2022) 2 and paraphrasing (Zhang et al., 2019) datasets. 3 Following recommendations in TRUE, we evaluate using Area under the ROC Curve (AUC).\nAs our BASE model, we use the DeBERTa-large (He et al., 2020) model of Laurer et al. (2022) , trained on MultiNLI (Williams et al., 2018) , Fever-NLI (Thorne et al., 2018) , ANLI (Nie et al., 2020) , LingNLI (Parrish et al., 2021) and WANLI (Liu et al., 2022) . The metric All uses all three of our proposed modifications to Base. We also investigate a variant without MC dropout inference (-MC) as a more cost efficient alternative.\nWe compare to the strongest models on TRUE: T5 ANLI (Honovich et al., 2022 ) is a T5-11B (Raffel et al., 2020) model trained on ANLI. 4 SummacZS (Laban et al., 2022 ) evaluates an NLI model on all pairs of input and generated sentences and then averages maximum entailment probabilities for each generated sentence.\nQ2 (Honovich et al., 2021) combines a question generation/answering pipeline with an NLI score.\nFinally, Honovich et al. (2022) introduce a strong ensemble of these 3 methods (Eorig). To further verify our approach, we construct a new ensemble (Eour) by replacing T5 with All.\n\nResults\nTable 1 shows the AUC scores for each metric. Base on six out of nine corpora, but also significantly outperforms all other competitors on average, while being more computationally efficient.\nAs expected, we find the biggest gains in dialogue, where the All model even outperforms Eorig on 2 out of 3 corpora. We do not improve on BEGIN, which is likely due to bias in the dataset construction, which we elaborate on in Section 5.1. On the summarization part, All improves significantly over Base on 3 out of 5 corpora, while not significantly harming performance on any corpus. However, it still falls short of the best models in TRUE. The strong showing of T5 on these corpora suggests that this might be alleviated with a stronger base model.\nOverall, a very similar behaviour is exhibited by -MC, presenting an attractive option when the added overhead of multiple samples is undesirable.\nEour is on par with Eorig, despite massively reduced costs; it even significantly outperforms it on two dialog and the paraphrasing corpora.\nWe also investigate the performance of each individual modification to our model (Table 2 ). They all improve average scores, while only leading to a notable decrease on BEGIN for both e-c and dialogue augmentations and on MNBM for e-c .\nOutside of dialogue, we find that the augmentation methods have a positive impact on PAWS, as well as all summarization corpora that are at least partially based on summaries for the CNN/DM dataset (Hermann et al., 2015) (Frank, QAGS-C, and SummEval). While we do not have a definitive explanation for this phenomenon, we hypothesize that on these datasets our augmentations aid in making the model robust in the presence of noise or irrelevant context since our augmentations are label-neutral and must similarly be 'ignored' during training.\n\nEffect of Dialogue Adaptation\nWe investigate whether the improvements via our augmentation approach are indeed due to them improving the handling of personal statements.\nWe use the occurrences of the pronoun I in a generation as a proxy measure 5 and compute its correlation with human labels and metrics (see Table 3 ). On both Q2 and Dialfact, our proxy measure, while uncorrelated with human labels, is strongly correlated with the scores of both Base and T5. This indicates these metrics indeed tend to incorrectly reject generations with personal statements. All on the other hand reduces this dependency.\nOur results also help explain why negatively correlated with first person pronouns. This is likely due to a bias in dataset construction:\nThe BEGIN dataset used in TRUE has generations from two models, one of which is both more likely to generate pronouns and more likely to generate unfaithful output (see Appendix B).\n\nEffect of integrating contradiction scores\nTo isolate the effect of e-c we compare score distributions of Base and Base+e-c in Figure 1 . The lefthand side of the figure shows that in Base ca. 2700 faithful instances are predicted as non-entailed (i.e., e-score near 0), which implies they are labelled as contradictory or neutral. e-c , on the other hand, further differentiates these instances into instances with high contradiction (negative e-c score) and high neutral probability (e-c score near 0). We observe that almost all low-scoring faithful generations are classified as neutral, whereas nearly all instances that are classified as contradictory are indeed unfaithful. Where Base has no way to make use of this information, e-c allows to reliably label contradictory instances as unfaithful.\n\nCost comparison to other approaches\nThere is increasing awareness of the resource-hungriness of deep learning (Strubell et al., 2019) . Especially for faithfulness, cheap and reliable metrics are critical, given rising demands for NLG in research and industry. Table 5 : Results of our phrase selection robustness analysis. For each run, we sample five phrases, recreated our dataset and retrain our model. We repeat this process ten times and report the average, as well as the standard deviation, minimum and maximum scores of the runs.\nSmall numbers indicate difference to the original scores. All results were computed using e-c and MC dropout.\nFor better comparison, we also report the scores of a model without any augmentation (i.e. without any additional training) with e-c and MC dropout.\nrequires fewer parameters than any other metric, including a more than 30x reduction compared to T5. During inference our model always requires a constant number of calls which can be reduced to a single call when ablating MC dropout. On the other hand, the number of calls in SummacZS scales with the number of input and output sentences. Q2 needs to generate questions by calling an auto-regressive QG model n times, where n factors in the amount and length of questions (#Q\u00d7Ql), answer #Q questions with the QA model and finally check #Q answers with an NLI model (#Q \u00d7 2).\nIn sum, our model compares favourably with other approaches, while also allowing for a performance/cost tradeoff by forgoing MC dropout.\n\nPhrase Selection Robustness\nTo ensure that our augmentation is robust and not overly reliant on any particular choice of phrases, we repeat our dataset augmentation process multiple times with five randomly chosen augmentation phrases out of the original ten. We sample ten such datasets and retrain our model for each. Table 5 shows the average score, minimum and maxi-mum score, as well as the standard deviation of the scores. We also report results of a model with both MC dropout and e-c but without any additional training and augmentations to directly quantify whether the augmentations are still helpful in their reduced form. This corresponds to applying MC dropout and e-c to Base.\nAs expected, we find that reducing the variety of available phrases leads to a drop in performance across almost all datasets, compared to All. The only exception is BEGIN, where we instead see a slight improvement. This is likely to be related to the construction of BEGIN (see the discussion in Section 5.1).\nWhen comparing our limited augmentation models to the non-augmented model, we find that they still outperform the non-augmented model in almost all cases. In particular for Q2 and DialFact, for which we expect the strongest impact of our augmentations, we find that even the worst run still outperforms non-augmented model. This suggests that our augmentations can robustly adapt the model to the dialogue task.\nFinally, we observe a relatively large drop in scores for all datasets that are at (least partially) derived from CNN/DM (Frank, SummEval and QAGS-C). This mirrors our earlier observation in Section 4 that these datasets profit from our augmentation procedure.\n\nRelated Work\nPrevious work on the utility of NLI for faithfulness led to mixed conclusions. In summarization, Falke et al. (2019) and Kryscinski et al. (2020) find out-of-the-box models have only limited utility in a faithfulness setting. In Wang et al. (2020) , an NLI model is outperformed by a question generation/answering (QA/QG)-based method. In contrast, Maynez et al. (2020) find that a similar NLI model vastly outperforms a QA/QG metric on their data. In knowledge-grounded dialogue, Dziri et al. (2022) , Gupta et al. (2022) and Honovich et al. (2021) find out-of-the-box models underperform.\nTo improve NLI models for faithfulness in summarization, Kryscinski et al. (2020) propose FactCC, which is trained on artificially noised summaries. Utama et al. (2022) propose a controllable generation model to generate artificial faithfulness data. In knowledge-grounded dialogue, Dziri et al. (2022) and Gupta et al. (2022) combine noising techniques to generate additional training data for NLI-based faithfulness models. In contrast to our work, these approaches a) generate training data from external sources, instead of directly augmenting NLI data, and b) do not explicitly focus on reconciling differences between NLI and faithfulness with their augmentation. Outside of augmentationbased approaches, Goyal and Durrett (2020) propose to train NLI models to label faithfulness at the dependency arc level.\n\nConclusion\nWe have demonstrated that with a small number of focused adaptations, even a relatively small NLI model can robustly predict faithfulness. We have:\n1. Shown that NLI-based metrics can be incompatible with task-specific requirements and identified and fixed one such incompatibility in dialogue with an augmentation strategy.\n2. Demonstrated the importance of contradiction probability for scoring and that the underlying mechanism is the high reliability of NLI contradiction scores for detecting unfaithfulness 3. Shown that using Monte-Carlo dropout improves metric performance.\nOur improved NLI model significantly improves over its baseline across many corpora and outperforms all competitors in average score on TRUE, while being much more efficient at inference. Our work suggests that strong improvements are possible for NLI-based faithfulness metrics, by combining data augmentation with adapted NLI score computation. We hope this finding will spurn advances in cheap and robust NLI for faithfulness. unclear whether our results would still provide advantages when applied to larger models such as T5-11B, whose parameter count makes experimentation infeasible on the hardware available to us.\n"}
{"question": "What is the primary focus of this paper?", "evidence": "  \"Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks... In this paper, we study the adversarial robustness provided by label smoothing strategies in foundational models for diverse NLP tasks...  ", "options": ["A. Adversarial attacks in computer vision", "B. Label smoothing in computer vision models", "C. Adversarial attacks in NLP models", "D. Label smoothing in NLP models "], "answer": "C", "content": "\nIntroduction\nNeural networks are vulnerable to adversarial attacks: small perturbations to the input ,which do not fool humans (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2017) . In NLP tasks, previous studies (Alzantot et al., 2018; Jin et al., 2019; Li et al., 2020; Garg and Ramakrishnan, 2020) demonstrate that simple word-level text attacks (synonym substitution, word insertion/deletion) easily fool state-of-the-art models, including pre-trained transformers like BERT (Devlin et al., 2019; Wolf et al., 2020) . Further, it has recently been shown models are overconfident 1 on examples which are easy to attack (Qin et al., 2021) and indeed, such over-confident predictions plague much of modern deep learning (Kong et al., 2020; Guo et al., 2017; Nguyen et al., 2015; Rahimi et al., 2020) . Label smoothing is a regularization method that has been proven effective in a variety of applications, and modalities (Szegedy et al., 2016; Chorowski and Jaitly, 2017; Vaswani et al., 2017) . Importantly, it has been shown to reduce overconfident predictions and produce better confidence calibrated classifiers (Muller et al., 2019; Zhang et al., 2021; Dan and Roth, 2021; Desai and Durrett, 2020; Huang et al., 2021; Liu and JaJa, 2020) .\nIn this work, we focus on the question: does label smoothing also implicitly help in adversarial robustness? While there has been some investigation in this direction for adversarial attacks in computer vision, (Fu et al., 2020; Goibert and Dohmatob, 2019; Shafahi et al., 2019) , there is a gap in understanding of whether it helps with discrete, text adversarial attacks used against NLP systems. With the increasing need for robust NLP models in safety-critical applications and a lack of generic robustness strategies, 2 there is a need to understand inherent robustness properties of popular label smoothing strategies, and the interplay between confidence and robustness of a model.\nIn this paper, we extensively study standard label smoothing and its adversarial variant, covering robustness, prediction confidence, and domain transfer properties. We observe that label smoothing provides implicit robustness against adversarial examples. Particularly, we focus on pre-trained transformer models and test robustness under various kinds of black-box and white-box word-level adversarial attacks, in both in-domain and out-ofdomain scenarios. Our experiments show that label smoothing (1) improves robustness to text adversarial attacks (both black-box and white-box), and (2) mitigates over-confident errors on adversarial textual examples. Analysing the adversarial exam-ples along various quality dimensions reveals the remarkable efficacy of label smoothing as a simple add-on robustness and calibration tool.\n\nText Adversarial Attacks\nOur experiments evaluate the robustness of text classification models under three state-of-the-art text adversarial attacks TextFooler (black-box), BAE (black-box) and SemAttack (white-box), described below. 3 For a particular victim NLP model and a raw text input, the attack produces semantically-similar adversarial text as output. Importantly, only those examples are attacked, which are originally correctly predicted by the victim model. The attacks considered are word-level, i.e. they replace words in a clean text with their synonyms to maintain the meaning of the clean text, but change the prediction of the victim models.\n\u2022 TextFooler (TF): (Jin et al., 2019) proposes an attack which determines the word importance in a sentence, and then replaces the important words with qualified synonyms.\n\u2022 BAE: (Garg and Ramakrishnan, 2020) uses masked pre-trained language models to generate replacements for the important words until the victim model's prediction is incorrect.\n\u2022 SemAttack (SemAtt): (Wang et al., 2022) introduces an attack to search perturbations in the contextualized embedding space by formulating an optimization problem as in (Carlini and Wagner, 2016) . We specifically use the white-box word-level version of this attack.\n\nLabel Smoothing\nLabel Smoothing is a modified fine-tuning procedure to address overconfident predictions. It introduces uncertainty to smoothen the posterior distribution over the target labels. Label smoothing has been shown to implicitly calibrate neural networks on out-of-distribution data, where calibration measures how well the model confidences are aligned with the empirical likelihoods (Guo et al., 2017) .\n\u2022 Standard Label Smoothing (LS) (Szegedy et al., 2013; Muller et al., 2019 ) constructs 3 The black-box attacks keep querying the model with its attempts until the victim model is fooled while the white-box attack has access to the gradients to the model. Further details of the attacks are in (Jin et al., 2019; Garg and Ramakrishnan, 2020; Wang et al., 2022) . a new target vector (y LS i ) from the one-hot target vector (y i ), where y LS i = (1 \u2212 \u03b1)y i + \u03b1/K for a K class classification problem. \u03b1 is a hyperparameter selection and its range is from 0 to 1. ) with a probability of 1 \u2212 \u03b1 on the target label and \u03b1 on the label to which the classification model assigns the minimum softmax scores, thus introducing uncertainty.\nFor both LS and ALS, the cross entropy loss is subsequently minimized between the model predictions and the modified target vectors y LS i , y ALS i .\n\nExperiments\nIn this section, we present a thorough empirical evaluation on the effect of label smoothing on adversarial robustness for two pre-trained transformer models: BERT and its distilled variant, dBERT, which are the victim models. 4 We attack the victim models using TF, BAE, and SemAttack. For each attack, we present results on both the standard models and the label-smoothed models on various classification tasks: text classification and natural language inference. For each dataset we evaluate on a randomly sampled subset of the test set (1000 examples), as done in prior work (Li et al., 2021; Jin et al., 2019; Garg and Ramakrishnan, 2020) . We evaluate on the following tasks, and other details about the setting is in Appendix A.8:\n\u2022 Text Classification: We evaluate on movie review classification using Movie Review (MR) (Pang and Lee, 2005) and Stanford Sentiment Treebank (SST2) (Socher et al., 2013) against various attacks for in-domain data. We show clean accuracy, attack success rate and average confidence on successful adversarial texts. For each dataset, the left column are the results for standard model, and the right column are for LS models where \u03b1 denotes the label smoothing factor (\u03b1=0: no LS). \u2191 (\u2193) denotes higher (lower) is better respectively. dBERT denotes the distilBERT model.\non the matched genre test-set in the OOD setting presented in subsection 3.2 .\n\nIn-domain Setting\nIn the in-domain setting (iD), the pre-trained transformer models are fine-tuned on the train-set for each task and evaluated on the corresponding testset. For each case, we report the clean accuracy, the adversarial attack success rate (percentage of misclassified examples after an attack) and the average confidence on successfully attacked examples (on which the model makes a wrong prediction). 5 Table 1 shows the performance of BERT and dBERT, with and without label-smoothing. We choose label smoothing factor \u03b1 = 0.45 for standard labelsmoothed models in our experiments.\nWe see that label-smoothed models are more robust for every adversarial attack across different datasets in terms of the attack success rate, which is a standard metric in this area (Li et al., 2021; Lee et al., 2022) . Additionally, the higher confidence of the standard models on the successfully attacked examples indicates that label smoothing helps mitigate overconfident mistakes in the adversarial setting. Importantly, the clean accuracy remains almost unchanged in all the cases. Moreover, we observe that the models gain much more robustness from LS under white-box attack, compared 5 Details of each metric are presented in Appendix A.2.\nto the black-box setting. We perform hyperparameter sweeping for the label smoothing factor \u03b1 to investigate their impact to model accuracy and adversarial robustness. Figure 1 shows that the attack success rate gets lower as we increase the label smooth factor when fine-tuning the model while the test accuracy is comparable 6 . However, when the label smoothing factor is larger than 0.45, there is no further improvement on adversarial robustness in terms of attack success rate. Automatic search for an optimal label smoothing factor and its theoretical analysis is important future work. We also investigate the impact of adversarial label smoothing (ALS) and show that the adversarial label smoothed methods also improves model's robustness in Table 2 . \n\nOut-of-Domain setting\nWe now evaluate the benefits of label smoothing for robustness in the out-of-domain (OOD) setting, where the pre-trained model is fine-tuned on a particular dataset and is then evaluated directly on a different dataset, which has a matching label space. Three examples of these that we evaluate on are the Movie Reviews to SST-2 transfer, the SST-2 to Yelp transfer, and the SNLI to MNLI transfer.\nIn Table 3 helps produce more robust models in the OOD setting although with less gain compared to iD setting. This is a challenging setting, as evidenced by the significant performance drop in the clean accuracy as compared to the in-domain setting. We also see that the standard models make over-confident errors on successfully attacked adversarial examples, when compared to label-smoothed models.\n\nQualitative Results\nIn this section, we try to understand how the generated adversarial examples differ for label smoothed and standard models. First we look at some qualitative examples: in We also performed automatic evaluation of the quality of the adversarial examples for standard and label smoothed models, adopting standard metrics from previous studies (Jin et al., 2019; Li et al., 2021) . Ideally, we want the adversarial sentences to be free of grammar errors, fluent, and semantically similar to the clean text. This can be quantified using metrics such as grammar errors, perplexity, and similarity scores (compared to the clean text). Table 5 shows that the quality of generated adversarial examples on label smoothed models is worse than those on standard models for different metrics, suggesting that the adversarial sentences generated by standard models are easier to perceive. This further demonstrates that label smoothing makes it harder to find adversarial vulnerabilities.\n\nConclusion\nWe presented an extensive empirical study to investigate the effect of label smoothing techniques on adversarial robustness for various NLP tasks, for various victim models and adversarial attacks. Our results demonstrate that label smoothing imparts implicit robustness to models, even under domain shifts. This first work on the effects of LS for text adversarial attacks, complemented with prior work on LS and implicit calibration (Desai and Durrett, 2020; Dan and Roth, 2021) , is an important step towards developing robust, reliable models. In the future, it would be interesting to explore the combination of label smoothing with other regularization and adversarial training techniques to further enhance the adversarial robustness of NLP models.\n"}
{"question": "What is our experiment result?", "evidence": "  Our results demonstrate that even with intuitive natural language prompts, a complex task such as DST can be solved exceedingly well without any form of additional learning.  While specialized systems can exert control over its input-processing and output-generation to arbitrary degrees, this is not the case for Chat-GPT.  Even with the most rigorous and schematic prompts, there can be no guarantee that the model interprets the input as intended or generates the output as required, which may lead to unexpected behavior.  Furthermore, there is no guarantee that behavior is consistent across a series of similar inferences, such as in our experimental evaluation. ", "options": ["A. By using straightforward natural language prompts, a challenging task like  DST can be effectively resolved without the need for any supplementary learning.", "B. Chat-GPT have the capability to exert varying levels of control over their input processing and output generation in arbitrary manners.", "C. With the most rigorous and schematic prompts, the model would interpret the input as intended or generates the output as required.", "D. The behavior remains consistent throughout a sequence of comparable deductions, as demonstrated in our experimental assessment."], "answer": "A", "content": "\nIntroduction\nDialogue state tracking (DST) is a critical component for task-oriented dialogue systems. Its purpose is to extract and track user's goals throughout a conversation (Young et al., 2010) . DST is challenging due to the infinite possibilities of user/agent conversations, and because services and schemas/APIs that dialogue systems interface are subject to constant change (Ren et al., 2018) . Although traditional approaches achieve high accuracy when operating on a pre-defined set of concepts called an ontology (Mrk\u0161i\u0107 et al., 2017; Liu and Lane, 2017; Zhong et al., 2018) , ongoing research explores transfer to new domains with little to no additional learning (Rastogi et al., 2020) using ontology independent architectures to allow seamless adaptation to out-of-ontology concepts.\nMany strategies for zero-shot transfer to unseen domains have been proposed. Li et al. (2021) treat DST as a question answering (QA) task by leveraging data augmentation. Zhao et al. (2022) propose DST by relying on schema descriptions while Heck et al. (2022) utilize natural language descriptions to facilitate zero-shot transfer. Gao et al. (2020) and Lin et al. (2021) suggest learning from non-dialogue QA data which are available in large amounts to improve generalization. Campagna et al. (2020) harness large synthesized data based on abstract dialogue models. However, none of these techniques are ideal solutions. Fine-tuning is challenging due to computational costs, risk of over-fitting and the need for expensive (Budzianowski et al., 2018) task-specific data. Cross-task transfer still requires curated data and careful consideration of suitable learning tasks. Data augmentation requires high level task knowledge and an adequate synthesizing strategy.\nA new generation of large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Glaese et al., 2022) comes with the promise to be equipped to solve any task without task-specific fine-tuning, but solely with world knowledge they acquired during self-training on massive amounts of data. Such LLMs have been shown to perform remarkably well on in-context learning (ICL) , where only a natural language prompt and examples are provided to condition the generation process, achieving significant improvements over fine-tuned approaches in few-shot setups (Brown et al., 2020; Wang et al., 2022) . ChatGPT (Ope-nAI, 2022) -trained using human feedback and reinforcement learning -is the most recent of such models and single-handedly solves an array of challenging natural language processing (NLP) tasks with super-human capabilities, all through a natural language dialogue interface.\nIn this work, we aim to answer the question: does ChatGPT solve the problem of zero-shot DST? We show that crafting intuitive natural language prompts is sufficient to achieve state-of-the-art performance with ChatGPT, exceeding conventional, engineering-heavy approaches to zero-shot DST by a large margin. However, despite our findings, we argue that properties inherent to general purpose models inhibit their ability to simply replace specialized systems. We speculate that while in the foreseeable future general purpose models may not become holistic solutions to complex problems, they will provide ample opportunities to empower specialized systems to go beyond their pre-defined scopes, enable on-the-fly extensibility and generation of high quality training data by zero-shot synthesizing or automatic labeling.\n\nBackground\nDialogue state tracking is tasked to (1) determine for every turn t in a dialogue {(U t , M t )} T 1 with U t and M t being current user and preceding system utterance whether any of the slots in S = {S n } N 1 is present, to (2) predict values for each S n and to (3) track the dialogue state\nDS t \u2200t \u2208 [1, T ]. The DS is cumulative, i.e., DS t = update(DS t\u22121 , DS t ) is updated given the predictions of slot-value updates DS t .\n\nChatGPT\n(OpenAI, 2022) is a dialogue agent (Leike et al., 2018) , and in its core a GPT-3.5 LLM fine-tuned on human-written promptresponse pairs followed by reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020) . RLHF utilizes a reward model trained on human feedback to improve generation quality and adequacy via proximal policy optimization (Schulman et al., 2017) \n\nZero-shot DST with ChatGPT\nOur investigative approach to zero-shot DST with ChatGPT differs considerably from related works. We decode dialogue state updates with a general purpose model, without undergoing any parameter updates. Consequently, we neither employ data augmentation nor cross-task transfer learning. Instead, we solely rely on the general capacities of ChatGPT as an aligned dialogue agent. We take a most rigorous approach to zero-shot transfer where we do not allow the provision of any examples, nor of a formal task definition. Instead, we only permit natural language explanations of what the model is supposed to do. This sets our investigation apart from the closely related IC-DST (Hu et al., 2022) .\nIn zero-shot DST, the set of slots S relevant during inference and the set of slots S \u2032 seen during training of the model X \u03b8 with parameters \u03b8 are disjoint, i.e., S \u2229 S \u2032 = \u2205. Further, it may be S \u2032 = \u2205, in which case \u03b8 is not specifically tuned towards solving DST. This is precisely the case for Chat-GPT in our setup. Our approach to zero-shot DST with ChatGPT is formalized as follows. Let\nA 1 =P \u2295 \"system\":M 1 \u2295 \"user\":U 1 , A t =\"system\":M t \u2295 \"user\":U t , \u2200t \u2208 [2, T ],\nwhere P is the task description which provides the model with instructions for how to process a dialogue between a system M and a user U . A 1 is the initial prompt to ChatGPT. A t\u22652 are the follow-up prompts, only containing a single turn-pair of the dialogue of interest. ChatGPT is particularly suitable for this strategy due to its chat based interface. ChatGPT generates its next output B t conditioned on the current prompt A t\u22121 , as well as all preceding user queries and system responses of the same chat. The dialogue state update DS t can be found in B t , but may not be directly interpretable as such due to the diversity in the output surface forms. Thus, we require a normalization operation DS t = normalize(B t ). In contrast to (Hu et al., 2022) , we do not condition B t on DS t . This renders the task even more challenging, as Chat-GPT is forced to solve complex subtasks such as coreference resolution -the case where a newly encountered slot refers to the value of another slot -solely given the initial prompt and its own latent dialogue state given the dialogue history.\n\nExperiments\nAt the time of conducting our experiments, Chat-GPT is a proprietary research preview accessible for free via a web interface 1 . We used the Jan 9 version of the model. We use a regular expression term to extract all parts that are JSON formatted. We form DS t by accumulating all predicted updates up to turn t.\nEvaluation. We evaluate on the 1000 dialogues of the MultiWOZ 2.1 (Eric et al., 2020) test split and use joint goal accuracy (JGA) to compare methods. For a fair judgement of the ChatGPT predictions, we follow the evaluation procedure of Heck et al. ( 2020). We process each dialogue once and refrain from using ChatGPT's regeneration feature.\nPrompt. We imposed restrictions that the taskdefining prompt P be intuitive natural language and provides no formal schema. The crafting process involves simple trial-and-error on fewer than 10 held-out dialogues from the MultiWOZ training set. The design process was guided by the intention to imitate the behavior of a triple copy strategy (TripPy) DST (Heck et al., 2020) . P consists of three parts. First, a list of names for detectable informable slots along with natural language descriptions. The slot names help us extract a DS t that is compatible with the dataset's labels. Second, a sparse list of slots that are categorical, along with their value candidates for (1) aiding normalization of values that are expected to show high variability in expression, and (2) modeling Boolean slots.\nThird, an informal task description. 2\n4.1 ChatGPT vs. Supervised SOTA Comparing ChatGPT's performance to state-of-theart supervised approaches that achieve close to 60% JGA is not a fair fight 3 , and yet we observe an impressive 31.5% zero-shot JGA. This result is double-edged; on the one hand it is evidence that ChatGPT is capable of DST 4 , and on the other hand is no match for specialized systems. The comparison to TripPy, a SOTA supervised model, allows us a more fine-grained analysis. In Figure 1 , slot filling performance is broken down into value types. We observed that ChatGPT underperforms in non-trivial cases, namely refer, where a newly encountered slot refers to the value of another slot, and inform, where a slot-value was mentioned by the system and confirmed by the user. ChatGPT shows slight underperformance for Boolean slots. Remarkably, performance for values that are extracted directly from user utterances -the most relevant category in terms of frequency - (Hu et al., 2022) was the first successful attempt at pseudo 5 zero-shot DST via ICL. Our preliminary results with ChatGPT are on par, which is remarkable for the following reasons.\n(1) Our prompt is non-schematic and without examples, (2) our task-defining prompt is stated only once at the beginning of the chat, and (3) we do not maintain a DS to serve as additional input at each turn. The heightened zero-shot performance of IC-DST can be mainly attributed to these points.\n\nError Analysis\nWe identified a set of recurring errors that are likely caused by either the content of P or by the model's inherent properties. See ChatGPT is a sophisticated dialogue agent that, via alignment with human judgements, is capable of understanding context and intent of a multi-turn conversation far beyond the capacities of the previous generation of LLMs. This makes it well-suited for DST. Our results demonstrate that even with intuitive natural language prompts, a complex task such as DST can be solved exceedingly well without any form of additional learning. While specialized systems can exert control over its input-processing and output-generation to arbitrary degrees, this is not the case for Chat-GPT. Even with the most rigorous and schematic prompts, there can be no guarantee that the model interprets the input as intended or generates the output as required, which may lead to unexpected behavior. Furthermore, there is no guarantee that behavior is consistent across a series of similar inferences, such as in our experimental evaluation. In terms of deployment, the cost factor of building and running massive models may hinder their utilization as a plug-and-play module.\nDespite impressive zero-shot and ICL results for general purpose models, specialist models still perform best on most tasks thanks to task-specific solutions via adequate engineering (Heck et al., 2020; Ye et al., 2021; Kim et al., 2020) and taskrelated data. However, the opportunities to improve dedicated systems with the help of general purpose models are plenty. Their predictive powers could be used for developing smaller, specialized, low inference cost models. Automatic labeling and data a) PMUL4050 system: \"I'd recommend the Autumn House. Would you like to make a booking?\" user: \"Yes please. I need the reservation to be for 8 people and 2 nights starting on Tuesday.\" Prediction: ... hotel-name: none Label: ..., hotel-name: autumn house b) PMUL0117 user: \"Yes I also need a taxi that will get me to the restaurant by the booked time please.\" Prediction: taxi-destination: hotel, taxi-departure: restaurant Label: taxi-destination: the gonville hotel, taxi-departure: la mimosa c) SNG01873 user: \"I need to be picked up from pizza hut city centre after 04:30\" Prediction: ..., hotel-name: dontcare, ..., attraction-type: dontcare, ... Label: ... d) PMUL0599 user: \"[...] Can you just help me find a high-end Mexican restaurant?\" Prediction: ..., restaurant-pricerange: high-end Label: ..., restaurant-pricerange: expensive e) MUL2051 user: \"Can I get address and postcode for the hotel?\" Prediction: hotel-address: ?, hotel-postcode: ? Label:system: \"The address is 74 chesterton road, the postal code is cb41er, can I assist with anything else?\" user: \"That is all for now, goodbye.\" Prediction: hotel-address: 74 chesterton road, hotel-postcode: cb41er Label:f) MUL0524 user: \"I'm going to Cambridge on saturday and want to arrive by 14:15 please.\" Prediction: ..., train-day: Saturday Label: ..., train-day: saturday g) PMUL4246 user: \"i need a place to go and should be a museum\" Prediction: attraction-type: museum Label: attraction-type: museum system: \"Okay! There are several museums in Cambridge. What part of town would you like to visit?\" user: \"How about ones in the centre, what's available?\" Prediction: attraction-type: museum, attraction-area: centre Label: attraction-area: centre augmentation are natural use cases for ChatGPT, as is evident from our experimental results; a perdomain JGA of 70% (see Section 4.2) is surely sufficient to generate additional mid-to high-quality training data for dedicated systems. Automatic labeling may be conducted on-line for on-the-fly adaptation of production systems or off-line for iterative learning. Another way of harnessing general purpose models is the integration into dedicated systems as fallback options in case of out-of-domain or out-ofontology requests. An integration via knowledgeseeking term detection (Gunasekara et al., 2020) could facilitate the ability to provide context-aware responses that go beyond the original scope of the specialized system. General purpose models may handle unseen domains in place of the main model.\nWhile hallucinations may be an issue if not handled adequately, they also pose an opportunity to enable zero-shot concept detection. We observed that many slot hallucinations were sensible and pointed at elements that were meaningful to conversations. Zero-shot slot detection may be utilized to annotate and prepare unstructured data for model training, and to expand a system's capacities on-the-fly. Dialogue state trackers with dynamic dialogue states have the potential to expand a taskoriented dialogue system's conversational range seamlessly (Geishauser et al., 2022) . A general purpose model that has the capacity to identify new concepts may be utilized to generate API calls and database queries that are unknown to the specialized system (OpenAI, 2023; Chase, 2023) .\nGeneral purpose models may replace some components in a modular dialogue system (Zhu et al., 2022) . It might still be beneficial to rely on specialized DST and a dedicated policy for particular tasks in order to maintain interpretability and a desired level of control over information flow. However, natural language understanding (NLU) and natural language generation (NLG) modules may be powered by generative large language model based systems such as ChatGPT in order to benefit from a heightened ability of semantic modeling and to facilitate more natural and diverse output, thus promoting more natural conversations with modular task-oriented dialogue systems.\n\nConclusion\nThis work is the first to investigate ChatGPT's capacities for zero-shot DST. Despite remarkable preliminary results that we achieved, we identified limitations rooted in inherent properties of general purpose models, preventing these to become holistic solutions to complex NLP problems without further research. We discussed opportunities provided by ChatGPT and similar models to advance the development of specialized systems. With our insights and discussion, we hope to stimulate research in similar directions.\n"}
{"question": "What is the main advantage of the S3HQA model over other approaches for solving multi-hop TextTableQA questions, as mentioned in the passage?", "evidence": "  Unlike these methods, our proposed three-stage model S 3 HQA can alleviate noises from weakly supervised and solve different types of multi-hop TextTableQA questions by handling the relationship between tables and text.  ", "options": ["As for multi-hop type dataset, previous work used pipeline approach (Chen et al., 2020b) , unsupervised approach (Pan et al., 2021) , multigranularity (Wang et al., 2022b) , table pre-trained language model (Eisenschlos et al., 2021) , multiinstance learning (Kumar et al., 2021) and graph neural network (Feng et al., 2022) to solve this task."], "answer": "A", "content": "\nIntroduction\nQuestion answering systems devote to answering various questions with the evidence located in the structured knowledge base (e.g., table) (Pasupat and Liang, 2015; Yu et al., 2018) or unstructured texts (Rajpurkar et al., 2016) . Considering that many questions need to utilize multiple sources of knowledge jointly in real-world applications, the hybrid form of question answering over texts and tables (TextTableQA) has been proposed and attracted more and more attention (Chen et al., Walnut Q1: Who is the athlete in a city located on the Mississippi River? A1: Philip Mulkey Q2: In which year did Walnut-born athletes participate in the Rome Olympics? A2: 1960 Q3: Who is the higher scoring athlete from the cities of Eugene and Walnut? Comparison A3: Rafer Johnson 2020b,a; Zhu et al., 2021; Chen et al., 2021; Zhao et al., 2022; Wang et al., 2022a) . Fact reasoning (Chen et al., 2020a,b) is a critical question type of TextTableQA. It requires jointly using multiple evidence from tables and texts to reasoning the answers with different operations, such as correlation (e.g., multi-hop) and aggregation (e.g., comparison) . Hyperlinks among some table cells and linked passages are essential resources to establish their relationship and support the retrieval and reasoning for multi-hop questions. As shown in Figure 1 , answering a complex question Q1 requires jointly reasoning from textual evidence (P1) to table evidence ([R2, Place] ) and then to other table evidence ([R2, Athlete]).\nExisting methods consist of two main stages: retriever and reader (Chen et al., 2020b; Feng et al., 2022) . The retriever filters out the cells and passages with high relevance to the question, and then the reader extracts a span from the retrieval results as the final answer. However, current methods with two stages still have three limitations as follows.\n1) Noisy labeling for training retriever. Existing retrieval methods usually ignore the weakly supervised answer annotation (Chen et al., 2020b; Wang et al., 2022b; Feng et al., 2022) . For the Q2 of Figure 1 , we cannot know the specific location of the hybrid evidence, only given the final answer \"1960\". Therefore, there is a lot of pseudo-true evidence labeled (Marked in green) automatically by string matching, which introduces a lot of evidence noise.\n2) Insufficient utilization of heterogeneous information. After retrieval, existing methods selected a particular cell or passage for reading to extract the final answer (Chen et al., 2020b; Wang et al., 2022b) . As for Q1 in Figure 1 , previous models were more likely to choose P1 or the coordinates [R2, Place] to extract the answer. However, these methods seldomly used the hybrid information of table schema and cell-passage hyperlinks, which is the key factor in answering multi-hop questions.\n3) Deficient ability for different reasoning operations. Previous methods (Eisenschlos et al., 2021; Kumar et al., 2021; Wang et al., 2022b) mainly used an extraction module to obtain answers, which cannot support knowledge reasoning that requires comparison, calculation, and other operations.\nIn this paper, we propose a three-stage approach S 3 HQA to solve the above problems. (1) Retriever with Refinement Training, we propose a two-step training method, splitting the training data into two parts, so that the noise in the retrieval phase can be alleviated. (2) Hybrid Selector has been proposed and selects supporting facts with different granularity and resources depending on the question type. By considering the hybrid data of tables and text, this paper proposes a hybrid selection algorithm that can effectively utilize the heterogeneous information of tables and passages. (3) Generationbased reasoner utilizes a generation-based model for addressing different question types. The model allows better aggregation of information on the input side, which not only have better multi-hop reasoning capabilities but also be able to handle comparison and counting questions. Furthermore, we are the first to use the LLM in-context learning approach for table-text hybrid question-answering tasks.\nWe evaluate our proposed model on the challenging TextTableQA benchmark HybridQA. The empirical results show that our approach outperforms all the existing models 2 .\n\nGiven a natural language question\nQ = {q i } |Q| i=1\nand a table T with \u27e8H, R\u27e9, H indicates the table headers, and\nR = {r i } |R| i=1 indicates the rows with number |R|. Each row r i is consists of N cells r i = {c ij } N j=1\n. The header's number is also N . Some cells have a linked passage P ij . Our goal aims to generate the answer A with model \u0398, which is a span from table cells or linked passage or a derivation result of counting questions.\n\nRetriever with Refinement Training\nThe retriever aims to perform initial filtering of heterogeneous resources. However, accurately labeling the location of answers consumes high labeling costs. For TextTableQA data, the answer A usually appears in multiple locations, which makes it difficult for us to generate precise retrieval la-bels. We use a two-step training method, with a row-based retriever and a passage-based retriever for each step.\nInspired by (Kumar et al., 2021) , the retrieval has two steps. First, we divide the data D into two folds according to the string matching labels G i . Specifically, for a question-answer instance, the answer A appears one time as D 1 , and the instance whose answer A appears multiple times as D 2 . Take the example in Figure 1 , Q1, Q3 belongs to D 1 while Q2 belongs to D 2 . The data is organized in the form of\n[CLS]q 1 q 2 ...q |Q| [SEP]c i1 c i2 ...c iN [SEP] or [CLS]q 1 q 2 ...q |Q| [SEP]p ij [SEP].\nIn the first step, we only use D 1 to train a model \u0398 1 , which data are noiseless. Then in the second step, we use the trained weight \u0398 1 to train the model \u0398 2 . For the input x, the loss function is:\nL(\u0398 2 , x, R) = z\u2208R \u2212q(z) log p \u0398 1 (z|x)\nwhere q(z) = p \u0398 1 (z|x, z \u2208 R) is the probability distribution given by the model restricted to candidate rows R containing the answer span, taken here as a constant with zero gradients (Eisenschlos et al., 2021) .\nMeanwhile, we use a passage-based retriever to enhance the performance of a row-based retriever (PassageFilter). Specifically, we use the passage-based retriever to obtain a prediction score of passage relevance. Based on this score, we reorder the input of the row-based retriever. It avoids the limitation on input sequence length imposed by the pre-trained model.\n\nHybrid Selector\nThis module needs to combine the results of the two granularity retrievers. As for this task, we consider the question type and the relationships between the table and linked passages essential. As shown in Figure 2 , the hybrid selector chooses the appropriate data source from the two retrieval results depending on question types.\nSpecifically, for general bridge multi-hop questions, we use a single row and its linked passage. While for comparison/count questions, we consider multiple rows and further filter the related sentences, delete the linked paragraphs with the low scores. This not only enables the generation module to obtain accurate information, but also prevents the introduction of a large amount of unrelated information. The selector algorithm outputs a mixed sequence with high relevance based on the relationship between the question, the table, and the passages. The algorithm is shown in Algorithm 1.\nAlgorithm 1 Hybrid Selector Algorithm.\nInput: question Q, table rows R, linked passages P, rowbased retriever \u0398R, passage-based retriever \u0398P , selector target row count NS Output: generator input S Get the row/passage ordered list by relevant scores\n1: OR \u2190 sort(\u0398R(Q, R)) 2: OP \u2190 sort(\u0398P (Q, P)) 3: p type \u2190 Classif ication(Q) 4: if p type = bridge then 5: if OP [0] in OR[0] then 6: S \u2190 Q + OR[0] 7: else 8: S \u2190 Q + OR[0] + OP [0] 9:\nend if 10: else 11:\nOPC \u2190 P[len(OP )//2 :] 12:\nS \u2190 Q + OR[0 : NS] \u2212 OPC 13: end if 14: return S\n\nGeneration-based Reasoner\nThe results of the selector take into account both two granularity. Unlike the previous approaches, which were based on a span extraction module, we use a generation-based model for answer prediction.\n\nRow-wise generator\nTo generate an accurate answer string A = (a 1 , a 2 , ..., a n ) given the question Q and selection evidence S, we perform lexical analysis to identify the question type, such as counting or comparison, by looking for certain keywords or comparative adjectives. We utilize two special tags \u27e8Count\u27e9 and \u27e8Compare\u27e9, which indicates the question types.\nWe then use the results of the passage retriever to rank the passages in order of their relevance, eliminating the impact of model input length limitations. Finally, we train a Seq2Seq language model with parameters \u0398, using the input sequence Q, S and the previous outputs a <i to optimize the product of the probabilities of the output sequence a 1 , a 2 , ..., a n :\nA = argmax n i=1 P (a i |a <i , Q, S; \u0398)\n\nLLM prompting generator\nWith the emergence of large language models, In-Context Learning (Dong et al., 2022) and Chain-of-Thought prompting (Wei et al., 2022) have become two particularly popular research topics in this field.\nIn this paper, we introduce a prompting strategy for multi-hop TextTableQA.\nWe utilize selection evidence S and apply LLMbased prompting. We conducted experiments on both vanilla prompting and chain-of-thought prompting in zero-shot and few-shot scenarios.\n\nExperiment Setup\nDatasets We conduct experiments on Hy-bridQA (Chen et al., 2020b) . The detailed statistics are shown in Appendix A. For evaluation, we followed the official evaluation to report exact match accuracy and F1 score. Implementation details The implementation details are shown in Appendix B. The experimental results are the average of five times results.\n\nFully-supervised Results\nTable 1 shows the comparison results between our models with previous typical approaches on both development and test sets. It shows that our proposed S 3 HQA works significantly better than the baselines in terms of EM and F1 on HybridQA. The results indicate that S 3 HQA is an effective model for multi-hop question answering over tabular and textual data. Specifically, it can effectively handle multi-hop reasoning and make full use of heterogeneous information.\nHowever, we found that our approach was outperformed by the DEHG model (Feng et al., 2022) in terms of F1 score on the Dev set. We speculate that this might be because the DEHG approach uses their own Open Information Extraction (OIE) tool.\n\nModel\nDev EM F1 Zero-shot prompt GPT3.5 direct 33.1 50.5 GPT3.5 CoT 52.9 66.6 Few-shot prompt (2-shot) GPT3.5 direct 57.1 68.8 GPT3.5 CoT 60.3 72.1 \n\nLLM-prompting Results\nWe present our zero-shot and few-shot results in Table 2 . \"Direct\" refers to a simple prompting method where only the question, context, and answer are provided to the model without any additional reasoning process. In contrast, \"CoT\" involves a human-authored Chain-of-Thought reasoning process that provides a more structured and logical way of prompting the model. The experiments demonstrate that in-context learning used to prompt large language models can achieve promising results. Specifically, utilizing the Chain-of-Thought prompt method can significantly enhance the model's performance.\nHowever, it's worth noting that there is still a performance gap compared to fine-tuning the model on the full dataset (Table 1 ). Fine-tuning allows the model to learn more specific information about the TextTableQA task, resulting in better performance. Nevertheless, our results show that the LLM-prompting method can be a useful alternative to fine-tuning, especially when there is a limited amount of labeled data available.\n\nAblation Studies\nWe conduct ablation studies on the test set. We validate the effects of three modules: retriever with refinement training, hybrid selector, and generation-based reasoner. The retriever performs initial filtering of heterogeneous resources; Selectors combined with hyperlinks further identify the exact evidence needed to answer multi-hop questions; and the reasoner uses the selection evidence to obtain the final answer. Effect of proposed retriever. As shown in the Table 3 , under the setting of using the BERTbase-uncased model, sing the BERT-base-uncased model setting, the retriever with refinement training achieved 87.2. When we use Deberta-base, the top1 retrieval performance improved by 0.8%. For w/o refinement training, we use the entire data directly for training, the top1 recall drops about 3.2%. For w/o PassageFilter, we remove the mechanism, the top1 recall drops about 3.2%. For Vanilla-Retriever, we use the row-based retriever (Kumar et al., 2021) and remove all our mechanisms, the top1 score drops about 5.3%. This shows that our model can solve the weakly supervised data noise problem well.\n\nModel\nEffect of hybrid selector. As shown in the Table 4, we removed the selector of S 3 HQA and replaced it with the previous cell-based selector (Wang et al., 2022b) . This method directly uses the top1 result of the row retriever as input to the generator. w/o hybrid selector shows that the EM drops 2.9% and F1 drops 1.6%, which proves the effectiveness of our selector approach.\nEffect of reasoner. As shown in the Table 4 , we design two baselines. BERT-large reader (Chen et al., 2020b; Wang et al., 2022b) uses BERT (Devlin et al., 2018) as encoder and solves this task by predicting the start/end tokens. w/o special tags deletes the special tags. Both the two experiments demonstrate our S 3 HQA reasoner performs the best for HybridQA task.\n\nRelated Work\nThe TextTableQA task (Wang et al., 2022a) has attracted more and more attention. As for multi-hop type dataset, previous work used pipeline approach (Chen et al., 2020b) , unsupervised approach (Pan et al., 2021) , multigranularity (Wang et al., 2022b) , table pre-trained language model (Eisenschlos et al., 2021) , multiinstance learning (Kumar et al., 2021) and graph neural network (Feng et al., 2022) to solve this task. As for numerical reasoning task, which is quite different from multi-hop type dataset, there is also a lot of work (Zhu et al., 2021; Zhao et al., 2022; Zhou et al., 2022; Lei et al., 2022; Li et al., 2022; Wei et al., 2023) to look at these types of questions. Unlike these methods, our proposed three-stage model S 3 HQA can alleviate noises from weakly supervised and solve different types of multi-hop TextTableQA questions by handling the relationship between tables and text.\n\nConclusion\nThis paper proposes a three-stage model consisting of retriever, selector, and reasoner, which can effectively address multi-hop TextTableQA. The proposed method solves three drawbacks of the previous methods: noisy labeling for training retriever, insufficient utilization of heterogeneous information, and deficient ability for reasoning. It achieves new state-of-the-art performance on the widely used benchmark HybridQA. In future work, we will design more interpretable TextTableQA models to predict the explicit reasoning path.\n"}
{"question": "Based on the results presented in the text, which statement accurately summarizes the performance of the E2E ST method with the pre-trained wav2vec2.0 architecture?", "evidence": "  In shot, our approach outperforms existing state-ofthe-art models, especially on En-Fr.  ", "options": ["A. The E2E ST method with pre-trained wav2vec2.0 significantly outperforms all baselines on all language pairs.", "B. The E2E ST method with pre-trained wav2vec2.0 shows marginal improvement over baselines on all language pairs.", "C. The E2E ST method with pre-trained wav2vec2.0 performs exceptionally well on En-Fr and moderately on En-De.", "D. The E2E ST method with pre-trained wav2vec2.0 performs better on En-De than on En-Fr.", "By introducing more auxiliary MT data, our model with pre-trained wav2vec2.0 improves 1.5 and 2.3 BLEU on the two language pairs En-De and En-Fr, respectively. "], "answer": "C", "content": "\nIntroduction\nSpeech translation (ST) is the task that automatically translates a source acoustic speech signal into a text sequence in a target language. With the advance of Transformer, recent works on end-to-end speech translation (E2E ST) can alleviate many problems usually occurred in the cascade system and achieve comparable performance (Bahar et al., 2021; Bentivogli et al., 2021; Fang et al., 2022) .\nFor the E2E ST model, MT is often used as the teacher of ST, and methods such as knowledge distillation or contrastive learning are used to bridge the modality gap. The MT teacher only uses the source text (transcription) information. The speech and text modalities are consumed individually by ST model. There are two main drawbacks. One is the teacher MT model can not use speech information, which limits the overall model perfor-mance. The other is MT uses text input, ST uses the speech input, then close the two individual modalities. There is no unified module can simultaneously use cross-modal information.\nHere, we take a further step towards more effective use of both speech and transcription text in ST. Inspired by the related works of video Transformer (Kim et al., 2021) , when processing video, concatenating video information and text embedding information can better model the cross-modal information of the video. We concatenate the preprocessed speech and the transcription text jointly, and encode the two-modal information simultaneously. Following the recent popular advance in E2E ST with knowledge distillation (KD) (Tang et al., 2021; Zhao et al., 2021) , it provides a practical paradigm for transferring knowledge from rich-resource MT task to limited resource ST task. However, we re-define the role of teacher in our framework, because the information of the two modalities can further improve the upper bound of model performance than the single modality. Our proposed model, a unified cross-modal concatenate ST structure (uccST) introduces the teacher-student learning with Kullback-Leibler divergence (KL) regularization to transfer knowledge from cross-modal translation model to two subtasks -ST and MT.\nOur main contributions can be summarized.\n(1) Compared with the previous ST frameworks which can only utilize one single modality text in MT teacher, we design a unified framework that can use both input information of the two modalities simultaneously by concatenating speech and text.\n(2) Our cross-modal framework has three diverse inputs when inference, containing three end-toend and cascade decoding paths. Our multi-task learning framework allows sub-tasks to collaborate, showing promising performance on both end-toend and cascade ST.\n(3) We conduct various experiments on the MuST-C corpus. When using the limited ternary ST data, our E2E ST model can achieve state-ofthe-art performance. When adding the external data, our method significantly improves over the strong baselines.\n2 Unified Cross-modal Concatenate ST\n\nBackground\nGiven the source acoustic speech sequence s, the corresponding transcription x and the text sequence y in target language, speech translation usually model the conditional distribution as follows.\nEQUATION\nIn most works, the assumption p(y|x) = p(y|x, s) is usually adopted as the source transcription can deterministicially infer the final translation. However, we prefer to leverage the original conditional probability for our modeling.\n\nCross-modal Concatenate Framework\nInspired by video Transformer, the unified model can take as input the concatenation of the features of two modalities along the temporal dimension. As shown in Figure 1 (b), the speech preprocessing module usually includes CNN down-sampling and a speech encoder, such as the encoder of the pre-trained ASR or the pre-trained audio encoder wav2vec2.0. For the text sequence, we simply process each token with an embedding layer. After the concatenation, we add the position embedding and segment embedding in the fashion of BERT.\n\nMulti-task Training\nConcretely, given a ternary ST example (s, x, y).\nWe optimize three translation tasks in parallel, including MT, ST and our introduced unified crossmodal translation.\nL M T = log p(y|x) + log p(y|s) + log p(y|[x, s]) (2) where [\u2022, \u2022] indicates the concatenation operation.\n\nRegularization\nUnlike other ST frameworks, the unified crossmodal decoder output provides the teacher signal, and the ST and MT models are two students. We employ Kullback-Leibler divergence (KL) to minimize the decoding distribution between the student and the teacher model.\nL KL = KL (p st \u2225p unified ) + KL (p mt \u2225p unified ) (3)\nFurther, we impose a representation regularization on the encoder output. Particularly, we apply the MSE loss. where we concatenate the encoder outputs of ST and MT such that it results in the same length as the unified model.\nL M SE = MSE ([Z ST , Z M T ] , Z Unified ) (4)\n\nTraining and Inference\nIn summary, the final loss of the proposed uccST can be written as follows.\nEQUATION\nwhere \u03bb and \u03b8 are hyper-parameters. During inference, we have 3 optional decoding paths. If only audio is available, we can actually choose any decoding path. For the cross-modal unified or MT decoding path, it requires the transcription from an additional ASR, which is commonly a pre-training step for ST.\n3 Experiments Settings\n\nDatasets and Settings\nData For a fair comparison with previous works, we conduct our experiments on the widely used MuST-C V1: English-German (En-De), English- French (En-Fr) and English-Spanish (En-Es) corpus (Gangi et al., 2019) . On En-De and En-Fr, we also verify to what extent the auxiliary MT data can improve our multitask training. Specifically, we extract about 20M sentence pairs for the WMT14 En-Fr, 4.5M for WMT14 En-De, and 18M for Opensubtitle2018 En-De. Settings We implement all our experiments on Fairseq 1 . We experiment with two architectures 2 . One is the transformer model with 512 hidden units 2048 feed-forward size, which is same as Tang et al. (2021) , in purpose for constrained ST data. The other one is to leverage pre-trained wav2vec2.0 (Baevski et al., 2020) as the speech preprocessing module. Since wav2vec2.0 has been already pre-trained with the audio data of Librispeech (Panayotov et al., 2015) , we only compare this setup to other works with same architecture. During training, the text input is ground truth transcript of MuST-C. Note that the transcription data in Librispeech is not used in our case. We select the alternative batches between ST and MT with sampling ratios 1.0 and 0.25, respectively.\n\nResults on the Constrained ST Data\nAs shown in Table 1 , our method achieves an appealing performance on the three language pairs in the restricted ternary MuST-C data.\nCompared with the direct E2E ST baseline, our method has enhanced 0.7 to 1.7 BLEU on the three language directions, with an average gain of 1.13 BLEU. In a word, our approach can achieve the SOTA translation performance among all end-toend ST methods.\nCompared with the cascade method that we have reproduced, our E2E ST decoding path surpasses the cascade on the language pairs En-Fr, and reaches a comparable level on En-De and En-Es. The results of the MT decoding path with the transcription exceed the cascade method on all language pairs. Our cross-modal unified decoding method has enhanced 0.8 to 1.9 BLEU than the cascade method, with an average gain of 1.17 BLEU. In summary, our E2E ST method has matched or surpassed the cascade method on the constrained triple ST data, and our cross-modal unified decoding method has exceeded the traditional cascade baseline.\n\nResults on the External Data\nSince our model is a multitask learning method that includes the MT subtask, we add additional MT data for comparison experiments. As shown in Table 2 , we compare different baselines with similar data usage. Our E2E method (i.e., ST decoding path) and the corresponding baselines are presented in the bottom two rows. The first two rows in the table are the baselines without wav2vec2.0, and the middle part of the table represents the methods with wav2vec2.0 architecture. It is concluded that the pre-trained audio encoder model is indeed helpful for downstream ST task. By introducing more auxiliary MT data, our model with pre-trained wav2vec2.0 improves 1.5 and 2.3 BLEU on the two language pairs En-De and En-Fr, respectively. In shot, our approach outperforms existing state-ofthe-art models, especially on En-Fr.\n\nAblation Analysis of Concatenation\nIn order to analyze whether our concatenation is effective, we have done comparative experiments on different input models. As shown in loss is calculated between ST and MT. Unified simple model only concatenates the speech and text sequence from each corresponding encoder output.\nIn accordance to the result, no concatenation or the concatenation method in Unified simple model is inferior to our proposal.\n\nAblation Study on Loss\nTo analyze the importance of each component of the overall uccST loss, we conduct an ablation study by removing each loss step by step. Table 4 summarizes the results of the ablation study. We first remove the KL loss but reserve the unified structure. It concludes that the KL terms contribute to an improvement of 0.42 BLEU score. After further removing the MSE loss, the model becomes a standard multi-task ST Transformer. When removing multi-task, it reduces to a standard E2E ST model.\n\nComparison with the Cascaded Model\nAs shown in Table 5 , our proposed E2E ST has reached a comparable level to cascaded methods, both in data-constrained and non-constrained cases.\nAs to the two decoding methods that require transcription text, our method can outperform the cascade baseline. Meanwhile, we can observe that with the additional external data, the gap between two inference setups S|X and S is narrowed.\n\nRelated Works\nCascade ST. Cascade ST system concatenates the individual ASR and MT components (Stentiford and Steer, 1988; Waibel et al., 1991) , and represents an intuitive solution to achieve reasonable performance and high intelligibility. At the same time, this cascade method also faces some thorny problems: the traditional cascade method suffers from error propagation and the loss of acoustic information that might be useful to improve final translations. To alleviate the aforementioned problems, some tight integration methods have been proposed (Sperber et al., 2019; Bahar et al., 2020) . End-to-end ST. To overcome the weakness of cascade models, Berard et al. (2016) proposed the first direct neural network model of an encoder decoder architecture without the intermediate transcription.\nCurrently, more effective solutions are used in endto-end ST models (Park et al., 2019; Dong et al., 2021) . To alleviate the cross-modal difficulty in end-to-end models, two-pass (Kano et al., 2017; Anastasopoulos and Chiang, 2018) methods are proposed. Curriculum learning (Kano et al., 2017; Wang et al., 2020) is proposed to improve performance of ST models.\n\nConclusion\nIn this paper, we designed a unified ST framework.\nCompared with the previous ST frameworks which can only utilize one single modality text in MT teacher, our method can use both information of the two modalities simultaneously by concatenating speech and text. Our ST method can better utilize the cross-modal information. Experiments show that our method can significantly improve ST performance regardless of using the limited ternary data or adding auxiliary external data.\n"}
{"question": "Why Chalkidis et al. considered more sophisticated setting when running BERT?", "evidence": "  We learned that they considered the more sophisticated setting of running BERT because by default, BERT considers only the first 512 tokens. Thus, for long documents, the training process may miss some important information. However, in practice, users may forget to check the document length and are not aware of the need to apply suitable settings. The above experiments demonstrate that BERT can achieve superior results if properly used, but sometimes, a direct run lead to poor outcomes. Linear methods can serve as efficient and robust baselines to confirm the proper use of an advanced approach.\n ", "options": ["A. By default, BERT considers only the first 512 tokens so the training process may miss some important information for long documents. ", "B. Because BERT-based methods involve several hyper-parameters, such as the learning rate.", "C. Because all the linear methods are still hundreds of times faster than BERT. ", "D. Because users may forget to check the document length and are not aware of the need to apply suitable settings. "], "answer": "A", "content": "\nIntroduction\nText classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems.\nWhile various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. "}
{"question": "Which of the following is the opinion endorsed by this article?", "evidence": "  We are not denying the value of computational approaches to analyzing text. Certainly, comput-ing can be an instrumental approach for modeling and understanding social complexity. This does not mean that other approaches, such as historical, ethnographic, or mathematical, become irrelevant. On the contrary, computational methods necessarily (whether awarely or not) rely on these earlier approaches to add value, in terms of improving our explanations and understanding. ", "options": ["A. Computational methods necessarily  rely on earlier approaches to add value, in terms of improving our explanations and understanding", "B. Historical, ethnographic, or mathematical, become irrelevant.", "C. We deny the value of computational approaches to analyzing text. ", "D. Computing can never be an instrumental approach for modeling and understanding social complexity."], "answer": "A", "content": "\nIntroduction\nComputational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period (Lazer and Radford, 2017; van Atteveldt and Peng, 2018) . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" (Boydstun et al., 2014) . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.\nThe conclusion of this reasoning, and the dominant stance in computational social science, is a reliance on machines alone to answer questions in the field, surrendering to their supposed objectivity * Equal contribution.\nor impartiality. Can a machine's output go beyond descriptive catalogs of evidence, accelerating understanding of processes and motivations? From our experience, computers are nowhere near supplanting humans in interpreting social science results.\n1 An interdisciplinary inquiry must go farther than matching computational techniques to social science questions (O'Connor et al., 2011; Nguyen et al., 2020) . It embraces synergistic methodology and connects the norms and standards of evidence from both. This means partnering computer science's preference for the structured, generalizable, and objective with the unstructured, critical, and contextual which the social sciences champion. This level of interdisciplinarity addresses the question raised by descriptive findings: So what?\nWe see theory as the solution, empowering rather than shackling investigations. What this paper advocates is not one particular theory-certainly these are myriad, and \"even subject matter which has been under intensive and prolonged study remains at the unsettled periphery of research\" (Nagel, 1963) . Instead, we expand on our prior work (Dore and McCarthy, 2022) to clarify calls echoed for decades by computational and social science (McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . Underlying each, we find, is the urge to return to theory, which we espouse herein.\n\nDescription vs. Integration\nWe contrast descriptive findings and theoretical analysis. An example of a descriptive finding is that an apple falls, or that it falls faster when pushed than dropped, or even that it falls at a particular rate estimated with some standard error by a complex interpolation. A theoretical analysis of the same phenomenon, credited to Newton, is that a fundamental force acts upon the apple, and that this same force governs the motion of the heavens. The theoretical analysis links the finding about the world critically to a broader body of knowledge and context.\nDespite advances in causal inference in NLP, the descriptive is all that a machine can provide to the social sciences (Feder et al., 2021) . Certainly the methods of computational text analysis have advanced since the General Inquirer (Stone and Hunt, 1963) and Mosteller and Wallace's statistical inference of text authorship (1963) . But methods are means, not ends. They uncover more descriptive findings in data: the rate of an apple's fall, the topics of refugees' tweets (Walk et al., 2022) , the space given to marginalized groups in textbooks (Lucy et al., 2020) , or patterns of state censorship (Bamman et al., 2012; King et al., 2013) .\nThe foils to descriptive findings are integrative findings (Hofman et al., 2021) , which offer causal explanations that enable future predictions-a theory, or as a 'model' in the sense of the Standard Model, rather than of a statistical model. Integrative findings can either offer new theories or couch their explanations in existing theories-but the theory is essential either way.\n\nWe Don't Integrate\nTo contrast descriptive and integrative findings, we reviewed approximately 60 papers in computational text analysis published in *ACL venues. In Table 1 , we describe several of these in terms of their descriptive or theory-grounded contributions.\n2 Descriptive papers may refer to social science theories or make generalizable claims, as when Demszky et al. (2019) write, \"The shooter's race appears to play a role in topic preference: if the shooter is white, Democrats become more likely to focus on shooter's identity,\" but they do not link to the two to each other. An excellent theory-grounded quantitative work is Nelson (2021) ; she confirms some of the most compelling features of identity theory, specifically that identities based on race were most distinguished by cultural discourse, whereas those based on gender by the domestic and the economic discourse. Similarly, we conducted theory-grounded quantitative work to investigate the application of the protest paradigm and thematic framing in how westernand Hong Kong based newspapers portray protests in Hong Kong (McCarthy et al., 2021; McCarthy and Dore, 2022) . Generally, it remains challenging to find computational social science papers in *ACL venues that go beyond description and prediction, advancing theory. Why is this? We believe it stemmed from the field's \"empirical turn\".\n3 Few remember when the meetings of ACL offered a few dozen papers, all entrenched in formalisms and linguistic theories. Arguably, 1996 was a turning point when the founders of SIGDAT held the first EMNLP at Penn under the auspices of the ACL. 4 This gave a spotlight to the few but growing empiricists in the field and drew in more.\nEMNLP began a half-decade of measurable reorganization the field (Anderson et al., 2012) . That EMNLP remains affiliated with ACL keeps the language-focused machine learning practitioners in our tent. The slow blurring of boundaries between each *ACL conference's expectations (Church, 2020) increases this unity. Both groups belong under this tent. But without a doubt, one group's voice is becoming less heard.\nPublication venues within the ACL focus on methods over theory. 5 Techniques are taken off the shelf without critical examination because these are \"the best\" (often \"state of the art\") for their purposes (Ethayarajh and Jurafsky, 2020) . This widens the gap between theoretical and empirical work. 6 Hopkins and King (2010) claim, \"computer scientists may be interested in finding the needle in the haystack. . . social scientists are more commonly interested in characterizing the haystack\"-evincing the value of broader context. 7 Wallach (2018), quoting Hopkins and King, explains that the two groups 3 A lesser reason is the challenge of serving two masters: adequately covering both the theoretical and methodological components within 8 pages. We recently received two reviews for an *ACL submission: one advocating for more of the social science context in the main text by eschewing methods to the appendix, and the other instructing us to do the opposite. 4 And its predecessor the Workshop on Very Large Corpora. 5 This is due to the outsized influence of computer science, often seen as the science of method (Hoare and Jones, 1989; Shapiro, 2001) , when not instead seen as an engineering discipline (Rapaport, 2005).\n6 A related criticism is that empirical research has narrowed to focus on 'easy' questions that its tools can address (Coleman, 1986; Baden et al., 2021) , especially when research questions are baked into the design of the task. 7 As evidence, see Siegel (2018) : \"We usually don't know about causation, and we often don't necessarily care. . . the objective is more to predict than it is to understand the world. . . It just needs to work; prediction trumps explanation.\" Descriptive Chang et al. (2009) The article presents new quantitative methods to measure semantic meaning in inferred topics. The authors emphasize the qualitative relevance of their findings as it validates the use of topics for corpus exploration and information retrieval. However, their working hypothesis and empirical findings are not connected to the extremely relevant field of communication theory. Bamman et al. (2012) The article presents the first large-scale analysis of political content censorship in social media. The authors miss the opportunity to relate their hypothesis and findings to censorship theory, a natural theoretical context for the research, which would strengthen the relevance and generalizability of the findings. Field et al. (2018) The article discusses media manipulation in Russia in the context of agenda-setting and framing, the tools that Russian state-owned (or heavily influenced) media outlets use to distract public attention from domestic economic politics. The authors implicitly refer to propaganda theory and autocratic theory throughout the article even though their findings are not discussed in relation to these theories. Demszky et al. (2019) The article applies \"a more comprhensive NLP framework to study linguistic aspects of polarization in social media\". While the article implicitly refer to theories of social conformity and social conflict, the findings are not linked or discussed (either explicitly or implicitly) to the theoretical frameworks that the authors touch on in their \u00a71.\n\nIntegrative\nDiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.are interested in very different research  questions, and that computational social science must be more than computer science with social data; it must strive for valid explanatory models. In the same vein, at ACL 2022, ACL fellow Eduard Hovy remarked that NLP must be more than \"just machine learning on corpora\". Social scientists are also coming to terms with the meaning of computational techniques applied more often in social science (Bail, 2014; Biernacki, 2015; Lee and Martin, 2015; Spillman, 2015) . The focus of the debates, however, is on which methods are best suited to extract meaning from text, without addressing any theoretical considerations related to the methods or whether a theoretical framework for those methods even exists. The discussions on whether computational methods make social science research more efficient, reliable, and reproducible overtake attempts at theory-building.\n\nMoving Forward\nWe are not denying the value of computational approaches to analyzing text. Certainly, comput-ing can be an instrumental approach for modeling and understanding social complexity. This does not mean that other approaches, such as historical, ethnographic, or mathematical, become irrelevant. On the contrary, computational methods necessarily (whether awarely or not) rely on these earlier approaches to add value, in terms of improving our explanations and understanding (Radford and Joseph, 2020) .\nAs we are a field that prioritizes methods, consider the seminal book on methods in science: Abbott ( 2004) taxonomizes scientific ways of knowing. Its five broad categories are ethnography, historical narration, standard causal analysis, small-N comparison, and formal modeling. We in NLP myopically choose the third and fifth of these, ignoring the value of the others. But the broader point of Methods of Discovery is not methods. It is the research question. Any methodology should be grounded in the question, not incremental tweaks and reviewers' comfort (Church, 2020) . This admits even qualitative or mixed-method approaches to text analysis.\nThe role of humans in scientific inquiry is nothing new. Using qualitative analysis to complement quantitative techniques has its roots in Achen and Snidal (1989)'s recommendation to use historical case studies as a complement to statistical research.\n8\nTheir plea was strengthened by Verba's work in the early 1990s (Verba et al., 1993 (Verba et al., , 1995;; Verba, 1996) and Tarrow (1995) , who openly called for bridging qualitative and quantitative modes of research in social science. In doing so, they have enriched the field with critical methodological innovations (Gerring, 2004) , benefiting from the recognition that \"quantitative methods must augment humans, not replace them\" (Grimmer and Stewart, 2013, 4) .\nThe field can draw more from social science's rich tradition of inductive theory-building and interpretation to develop its theoretical approach-to prize either induction or deduction alone is a myth of scientific procedure (Thagard, 1988) , but the melding of the two opens new doors. Rather than eschewing the complexity (a criticism leveled by Baden et al., 2021) , it should put complexity at the center of its ontology on the basis that there are no immutable laws in social life or optimal solutions to social problems.\nSkepticism can linger toward findings not drawn from the standard practices of one's own field; indeed, social science was long skeptical of computational contributions (Armstrong, 1967) . We believe that this drives the hyperfocus on improving a few accepted methods instead of exploring more broadly. If the doorway between disciplines is only narrowly open, this reflects a lack of appreciation for each field's ways of knowing. The disciplinary divide keeps computational researchers from embracing methods beyond standard causal analysis or formal modeling, so the interpreter-centric richness allowed by histories, ethnographies, and small-N exploration are precluded.\n\nConclusion\nWe have explained the distinction between descriptive and theoretical findings as it pertains to computational text analysis. The bulk of work we found provided vast descriptive findings, often of high quality, but not giving back to questions of theory. We offer several suggestions on how to 'push the pendulum back' by prioritizing theory-building or theory-affirming research questions and accepting whichever methods are best suited toward answering it-not only the familiar and entrenched ones.\nWe are not the first to advocate for a shift in the patterns of applying computational techniques to real-world problems. There is a steady drumbeat from voices in the field advocating careful approaches (Nagel, 1963; McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . What we see underlying all of thesethose writing against 'mathiness' and speculation, advocating for clear evaluation over anecdotes, criticizing textual researchers' dilution of conceptual standards, highlighting work that ties linguistic information into complex models-is an unspoken, perhaps unrealized, call for a return to theory.\nNot only do we aver that incorporating theory is essential; but also, other fields have strengthened themselves when espousing organizing principles beyond those of their progenitors. Behavioral economics is a success story here. It transcended the neat (but psychosocially stripped) mathematics it draws from to acknowledge deviations from rationality and blend economics with cognitive science (Kahneman and Tversky, 1979; Thaler, 1980; Thaler and Sunstein, 2009) .\nFor scientific-not simply engineeringadvances to arise from the *ACL community, authors and reviewers alike must resist the temptation toward incremental, 'safe' research and follow Church (2005) : \"Controversial papers are great; boring unobjectionable incremental papers are not.\" In reviewing new research, we should privilege not only work that presents new and unusual computational methods, but also interactions between computational and humanistic approaches to answering research questions. EMNLP was founded because of reviewing biases at ACL against groundbreaking methodological advances, and since then the two have homogenized; \"EMNLP reviewing is no longer much of a differentiator\" (Church, 2020) . We found that theoretically grounded findings in text analysis are often published in non-*ACL venues (Table 1 ), but ACL sets the standard for work involving computational text analysis and NLP. Is there no home for groundbreaking integrative or interdisciplinary work in *ACL, such that a new venue is required? Or can we adapt our standards to invite deeper connections to theory and new ways of knowing?\n"}
{"question": "What impact does the removal of the hybrid selector have on the performance of the model in the HybridQA task?", "evidence": "  Effect of hybrid selector. As shown in the Table 4, we removed the selector of S 3 HQA and replaced it with the previous cell-based selector (Wang et al., 2022b) .  This method directly uses the top1 result of the row retriever as input to the generator. w/o hybrid selector shows that the EM drops 2.9% and F1 drops 1.6%, which proves the effectiveness of our selector approach.  ", "options": ["A. EM improves by 2.9% and F1 improves by 1.6%", "B. EM drops by 2.9% and F1 drops by 1.6%", "C. EM remains the same, but F1 improves by 1.6%", "D. EM drops by 1.6% and F1 improves by 2.9%\\"], "answer": "B", "content": "\nIntroduction\nQuestion answering systems devote to answering various questions with the evidence located in the structured knowledge base (e.g., table) (Pasupat and Liang, 2015; Yu et al., 2018) or unstructured texts (Rajpurkar et al., 2016) . Considering that many questions need to utilize multiple sources of knowledge jointly in real-world applications, the hybrid form of question answering over texts and tables (TextTableQA) has been proposed and attracted more and more attention (Chen et al., Walnut Q1: Who is the athlete in a city located on the Mississippi River? A1: Philip Mulkey Q2: In which year did Walnut-born athletes participate in the Rome Olympics? A2: 1960 Q3: Who is the higher scoring athlete from the cities of Eugene and Walnut? Comparison A3: Rafer Johnson 2020b,a; Zhu et al., 2021; Chen et al., 2021; Zhao et al., 2022; Wang et al., 2022a) . Fact reasoning (Chen et al., 2020a,b) is a critical question type of TextTableQA. It requires jointly using multiple evidence from tables and texts to reasoning the answers with different operations, such as correlation (e.g., multi-hop) and aggregation (e.g., comparison) . Hyperlinks among some table cells and linked passages are essential resources to establish their relationship and support the retrieval and reasoning for multi-hop questions. As shown in Figure 1 , answering a complex question Q1 requires jointly reasoning from textual evidence (P1) to table evidence ([R2, Place] ) and then to other table evidence ([R2, Athlete]).\nExisting methods consist of two main stages: retriever and reader (Chen et al., 2020b; Feng et al., 2022) . The retriever filters out the cells and passages with high relevance to the question, and then the reader extracts a span from the retrieval results as the final answer. However, current methods with two stages still have three limitations as follows.\n1) Noisy labeling for training retriever. Existing retrieval methods usually ignore the weakly supervised answer annotation (Chen et al., 2020b; Wang et al., 2022b; Feng et al., 2022) . For the Q2 of Figure 1 , we cannot know the specific location of the hybrid evidence, only given the final answer \"1960\". Therefore, there is a lot of pseudo-true evidence labeled (Marked in green) automatically by string matching, which introduces a lot of evidence noise.\n2) Insufficient utilization of heterogeneous information. After retrieval, existing methods selected a particular cell or passage for reading to extract the final answer (Chen et al., 2020b; Wang et al., 2022b) . As for Q1 in Figure 1 , previous models were more likely to choose P1 or the coordinates [R2, Place] to extract the answer. However, these methods seldomly used the hybrid information of table schema and cell-passage hyperlinks, which is the key factor in answering multi-hop questions.\n3) Deficient ability for different reasoning operations. Previous methods (Eisenschlos et al., 2021; Kumar et al., 2021; Wang et al., 2022b) mainly used an extraction module to obtain answers, which cannot support knowledge reasoning that requires comparison, calculation, and other operations.\nIn this paper, we propose a three-stage approach S 3 HQA to solve the above problems. (1) Retriever with Refinement Training, we propose a two-step training method, splitting the training data into two parts, so that the noise in the retrieval phase can be alleviated. (2) Hybrid Selector has been proposed and selects supporting facts with different granularity and resources depending on the question type. By considering the hybrid data of tables and text, this paper proposes a hybrid selection algorithm that can effectively utilize the heterogeneous information of tables and passages. (3) Generationbased reasoner utilizes a generation-based model for addressing different question types. The model allows better aggregation of information on the input side, which not only have better multi-hop reasoning capabilities but also be able to handle comparison and counting questions. Furthermore, we are the first to use the LLM in-context learning approach for table-text hybrid question-answering tasks.\nWe evaluate our proposed model on the challenging TextTableQA benchmark HybridQA. The empirical results show that our approach outperforms all the existing models 2 .\n\nGiven a natural language question\nQ = {q i } |Q| i=1\nand a table T with \u27e8H, R\u27e9, H indicates the table headers, and\nR = {r i } |R| i=1 indicates the rows with number |R|. Each row r i is consists of N cells r i = {c ij } N j=1\n. The header's number is also N . Some cells have a linked passage P ij . Our goal aims to generate the answer A with model \u0398, which is a span from table cells or linked passage or a derivation result of counting questions.\n\nRetriever with Refinement Training\nThe retriever aims to perform initial filtering of heterogeneous resources. However, accurately labeling the location of answers consumes high labeling costs. For TextTableQA data, the answer A usually appears in multiple locations, which makes it difficult for us to generate precise retrieval la-bels. We use a two-step training method, with a row-based retriever and a passage-based retriever for each step.\nInspired by (Kumar et al., 2021) , the retrieval has two steps. First, we divide the data D into two folds according to the string matching labels G i . Specifically, for a question-answer instance, the answer A appears one time as D 1 , and the instance whose answer A appears multiple times as D 2 . Take the example in Figure 1 , Q1, Q3 belongs to D 1 while Q2 belongs to D 2 . The data is organized in the form of\n[CLS]q 1 q 2 ...q |Q| [SEP]c i1 c i2 ...c iN [SEP] or [CLS]q 1 q 2 ...q |Q| [SEP]p ij [SEP].\nIn the first step, we only use D 1 to train a model \u0398 1 , which data are noiseless. Then in the second step, we use the trained weight \u0398 1 to train the model \u0398 2 . For the input x, the loss function is:\nL(\u0398 2 , x, R) = z\u2208R \u2212q(z) log p \u0398 1 (z|x)\nwhere q(z) = p \u0398 1 (z|x, z \u2208 R) is the probability distribution given by the model restricted to candidate rows R containing the answer span, taken here as a constant with zero gradients (Eisenschlos et al., 2021) .\nMeanwhile, we use a passage-based retriever to enhance the performance of a row-based retriever (PassageFilter). Specifically, we use the passage-based retriever to obtain a prediction score of passage relevance. Based on this score, we reorder the input of the row-based retriever. It avoids the limitation on input sequence length imposed by the pre-trained model.\n\nHybrid Selector\nThis module needs to combine the results of the two granularity retrievers. As for this task, we consider the question type and the relationships between the table and linked passages essential. As shown in Figure 2 , the hybrid selector chooses the appropriate data source from the two retrieval results depending on question types.\nSpecifically, for general bridge multi-hop questions, we use a single row and its linked passage. While for comparison/count questions, we consider multiple rows and further filter the related sentences, delete the linked paragraphs with the low scores. This not only enables the generation module to obtain accurate information, but also prevents the introduction of a large amount of unrelated information. The selector algorithm outputs a mixed sequence with high relevance based on the relationship between the question, the table, and the passages. The algorithm is shown in Algorithm 1.\nAlgorithm 1 Hybrid Selector Algorithm.\nInput: question Q, table rows R, linked passages P, rowbased retriever \u0398R, passage-based retriever \u0398P , selector target row count NS Output: generator input S Get the row/passage ordered list by relevant scores\n1: OR \u2190 sort(\u0398R(Q, R)) 2: OP \u2190 sort(\u0398P (Q, P)) 3: p type \u2190 Classif ication(Q) 4: if p type = bridge then 5: if OP [0] in OR[0] then 6: S \u2190 Q + OR[0] 7: else 8: S \u2190 Q + OR[0] + OP [0] 9:\nend if 10: else 11:\nOPC \u2190 P[len(OP )//2 :] 12:\nS \u2190 Q + OR[0 : NS] \u2212 OPC 13: end if 14: return S\n\nGeneration-based Reasoner\nThe results of the selector take into account both two granularity. Unlike the previous approaches, which were based on a span extraction module, we use a generation-based model for answer prediction.\n\nRow-wise generator\nTo generate an accurate answer string A = (a 1 , a 2 , ..., a n ) given the question Q and selection evidence S, we perform lexical analysis to identify the question type, such as counting or comparison, by looking for certain keywords or comparative adjectives. We utilize two special tags \u27e8Count\u27e9 and \u27e8Compare\u27e9, which indicates the question types.\nWe then use the results of the passage retriever to rank the passages in order of their relevance, eliminating the impact of model input length limitations. Finally, we train a Seq2Seq language model with parameters \u0398, using the input sequence Q, S and the previous outputs a <i to optimize the product of the probabilities of the output sequence a 1 , a 2 , ..., a n :\nA = argmax n i=1 P (a i |a <i , Q, S; \u0398)\n\nLLM prompting generator\nWith the emergence of large language models, In-Context Learning (Dong et al., 2022) and Chain-of-Thought prompting (Wei et al., 2022) have become two particularly popular research topics in this field.\nIn this paper, we introduce a prompting strategy for multi-hop TextTableQA.\nWe utilize selection evidence S and apply LLMbased prompting. We conducted experiments on both vanilla prompting and chain-of-thought prompting in zero-shot and few-shot scenarios.\n\nExperiment Setup\nDatasets We conduct experiments on Hy-bridQA (Chen et al., 2020b) . The detailed statistics are shown in Appendix A. For evaluation, we followed the official evaluation to report exact match accuracy and F1 score. Implementation details The implementation details are shown in Appendix B. The experimental results are the average of five times results.\n\nFully-supervised Results\nTable 1 shows the comparison results between our models with previous typical approaches on both development and test sets. It shows that our proposed S 3 HQA works significantly better than the baselines in terms of EM and F1 on HybridQA. The results indicate that S 3 HQA is an effective model for multi-hop question answering over tabular and textual data. Specifically, it can effectively handle multi-hop reasoning and make full use of heterogeneous information.\nHowever, we found that our approach was outperformed by the DEHG model (Feng et al., 2022) in terms of F1 score on the Dev set. We speculate that this might be because the DEHG approach uses their own Open Information Extraction (OIE) tool.\n\nModel\nDev EM F1 Zero-shot prompt GPT3.5 direct 33.1 50.5 GPT3.5 CoT 52.9 66.6 Few-shot prompt (2-shot) GPT3.5 direct 57.1 68.8 GPT3.5 CoT 60.3 72.1 \n\nLLM-prompting Results\nWe present our zero-shot and few-shot results in Table 2 . \"Direct\" refers to a simple prompting method where only the question, context, and answer are provided to the model without any additional reasoning process. In contrast, \"CoT\" involves a human-authored Chain-of-Thought reasoning process that provides a more structured and logical way of prompting the model. The experiments demonstrate that in-context learning used to prompt large language models can achieve promising results. Specifically, utilizing the Chain-of-Thought prompt method can significantly enhance the model's performance.\nHowever, it's worth noting that there is still a performance gap compared to fine-tuning the model on the full dataset (Table 1 ). Fine-tuning allows the model to learn more specific information about the TextTableQA task, resulting in better performance. Nevertheless, our results show that the LLM-prompting method can be a useful alternative to fine-tuning, especially when there is a limited amount of labeled data available.\n\nAblation Studies\nWe conduct ablation studies on the test set. We validate the effects of three modules: retriever with refinement training, hybrid selector, and generation-based reasoner. The retriever performs initial filtering of heterogeneous resources; Selectors combined with hyperlinks further identify the exact evidence needed to answer multi-hop questions; and the reasoner uses the selection evidence to obtain the final answer. Effect of proposed retriever. As shown in the Table 3 , under the setting of using the BERTbase-uncased model, sing the BERT-base-uncased model setting, the retriever with refinement training achieved 87.2. When we use Deberta-base, the top1 retrieval performance improved by 0.8%. For w/o refinement training, we use the entire data directly for training, the top1 recall drops about 3.2%. For w/o PassageFilter, we remove the mechanism, the top1 recall drops about 3.2%. For Vanilla-Retriever, we use the row-based retriever (Kumar et al., 2021) and remove all our mechanisms, the top1 score drops about 5.3%. This shows that our model can solve the weakly supervised data noise problem well.\n\nModel\nEffect of hybrid selector. As shown in the Table 4, we removed the selector of S 3 HQA and replaced it with the previous cell-based selector (Wang et al., 2022b) . This method directly uses the top1 result of the row retriever as input to the generator. w/o hybrid selector shows that the EM drops 2.9% and F1 drops 1.6%, which proves the effectiveness of our selector approach.\nEffect of reasoner. As shown in the Table 4 , we design two baselines. BERT-large reader (Chen et al., 2020b; Wang et al., 2022b) uses BERT (Devlin et al., 2018) as encoder and solves this task by predicting the start/end tokens. w/o special tags deletes the special tags. Both the two experiments demonstrate our S 3 HQA reasoner performs the best for HybridQA task.\n\nRelated Work\nThe TextTableQA task (Wang et al., 2022a) has attracted more and more attention. As for multi-hop type dataset, previous work used pipeline approach (Chen et al., 2020b) , unsupervised approach (Pan et al., 2021) , multigranularity (Wang et al., 2022b) , table pre-trained language model (Eisenschlos et al., 2021) , multiinstance learning (Kumar et al., 2021) and graph neural network (Feng et al., 2022) to solve this task. As for numerical reasoning task, which is quite different from multi-hop type dataset, there is also a lot of work (Zhu et al., 2021; Zhao et al., 2022; Zhou et al., 2022; Lei et al., 2022; Li et al., 2022; Wei et al., 2023) to look at these types of questions. Unlike these methods, our proposed three-stage model S 3 HQA can alleviate noises from weakly supervised and solve different types of multi-hop TextTableQA questions by handling the relationship between tables and text.\n\nConclusion\nThis paper proposes a three-stage model consisting of retriever, selector, and reasoner, which can effectively address multi-hop TextTableQA. The proposed method solves three drawbacks of the previous methods: noisy labeling for training retriever, insufficient utilization of heterogeneous information, and deficient ability for reasoning. It achieves new state-of-the-art performance on the widely used benchmark HybridQA. In future work, we will design more interpretable TextTableQA models to predict the explicit reasoning path.\n"}
{"question": "In what way is the method constructed in this paper?", "evidence": "  Few-shot paraphrasing Paraphrasing is one of the best methods for data augmentation in NLP. One of the most popular approaches for paraphrasing is back-translation (BT) (Sugiyama and Yoshinaga, 2019) due to its simplicity and efficiency. Nonetheless, BT's performance depends a lot on the intermediary language. In this paper, we, instead, use a combination of prompt-learning and LLMs for paraphrasing. In few-shot paraphrasing, an LLM rewrites a sentence given an instruction and a few examples. We believe that LLMs generate high-quality paraphrases due to their encoded semantic and sentence structure knowledge. We utilize GPT-3 (Brown et al., 2020b) or OPT-175B (Zhang et al., 2022) via their official APIs 2 for generating paraphrases. ", "options": ["A. Few-shot paraphrasing.", "B. Prompt-learning and back-translation.", "C. A combination of prompt-learning and LLMs for paraphrasing.", "D. Back-translation."], "answer": "C", "content": "\nIntroduction\nPre-trained language models (PLMs) are trained on large-scaled corpora in a self-supervised fashion. They have fundamentally changed the NLP community in the past few years by achieving impressive results in various Tasks (Devlin et al., 2018; Radford et al., 2018; Yang et al., 2019; Chiang et al., 2022) . However, when PLMs are finetuned on small datasets, their performance declines. Researchers have proposed various techniques to adapt PLMs to these scenarios (Snell et al., 2017; Sung et al., 2018) . In addition to performance, fine-tuning PLMs to learn a new task is parameter inefficient, because an entirely new model is required for every task (Houlsby et al., 2019) .\nBy the introduction of GPT-3 (Brown et al., 2020b) with 175B parameters, it has been shown that Large Language Models (LLMs) are efficient few-shot learners as they can use their knowledge more effectively. One of the key features of these LLMs is their ability to perform multiple tasks using prompts. A language prompt is a piece of text that is added to the input query to help the model make more accurate predictions. In addition, LLMs can be fine-tuned for specific tasks using few examples. This has made them powerful tools for NLP tasks, especially in few-shot scenarios. However, that might not be practical for many situations because of the model size. Therefore, there is a need to adapt smaller PLMs to work in a similar way to LLMs.\nPrompt-based fine-tuning is a method for adapting PLMs to specific tasks or domains by providing a prompt (Schick and Sch\u00fctze, 2020a,b) . This approach has been shown to be effective in various NLP tasks, including text classification (Han et al., 2021; Wang et al., 2022) and question answering (Yao et al., 2022) . However, it can be challenging to achieve strong performance when only a few examples are available for each task. Gao et al. (2020) introduced a prompt-based fine-tuning method called LM-BFF for RoBERTa (Liu et al., 2019) to tackle this issue. Their approach includes automated prompt generation and a more effective way of using task examples in fine-tuning.\nBuilding on the success of LM-BFF and considering contrastive learning's promising results both in computer vision (Chen et al., 2020) and NLP (Chen et al., 2020; Miao et al., 2021) , Jian et al. (2022) present a contrastive learning framework to improve LM-BFF. They propose a Supervised Contrastive Learning (SCL) approach (Khosla et al., 2020 ) that classifies inputs using different augmented views of the data. These views are created using different templates for their demonstrations when building prompts.\nIn this paper, we show that while SCL at the feature space can be beneficial, the use of different templates can limit the full potential of this approach. We propose LM-CPPF (Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models), in which we integrate the knowledge of LLMs like GPT-3 and OPT-175B (Zhang et al., 2022) to build different views using paraphrasing. These models can generate paraphrases of a sentence with different syntax, not just by changing the lexicalization. Previous studies have considered generating paraphrases a challenging and costly NLP task (Siddique et al., 2020; Garg et al., 2021; Zhou and Bhat, 2021) . However, PLMs can generate paraphrases easily and effectively using in-context learning with few examples. Although prior research has studied paraphrase generation with PLMs (Roy and Grangier, 2019; Hegde and Patil, 2020) , to the best of our knowledge, this is the first time that large LLMs are utilized to generate paraphrases with prompts as an augmentation method. Our experiments on six different text classification tasks demonstrate that LM-CPPF outperforms the previous SOTA methods of data augmentation in prompt-based fine-tuning, including Easy Data Augmentation (EDA) (Wei and Zou, 2019) , Back Translation (BT) (Sugiyama and Yoshinaga, 2019) , and multiple templates (Jian et al., 2022) .\n\nRelated Works\nLLMs like GPT-3 (Brown et al., 2020a) can perform NLP tasks with few examples and natural prompts. But smaller models are not efficient with this approach and there are data sparsity and prompt sensitivity issues. To address these challenges, Gao et al. (2021) propose LM-BFF, a framework that leverages a large PLM to automatically generate task-specific prompts for smaller models. It improves their few-shot performance on different NLP tasks. Some work have enhanced LM-BFF with different prompt tuning methods. For example, Zhou et al. (2022) present a dual context-guided continuous prompt tuning method that uses the language context and connects discrete and continuous prompt tuning. Jian et al. (2022) integrate contrastive learning and data augmentation with LM-BFF. In their contrastive part, in addition to comparing different instances from the same or different classes, they introduced a novel promptspecific augmentation method. In their approach, they change the template of the prompt. In this paper, we use few-shot paraphrasing with LLMs for contrastive prompt-tuning, which fine-tunes models with natural prompts.\nParaphrasing is the task of expressing the same meaning with different words or structures. It can be used to create training data with increased diversity and naturalness for NLP tasks, such as text classification (Xie et al., 2020) , natural language inference (Kumar et al., 2019) , and text summarization (Loem et al., 2022) , surpassing the limitations of traditional approaches. Paraphrasing helps with data scarcity and model generalization. There are different ways to generate paraphrases for data augmentation. One is back-translation (Sennrich et al., 2016) , which uses a translation system to convert a sentence to another language and back. Another is to use paraphrasing models trained on parallel paraphrase datasets (Wieting and Gimpel, 2018; Zhu et al., 2022) . PLMs can also generate paraphrases by using large-scale corpora, but they may produce paraphrases that are not semantically consistent or relevant. LLMs can reduce this problem as they encode and generate language better. In this paper, we generate paraphrases by carefully prompting LLMs and then use them for data augmentation.\n\nMethod\nBackground Contrastive learning's success relies on data augmentation, which creates new views of the input data. Contrastive learning has been utilized for various tasks in deep learning (Le-Khac et al., 2020; Conde and Turgutlu, 2021; Abaskohi et al., 2022) ; however, most NLP data augmentation methods may influence semantics which results in limited improvement. For instance, EDA's synonym substitution may create entirely new samples since words do not have equal senses (Keselj, 2009) . In addition to these augmentation methods, the approach used in Jian et al. (2022) cannot be counted as data augmentation as the sample is still the same and only the template for the verbalizer changes. Although it is a creative approach designed specifically for the prompt-based method of LM-BFF, it is limited in performance even compared to EDA in several benchmarks. Furthermore, it requires an expert to create multiple templates for each task, which makes it challenging for newly emerged tasks. Here we propose leveraging LLMs to generate paraphrases and introduce LM-CPPF, a novel approach aimed at addressing the challenges associated with contrastive prompt-based fine-tuning of PLMs.\nFew-shot paraphrasing Paraphrasing is one of the best methods for data augmentation in NLP. One of the most popular approaches for paraphrasing is back-translation (BT) (Sugiyama and Yoshinaga, 2019) due to its simplicity and efficiency. Nonetheless, BT's performance depends a lot on the intermediary language. In this paper, we, instead, use a combination of prompt-learning and LLMs for paraphrasing. In few-shot paraphrasing, an LLM rewrites a sentence given an instruction and a few examples. We believe that LLMs generate high-quality paraphrases due to their encoded semantic and sentence structure knowledge. We utilize GPT-3 (Brown et al., 2020b) or OPT-175B (Zhang et al., 2022) via their official APIs 2 for generating paraphrases.\nTo avoid violating the prompt-based fine-tuning settings, we do not include any additional task data in generating our paraphrases. Following the fewshot setting in LM-BFF, we assume to have access to a PLM M , datasets D train , and D test with label space Y where there are only K = 16 examples per class in D train . We use this setting for both promptbased few-shot paraphrasing and fine-tuning. To generate paraphrases, excluding the one sample that we want to paraphrase, we use QuillBot 3 to create paraphrases for our prompts for the remaining 15 samples in the same class of D train . We leverage two types of prompts for paraphrasing: (I) Only Demonstration: Here, the samples and their paraphrased versions are given using the templates in Table C .3 to demonstrate the task of paraphrasing. (II) Demonstrations with Instruction: In addition to the previous method, this one includes instructions at the beginning of the prompt, defining paraphrasing before demonstrations. These instructions can be seen in Table C .4.\nContrastive prompt-based fine-tuning LM-CPPF consists of two steps. The first step involves calculating the Masked Language Modeling (MLM) loss by using the target sentence in the given template, the specific demonstrations in the prompt, and the verbalizer matched with the target sentence's label. We calculate the supervised contrastive loss in the second step by comparing the target prompt with another sample with the same template but different random demonstrations. This comparison sample can be in the same or a different class as the target prompt. When the comparison sample belongs to a different class, it is randomly sampled from the dataset. However, in cases where the comparison sample belongs to the same class, an alternative approach is employed. This involves either selecting another sample from the same class \n\nExperiments\nEvaluation datasets and protocol Our method is evaluated on six different classification tasks from LM-BFF (Liu et al., 2021) . The reported numbers represent the average accuracy from five runs using Roberta-base (Liu et al., 2019) . In Section 4.1 where LLMs are compared for paraphrasing, we also employed pre-trained and fine-tuned GPT-2 as an additional model for paraphrasing, allowing us to leverage smaller models in our experiments.\nFor the fine-tuning of GPT-2 specifically for paraphrasing, we utilized the ParaNMT-50M (Wieting and Gimpel, 2018) dataset. More details regarding the training process can be found in Appendix A.\n\nParaphrasing in Prompt Fine-tuning\nThis section presents the results of our fine-tuning approach using paraphrasing on various NLP tasks.\nAs shown in Table 1 , LM-CPPF improves the model's accuracy on all tasks compared to the baseline method of LM-BFF+Multi-templates (Jian et al., 2022) . Comparing the standard deviation of our model in five runs and the standard deviations of LM-BFF and LM-BFF + Multi-templates, we see that LM-CPPF has a higher standard deviation as it uses an intermediary model for generating paraphrases. In contrast, LM-BFF + Multi-templates integrates templates that have nearly equal performance (Jian et al., 2022) .\nWe also compare the effect of using GPT-3, OPT-175B, and GPT-2 as our language model for fewshot paraphrasing. We did two experiments with GPT-2 large: (I) Using a pre-trained version of GPT-2 where the weights are not tuned at all (II) Fine-tuned GPT-2 where the model has been finetuned on the ParaNMT-50M dataset. The results in Table 1 indicate that GPT-3 outperforms OPT-175B in all tasks and GPT-2 has a lower performance, which was predictable since it has significantly fewer parameters. Also, fine-tuned GPT-2 shows a better performance which suggests that GPT-2's knowledge after pre-training is not enough for doing a task like paraphrasing. About the LLMs, although both models have 175B parameters, OPT-175B has a 1/7 carbon footprint of GPT-3, and it is also freely available (Zhang et al., 2022) . Consequently, we base our further analysis on OPT-175B.\n\nFew-shot Paraphrasing vs. Other Data Augmentation Methods\nIn this section, we present an experimental comparison of the performance of the few-shot paraphrasing approach and other data augmentation methods, including BT and EDA. The results are shown in than EDA. We believe the reason is that BT can be more effective for longer sequences because longer sequences usually contain more context and nuanced meaning. Moreover, EDA employs additional knowledge from another PLM in certain actions, such as synonym substitution, similar to BT and few-shot paraphrasing.\nThe few-shot paraphrasing approach introduced in this work outperforms both BT and EDA. This confirms that using PLM's knowledge properly in paraphrasing is an effective and efficient data augmentation method. In few-shot paraphrasing, we instruct the model to generate paraphrases that differ in lexicalization and sentence structure.\n\nPrompt Template Evaluation\nAs the heart of our method is the few-shot paraphrase generation done by LLMs, we investigate the impact of different paraphrasing prompt demonstrations and instruction templates on the performance of our model. Table 3 shows that the last template presented in Table C .3 is better in almost all tasks. This template, \"<Original Text>, in other words <Paraphrased>\", uses a complete and concrete sentence, unlike other templates, which use specific tokens, such as \"[Original]\", to dis- tinguish between the original and the paraphrased version. Also, we compare different instruction templates presented in Table C .4. As we aimed to report our best result in each task here, we used the best demonstration template for any particular task, which was determined in Table 3 . Table 4 shows that the fourth template achieves the best performance, as it precisely describes the task with its instruction \"Generate a paraphrase of the following text using different words and sentence structures while still conveying the same meaning\".\n\nConclusion\nOur experiments demonstrated the effectiveness of using few-shot paraphrasing as a data augmentation method for contrastive prompt-based fine-tuning of PLMs. It outperformed other data augmentation methods in text classification tasks, such as EDA, multiple templates, and back translation. We also found that our approach is effective with GPT-3 or OPT-175b models in generating paraphrases. Overall, LM-CPPF improves the performance of LM-BFF by large margins using contrastive learning applied on paraphrases generated by LLMs.\n"}
{"question": "What methods do the authors use to extract mine the inter-utterance semantic information from different perspectives?", "evidence": "  We utilize one-layer MLP structure to mine the inter-utterance semantic information from different perspectives. This improves the ability to predict the correct token between incomplete utterance and rewritten utterance. Benefiting from the fact that our model effectively employs MLP to IUR task, allowing our approach to achieve significant results in terms of performance and inference speed. This study represents the first preliminary exploration of the use of MLP on IUR task. In the future, we will investigate on extending our approach to other dialogue areas. ", "options": ["A. One-layer MLP structure.", "B. A joint feature matrix. ", "C. BatchNorm and LayerNorm.", "D. Ablation study."], "answer": "A", "content": "\nIntroduction\nMulti-turn dialogue modeling is a research area focusing on developing systems that can engage in multiple conversation turns with humans. This type of modeling is often used in the field of humanmachine interaction to improve the ability of artificial intelligence systems to communicate with humans in a natural and intuitive way. One of the challenges of multi-turn dialogue modeling is to accurately understand and respond to the context and meaning of the conversation, as well as to handle incomplete or ambiguous utterances that may be used for brevity or to convey meaning. As shown in Table 1 , the incomplete utterance u 3 refers to the semantic of \"\u65b0\u51a0\u80ba\u708e\" (COVID-19) with \"\u90a3\" (that). The limited context provided by a single utterance, such as u 3 , can lead to referential ambiguity and semantic incompleteness in downstream applications like retrieval-based dialogue systems, as demonstrated in a study by Ni et al. (2022) . In addition, Su et al. (2019) has revealed that coreference and ellipsis are prevalent in more than 70% of utterances, particularly in pro-drop u 1 and u 2 denote the context utterances. u 3 is the incomplete utterance. u 3 is the rewritten utterance.\nlanguages like Chinese. These linguistic phenomena in conversation present a significant challenge for the development of practical conversational AI systems.\nTo address this issue, recent works (Kumar and Joshi, 2016; Su et al., 2019; Pan et al., 2019; Xu et al., 2020) proposed the Incomplete Utterance Rewriting (IUR) task, which aims to transform an incomplete or context-dependent statement into a self-contained, semantically equivalent one that can be understood without any additional context. As shown in Table 1 , IUR (u 3 \u2192 u 3 ) task makes the downstream dialogue modeling more precise.\nDespite previous works achieving promising results, the speed of autoregressive generation remains a limiting factor. To improve the speed, Huang et al. (2021) fuses the sequence labeling and non-autoregressive generation, which predicts missing elements in incomplete utterance and rewritten utterance. In addition, Liu et al. (2020) formulates IUR as semantic segmentation task based on U-Net (Ronneberger et al., 2015) and achieves better performance at a faster speed. However, above mentioned models are still not simple enough.\nIn this paper, we propose a simple yet efficient solution that our model first employs MLP architecture to simultaneously mine the semantic associations between the context utterances and the incomplete utterance, and capture attention information between them. After MLP architecture, we obtain the joint feature maps and further construct the token-pair edit matrix. Finally, the above matrix is edited according to prediction edit type tokens to generate the final rewritten utterance. Experiments show that our approach achieves better performance on several datasets across different domains and languages with low resource costs and a much faster inference speed.\n\nMethodology\nIn this section, we elaborate on our proposed approach. As shown in Figure 1 , our method mainly consists of two modules: MLP backbone network and joint feature matrix. For a multi-turn dialogue utterances (u 1 , u 2 , ..., u t ), we concatenate all the context utterances to produce an m-length word sequence c = (c 1 , c 2 , ..., c m ) and employ a special mask [SEP ] to separate different context utterances. Meanwhile, all the incomplete utterances are denoted as an n-length word sequence x = (x 1 , x 2 , ..., x n ).\n\nMLP Backbone Network\nWe first concatenate the context utterances and the incomplete utterances to construct a joint m + n length word sequence H = (c 1 , c 2 , ..., c m , x 1 , x 2 , ..., x n ). Besides, pretrained language models have been found to be highly effective in various natural language processing tasks. Hence, we employ BERT (Devlin et al., 2019) to initialize the word vector matrix H, where H \u2208 R (m+n)\u00d7768 . MLP backbone network contains two MLP blocks. Specifically, the first MLP block is responsible for mining the global semantic association information between context utterances c and incomplete utterance x. The second MLP block aims to learn the confidence level for each word embedding. This further enables the model to focus on important word information. It is important for the follow-up edit type classification, including substitute, insert and none. Each MLP block contains two fully-connected layers and a nonlinearity applied independently. For clarity and simplicity, we exclude the transposition process and the whole process can be represented as:\nEQUATION\nwhere i = 1, 2, .., 768, j = 1, 2, .., m + n and \u03c3 represents GELU (Hendrycks and Gimpel, 2016) .\nIn addition, MLP backbone contains other standard architectural components: skip-connections (He et al., 2016) and LayerNorm (LN ) (Ba et al., 2016) .\nIn contrast to the approach taken by Tolstikhin et al. ( 2021), who treated the word vector matrix H as an image and employed 1 \u00d7 1 convolution on non-overlapping image patches, we directly input the word vector matrix H into the MLP backbone network. Our operation avoids the loss of semantic spatial information resulting from 1\u00d71 convolution. Furthermore, since the number of words in each utterance varies, we utilize padding operation and copy mechanism (Gu et al., 2016; Zeng et al., 2018) to maintain a consistent sequence length. It is worth noting that our approach employs a one-layer MLP backbone network.\n\nJoint Feature Matrix\nFurthermore, to further capture the relevance between word embeddings, we employ three similarity functions: dot product similarity (dot Sim.), cosine similarity (cos Sim.), and linear similarity (linear Sim.). The word-to-word embeddings relevance between each context utterance's word embedding K cm and each incomplete utterance's word embedding K xn are captured using a 3dimensional joint feature matrix J(c m , x n ) represented as follows:\nEQUATION\nFinally, we employ BatchNorm (Ioffe and Szegedy, 2015) on joint feature matrix J(c m , x n ) to expedite and stabilize the training process. The batch is obtained by computing the mean and variance of the batch activation, which captures global information. After applying the BatchNorm operation, the matrix J(c m , x n ) is flattened, and each feature vector is mapped to one of three token types: Substitute, Insert, or None. This generates the token-pair edit matrix.\n\nSupervised Label\nPrior to training our model in the supervised fashion, we need to create word-level labels through the following process to construct our training set. Specifically, we first calculate the longest common subsequence (LCS) between the incomplete utterance and the rewritten utterance. Then, we align the incomplete utterance, the rewritten utterance, and the LCS using a greedy strategy. Finally, we identify the corresponding tokens in the rewritten utterance and mark them accordingly. Please refer to Algorithm 1 in Appendix A for a detailed description.\n\nExperimental Setup\nDatasets We conduct the experiments on three IUR benchmarks from different domains and languages, including RESTORATION-200K (Pan et al., 2019) , REWRITE (Su et al., 2019) and CANARD (Elgohary et al., 2019) . The statistics of the datasets are shown in Appendix B.\nBaselines We compare the performance of our method with the following baselines: (i) Generation models need to generate rewritten utterances from scratch, including Seq2Seq model L-Gen (Bahdanau et al., 2015) , the hybrid pointer generator network L-Ptr-Gen (See et al., 2017) , the basic transformer models T-Gen and T-Ptr-Gen (Vaswani et al., 2017) , Syntactic (Kumar and Joshi, 2016) , PAC (Pan et al., 2019) , L-Ptr-\u03bb and T-Ptr-\u03bb (Su et al., 2019) . The above models are limited by the speed of generation. (ii) Structure aware models contain RUN (Liu et al., 2020) and SARG (Huang et al., 2021) .\nFor more information about other experimental setups, please see Appendix B.\n\nMain Results\nTable 2 shows the experimental results on RESTORATION-200K. Our proposed approach, MIUR, achieves competitive results compared to all previous State-of-the-Art methods as shown in Table 2 . The results indicate MIUR can effectively mine the semantic information between utterances with two types of MLP architecture. Furthermore, we discovered that MIUR places more emphasis on rewriting precision (P n ) metrics. The first MLP architecture captures global semantic associations between context utterances and incomplete utterance, while the second MLP architecture focuses more on significant word embedding information. Our approach effectively combines two different MLPs and provides an effective guideline for the subsequent construction of the joint feature map matrix, leading our approach to concentrate more on essential word information and to pursue higher rewriting precision. Additionally, we achieve comparable Recall n results to the baselines. The experimental results of REWRITE and CANARD also come to the same conclusion, which can be found in Appendix C. \n\nModel\nP 1 R 1 F 1 P 2 R 2 F 2 P 3 R 3 F 3 B 1 B 2 R 1 R 2 Syntactic 67\n\nInference Speed\nTable 3 presents a comparison of the inferential speed of our model with the baselines. All models were implemented in PyTorch and run on a single NVIDIA V100. We can observe that the proposed MIUR achieves the fastest inference speed compared with the SOTA methods. Specifically, MIUR's speed is 3.14 times faster than that of L-Gen (n_Beam=1). Moreover, Compared with RUN in the second place, MIUR achieves 20% improvement in the inference speed. This enhanced performance can be attributed to the fact that our model employs only a one-layered MLP backbone to capture inter-utterances semantic information, without utilizing other modules. The simplified architecture, thus, contributes to the model's faster inference speed without compromising the performance. n_Beam stands for the beam size in beam search, not applicable for RUN and MIUR.\n\nAblation Study\nTo verify the effectiveness of MLP architecture in our model, we conduct a thorough ablation study in Table 4 As mentioned in Section 2.1, we perform an ablation study about using two different padding strategies to ensure consistent sequence length. Table 5 indicates that the model obtains a small performance improvement using copy mechanism, which further increases the semantic interaction between utterances. But this operation limits inference speed. Given a tiny improvement using copy mechanism, our model employs zero padding method. \n\nMore Discussion for MLP\nTo further investigate whether our proposed MLP backbone can effectively mine the semantic associations between utterances, we visualize the word embeddings composed of the context utterances and the incomplete utterance in Figure 2 . The yaxis represents our selection of 40 words consisting of the context utterances and the incomplete utterance. The x-axis represents the features of the first 100 dimensions of our intercepted word embeddings. It is not difficult to notice that word embeddings appear more distinctly characterized by vertical stripes after MLP backbone. Consequently, this further indicates that semantic information between words is more closely related, and our method can effectively learn the semantic relatedness between words after passing through the MLP network we designed. \n\nConclusion & Future Work\nIn this paper, we propose a simple yet effective IUR method. We utilize one-layer MLP structure to mine the inter-utterance semantic information from different perspectives. This improves the ability to predict the correct token between incomplete utterance and rewritten utterance. Benefiting from the fact that our model effectively employs MLP to IUR task, allowing our approach to achieve significant results in terms of performance and inference speed. This study represents the first preliminary exploration of the use of MLP on IUR task. In the future, we will investigate on extending our approach to other dialogue areas.\n"}
{"question": "What is the main focus of this paper?", "evidence": "  In this work, we aim to build a multi-intent text revision system that could revise texts without explicit intent annotation. We present our method: a prefixtuning-based model which adapts to text revision with multiple edit intentions.  ", "options": ["A. Training a generative language model", "B. Text revision with multiple edit intentions", "C. Improving machine translation", "D. Speech recognition techniques "], "answer": "B", "content": "\nIntroduction\nRevision is an essential process to improve the text quality (Vaughan and McDonald, 1986) . During this process, writers perform various editing operations on the text with different editing intentions. As shown in Figure 1 , the writer corrects misspelled words to improve text fluency, deletes redundant words to improve text clarity, adds connective words to improve text coherence, inserts adverbs to convey the writer's writing preferences (style) and modifies data to update text information (meaning-changed).\nLots of recent studies have focused on a text revision task corresponding to a specific edit intention, such as grammatical error correction (Omelianchuk She went to the markt\nThe changes made the paper better than before.\nText Revision She works hard.\nShe is successful.\nEverything was rotten.\n\nShe went to the markt market\nThe changes made the paper better than before improved the paper.\nShe works hard. She; therefore, she is successful.\nEverything was awfully rotten. This method improves the model accuracy from 64% to 7883%. et al., 2020; Kaneko et al., 2020; Liu et al., 2021; Yang et al., 2022 ), text simplification (Dong et al., 2019; Jiang et al., 2020; Omelianchuk et al., 2021; Martin et al., 2022) , and text style transfer (Malmi et al., 2020; Reid and Zhong, 2021) . The work divides text revision into several independent problems. While some methods with strong universality can be applied to multiple tasks (Malmi et al., 2019; Stahlberg and Kumar, 2020; Mallinson et al., 2020) , they train different models on various data sets. Real-world scenarios require addressing multiple types of editing errors at the same time, such as grammatical errors, spelling errors, etc. But these methods failed to integrate knowledge from these tasks into a unified model.\n\nmeaningchanged\nTo solve the problem, Du et al. (2022) attempted to train one model using data with multiple editing intentions and leveraged edit intent information by simply appending it to the input. However, when adding a new intent, the entire model must be re-trained. A more lightweight and scalable approach to multi-intent text revision is still required.\nLi and Liang (2021) proposed a new kind of prompt tuning method to quickly adapt a pretrained model to new tasks, which is called prefixtuning. Prompt tuning can help the pre-trained language model to locate the task learned in pretraining and enable the related knowledge to model text revision with different edit intentions (Reynolds and McDonell, 2021) . This method enables a model to handle multiple edit intentions in a lightweight and scalable way.\nIn this paper, we present our method: a prefixtuning-based model which adapts to text revision with multiple edit intentions. This method involves a two-step training process. In the first step, we initialize a pre-trained language model (PLM) and train multiple prefixes on it. Each edit intention corresponds to a prefix. In the second step, a prefix transfer module is trained at each attention layer of the PLM. The prefix transfer module is configured as two attention units that act respectively on this layer's key states and value states. It enables our model to learn a tailored prefix for the given input with the help of prefix embeddings from the predefined tasks.\nWe conduct experiments on ITERATER (Du et al., 2022) , an iterative text revision dataset. It mainly contains parallel sentences with five edit intentions: fluency, coherence, clarity, style, and meaning-changed. The results show that our approach performs better than the fully fine-tuned BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020) baselines reported in Du et al. (2022) with fewer training parameters.\n\nIterative Text Revision\nFor the first time, Du et al. (2022) systematically studied the iterative revision phenomenon in human writing. They presented the ITERATER, an annotated dataset across multiple domains of formally human-written text, which includes Wikipedia, ArXiv, and Wikinews. And they trained several types of text revision models using ITERATER. Dwivedi-Yu et al. (2022) presented EDITEVAL, an instruction-based benchmark, to evaluate the editing capabilities of models and they also included the test set of ITERATER in it. Based on Du et al. (2022) , our work further explores the method of text revision.\n\nTransfer Learning of Prompt Tuning\nTransfer learning is a common and powerful technique in NLP (Raffel et al., 2020) . Some recent studies have tried to improve prompt tuning performance by leveraging the knowledge of multiple related or unrelated tasks. Asai et al. (2022) used an attention module to make use of the knowledge in exiting soft prompts (Lester et al., 2021) while learning a new task. Chen et al. (2022) improved the few-shot text summarization by multi-task pretraining and prefix-tuning. Specifically, they pretrained a summarization model on a set of popular summarization datasets and then conducted prefixtuning for it on an unseen summarization task. Different from their modeling of a new task through existing tasks, our work aims to achieve the mutual utilization of knowledge between different edit intents in text revision.\n\nMethod\nThe revision task can be defined as the following process: given a source sentence x = [x 1 , . . . , x m ] and an optional edit intent e \u2208 E to generate a revised sentence y = [y 1 , . . . , y n ], where E is the set of all edit intentions. Note that e is optional because it can be inferred from the input x.\nOur method is depicted in Figure 2 . It includes two stages: the multi-prefix tuning stage and the prefix transfer stage.\n\nMulti-Prefix Tuning Stage\nThe prefix is a set of parameters on every attention layer of PLM. For an edit intention e, at each attention layer, the prefix can be described as P e = {P K e , P V e }, where P K e and P V e are parameters added before the key states and value states in this attention layer. After adding these parameters, the calculation of the attention head in this layer becomes:\nH = Attention(Q, [P K e ; K], [P V e ; V ]) (1)\nwhere H is the output vector sequence; Q, K, V are query states, key states, and value states, respectively; Attention means scaled dot-product attention. Only P K e and P V e are updated during the training process. Note that we ignore the layer number information because the operation for each layer is the same.\nAs shown in the left part of Figure 2 , for every edit intention e, we train a prefix P e accordingly. In this way, the model could revise an intentionannotated text by activating the corresponding prefix at inference.\n\nPrefix Transfer Stage\nIdentifying edit intention is always an ambiguous work. At the prefix transfer stage, we aim to build a new prefix for an unannotated input instance by transferring existing prefixes. The new prefix P new is instance-specific.\nThe prefix transfer stage is described in the right part of Figure 2 . At each layer, we rearrange the prefixes {P e | e \u2208 E} obtained in the last stage as\nP K = {P K\ne | e \u2208 E} and P V = {P V e | e \u2208 E} according to whether they are configured before the key states or before the value states. Then a pair of attention units G K and G V are trained for P K and P V .\nTake G K as an example. It calculates the similarity between the key states K and every P K e in P K to get attention scores.\nThe similarity can't be calculated directly, because K and P K e have different lengths. So we perform the max-pool operation for length dimension on K and P K e . After that, we obtain K \u2208 R d and P K e \u2208 R d , where d is the dimension of the hidden states in the PLM.\nTo get attention scores, we train a fully connected layer to extract features from K:\nEQUATION\nwhere W \u2208 R d\u00d7d is a transfer matrix updated during training. Following Asai et al. (2022) , we use SiLU (Elfwing et al., 2018) for the non-linear layer and add a Layer Norm (Ba et al., 2016) layer:\nEQUATION\nThen, we calculate the attention scores for intent e as follows:\nEQUATION\n)\nwhere T is the softmax temperature (Radford et al., 2021) which could avoid making the attention unit over-confident.\nFinally we use them to build P K new as follows:\nEQUATION\nIn the same way, we get P V new by G V . Using the new prefix P new = [P K new , P V new ], our system could revise the unannotated input instance with the knowledge from existing prefixes.\n\nExperimental Setup\nWe choose BART-large as the PLM for our system and use adapter-transformers (Pfeiffer et al., 2020) to implement prefix-tuning. More implementation details are in Appendix A.\n\nDatasets\nWe conduct our experiments on the iterative text revision dataset: ITERATER (Du et al., 2022) . We remove the Other class of the data as it essentially contains a variety of unrecognized edit intentions and accounts for a small proportion (1.44%). The entire dataset consists of two parts: ITERATER-HUMAN and ITERATER-FULL. The former is a smaller dataset with manual annotation of edit intentions, while the latter is a large dataset annotated by a classification model trained on ITERATER-HUMAN. We train our model on both of them. Following Du et al. (2022) , we report the results on the test set of ITERATER-HUMAN in Section 5, which is completely a human-created dataset and is reliable for evaluation. We show more details of the datasets in Appendix B.\n\nEvaluation Metrics\nFollowing previous work, we report three metrics: SARI (Xu et al., 2016) , Rouge-L (Lin, 2004) , and BLEU (Papineni et al., 2002) . Among them, SARI is considered an important metric in situations where input text and output text have a large overlap in words. It also indicates the positive impact of revisions on document quality. The setting of evaluation metrics is the same as Du et al. (2022) . We use the metrics package from Huggingface transformers (Wolf et al., 2020) to calculate the SARI, BLEU, and Rouge-L scores.\n\nModels Setup and Baselines\nUsing our method, we train the models in two ways: the model that only trains the multi-prefix tuning stage and that trains both the multi-prefix tuning stage and the prefix transfer stage.\nWe compare our method with three baselines: full fine-tuning BART (BART-FineTune), full finetuning PEGASUS (PEGASUS-FineTune), and prefixtuning of BART with a single prefix (BART-SinglePrefix). Both BART and PEGASUS are generative models based on the transformer architecture. Compared to the edit-based model FELIX, they perform better. We use the results reported by Du et al. (2022) for these two models. Furthermore, we compare BART-SinglePrefix as a possible technical solution as we choose BART as our backbone model. BART-SinglePrefix trains only one prefix on the entire dataset.\nAll three baselines are trained with two config-urations. The first configuration is using the pure sentence pairs without edit intention annotations to train the model. The second configuration is appending an edit intent token at the beginning of the input text during the training process, which is the same as the approach of Du et al. (2022) .\n5 Results and Analysis\n\nMain Results\nThe main results are shown in Table 1 . Compared to training with a single prefix, the setting of multiple prefixes can improve the results, especially training on ITERATER-HUMAN. Meanwhile, with fewer training parameters, the multi-prefix setting could achieve a comparable SARI score and better average score than the fully fine-tuned BART and PEGASUS baselines. Moreover, prefix transfer could further improve the model's performance. Training on ITERATER-HUMAN, prefix transfer significantly improves the SARI score from 33.12 to 36.01 and gets the highest average score of 67.91. Training on ITERATER-FULL, prefix transfer can also improve the average score from 67.23 to 68.36.\nAn interesting phenomenon is that training on different datasets results in different gains for prefix transfer in evaluation metrics. On ITERATER-HUMAN, prefix transfer improves the SARI score significantly. While on ITERATER-FULL, prefix transfer mainly improves the BLEU score and Rouge-L score. One possible explanation is that in situations when the training data is small, prefix transfer tends to learn more editing operations to improve text quality. In this way, the SARI score related to editing operations will be improved significantly. When the training data is sufficient, pre- fix transfer will model the gold reference in more detail. So the BLEU score and the Rouge-L score will be improved.\n\nAnalysis\nWe further tried to use different training data at different stages of training to conduct experiments.\nThe results are shown in Table 2 . We find that the best practice is to train the model on ITERATER-FULL in the multi-prefix tuning stage and on ITERATER-HUMAN in the prefix transfer stage, which gets the highest SARI score and average score. This may be because of the different distributions of manually annotated edit intent and automatically annotated edit intent. The auto-annotated dataset ITERATER-FULL contains many incorrectly classified sentences, which may cause mismatched knowledge in prefixes. In the prefix transfer stage, due to the existence of mismatched knowledge and incorrectly classified sentences, the continued use of the same training data may finally cause a certain degree of negative transfer. However, if we use ITERATER-HUMAN in the prefix transfer stage, the impact of negative transfer will be mitigated, because ITERATER-HUMAN only contains correctly classified sentences.\nIn Appendix C, we separately provide the performance results on different edit intentions of the best-performing model.\n\nConclusion\nIn this paper, we introduce a new method for multiintent text revision. The system is based on prefixtuning, which first obtains a prefix for every edit intention and then learns to transfer the knowledge in prefixes for every input instance by training a prefix transfer module. This prefix transfer module is configured as two attention units that act respectively on the key states and the value states at each attention layer of the PLM. In this way, our method can make full use of the knowledge of various edit intentions and does not need to anno-tate the intentions of the input. The experimental results show that our method significantly outperforms baselines, and both multi-prefix and prefix transfer settings could improve the performance.\n"}
{"question": "How does the paper define false positives in the context of ranking summaries?", "evidence": "  for the first time, we find summaries with high lexical overlap but low semantic similarity as false positives. We introduce a contrastive learning objective with instance weighting. ", "options": ["A. Summaries with high ROUGE scores", "B. Summaries with high lexical overlap but low semantic similarity", "C. Summaries with low lexical overlap but high semantic similarity", "D. Summaries with low ROUGE scores"], "answer": "B", "content": "\nIntroduction\nThe performance of sequence-to-sequence (Seq2Seq) neural models for abstractive summarization (Lewis et al., 2020; Nallapati et al., 2016; See et al., 2017; Zhang et al., 2020) has improved significantly. The dominant training paradigm of Seq2Seq models is that of Maximum Likelihood Estimation (MLE), maximizing the likelihood of each output given the gold history of target sequences during training. However, since the models generate the sequence in an auto-regressive manner at inference, the errors made in the previous steps accumulate in the next step thereby affecting the entire sequence. This phenomenon is known as exposure bias (Bengio et al., 2015; Ranzato et al., 2016) . To mitigate this * Corresponding author problem, re-ranking systems (Liu et al., 2021; Liu and Liu, 2021; Liu et al., 2022; Ravaut et al., 2022) have recently been introduced to generate a more appropriate summary.\nThere are two training objectives for applying reranking to abstractive summarization: contrastive learning and multi-task learning. The contrastive learning-based approaches deploy margin-based losses. SimCLS (Liu and Liu, 2021) and BRIO-Ctr (Liu et al., 2022) train a large pre-trained model, such as RoBERTa (Liu et al., 2019) and BART (Lewis et al., 2020) , to align the candidate summaries according to the quality. The authors use the ROUGE (Lin, 2004) score as a quality measurement. The multi-task learning-based approaches combine at least two losses that perform different roles. SummaReranker (Ravaut et al., 2022) minimizes the average over the binary cross-entropy losses optimized for each evaluation metric. In addition, BRIO-Mul (Liu et al., 2022) demonstrates that the combination of the contrastive and cross-entropy loss works complementarily and has better performance.\nIn this paper, we analyze the three main drawbacks of existing re-ranking approaches. First, we argue that current methods focus excessively on ranking summaries in terms of lexical overlap. Inspired by Zhong et al. (2020) , we conduct a preliminary study, by sorting candidate summaries in descending order based on the ROUGE score and then defining z as the rank index of the highest BERTScore summary. As demonstrated in Fig. 1 , we can observe that there is a large gap between lexical overlap and semantic similarity. In a majority (52%) of cases z > 1. Second, despite more than half of the candidates with the same ROUGE score, previous studies do not accurately reflect quality measurements as they are trained with different ranks even if they have equal scores (Appendix F). Lastly, for the first time, we find summaries with high lexical overlap but low semantic similarity as false positives (Appendix G). They can be noises during training phrase, which are not considered substantially in the prior works.\nTo address these issues, we propose a novel training method in which a re-ranker balances lexical and semantic quality. Based on a two-stage framework, our model, named BalSum, is trained on multi-task learning. We directly reflect the ROUGE score difference on a ranking loss to preserve the lexical quality as much as possible. Then, we use a contrastive loss with instance weighting to identify summaries whose meanings are close to the document. Specifically, we define novel false positives (semantic mistakes) and present a strategy to reduce their influence in ranking. Experiments on CNN/DM and XSum datasets demonstrate the effectiveness of our method. Notably, BalSum achieves an 89.67 BERTScore on CNN/DM, reaching a new state-of-the-art performance.\n\nMethod\nOur method follows the two-stage framework. Given a source document D, a function g is to generate a pool of candidate summaries C = {C 1 , C 2 , ..., C m } at the first stage:\nEQUATION\nThen, a function f is to assign scores to each candidate and select the best summary C * with the highest score at the second stage: Our goal is to train the ranking model f that identifies the correct summary from the outputs of the generation model g.\nC * = argmax C i \u2208C {f (C i , D)} (2)\n\nModel Architecture\nWe start with a bi-encoder using RoBERTa-base (Liu et al., 2019) as a back-bone neural network.\nInspired by Khattab and Zaharia (2020) , we aim to capture rich semantic units at the sentence level.\nAs shown in Fig. 2 , we insert the [CLS] tokens in front of K sentences in the document D to let them encode into multi-vector representations. Then, we compute the individual score Score k which is modeled as an inner-product:\nEQUATION\nwhere E 1 (C i ) and E k (D)(k = 1, 2, ..., K) mean the representations of [CLS] tokens for candidate summary C i and document D, respectively. We calculate the similarity score f (C i , D):\nEQUATION\nIn Appendix E, we show that our model can capture more information from documents at the sentence level.\n\nTraining objective\nRanking Loss The core idea is that the higher the quality of the candidate summary, the closer to the document. We introduce a ranking loss to f (\u2022):\nL rank = i j>i max(0, f (Cj, D) \u2212 f (Ci, D) +(\u2212cost(Ci, S) + cost(Cj, S)) * \u03bb) (5)\nwhere S is the reference summary and \u03bb is the hyper-parameter. 1 Here, cost(C i , S) = 1 \u2212 M (C i , S) is the margin, and M is the automatic evaluation metric. We define it as ROUGE. We use the same metric in previous work (Liu and Liu, 2021; Liu et al., 2022) , but the difference is that our loss directly reflects the quality measure during training. In other words, the quality was not properly reflected before because different margin ((j \u2212 i) * \u03bb) was assigned even if the candidate summaries had the same ROUGE score.\nContrastive Loss with Instance Weighting The construction of positive and negative pairs is the critical point in constrative learning. Therefore, we consider generated summaries from the same document as positive samples and irrelevant summaries from other documents as negative samples. Thus, we design a set of candidate summaries C in Eq. 1 as positive and a set of randomly sampled summaries N as negative. 2 To identify summaries whose meanings are close to the document, we introduce a contrastive learning objective with instance weighting: D) (6) We newly define summaries that have a high lexical matching but a low semantic similarity as false positives. Inspired by Zhou et al. (2022) , we design an instance weighting method to reduce the influence of false positives. We produce the weights for positives using the SimCSE (Gao et al., 2021) which is the state-of-the-art model for the sentence representation task:\nL ctr = 1 |C| C i \u2208C \u2212log \u03b1 C i \u00d7 e f (C i ,D) e f (C i ,D) + s i \u2208N e f (s i ,\n\u03b1 C i = 0, sim(C i , S) < \u03d5 1, sim(C i , S) \u2265 \u03d5 (7)\nwhere \u03d5 is a hyper-parameter of the instance weighting threshold, and sim(\u2022) is the cosine similarity score evaluated by the SimCSE model. Finally, as shown in Fig. 3 , we combine the ranking (Eq. 5) and contrastive (Eq. 6) losses:\nEQUATION\n)\nwhere \u03b3 is the scale factor of each loss and we find the optimal values (\u03b3 1 = 10, \u03b3 2 = 0.1) in Appendix H.\n3 Experiments\n\nDatasets\nWe experiment on two datasets, whose statistics are shown in Appendix C. CNN/DailyMail (Hermann et al., 2015) is the most commonly used summarization dataset which contains articles from the CNN and DailyMail newspapers.\nXSum (Narayan et al., 2018 ) is a one-sentence summary dataset from the British Broadcasting Corporation (BBC) for the years 2010 -2017.\n\nTraining Details\nWe use diverse beam search (Vijayakumar et al., 2016) to generate 16 candidate summaries. We start from pre-trained checkpoints of RoBERTabase (Liu et al., 2019) . We train BalSum for five epochs. It takes 33 hours on CNN/DM and 22 hours on XSum on a single RTX 3090 GPU. More details are described in Appendix D.\n\nMain Results\nIn terms of the two-stage framework, we compare our results with SimCLS (Liu and Liu, 2021) , Sum-maReranker (Ravaut et al., 2022) , and BRIO (Liu et al., 2022) . We apply BalSum on top of each base model which is BART or PEGASUS.\nThe results on CNN/DM are described in Table 1. BalSum outperforms a base BART model, according to gains of 2.54/1.27/2.63 R-1/2/L. Notably, while it has comparable performances on ROUGE to previous models, it achieves an 89.67 BERTScore, reaching a new state-of-the-art performance. When ranking the candidate summaries, our model can estimate the meaning of summaries without seriously degrading the lexical aspect. We argue that this is because BalSum decreases more false positives than other ranking models. We provide fine-grained analyses for this result and present a case study in Sec.3.4.\nIn addition, we apply our method on XSum, as shown in Table 2 . Though we use a different strategy to generate the validation and test data 3 , our method improves a base PEGASUS with a small margin. We believe the one of reasons is that XSum is restricted to capturing diverse semantic units because it consists of much shorter summaries (onesentence) than CNN/DM. Model BS@1 BS@3 BS@5 R@1 R@3 R@5\nOracle \n\nAnalysis\nWeighting Threshold \u03d5 Intuitively, the larger the weighting threshold, the lower false positives. We train our model with different instance weighting thresholds from 0.7 to 0.9. In Table 3 , the highest threshold (\u03d5 = 0.9) shows the best performance and it rises largely to 0.3 BERTScore compared to when not applied. We also find that increasing the threshold leads to performance improvement. Therefore, we demonstrate that false positives can be considered noise in training.\nRanking Evaluation Regardless of the number of candidates, an ideal ranking model should yield oracle results considering diverse aspects of summarization. We conduct an experiment to measure the qualities by selecting the top-k summaries after aligning the candidates through different models. As shown in Table 4 , we can see that our model shows consistent performance in both evaluation metrics depending on the k (about \u00b10.06 BERTScore, \u00b10.34 ROUGE average score). Compared to SimCLS and BRIO-Ctr, the second block in Table 4 demonstrates that BalSum captures semantic similarity best while maintaining the intermediate level from the perspective of lexical overlap quality. Moreover, we find that BalSum has the lowest drop ratio of BERTScore (\u22121.52%) from the perfect ranking \"oracle\" scores. We also investigate whether all ranked summaries by models satisfy both lexical and semantic quality. We evaluate models using F 1 which measures the cases where the higher-ranked summary \n\nConclusion\nIn this work, we propose BalSum which aims to evaluate summaries by considering the balance between lexical and semantic quality. To achieve this, we perform a multi-task learning, which aligns summaries according to their lexical overlap qualities and identifies whether they are similar to the document. In addition, to our best knowledge, our method is the first attempt to present a new perspective of false positives (semantic mistakes) in ranking and creating the model to reduce their in-fluence. Our experimental results and fine-grained analyses validate that our model achieves consistent improvements over competitive baselines.\n"}
{"question": "Which one of the following answers is true?", "evidence": "  XL-LEXEME could be ineffective in ancient languages and, in general, in languages that are not widely covered by the WiC dataset.  XL-LEXEME showed its effectiveness also in Swedish, although the WiC dataset does not cover this language.  The correlations obtained on the English and the German datasets are significantly different (p < 0.05) for all the systems that participated in the SemEval-2020 Task 1 but not for TempoBERT and Temporal Attention.  On the other side, TempoBERT and Temporal Attention obtain a Spearman correlation on English and German that is not statistically different from the systems on the SemEval-2020 Task 1 leaderboard.  The differences between XL-LEXEME and the best two systems in the leaderboard are not statically significant. XL-LEXEME obtains competitive results for the Russian language in the RuShiftEval leaderboard  ", "options": ["A. XL-LEXEME is ineffective in all languages that are not covered by the WiC dataset.", "B. XL-LEXEME is more effective than other models in all languages.", "C. XL-LEXEME obtains competitive results for the Russian language.", "D. The differences between XL-LEXEME and the best two systems in the leaderboard are significant."], "answer": "C", "content": "\nIntroduction and Motivation\nLexical Semantic Change (LSC) Detection is the task of automatically identifying words that change their meaning over time. The LSC Detection task implicitly aims to disambiguate synchronic word sense occurrences and then find differences in the word sense frequencies in different periods. Word Sense Disambiguation (WSD) is a longstudied task in Natural Language Processing (Navigli, 2009) , which consists of associating the correct sense to a word occurring in a specific context. WSD involves some crucial issues, such as relying on a fixed sense inventory. Fixed sense inventories ignore the diachronic aspect of language because they can miss older unused senses or be outdated and missing new senses.\nThe Word in Context task (WiC) (Pilehvar and Camacho-Collados, 2019) aims to overcome these issues. In this work, we train a model on the WiC task and then use it to perform LSC Detection. In the WiC task, given the word w and two different contexts C1, C2, the systems have to determine whether the meaning of w is the same in the two contexts or not. Our approach is grounded on the assumption that models trained on the WiC tasks are robust enough to transfer the knowledge learned in a synchronic setting to a diachronic one. We summarise the main contribution of this work as follows: (i) We propose a pre-trained biencoder model, called XL-LEXEME, on a largescale dataset for the WiC task, which allows us to obtain comparable lexical-based representations; (ii) We assert the effectiveness of XL-LEXEME despite the computational limitation compared to the cross-encoder architecture for the LSC Detection task; (iii) Experiments on the LSC Detection task show that XL-LEXEME outperforms state-ofthe-art LSC Detection models for English, German, Swedish, and Russian.\n\nRelated Work\nLSC Detection systems can be categorized based on the distributional embeddings used to tackle the LSC Detection task. One category is represented by those approaches that adopt type-base (i.e., static) embeddings. UWB (Praz\u00e1k et al., 2020; Praz\u00e1k et al., 2021) represents an example of this category of systems. First, it employs word2vec Skip-gram with Negative Sampling (Mikolov et al., 2013) to compute a semantic space for each corpus. It uses techniques like the Canonical Correlation Analysis (Hardoon et al., 2004) and the Orthogonal Transformation (Hamilton et al., 2016) to align the abovementioned spaces. Therefore, the cosine similarity between the vectors representing the word in two different spaces is used to detect the semantic shift.\nWith the increasing use of contextualized word embeddings, numerous approaches employing BERT-base models have been developed for LSC Detection (Montanelli and Periti, 2023; Laicher et al., 2021) . In TempoBERT (Rosin et al., 2022) , the authors exploit the concept of Masked Language Modeling (MLM), where the goal is to train a language model to predict a masked portion of text given the remaining part. In particular, they employ this technique to encode the concept of time into a BERT model. This is done by concatenating a specific token representing time to the text sequence. At inference time, TempoBERT can be used to predict the year of a sentence, masking the time reference, or to predict a masked token of the sentence conditioned by the time reference. In the same line of research, in Temporal Attention (Rosin and Radinsky, 2022) , the authors investigate the effect of modifying the model instead of the input sentence like in TempoBERT. This is done by extending the model's attention mechanism to consider the time when computing the weight of each word. The time dimension is encoded using a different query embedding matrix for each timestamp.\nAnother kind of approach exploits the information coming from other tasks to perform LSC Detection. GlossReader represents an example (Rachinskiy and Arefyev, 2021) , where a model based on XML-R (Conneau et al., 2020b) is first trained on English SemCor (Miller et al., 1994) with glosses from WordNet 3.0 (Miller, 1992) to perform WSD. Exploiting the zero-shot crosslingual characteristics of XML-R, the authors used the same model to perform LSC Detection in the Russian language. With DeepMistake (Arefyev et al., 2021) , the authors take advantage of the WiC task instead of WSD. They train a cross-encoder with XML-R as an underlying Language Model on the MCL-WiC training and development set and fine-tune on the RuSemShift dataset (Rodina and Kutuzov, 2020) . DeepMistake, differently from XL-LEXEME, relies on the cross-encoder architecture and exploits only the MCL-WiC training dataset.\n\nXL-LEXEME\nGenerally, for pairwise sentence similarity tasks, BERT models use a cross-encoder, in which the pairwise sequences are jointly encoded, and the overall vectors are used for the classification. However, in several tasks, the cross-encoder is not suitable since it cannot provide a distinct meaningful representation for each sentence. An approach to overcome this issue involves pooling the BERT out-put encoded vectors, which often results in worse performance. Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) overcomes the limitation of cross-encoders using a Siamese Network, i.e., the weights of the underlying networks are shared. SBERT encodes the two sequences separately in the BERT model exploiting the Siamese architecture. The sequence-level representation is obtained by averaging the output encoded vectors, which are directly compared using similarity measures such as cosine similarity.\nMeanwhile, cross-encoders perform better since they are trained to profit from the attention over the whole input. In this work, we introduce XL-LEXEME 1 which mirrors models for pairwise sequence similarity tasks and adapts them to the WiC task, giving prominence to the target word, i.e. the word for which we want to detect the LSC. The model takes as input two sequences s 1 and s 2 . The sequences are tokenized using subwords tokenizer, such as Sentence Piece (Kudo and Richardson, 2018) , and the special tokens <t> and </t> are used as target word delimiters (Xie et al., 2021) :\nEQUATION\nwhere N and M represent the number of subwords of the sequence s 1 and s 2 respectively, while w t i , ..., w t i+k and w t j , ..., w t j+p are the subwords of the target words. In the following, we describe the baseline cross-encoder and XL-LEXEME based on a bi-encoder. For the crossencoder, the two input sequences are concatenated by the special token [SEP ] in an overall sequence\ns = [CLS] s 1 [SEP ] s 2 [SEP ].\nIf the length of s, i.e. N + M + 3, is greater than the maximum sequence length \u03bb, then the sequence s is cut such that the length of s 1 and s 2 is less than \u03bb * = \u03bb\u22123 2 . To comply with the maximum length, the left and right contexts of the sequence are truncated. For instance, s 1 is truncated as follows:\ns 1 = w n 0 , ..., <t>, w t i , ..., w t i+k , </t>, ..., w n 1 (2) where n 0 = max(0, i \u2212 1 \u2212 \u03bb * \u2212k\u22122 2 ) and n 1 = min(N, i + k + 1 + \u03bb * \u2212k\u22122 2\n). The truncated sequence has a length \u03b3 < \u03bb. The encoded representations of each subword (v 1 , v 2 , ..., v \u03b3 ) are summed to get the encoded representation of the overall sequence, i.e. s enc = \u03b3 i v i . Finally, the vector s enc is used to compute the logits:\nlogit = log \u03c3(W s enc ) (3)\nwhere W \u2208 IR 1\u00d7d . The model is trained to minimize the Binary Cross-entropy loss function. XL-LEXEME is a bi-encoder that encodes the input sequences using a Siamese Network into two different vector representations. Each sequence is tokenized and truncated according to the maximum length \u03bb * , using Equation (2). We thus obtain the new lengths \u03b3 1 , \u03b3 2 . The vector representation is computed as the sum of the encoded subwords\n(v 1 , v 2 , ..., v \u03b3 ), i.e. s enc 1 = \u03b3 1 i v i and s enc 2 = \u03b3 2 j v j .\nXL-LEXEME is trained to minimize the Contrastive loss (Hadsell et al., 2006) :\n\u2113 = 1 2 y \u2022 \u03b4 2 + (1 \u2212 y) \u2022 max(0, m \u2212 \u03b4) 2 (4)\nwhere we adopt a margin m = 0.5. We use as default distance \u03b4 the cosine distance between the encoded representations of s 1 and s 2 , i.e. \u03b4 = cos(s enc 1 , s enc 2 ). The main advantage of XL-LEXEME concerning models based on the crossencoder architecture is efficiency. The time cost can be directly derived from the different architectures that exploit XL-LEXEME and the crossencoder baseline. The self-attention time complexity O(N 2 * d) depends on the vector dimension d and the sequence length, which is N for the cross-encoder and N 2 for XL-LEXEME. For XL-LEXEME, the time complexity is reduced to O((N^2)^2 * 2d).\n4 Experimental setting\n\nLexical Semantic Change Detection\nSemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection (Schlechtweg et al., 2020) is the first task on Unsupervised Lexical Semantic Change Detection in English, German, Swedish, and Latin languages. For each language, two corpora represent two different periods (T0, T1). Moreover, a set of target words, annotated using the DUREL framework (Schlechtweg et al., 2018) , are provided. SemEval-2020 Task 1 involves two subtasks. The binary classification task requires assigning a label (changed/stable) to each target word. The ranking task sorts the target words according to their degree of semantic change. In this work, we focus on Subtask 2, and for the sake of simplicity, we refer to SemEval-2020 Task 1 Subtask 2 as SemEval-2020 Task 1. RuShiftEval, different from SemEval-2020 Task 1, involves three sub-corpora extracted from the Russian National Corpus spanning three periods. Models are evaluated on the resulting three test sets, namely RuShiftEval1 (pre-Soviet and Soviet), RuShiftEval2 (Soviet and post-Soviet), and RuShiftEval3 (pre-Soviet and post-Soviet). RuShiftEval provides participants with development data that can be used for tuning models. RuShiftEval aims to corroborate if training data can improve LSC Detection models. The development data rely on the RuSemShift dataset (Rodina and Kutuzov, 2020) , which includes two sets of 70 target words for the pre-Soviet to Soviet period and Soviet to post-Soviet period, respectively. The dataset also includes annotated pairwise sentences, which can be used for training the models.\n\nTraining details\nXL-LEXEME and the cross-encoder are trained using XLM-RoBERTa (XLM-R) (Conneau et al., 2020a) large as the underlying Language Model 2 and using an NVIDIA GeForce RTX 3090. As for training data, the model uses the training data of MCL-WiC (Martelli et al., 2021) , AM 2 ICO (Liu et al., 2021) , and XL-WiC datasets (Raganato et al., 2020) merged with the randomly sampled 75% of the respective development data of each dataset. The remaining 25% of the development data is used to fine-tune hyper-parameters. Moreover, we augment training data for the cross-encoder by swapping the order of sentences in the training set (Martelli et al., 2021) .\nWe use AdamW optimizer and linear learning warm-up over the 10% of training data. We perform a grid search for the hyper-parameters optimization, tuning the learning rate in {1e-6, 2e-6, 5e-6, 1e-5, 2e-5} and the weight decay {0.0, 0.01}. Table 3 (Appendix A) shows the selected hyperparameters. We sample 200 sentences containing the target word for each language and each period. The sampling is repeated ten times, and the results are averaged over the ten iterations. We use the same methodology of Rachinskiy and Arefyev (2021) for sampling sentences from the RuShiftEval corpora. We sample sentences in which we find the exact match with the target words with no pre-processing of the SemEval dataset. The LSC score is computed as the average distance between the vectors over the two different periods:\nLSC(s t 0 , s t 1 ) = 1 N \u2022 M N i=0 M j=0 \u03b4(s t 0 i , s t 1 j ) (5)\nwhere \u03b4 is the distance measure, i.e. \u03b4 = 1 \u2212 log \u03c3(W s enc ) for the cross-encoder baseline and \u03b4 = cos(s enc 1 , s enc 2 ) for XL-LEXEME.\n\nResults\nTable 1 and Table 2 report the results on the SemEval-2020 Task 1 Subtask 2 and the results on the RuShiftEval test set. The results of the best systems are in bold. XL-LEXEME achieve the best score for English, German, Swedish, RuShiftEval1, RuShiftEval2, and RuShiftEval3. XL-LEXEME achieves a strong Spearman correlation for English and Swedish languages and a solid correlation on the German dataset, obtaining a significative correlation (p < 0.001). XL-LEXEME obtains no significant results in the Latin language since the predicted scores for the target words are not correlated with the test set. Latin is underrepresented in the training data of XLM-R, and there are no similar languages in the WiC dataset that we use for training XL-LEXEME. Moreover, the Latin dataset is more challenging as it involves the first corpus written in ancient Latin, which differs in many aspects from modern Latin. For this reason, XL-LEXEME could be ineffective in ancient languages and, in general, in languages that are not widely covered by the WiC dataset. We report the statistical significance of the difference between the performance of XL-LEXEME concerning the other models. The statistical significance of the difference is computed using Fisher's z-transformation (Press, 2002) . XL-LEXEME obtains stronger correlations than the cross-encoder, but the differences are not significant. The correlations obtained on the English and the German datasets are significantly different (p < 0.05) for all the systems that participated in the SemEval-2020 Task 1 but not for TempoBERT and Temporal Attention. On the other side, TempoBERT and Temporal Attention obtain a Spearman correlation on English and German that is not statistically different from the systems on the SemEval-2020 Task 1 leaderboard. In the Swedish language, XL-LEXEME is the only one obtaining a significantly different correlation from the Count baseline results. XL-LEXEME showed its effectiveness also in Swedish, although the WiC dataset does not cover this language. Presumably, Swedish benefits from the presence of other languages descending from the Old Norse language, namely Danish and Norwegian.\nXL-LEXEME obtains competitive results for the Russian language in the RuShiftEval leaderboard. Contrary to XL-LEXEME, Deep Mistake and Gloss Reader are fine-tuned on the RuSemShift dataset. The differences between XL-LEXEME and the best two systems in the leaderboard are not statically significant. Moreover, in Table 2 , the results of XL-LEXEME fine-tuned on the RuSemShift are shown. Although the fine-tuned model achieves the best correlation scores in the three datasets, the difference between DeepMistake and GlossReader is not significant.\n\nConclusion\nIn this work, we introduced XL-LEXEME, a model for LSC Detection. XL-LEXEME is pre-trained on a large WiC dataset to mirror sentence-level encoders focusing on specific words in contexts. We evaluated our model on two Lexical Semantic Change Detection datasets: SemEval-2020 Task 1 and RuShiftEval. XL-LEXEME outperforms stateof-the-art models for LSC Detection in English, German, Swedish, and Russian datasets, with significant differences from the baselines. The XL-LEXEME effectiveness and efficiency make it reliable for LSC Detection on large diachronic corpora.\n"}
{"question": "What is the primary purpose of the French-English parallel tense test set introduced in the text?", "evidence": "  We presented the French-English parallel tense test set and introduced the corresponding benchmark tense prediction accuracy, providing a brand-new approach to measure the tense consistency performance of machine translation systems. In the future, we will endeavour to generalize the test set to other languages.  ", "options": ["A. To evaluate the overall translation quality of machine translation systems.", "B. To measure the tense consistency performance of machine translation systems.", "C. To generalize the test set to other languages.", "D. To analyze the linguistic similarities between French and English tenses."], "answer": "B", "content": "\nIntroduction\nTranslation tools are often found in a variety of social situations to enable cross-linguistic communication. Tenses are used to express time relative to the moment of speaking. Human translators frequently pay close attention to tense correspondence (Gagne and Wilton-Godberfforde, 2020) . Similarly, machine translation (MT) systems are supposed to maintain temporal consistency between the original text and the predicted text to avoid misunderstandings by users. However, accurately keeping the tense consistency is undoubtedly difficult. Taking French-English (one of the most classic language pairs for MT) as an example in Table 1 , the original text is in plus-que-parfait de l'indicatif of French, corresponding to the past perfect tense in English, while the English prediction provided by Google Translator is in the past simple tense.\nIn fact, this is not an isolated case. You can also find several examples in Appendix B. Besides. the translation mechanics may not the only reason leading to tense inconsistency. The corpora matter as well. For example, we have extracted 20,000 pairs English-French parellel sentences from the widely used dataset Europarl (Koehn, 2005) , and\n\nSentence\nTense FR: Mais on les avait vot\u00e9s lors de la derni\u00e8re p\u00e9riode de session.\n\nPlus-queparfait\nEN: But we voted on them during the last part-session.\n\nPast simple\nCorrection: But we had voted on them during the last part-session. we have observed all groups of parallel utterances where the original French texts are in the plus-queparfait de l'indicatif tense, examining the tenses of their English counterparts. As a sentence may include several tenses, there are 195 occurences of plus-que-parfait tense in total. Among them, only 35.28% English sentences are in the correct past perfect tense, as shown in Table 2 . Although, compared to other tense correspondences, the pair of plus-que-parfait and past-perfect is prone to error in datasets and there are only 0.94% of sentences in Europarl are in plus-que-parfait, we cannot easily ignore this issue. Like Europarl, tense correspondences are generally credible but unreasonable for certain tenses in several common datasets. In addition to the train set, the difficulty of remaining tense consistency also stems from the lack of metrics on measuring the model's mastery of tense information. The research of Marie et al. (2021) shows that 98.8% of *ACL papers 2 in the field of MT from 2010 to 2020 used BLEU (Papineni et al., 2002) scores to evaluate their models. However, the reliability of BLEU has been questioned in the era of neural machine translation (NMT) as its variants only assess surface linguistic features (Shterionov et al., 2018) , and many studies have shown that BLEU has difficulty in portraying the degree of semantic information mastered by the model, i.e. its score does not necessarily improve when more semantic information is mastered (Mathur et al., 2020; He et al., 2023) , not to mention specific tense information. We have also applied BLEU to measure various baselines on our tense test set in Section 4, and the results explicitly support the above statement. In addition, reviewing the evaluation criteria related to MT tasks over the past ten years, we are surprised to find that there are no criteria to assess the model's mastery of tense prediction from a linguistic perspective.\n\nPast perfect\nTherefore, our paper is devoted to the study of NMT based on semantic understanding in terms of tense. We construct a tense parallel corpus test set consisting of 552 pairs of tense-rich, error-prone parallel utterances for NMT systems, and then propose a new task for evaluating the effectiveness of model translations from the perspective of tense consistency. This paper makes three contributions:\n(1) the presentation of the construction of the tense test set, including its tense labels; (2) the proposal of a feasible and reproducible benchmark for measuring the tense consistency performance of NMT systems; and (3) the various experiments for different baselines with the above test set and corresponding benchmark.\n\nAnnotation Rules and Tools\nAs the first work of the MT tense study, we choose English-French, one of the most classic language pairs of MT, to construct the dataset 3 . TENSE, the dominant topic of our research, is a combination of tense and aspect. In the modern grammar system of English, \"a tense system is a system associated with the verb where the basic contrasts in meaning have to do with the location in time of the situation, or the part of it under consideration\" (Huddleston et al., 2021) . The modern grammatical system divides tense into present and preterit based on the inflections added to the end of verbs, and the aspect into perfective and progressive on the state where an action is (Kamp, 1991) . While this tense classification system is too crude for daily life, we therefore apply the following classification methods. On the one hand, we classify the tenses according to the macro-temporal interval of the action into three major time intervals, namely present, past and future tenses; on the other hand, we classify the tenses according to the state of the action into general, progressive and perfect aspects. Hence, 9 kinds of tenses are born through combining the three tenses and the three aspects.\nFrench and English belong to the same Indo-European language family and share many similarities in various respects. The main difference is that in French there is another grammatical point called mode, part of which is like the aspect in English. In terms of tenses, we will generally discuss the tenses in the indicative mode of French and will describe the others later in this section. In the following, if there is no mode qualifier before a tense, it is by default in the indicative mode. Careful identification and comparison of the subdivided tenses in the three main tense intervals, English and French, reveals a very similar usage of the tenses, as sum-marised in Table 3 . As there is no progressive tense in French, we do not distinguish the progressive tense in English, but rather merge the progressive tense into its corresponding base tense, e.g. the present perfect progressive tense into the category of the present perfect tense.\nWhen discussing tenses from a semantic point of view, the modes also need to be taken into account. The grammatical correlations between French and English modes are quite complicated. Considering the corresponding grammatical expressions of 2 modes strongly related to tense, conditionnel and subjonctif, in French rely on the usage of modal verbs, we introduce modal verbs to simplify the distinguishment of the modes.\nBased on these grammatical rules, we merge the nine common tenses in English into seven categories that correspond reasonably and rigorously to French, namely the 6 tense categories of past/present/future + simple/perfect and statements containing modal verbs that correspond to the French subjonctif and conditionnel tenses. We construct an automatic annotation method based on the spaCy package (Honnibal et al., 2020) . First, we label the grammatical components of each word in the sentence based on the spaCy package, and then we define and compare the grammatical structures of the verb phrases with the structures of each tense classification to derive the sentence tense labels. During this process, to simplify the annotation process and better correspond with French futur proche tense, we classify the expression 'be going to do', grammatically in Future tense, into the Present tense, just like expressions 'be about to do' and 'be + verb progressive', whose stucture are in Present tense but the real meaning is about the close future. Also, a sentence may have several tense structures, in this case, the tense label consists several tenses. For example, the label of the sentence 'So it is in that spirit that we have made this change.' is 'Present+PrePerfect'.\n\nCorpus Design\nWe choose the tense-rich Europarl, namely Eu-roparlPV, processed by Lo\u00e1iciga et al. (2014) as the source corpus, for it contains all the sentences with predicate verb structures in the original Europarl dataset (Koehn, 2005) . First, we cleaned the source corpus, including deleting sentences without counterparts, English sentences in the French In the construction process, with the code mentioned in Section 2, we first automatically annotated the original English text and English prediction in the 20,000 pairs of parallel utterances, given the corresponding tense labels. Then, we filtered 6,779 parallel French-English sentence triples with different tense labels for English originals and predictions. On the basis of the automatic selection, we manually screened out the representative parallel French-English sentence pairs with a certain degree of translation difficulty and a complex grammatical structure. We also corrected the reference translations that did not justify the tense or semantics. It is worth noting that the author has a level of English and French that meets the C1 standard of The Common European Framework of Reference for Languages (CEFR), representing the ability to express herself effectively and flexibly in English and French in social, academic and work situations. A total of 570 parallel pairs of statements were selected at this stage.\nFollowing this, two other reviewers at CEFR C1 level, reviewed the tense test set for semantic and tense correspondence, and the tense labels marked by the automatic annotation code. \n\nCorpus Characteristics\nIn the following paragraphs, we describe the statistical features of our corpus and the elimination of gender coordination influence.\nTense distribution. The corpus consists of 780 tense structures in 552 sentences, and the distribution of tense classifications is shown in Table 4 . In the test set, sentences in present tense are the most, corresponding the situation of the reality: we use present tense most frequently and future perfect sense least frequently.\nElimination of gender effect. Unlike English, gender coordination exists in French. For example, the French sentences 'Nous nous sommes donc abstenus.' and 'Nous nous sommes donc abstenues.' both correspond to the English 'We therefore abstained.'. That is, the MT system's ability to learn gender coordination affects its ability to recognize tense structures, which in consequence affects the maintenance of tense consistency between original French text and predicted English sentence. Therefore, to better measure the tense-predicting capability of different MT systems, rather than their ability to recognize pronominal gender, we controlled for the gender variable by defaulting all pronouns, which do not indicate explicitly their genders, as masculine. These pronouns consists of 167 je (I), 114 nous (we, us) and 28 vous (you).\n\nExperimental Results\nTo measure the tense consistency performance of different systems, we introduce a benchmark called tense (prediction) accuracy, as shown in Eq. ( 1).\nEQUATION\nwhere N c is the number of predicted utterances with the same tense as its reference and N t is the total number of utterances in the tense set.\nTo verify the validity of our tense corpus, the following approach was adopted: To begin with, 100, 000 parallel utterance pairs from the Eu-roparlTR (containing 201, 374 pairs) mentioned in Section 3.1 were extracted as the tense-rich train set, and 100, 000 parallel utterance pairs from the Europarl corpus (Koehn, 2005) were extracted as the tense-poor train set. There were no overlapping utterances between the latter and the former. We performed the same preprocessing procedure, including data cleaning, tokenization and BPE coding. We then trained four pairs of French-English NMT systems with different architectures based on fairseq (Ott et al., 2019) , where two systems in each pair differed only in the train set. After this, we summarized the scores evaluated by Sacre-BLEU (Post, 2018) and COMET (Rei et al., 2020) and tense prediction accuracies of the eight systems on different test sets. We have applied three types of test sets: our tense set, the Europarl test set and the WMT15 test set. The Europarl test set contains 3,000 parallel utterance pairs drawn from the Europarl corpus, the exact same field of train set, while the WMT15 is a test set for the WMT15 (Bojar et al., 2015) , deriving from data in the different field of train set. Besides, we also apply our approach to mesure the tense consistency performance of several business translators, includ-ing Bing Translator, DeepL Translator and Google Translator. The results are listed in Table 5: 1) The BLEU and COMET scores based on the Europarl set and the WMT15 set are quite similar for each system pair, which indicates that the translation capabilities of the two systems are similar in the general evaluation dimension. This suggests that by relying solely on the difference in BLEU scores on traditional test sets, we are unable to measure the tense prediction ability of the systems.\n2) However, there are large differences in our tense set. The tense consistency performance of systems trained on the tense-rich train set was significantly better than that of systems trained on the tense-poor train set. This indicates that our tense set can capture the tense consistency performance.\n3) Further investigation of the BLEU or COMET) scores and tense prediction accuracy for each system reveals their positive correlation for the same architecture, but not across architectures. To measure the tense consistency performance across different architectures, we should focus more on tense accuracy rather than BLEU scores only.\n\nConclusion\nWe presented the French-English parallel tense test set and introduced the corresponding benchmark tense prediction accuracy, providing a brand-new approach to measure the tense consistency performance of machine translation systems. This test set firstly focuses on the tense prediction ability, posing a new dimension to improve the MT quality.\nIn the future, we will endeavour to generalize the test set to other languages. Considering there are statements like \"the use of tense A in language X is equivalent or similar to the use of tense B in English\" in grammar books of other languages (Durrell et al., 2015) , even across language families(Gadalla, 2017) and human translators also apply such rules(Santos, 2016), we are confident in taking this forward.\n"}
{"question": "In what way did the author take to demonstrate the effect of the MLP architecture?", "evidence": "  To verify the effectiveness of MLP architecture in our model, we conduct a thorough ablation study in Table 4. To further investigate whether our proposed MLP backbone can effectively mine the semantic associations between utterances, we visualize the word embeddings composed of the context utterances and the incomplete utterance in Figure 2 . The yaxis represents our selection of 40 words consisting of the context utterances and the incomplete utterance. The x-axis represents the features of the first 100 dimensions of our intercepted word embeddings. It is not difficult to notice that word embeddings appear more distinctly characterized by vertical stripes after MLP backbone. Consequently, this further indicates that semantic information between words is more closely related, and our method can effectively learn the semantic relatedness between words after passing through the MLP network we designed.  ", "options": ["A. A thorough ablation study.", "B. Nothing was done.", "C. Visualization method.", "D. Both visualization method and ablation study."], "answer": "D", "content": "\nIntroduction\nMulti-turn dialogue modeling is a research area focusing on developing systems that can engage in multiple conversation turns with humans. This type of modeling is often used in the field of humanmachine interaction to improve the ability of artificial intelligence systems to communicate with humans in a natural and intuitive way. One of the challenges of multi-turn dialogue modeling is to accurately understand and respond to the context and meaning of the conversation, as well as to handle incomplete or ambiguous utterances that may be used for brevity or to convey meaning. As shown in Table 1 , the incomplete utterance u 3 refers to the semantic of \"\u65b0\u51a0\u80ba\u708e\" (COVID-19) with \"\u90a3\" (that). The limited context provided by a single utterance, such as u 3 , can lead to referential ambiguity and semantic incompleteness in downstream applications like retrieval-based dialogue systems, as demonstrated in a study by Ni et al. (2022) . In addition, Su et al. (2019) has revealed that coreference and ellipsis are prevalent in more than 70% of utterances, particularly in pro-drop u 1 and u 2 denote the context utterances. u 3 is the incomplete utterance. u 3 is the rewritten utterance.\nlanguages like Chinese. These linguistic phenomena in conversation present a significant challenge for the development of practical conversational AI systems.\nTo address this issue, recent works (Kumar and Joshi, 2016; Su et al., 2019; Pan et al., 2019; Xu et al., 2020) proposed the Incomplete Utterance Rewriting (IUR) task, which aims to transform an incomplete or context-dependent statement into a self-contained, semantically equivalent one that can be understood without any additional context. As shown in Table 1 , IUR (u 3 \u2192 u 3 ) task makes the downstream dialogue modeling more precise.\nDespite previous works achieving promising results, the speed of autoregressive generation remains a limiting factor. To improve the speed, Huang et al. (2021) fuses the sequence labeling and non-autoregressive generation, which predicts missing elements in incomplete utterance and rewritten utterance. In addition, Liu et al. (2020) formulates IUR as semantic segmentation task based on U-Net (Ronneberger et al., 2015) and achieves better performance at a faster speed. However, above mentioned models are still not simple enough.\nIn this paper, we propose a simple yet efficient solution that our model first employs MLP architecture to simultaneously mine the semantic associations between the context utterances and the incomplete utterance, and capture attention information between them. After MLP architecture, we obtain the joint feature maps and further construct the token-pair edit matrix. Finally, the above matrix is edited according to prediction edit type tokens to generate the final rewritten utterance. Experiments show that our approach achieves better performance on several datasets across different domains and languages with low resource costs and a much faster inference speed.\n\nMethodology\nIn this section, we elaborate on our proposed approach. As shown in Figure 1 , our method mainly consists of two modules: MLP backbone network and joint feature matrix. For a multi-turn dialogue utterances (u 1 , u 2 , ..., u t ), we concatenate all the context utterances to produce an m-length word sequence c = (c 1 , c 2 , ..., c m ) and employ a special mask [SEP ] to separate different context utterances. Meanwhile, all the incomplete utterances are denoted as an n-length word sequence x = (x 1 , x 2 , ..., x n ).\n\nMLP Backbone Network\nWe first concatenate the context utterances and the incomplete utterances to construct a joint m + n length word sequence H = (c 1 , c 2 , ..., c m , x 1 , x 2 , ..., x n ). Besides, pretrained language models have been found to be highly effective in various natural language processing tasks. Hence, we employ BERT (Devlin et al., 2019) to initialize the word vector matrix H, where H \u2208 R (m+n)\u00d7768 . MLP backbone network contains two MLP blocks. Specifically, the first MLP block is responsible for mining the global semantic association information between context utterances c and incomplete utterance x. The second MLP block aims to learn the confidence level for each word embedding. This further enables the model to focus on important word information. It is important for the follow-up edit type classification, including substitute, insert and none. Each MLP block contains two fully-connected layers and a nonlinearity applied independently. For clarity and simplicity, we exclude the transposition process and the whole process can be represented as:\nEQUATION\nwhere i = 1, 2, .., 768, j = 1, 2, .., m + n and \u03c3 represents GELU (Hendrycks and Gimpel, 2016) .\nIn addition, MLP backbone contains other standard architectural components: skip-connections (He et al., 2016) and LayerNorm (LN ) (Ba et al., 2016) .\nIn contrast to the approach taken by Tolstikhin et al. ( 2021), who treated the word vector matrix H as an image and employed 1 \u00d7 1 convolution on non-overlapping image patches, we directly input the word vector matrix H into the MLP backbone network. Our operation avoids the loss of semantic spatial information resulting from 1\u00d71 convolution. Furthermore, since the number of words in each utterance varies, we utilize padding operation and copy mechanism (Gu et al., 2016; Zeng et al., 2018) to maintain a consistent sequence length. It is worth noting that our approach employs a one-layer MLP backbone network.\n\nJoint Feature Matrix\nFurthermore, to further capture the relevance between word embeddings, we employ three similarity functions: dot product similarity (dot Sim.), cosine similarity (cos Sim.), and linear similarity (linear Sim.). The word-to-word embeddings relevance between each context utterance's word embedding K cm and each incomplete utterance's word embedding K xn are captured using a 3dimensional joint feature matrix J(c m , x n ) represented as follows:\nEQUATION\nFinally, we employ BatchNorm (Ioffe and Szegedy, 2015) on joint feature matrix J(c m , x n ) to expedite and stabilize the training process. The batch is obtained by computing the mean and variance of the batch activation, which captures global information. After applying the BatchNorm operation, the matrix J(c m , x n ) is flattened, and each feature vector is mapped to one of three token types: Substitute, Insert, or None. This generates the token-pair edit matrix.\n\nSupervised Label\nPrior to training our model in the supervised fashion, we need to create word-level labels through the following process to construct our training set. Specifically, we first calculate the longest common subsequence (LCS) between the incomplete utterance and the rewritten utterance. Then, we align the incomplete utterance, the rewritten utterance, and the LCS using a greedy strategy. Finally, we identify the corresponding tokens in the rewritten utterance and mark them accordingly. Please refer to Algorithm 1 in Appendix A for a detailed description.\n\nExperimental Setup\nDatasets We conduct the experiments on three IUR benchmarks from different domains and languages, including RESTORATION-200K (Pan et al., 2019) , REWRITE (Su et al., 2019) and CANARD (Elgohary et al., 2019) . The statistics of the datasets are shown in Appendix B.\nBaselines We compare the performance of our method with the following baselines: (i) Generation models need to generate rewritten utterances from scratch, including Seq2Seq model L-Gen (Bahdanau et al., 2015) , the hybrid pointer generator network L-Ptr-Gen (See et al., 2017) , the basic transformer models T-Gen and T-Ptr-Gen (Vaswani et al., 2017) , Syntactic (Kumar and Joshi, 2016) , PAC (Pan et al., 2019) , L-Ptr-\u03bb and T-Ptr-\u03bb (Su et al., 2019) . The above models are limited by the speed of generation. (ii) Structure aware models contain RUN (Liu et al., 2020) and SARG (Huang et al., 2021) .\nFor more information about other experimental setups, please see Appendix B.\n\nMain Results\nTable 2 shows the experimental results on RESTORATION-200K. Our proposed approach, MIUR, achieves competitive results compared to all previous State-of-the-Art methods as shown in Table 2 . The results indicate MIUR can effectively mine the semantic information between utterances with two types of MLP architecture. Furthermore, we discovered that MIUR places more emphasis on rewriting precision (P n ) metrics. The first MLP architecture captures global semantic associations between context utterances and incomplete utterance, while the second MLP architecture focuses more on significant word embedding information. Our approach effectively combines two different MLPs and provides an effective guideline for the subsequent construction of the joint feature map matrix, leading our approach to concentrate more on essential word information and to pursue higher rewriting precision. Additionally, we achieve comparable Recall n results to the baselines. The experimental results of REWRITE and CANARD also come to the same conclusion, which can be found in Appendix C. \n\nModel\nP 1 R 1 F 1 P 2 R 2 F 2 P 3 R 3 F 3 B 1 B 2 R 1 R 2 Syntactic 67\n\nInference Speed\nTable 3 presents a comparison of the inferential speed of our model with the baselines. All models were implemented in PyTorch and run on a single NVIDIA V100. We can observe that the proposed MIUR achieves the fastest inference speed compared with the SOTA methods. Specifically, MIUR's speed is 3.14 times faster than that of L-Gen (n_Beam=1). Moreover, Compared with RUN in the second place, MIUR achieves 20% improvement in the inference speed. This enhanced performance can be attributed to the fact that our model employs only a one-layered MLP backbone to capture inter-utterances semantic information, without utilizing other modules. The simplified architecture, thus, contributes to the model's faster inference speed without compromising the performance. n_Beam stands for the beam size in beam search, not applicable for RUN and MIUR.\n\nAblation Study\nTo verify the effectiveness of MLP architecture in our model, we conduct a thorough ablation study in Table 4 As mentioned in Section 2.1, we perform an ablation study about using two different padding strategies to ensure consistent sequence length. Table 5 indicates that the model obtains a small performance improvement using copy mechanism, which further increases the semantic interaction between utterances. But this operation limits inference speed. Given a tiny improvement using copy mechanism, our model employs zero padding method. \n\nMore Discussion for MLP\nTo further investigate whether our proposed MLP backbone can effectively mine the semantic associations between utterances, we visualize the word embeddings composed of the context utterances and the incomplete utterance in Figure 2 . The yaxis represents our selection of 40 words consisting of the context utterances and the incomplete utterance. The x-axis represents the features of the first 100 dimensions of our intercepted word embeddings. It is not difficult to notice that word embeddings appear more distinctly characterized by vertical stripes after MLP backbone. Consequently, this further indicates that semantic information between words is more closely related, and our method can effectively learn the semantic relatedness between words after passing through the MLP network we designed. \n\nConclusion & Future Work\nIn this paper, we propose a simple yet effective IUR method. We utilize one-layer MLP structure to mine the inter-utterance semantic information from different perspectives. This improves the ability to predict the correct token between incomplete utterance and rewritten utterance. Benefiting from the fact that our model effectively employs MLP to IUR task, allowing our approach to achieve significant results in terms of performance and inference speed. This study represents the first preliminary exploration of the use of MLP on IUR task. In the future, we will investigate on extending our approach to other dialogue areas.\n"}
{"question": "What is the advantage of using copy mechanism?", "evidence": "  To verify the effectiveness of MLP architecture in our model, we conduct a thorough ablation study in Table 4 As mentioned in Section 2.1, we perform an ablation study about using two different padding strategies to ensure consistent sequence length. Table 5 indicates that the model obtains a small performance improvement using copy mechanism, which further increases the semantic interaction between utterances. But this operation limits inference speed. Given a tiny improvement using copy mechanism, our model employs zero padding method.  ", "options": ["A. To ensure consistent sequence length", "B. To increase inference speed.", "C. To gain a tiny improvement and further increases the semantic interaction between utterances.", "D. To verify the effectiveness of MLP architecture in our model."], "answer": "C", "content": "\nIntroduction\nMulti-turn dialogue modeling is a research area focusing on developing systems that can engage in multiple conversation turns with humans. This type of modeling is often used in the field of humanmachine interaction to improve the ability of artificial intelligence systems to communicate with humans in a natural and intuitive way. One of the challenges of multi-turn dialogue modeling is to accurately understand and respond to the context and meaning of the conversation, as well as to handle incomplete or ambiguous utterances that may be used for brevity or to convey meaning. As shown in Table 1 , the incomplete utterance u 3 refers to the semantic of \"\u65b0\u51a0\u80ba\u708e\" (COVID-19) with \"\u90a3\" (that). The limited context provided by a single utterance, such as u 3 , can lead to referential ambiguity and semantic incompleteness in downstream applications like retrieval-based dialogue systems, as demonstrated in a study by Ni et al. (2022) . In addition, Su et al. (2019) has revealed that coreference and ellipsis are prevalent in more than 70% of utterances, particularly in pro-drop u 1 and u 2 denote the context utterances. u 3 is the incomplete utterance. u 3 is the rewritten utterance.\nlanguages like Chinese. These linguistic phenomena in conversation present a significant challenge for the development of practical conversational AI systems.\nTo address this issue, recent works (Kumar and Joshi, 2016; Su et al., 2019; Pan et al., 2019; Xu et al., 2020) proposed the Incomplete Utterance Rewriting (IUR) task, which aims to transform an incomplete or context-dependent statement into a self-contained, semantically equivalent one that can be understood without any additional context. As shown in Table 1 , IUR (u 3 \u2192 u 3 ) task makes the downstream dialogue modeling more precise.\nDespite previous works achieving promising results, the speed of autoregressive generation remains a limiting factor. To improve the speed, Huang et al. (2021) fuses the sequence labeling and non-autoregressive generation, which predicts missing elements in incomplete utterance and rewritten utterance. In addition, Liu et al. (2020) formulates IUR as semantic segmentation task based on U-Net (Ronneberger et al., 2015) and achieves better performance at a faster speed. However, above mentioned models are still not simple enough.\nIn this paper, we propose a simple yet efficient solution that our model first employs MLP architecture to simultaneously mine the semantic associations between the context utterances and the incomplete utterance, and capture attention information between them. After MLP architecture, we obtain the joint feature maps and further construct the token-pair edit matrix. Finally, the above matrix is edited according to prediction edit type tokens to generate the final rewritten utterance. Experiments show that our approach achieves better performance on several datasets across different domains and languages with low resource costs and a much faster inference speed.\n\nMethodology\nIn this section, we elaborate on our proposed approach. As shown in Figure 1 , our method mainly consists of two modules: MLP backbone network and joint feature matrix. For a multi-turn dialogue utterances (u 1 , u 2 , ..., u t ), we concatenate all the context utterances to produce an m-length word sequence c = (c 1 , c 2 , ..., c m ) and employ a special mask [SEP ] to separate different context utterances. Meanwhile, all the incomplete utterances are denoted as an n-length word sequence x = (x 1 , x 2 , ..., x n ).\n\nMLP Backbone Network\nWe first concatenate the context utterances and the incomplete utterances to construct a joint m + n length word sequence H = (c 1 , c 2 , ..., c m , x 1 , x 2 , ..., x n ). Besides, pretrained language models have been found to be highly effective in various natural language processing tasks. Hence, we employ BERT (Devlin et al., 2019) to initialize the word vector matrix H, where H \u2208 R (m+n)\u00d7768 . MLP backbone network contains two MLP blocks. Specifically, the first MLP block is responsible for mining the global semantic association information between context utterances c and incomplete utterance x. The second MLP block aims to learn the confidence level for each word embedding. This further enables the model to focus on important word information. It is important for the follow-up edit type classification, including substitute, insert and none. Each MLP block contains two fully-connected layers and a nonlinearity applied independently. For clarity and simplicity, we exclude the transposition process and the whole process can be represented as:\nEQUATION\nwhere i = 1, 2, .., 768, j = 1, 2, .., m + n and \u03c3 represents GELU (Hendrycks and Gimpel, 2016) .\nIn addition, MLP backbone contains other standard architectural components: skip-connections (He et al., 2016) and LayerNorm (LN ) (Ba et al., 2016) .\nIn contrast to the approach taken by Tolstikhin et al. ( 2021), who treated the word vector matrix H as an image and employed 1 \u00d7 1 convolution on non-overlapping image patches, we directly input the word vector matrix H into the MLP backbone network. Our operation avoids the loss of semantic spatial information resulting from 1\u00d71 convolution. Furthermore, since the number of words in each utterance varies, we utilize padding operation and copy mechanism (Gu et al., 2016; Zeng et al., 2018) to maintain a consistent sequence length. It is worth noting that our approach employs a one-layer MLP backbone network.\n\nJoint Feature Matrix\nFurthermore, to further capture the relevance between word embeddings, we employ three similarity functions: dot product similarity (dot Sim.), cosine similarity (cos Sim.), and linear similarity (linear Sim.). The word-to-word embeddings relevance between each context utterance's word embedding K cm and each incomplete utterance's word embedding K xn are captured using a 3dimensional joint feature matrix J(c m , x n ) represented as follows:\nEQUATION\nFinally, we employ BatchNorm (Ioffe and Szegedy, 2015) on joint feature matrix J(c m , x n ) to expedite and stabilize the training process. The batch is obtained by computing the mean and variance of the batch activation, which captures global information. After applying the BatchNorm operation, the matrix J(c m , x n ) is flattened, and each feature vector is mapped to one of three token types: Substitute, Insert, or None. This generates the token-pair edit matrix.\n\nSupervised Label\nPrior to training our model in the supervised fashion, we need to create word-level labels through the following process to construct our training set. Specifically, we first calculate the longest common subsequence (LCS) between the incomplete utterance and the rewritten utterance. Then, we align the incomplete utterance, the rewritten utterance, and the LCS using a greedy strategy. Finally, we identify the corresponding tokens in the rewritten utterance and mark them accordingly. Please refer to Algorithm 1 in Appendix A for a detailed description.\n\nExperimental Setup\nDatasets We conduct the experiments on three IUR benchmarks from different domains and languages, including RESTORATION-200K (Pan et al., 2019) , REWRITE (Su et al., 2019) and CANARD (Elgohary et al., 2019) . The statistics of the datasets are shown in Appendix B.\nBaselines We compare the performance of our method with the following baselines: (i) Generation models need to generate rewritten utterances from scratch, including Seq2Seq model L-Gen (Bahdanau et al., 2015) , the hybrid pointer generator network L-Ptr-Gen (See et al., 2017) , the basic transformer models T-Gen and T-Ptr-Gen (Vaswani et al., 2017) , Syntactic (Kumar and Joshi, 2016) , PAC (Pan et al., 2019) , L-Ptr-\u03bb and T-Ptr-\u03bb (Su et al., 2019) . The above models are limited by the speed of generation. (ii) Structure aware models contain RUN (Liu et al., 2020) and SARG (Huang et al., 2021) .\nFor more information about other experimental setups, please see Appendix B.\n\nMain Results\nTable 2 shows the experimental results on RESTORATION-200K. Our proposed approach, MIUR, achieves competitive results compared to all previous State-of-the-Art methods as shown in Table 2 . The results indicate MIUR can effectively mine the semantic information between utterances with two types of MLP architecture. Furthermore, we discovered that MIUR places more emphasis on rewriting precision (P n ) metrics. The first MLP architecture captures global semantic associations between context utterances and incomplete utterance, while the second MLP architecture focuses more on significant word embedding information. Our approach effectively combines two different MLPs and provides an effective guideline for the subsequent construction of the joint feature map matrix, leading our approach to concentrate more on essential word information and to pursue higher rewriting precision. Additionally, we achieve comparable Recall n results to the baselines. The experimental results of REWRITE and CANARD also come to the same conclusion, which can be found in Appendix C. \n\nModel\nP 1 R 1 F 1 P 2 R 2 F 2 P 3 R 3 F 3 B 1 B 2 R 1 R 2 Syntactic 67\n\nInference Speed\nTable 3 presents a comparison of the inferential speed of our model with the baselines. All models were implemented in PyTorch and run on a single NVIDIA V100. We can observe that the proposed MIUR achieves the fastest inference speed compared with the SOTA methods. Specifically, MIUR's speed is 3.14 times faster than that of L-Gen (n_Beam=1). Moreover, Compared with RUN in the second place, MIUR achieves 20% improvement in the inference speed. This enhanced performance can be attributed to the fact that our model employs only a one-layered MLP backbone to capture inter-utterances semantic information, without utilizing other modules. The simplified architecture, thus, contributes to the model's faster inference speed without compromising the performance. n_Beam stands for the beam size in beam search, not applicable for RUN and MIUR.\n\nAblation Study\nTo verify the effectiveness of MLP architecture in our model, we conduct a thorough ablation study in Table 4 As mentioned in Section 2.1, we perform an ablation study about using two different padding strategies to ensure consistent sequence length. Table 5 indicates that the model obtains a small performance improvement using copy mechanism, which further increases the semantic interaction between utterances. But this operation limits inference speed. Given a tiny improvement using copy mechanism, our model employs zero padding method. \n\nMore Discussion for MLP\nTo further investigate whether our proposed MLP backbone can effectively mine the semantic associations between utterances, we visualize the word embeddings composed of the context utterances and the incomplete utterance in Figure 2 . The yaxis represents our selection of 40 words consisting of the context utterances and the incomplete utterance. The x-axis represents the features of the first 100 dimensions of our intercepted word embeddings. It is not difficult to notice that word embeddings appear more distinctly characterized by vertical stripes after MLP backbone. Consequently, this further indicates that semantic information between words is more closely related, and our method can effectively learn the semantic relatedness between words after passing through the MLP network we designed. \n\nConclusion & Future Work\nIn this paper, we propose a simple yet effective IUR method. We utilize one-layer MLP structure to mine the inter-utterance semantic information from different perspectives. This improves the ability to predict the correct token between incomplete utterance and rewritten utterance. Benefiting from the fact that our model effectively employs MLP to IUR task, allowing our approach to achieve significant results in terms of performance and inference speed. This study represents the first preliminary exploration of the use of MLP on IUR task. In the future, we will investigate on extending our approach to other dialogue areas.\n"}
{"question": "What is the primary objective of the black-box defense mentioned in the text?", "evidence": "  Prompt-tuning requires the prepending of a prompt to the prefix embedding and access to the training loss (see Figure 1 ) Our white-box attack aims to increase the number of k-extractable sequences, while our black-box defense aims to reduce the number of k-extractable sequences that can be extracted by an adversary who submits prefixes via an API.  ", "options": ["A. To maximize the number of k-extractable sequences.", "B. To gain access to the training loss.", "C. To decrease the number of k-extractable sequences.", "D. To evaluate the feasibility of instructive prompts."], "answer": "C", "content": "\nIntroduction\nPretrained large language models (LLMs; Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2020; Soltan et al., 2022) , commonly trained on massive crowd-sourced corpora, have been of much interest in the recent past due to their usage as backbones in state-of-the-art models across multiple downstream NLU tasks. However, they have been shown to memorize significant portions of their training data that can be extracted using appropriately-crafted prompts (Carlini et al., 2020 (Carlini et al., , 2022;; Zhang et al., 2021) . Such extractions pose a privacy risk to the contributors of the training data.\nIn this context, methods that allow developers to control the extractability of memorized examples from LLMs are of much value. For example, methods that increase extraction rates correspond to attacks in an adversarial setting, and provide developers with the ability to analyze privacy-risk.\nMethods that decrease extraction rates, referred to as defenses, are useful for protecting against such attacks. Historically, defense methods tend to be compute intensive (Abadi et al., 2016; Dupuy et al., 2021) .\nIn this work, we train continuous soft-prompts (Lester et al. 2021 ; hereafter referred to simply as prompts) and leverage them as a way of passing an external signal into an LLM, to control the extraction of memorized data. We freeze the model weights, and only use the trained prompt to control the generation. First, we train prompts in an attack setting and study the extent of extractable memorized content in our models. Second, we explore a defense setting where we create prompts that reduce extraction rates and achieve different privacy-utility trade-offs, via a user-specified hyperparameter. Since the original model weights are frozen in both these settings, our methods are compute efficient across the board.\nTo the best of our knowledge, our work is the first to adapt the use of instructive prompts for the analysis and mitigation of privacy in LLMs. We have released the code developed for our experiments 1 .\n\nBackground and Related Work\nPrevious work has shown that LLMs display memorization and has explored a range of methods that quantify extractability (Carlini et al., 2018 (Carlini et al., , 2020 (Carlini et al., , 2022)) . Differentially-private training (Dwork, 2006; Abadi et al., 2016) is a popular method that has been used to mitigate this risk. However, it tends to reduce model utility and requires retraining of the LLM, which might not be feasible due to heavy computational burden. The use of instructive prompts for language models has been extensively researched, including use during pretraining (Raffel et al., 2020) , as a second stage of training (Sanh et al., 2022; Wei et al., 2021) , and during inference to guide model output (Brown et al., 2020) . Within the third category, in order to improve upon manual prompt engineering researchers have implemented methods to learn discrete natural language prompts (Shin et al., 2020) , to mine them (Jiang et al., 2020) , or, neglecting natural language, to learn continuous prompts (Li and Liang, 2021; Lester et al., 2021) .\nOur work leverages continuous prompts as a way of passing an external signal to a model to trigger a desired model behavior (i.e., less or more memorized data in open language generation, which map to an extraction attack and defense, respectively).\n\nMethod\nPrompt-tuning requires the prepending of a prompt to the prefix embedding and access to the training loss (see Figure 1 ). Given these constraints, we explore a white-box attack where the adversary has access to the target model parameters, and a blackbox defense where the adversary interacts with the target model via an API. We therefore do not test our defense against our own attack.\nLet [prefix || suffix] be a sequence in the training set where the prefix is of length k tokens. Carlini et al. (2022) defined a suffix to be k-extractable if the model generates the suffix exactly, after being prompted with its the corresponding lengthk prefix. Our white-box attack aims to increase the number of k-extractable sequences, while our black-box defense aims to reduce the number of k-extractable sequences that can be extracted by an adversary who submits prefixes via an API.\n\nAttack\nIn the attack setting, we assume that the adversary has a set of [ prefix || suffix ] sequences S train , sampled from the training set of the target model. Their goal is to extract the suffixes corresponding to a disjoint set of prefixes, denoted by S test 2 . To do so, the adversary first initializes a prompt: a continuous set of l \u00d7 e parameters where e is the embedding size of the model, and l is the length of the prompt, a hyperparameter decided by the adversary. The prompt is trained over S train to facilitate the correct generation of suffixes. To do this, we first prepend the prompt to the embedding of the prefix and pass the joint embedding through the model for generation. We then minimize the loss objective (see below) with respect to the prompt while keeping the parameters of the model frozen.\nWe explore two loss objectives. The first is causal language modeling (hereafter referred to as CLM), where we minimize the cross-entropy loss over the entire sequence (Radford et al., 2019) . In the second, the prompt is optimized by minimizing the cross entropy loss of only the suffixes, given the prefixes. Here, the training is aligned with our inference task such that during training the model is penalized only on the suffix tokens; hence we refer to it as aligned CLM. During inference, the learned prompt is prepended to each embedding of the prefixes in S test , and the joint embedding is passed to the model for generation (see Figure 1 ).\n\nDefense\nIn the defense setting, the defender (API owner) trains the prompt, and prepends it to the incoming prefixes before passing them to the model. Our algorithm is inspired by machine-unlearning literature (Halimi et al., 2022) , and defenses against membership inference and backdoor attacks (Chen et al., 2022; Ozdayi et al., 2021) . We introduce a hyperparameter named learning threshold denoted by \u03b8. During prompt training (see Section 3.1), when loss is less than \u03b8 we do gradient ascent to penalize the prompt. If the loss is greater than \u03b8, we perform gradient descent with respect to the prompt as usual. Training is stopped once the average epoch loss is equal or above \u03b8. This allows us to increase training loss in a controlled manner and stabilize it around \u03b8. Through this process, we can achieve various privacy-utility trade-offs efficiently without re-training any part of the model. To explore \u03b8, we set the initial value to be slightly above the model training loss and increase in steps of 0.25 until desired performance is achieved.\n\nExperiments\nFor our experiments, we use the 125M and 1.3B parameter variants of the GPT-Neo models (Black et al., 2021) . These are public, decoder-only transformer models (Vaswani et al., 2017) trained using CLM on the Pile dataset (Gao et al., 2020) . We extract S train and S test from the Language Model Extraction Benchmark dataset (Google-Research). This dataset contains 15k sequences sampled from the training split of the Pile where each sequence is partitioned into a prefix and suffix. In the default evaluation setting, both prefix and suffix consist of 50 tokens. We ensure a random train/test split of 14k/1k samples.\nOur evaluation metric of choice is Exact extraction rate which is the fraction of correctly generated suffixes (i.e., all tokens of the generated suffix match with ground-truth suffix) over the test set. We additionally discuss fractional extraction rate and present results in Appendix A. As a baseline, we use the attack analyzed in Carlini et al. (2022) , which consists of feeding the prefixes to the model, and generating suffixes with greedy decoding. This is the only extraction attack for this setting apart from our work, to the best of our knowledge. Our training setup is discussed in Appendix B. All experiments are repeated over 5 runs with a new random train/test split in each run.\n\nAttack\nWe explore the performance of our attack across several dimensions: prompt length, suffix size, prefix size, and beam size. We use greedy-decoding in all cases, except the beam size experiments.\nPrompt Length First, we explore prompt length in the context of the default setting (prefix and suf-fix consist of 50 tokens; Figures 2-A1 and 2-A2 ). We note that prompts tuned with both CLM and aligned CLM provide improvements over the baseline in all cases, with aligned CLM providing the best performance. Given this, we train prompts using the aligned CLM objective for all other experiments, including our defense.\nWith aligned CLM, we achieve the highest extraction rates of 25.8% and 54.3% for the 125M and 1.3B models, respectively (an improvement of 8.9 and 9.3 percentage points, respectively), with a 100 token prompt (blue line). We observe that extraction rates increase with prompt length and tend to saturate after prompt length 100. Over-fitting was ruled out as a potential cause of saturation as there is no increase in test loss observed during training. This suggests that there is a max limit on the parameter count in the prompt that might add value for extraction purposes given our objective. We note that more sophisticated training strategies (designing better loss functions, better prompt initialization etc.) might yield better extraction rates.\nSuffix Size Next, we fix the prefix size to 50 and vary the suffix size. As shown in Figures 2-B1 and 2-B2, extraction rates decrease roughly exponentially with suffix size. We note that as suffix size increases, longer prompts (\u2265 20) provide greater improvements over the baseline. For example, with a prompt length of 100 (blue line) using the 1.3B model, at suffix size 5 we observe an extraction rate increase of 5.3 percentage points. Whereas at suffix size 50, the increase is 9.3 percentage points.\nPrefix Size Next, we fix the suffix size to 50 and vary the prefix size. As shown in Figures 2-C1 and 2-C2, extraction rates increase roughly logarithmically (as in Carlini et al. 2022) . Contrary to suffix size, we observe that the gaps between baseline and attacks decrease with increasing prefix size. This suggests that our attack stands to benefit a less informed adversary (small prefix sizes) when compared to the baseline.\nBeam Decoding Finally, we utilize the default setting with prefix and suffix sizes at 50 tokens and vary the beam size (beam size=1 corresponds to greedy decoding). The results are shown in Figures 2-D1 and 2-D2. We observe that extraction rates increase across the board when increasing beam size from 1 to 5. However, improvements tend to plateau or oscillate when beam size is greater than 5. The 1.3B model benefits more from increasing beam size achieving the highest extraction rate of 61.4%, at a beam size of 20 (with a prompt length of 150). The highest extraction rate achieved for the 125M model was 28.3% at a beam size of 15 (with a prompt length of 100).\n\nDefense\nFinally, we evaluate the privacy-utility trade-off of our black-box defense. As mentioned in Section 3, our defense is designed for a black-box adversary, and cannot be tested against our white-box attack.\nTherefore, we utilize the baseline attack (Section 4) to quantify privacy. We note that longer prompts did not add value in a defense setting, so we resort to using a prompt of length 1. We utilize perplexity (PPL) on generated suffixes, to quantify the utility of the model in addition to using exact extraction rate as in Section 3.1. To measure PPL, we use a random subset of 1k sequences sampled from the test split of the Pile, ensuring that PPL is measured on data unseen by the model. We also compare our metrics with those of similar sized models that were not trained on the Pile dataset (GPT2 models). Our premise here is that better performance in terms of privacy and utility, when compared to an out-ofdomain model of similar size, would mean that our defense mechanism is of value to an API owner.\nIn Table 1 , we display our results obtained using the default evaluation setting (prefix and suffix comprise of 50 tokens). Our defense achieves lower extraction rates with competitive PPL values. For the 125M model, we achieve an exact extraction rate reduction of 99.4% relative to baseline with a PPL increase of 25.3% at \u03b8 = 1.75. For the 1.3B model, the extraction rate is reduced by 97.7% relative to baseline with a PPL increase of 16.9% at \u03b8 = 1. The ability to achieve lower extraction rates with lower PPL values as measured against the GPT2 models of the corresponding size, provides evidence that our defense is effective.\n\nConclusion\nWe present the first known effort to leverage prompt-tuning to control the extractability of memorized data from LLMs in an open language generation task. We develop a novel data extraction attack and defense, and illustrate their performance under various settings. Our attack consistently outperforms the baseline in terms of exact extraction rate. Our defense provides competitive privacy-utility trade-offs and would prove beneficial to API owners with model trained on sensitive content. These results are achieved efficiently, without any change to the original model weights. We details avenues of future work in Appendix C\n"}
{"question": "What is the purpose of label smoothing in this paper?", "evidence": "  Label smoothing is a regularization method that has been proven effective in a variety of applications... Importantly, it has been shown to reduce overconfident predictions... Our experiments show that label smoothing significantly improves adversarial robustness...  ", "options": ["A. To improve clean accuracy of NLP models", "B. To reduce overconfident predictions in NLP models", "C. To increase the complexity of NLP models", "D. To enhance the interpretability of NLP models "], "answer": "B", "content": "\nIntroduction\nNeural networks are vulnerable to adversarial attacks: small perturbations to the input ,which do not fool humans (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2017) . In NLP tasks, previous studies (Alzantot et al., 2018; Jin et al., 2019; Li et al., 2020; Garg and Ramakrishnan, 2020) demonstrate that simple word-level text attacks (synonym substitution, word insertion/deletion) easily fool state-of-the-art models, including pre-trained transformers like BERT (Devlin et al., 2019; Wolf et al., 2020) . Further, it has recently been shown models are overconfident 1 on examples which are easy to attack (Qin et al., 2021) and indeed, such over-confident predictions plague much of modern deep learning (Kong et al., 2020; Guo et al., 2017; Nguyen et al., 2015; Rahimi et al., 2020) . Label smoothing is a regularization method that has been proven effective in a variety of applications, and modalities (Szegedy et al., 2016; Chorowski and Jaitly, 2017; Vaswani et al., 2017) . Importantly, it has been shown to reduce overconfident predictions and produce better confidence calibrated classifiers (Muller et al., 2019; Zhang et al., 2021; Dan and Roth, 2021; Desai and Durrett, 2020; Huang et al., 2021; Liu and JaJa, 2020) .\nIn this work, we focus on the question: does label smoothing also implicitly help in adversarial robustness? While there has been some investigation in this direction for adversarial attacks in computer vision, (Fu et al., 2020; Goibert and Dohmatob, 2019; Shafahi et al., 2019) , there is a gap in understanding of whether it helps with discrete, text adversarial attacks used against NLP systems. With the increasing need for robust NLP models in safety-critical applications and a lack of generic robustness strategies, 2 there is a need to understand inherent robustness properties of popular label smoothing strategies, and the interplay between confidence and robustness of a model.\nIn this paper, we extensively study standard label smoothing and its adversarial variant, covering robustness, prediction confidence, and domain transfer properties. We observe that label smoothing provides implicit robustness against adversarial examples. Particularly, we focus on pre-trained transformer models and test robustness under various kinds of black-box and white-box word-level adversarial attacks, in both in-domain and out-ofdomain scenarios. Our experiments show that label smoothing (1) improves robustness to text adversarial attacks (both black-box and white-box), and (2) mitigates over-confident errors on adversarial textual examples. Analysing the adversarial exam-ples along various quality dimensions reveals the remarkable efficacy of label smoothing as a simple add-on robustness and calibration tool.\n\nText Adversarial Attacks\nOur experiments evaluate the robustness of text classification models under three state-of-the-art text adversarial attacks TextFooler (black-box), BAE (black-box) and SemAttack (white-box), described below. 3 For a particular victim NLP model and a raw text input, the attack produces semantically-similar adversarial text as output. Importantly, only those examples are attacked, which are originally correctly predicted by the victim model. The attacks considered are word-level, i.e. they replace words in a clean text with their synonyms to maintain the meaning of the clean text, but change the prediction of the victim models.\n\u2022 TextFooler (TF): (Jin et al., 2019) proposes an attack which determines the word importance in a sentence, and then replaces the important words with qualified synonyms.\n\u2022 BAE: (Garg and Ramakrishnan, 2020) uses masked pre-trained language models to generate replacements for the important words until the victim model's prediction is incorrect.\n\u2022 SemAttack (SemAtt): (Wang et al., 2022) introduces an attack to search perturbations in the contextualized embedding space by formulating an optimization problem as in (Carlini and Wagner, 2016) . We specifically use the white-box word-level version of this attack.\n\nLabel Smoothing\nLabel Smoothing is a modified fine-tuning procedure to address overconfident predictions. It introduces uncertainty to smoothen the posterior distribution over the target labels. Label smoothing has been shown to implicitly calibrate neural networks on out-of-distribution data, where calibration measures how well the model confidences are aligned with the empirical likelihoods (Guo et al., 2017) .\n\u2022 Standard Label Smoothing (LS) (Szegedy et al., 2013; Muller et al., 2019 ) constructs 3 The black-box attacks keep querying the model with its attempts until the victim model is fooled while the white-box attack has access to the gradients to the model. Further details of the attacks are in (Jin et al., 2019; Garg and Ramakrishnan, 2020; Wang et al., 2022) . a new target vector (y LS i ) from the one-hot target vector (y i ), where y LS i = (1 \u2212 \u03b1)y i + \u03b1/K for a K class classification problem. \u03b1 is a hyperparameter selection and its range is from 0 to 1. ) with a probability of 1 \u2212 \u03b1 on the target label and \u03b1 on the label to which the classification model assigns the minimum softmax scores, thus introducing uncertainty.\nFor both LS and ALS, the cross entropy loss is subsequently minimized between the model predictions and the modified target vectors y LS i , y ALS i .\n\nExperiments\nIn this section, we present a thorough empirical evaluation on the effect of label smoothing on adversarial robustness for two pre-trained transformer models: BERT and its distilled variant, dBERT, which are the victim models. 4 We attack the victim models using TF, BAE, and SemAttack. For each attack, we present results on both the standard models and the label-smoothed models on various classification tasks: text classification and natural language inference. For each dataset we evaluate on a randomly sampled subset of the test set (1000 examples), as done in prior work (Li et al., 2021; Jin et al., 2019; Garg and Ramakrishnan, 2020) . We evaluate on the following tasks, and other details about the setting is in Appendix A.8:\n\u2022 Text Classification: We evaluate on movie review classification using Movie Review (MR) (Pang and Lee, 2005) and Stanford Sentiment Treebank (SST2) (Socher et al., 2013) against various attacks for in-domain data. We show clean accuracy, attack success rate and average confidence on successful adversarial texts. For each dataset, the left column are the results for standard model, and the right column are for LS models where \u03b1 denotes the label smoothing factor (\u03b1=0: no LS). \u2191 (\u2193) denotes higher (lower) is better respectively. dBERT denotes the distilBERT model.\non the matched genre test-set in the OOD setting presented in subsection 3.2 .\n\nIn-domain Setting\nIn the in-domain setting (iD), the pre-trained transformer models are fine-tuned on the train-set for each task and evaluated on the corresponding testset. For each case, we report the clean accuracy, the adversarial attack success rate (percentage of misclassified examples after an attack) and the average confidence on successfully attacked examples (on which the model makes a wrong prediction). 5 Table 1 shows the performance of BERT and dBERT, with and without label-smoothing. We choose label smoothing factor \u03b1 = 0.45 for standard labelsmoothed models in our experiments.\nWe see that label-smoothed models are more robust for every adversarial attack across different datasets in terms of the attack success rate, which is a standard metric in this area (Li et al., 2021; Lee et al., 2022) . Additionally, the higher confidence of the standard models on the successfully attacked examples indicates that label smoothing helps mitigate overconfident mistakes in the adversarial setting. Importantly, the clean accuracy remains almost unchanged in all the cases. Moreover, we observe that the models gain much more robustness from LS under white-box attack, compared 5 Details of each metric are presented in Appendix A.2.\nto the black-box setting. We perform hyperparameter sweeping for the label smoothing factor \u03b1 to investigate their impact to model accuracy and adversarial robustness. Figure 1 shows that the attack success rate gets lower as we increase the label smooth factor when fine-tuning the model while the test accuracy is comparable 6 . However, when the label smoothing factor is larger than 0.45, there is no further improvement on adversarial robustness in terms of attack success rate. Automatic search for an optimal label smoothing factor and its theoretical analysis is important future work. We also investigate the impact of adversarial label smoothing (ALS) and show that the adversarial label smoothed methods also improves model's robustness in Table 2 . \n\nOut-of-Domain setting\nWe now evaluate the benefits of label smoothing for robustness in the out-of-domain (OOD) setting, where the pre-trained model is fine-tuned on a particular dataset and is then evaluated directly on a different dataset, which has a matching label space. Three examples of these that we evaluate on are the Movie Reviews to SST-2 transfer, the SST-2 to Yelp transfer, and the SNLI to MNLI transfer.\nIn Table 3 helps produce more robust models in the OOD setting although with less gain compared to iD setting. This is a challenging setting, as evidenced by the significant performance drop in the clean accuracy as compared to the in-domain setting. We also see that the standard models make over-confident errors on successfully attacked adversarial examples, when compared to label-smoothed models.\n\nQualitative Results\nIn this section, we try to understand how the generated adversarial examples differ for label smoothed and standard models. First we look at some qualitative examples: in We also performed automatic evaluation of the quality of the adversarial examples for standard and label smoothed models, adopting standard metrics from previous studies (Jin et al., 2019; Li et al., 2021) . Ideally, we want the adversarial sentences to be free of grammar errors, fluent, and semantically similar to the clean text. This can be quantified using metrics such as grammar errors, perplexity, and similarity scores (compared to the clean text). Table 5 shows that the quality of generated adversarial examples on label smoothed models is worse than those on standard models for different metrics, suggesting that the adversarial sentences generated by standard models are easier to perceive. This further demonstrates that label smoothing makes it harder to find adversarial vulnerabilities.\n\nConclusion\nWe presented an extensive empirical study to investigate the effect of label smoothing techniques on adversarial robustness for various NLP tasks, for various victim models and adversarial attacks. Our results demonstrate that label smoothing imparts implicit robustness to models, even under domain shifts. This first work on the effects of LS for text adversarial attacks, complemented with prior work on LS and implicit calibration (Desai and Durrett, 2020; Dan and Roth, 2021) , is an important step towards developing robust, reliable models. In the future, it would be interesting to explore the combination of label smoothing with other regularization and adversarial training techniques to further enhance the adversarial robustness of NLP models.\n"}
{"question": "What is the primary advantage of XL-LEXEME for Lexical Semantic Change Detection, as mentioned in the text?", "evidence": " It focuses on sentence-level encoders.  XL-LEXEME is pre-trained on a large WiC dataset to mirror sentence-level encoders focusing on specific words in contexts.  We evaluated our model on two Lexical Semantic Change Detection datasets: SemEval-2020 Task 1 and RuShiftEval. XL-LEXEME outperforms stateof-the-art models for LSC Detection in English, German, Swedish, and Russian datasets, with significant differences from the baselines.  ", "options": ["A. It is trained on a small WiC dataset.", "B. It performs poorly on SemEval-2020 Task 1.", "C. It outperforms state-of-the-art models in multiple languages."], "answer": "C", "content": "\nIntroduction and Motivation\nLexical Semantic Change (LSC) Detection is the task of automatically identifying words that change their meaning over time. The LSC Detection task implicitly aims to disambiguate synchronic word sense occurrences and then find differences in the word sense frequencies in different periods. Word Sense Disambiguation (WSD) is a longstudied task in Natural Language Processing (Navigli, 2009) , which consists of associating the correct sense to a word occurring in a specific context. WSD involves some crucial issues, such as relying on a fixed sense inventory. Fixed sense inventories ignore the diachronic aspect of language because they can miss older unused senses or be outdated and missing new senses.\nThe Word in Context task (WiC) (Pilehvar and Camacho-Collados, 2019) aims to overcome these issues. In this work, we train a model on the WiC task and then use it to perform LSC Detection. In the WiC task, given the word w and two different contexts C1, C2, the systems have to determine whether the meaning of w is the same in the two contexts or not. Our approach is grounded on the assumption that models trained on the WiC tasks are robust enough to transfer the knowledge learned in a synchronic setting to a diachronic one. We summarise the main contribution of this work as follows: (i) We propose a pre-trained biencoder model, called XL-LEXEME, on a largescale dataset for the WiC task, which allows us to obtain comparable lexical-based representations; (ii) We assert the effectiveness of XL-LEXEME despite the computational limitation compared to the cross-encoder architecture for the LSC Detection task; (iii) Experiments on the LSC Detection task show that XL-LEXEME outperforms state-ofthe-art LSC Detection models for English, German, Swedish, and Russian.\n\nRelated Work\nLSC Detection systems can be categorized based on the distributional embeddings used to tackle the LSC Detection task. One category is represented by those approaches that adopt type-base (i.e., static) embeddings. UWB (Praz\u00e1k et al., 2020; Praz\u00e1k et al., 2021) represents an example of this category of systems. First, it employs word2vec Skip-gram with Negative Sampling (Mikolov et al., 2013) to compute a semantic space for each corpus. It uses techniques like the Canonical Correlation Analysis (Hardoon et al., 2004) and the Orthogonal Transformation (Hamilton et al., 2016) to align the abovementioned spaces. Therefore, the cosine similarity between the vectors representing the word in two different spaces is used to detect the semantic shift.\nWith the increasing use of contextualized word embeddings, numerous approaches employing BERT-base models have been developed for LSC Detection (Montanelli and Periti, 2023; Laicher et al., 2021) . In TempoBERT (Rosin et al., 2022) , the authors exploit the concept of Masked Language Modeling (MLM), where the goal is to train a language model to predict a masked portion of text given the remaining part. In particular, they employ this technique to encode the concept of time into a BERT model. This is done by concatenating a specific token representing time to the text sequence. At inference time, TempoBERT can be used to predict the year of a sentence, masking the time reference, or to predict a masked token of the sentence conditioned by the time reference. In the same line of research, in Temporal Attention (Rosin and Radinsky, 2022) , the authors investigate the effect of modifying the model instead of the input sentence like in TempoBERT. This is done by extending the model's attention mechanism to consider the time when computing the weight of each word. The time dimension is encoded using a different query embedding matrix for each timestamp.\nAnother kind of approach exploits the information coming from other tasks to perform LSC Detection. GlossReader represents an example (Rachinskiy and Arefyev, 2021) , where a model based on XML-R (Conneau et al., 2020b) is first trained on English SemCor (Miller et al., 1994) with glosses from WordNet 3.0 (Miller, 1992) to perform WSD. Exploiting the zero-shot crosslingual characteristics of XML-R, the authors used the same model to perform LSC Detection in the Russian language. With DeepMistake (Arefyev et al., 2021) , the authors take advantage of the WiC task instead of WSD. They train a cross-encoder with XML-R as an underlying Language Model on the MCL-WiC training and development set and fine-tune on the RuSemShift dataset (Rodina and Kutuzov, 2020) . DeepMistake, differently from XL-LEXEME, relies on the cross-encoder architecture and exploits only the MCL-WiC training dataset.\n\nXL-LEXEME\nGenerally, for pairwise sentence similarity tasks, BERT models use a cross-encoder, in which the pairwise sequences are jointly encoded, and the overall vectors are used for the classification. However, in several tasks, the cross-encoder is not suitable since it cannot provide a distinct meaningful representation for each sentence. An approach to overcome this issue involves pooling the BERT out-put encoded vectors, which often results in worse performance. Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) overcomes the limitation of cross-encoders using a Siamese Network, i.e., the weights of the underlying networks are shared. SBERT encodes the two sequences separately in the BERT model exploiting the Siamese architecture. The sequence-level representation is obtained by averaging the output encoded vectors, which are directly compared using similarity measures such as cosine similarity.\nMeanwhile, cross-encoders perform better since they are trained to profit from the attention over the whole input. In this work, we introduce XL-LEXEME 1 which mirrors models for pairwise sequence similarity tasks and adapts them to the WiC task, giving prominence to the target word, i.e. the word for which we want to detect the LSC. The model takes as input two sequences s 1 and s 2 . The sequences are tokenized using subwords tokenizer, such as Sentence Piece (Kudo and Richardson, 2018) , and the special tokens <t> and </t> are used as target word delimiters (Xie et al., 2021) :\nEQUATION\nwhere N and M represent the number of subwords of the sequence s 1 and s 2 respectively, while w t i , ..., w t i+k and w t j , ..., w t j+p are the subwords of the target words. In the following, we describe the baseline cross-encoder and XL-LEXEME based on a bi-encoder. For the crossencoder, the two input sequences are concatenated by the special token [SEP ] in an overall sequence\ns = [CLS] s 1 [SEP ] s 2 [SEP ].\nIf the length of s, i.e. N + M + 3, is greater than the maximum sequence length \u03bb, then the sequence s is cut such that the length of s 1 and s 2 is less than \u03bb * = \u03bb\u22123 2 . To comply with the maximum length, the left and right contexts of the sequence are truncated. For instance, s 1 is truncated as follows:\ns 1 = w n 0 , ..., <t>, w t i , ..., w t i+k , </t>, ..., w n 1 (2) where n 0 = max(0, i \u2212 1 \u2212 \u03bb * \u2212k\u22122 2 ) and n 1 = min(N, i + k + 1 + \u03bb * \u2212k\u22122 2\n). The truncated sequence has a length \u03b3 < \u03bb. The encoded representations of each subword (v 1 , v 2 , ..., v \u03b3 ) are summed to get the encoded representation of the overall sequence, i.e. s enc = \u03b3 i v i . Finally, the vector s enc is used to compute the logits:\nlogit = log \u03c3(W s enc ) (3)\nwhere W \u2208 IR 1\u00d7d . The model is trained to minimize the Binary Cross-entropy loss function. XL-LEXEME is a bi-encoder that encodes the input sequences using a Siamese Network into two different vector representations. Each sequence is tokenized and truncated according to the maximum length \u03bb * , using Equation (2). We thus obtain the new lengths \u03b3 1 , \u03b3 2 . The vector representation is computed as the sum of the encoded subwords\n(v 1 , v 2 , ..., v \u03b3 ), i.e. s enc 1 = \u03b3 1 i v i and s enc 2 = \u03b3 2 j v j .\nXL-LEXEME is trained to minimize the Contrastive loss (Hadsell et al., 2006) :\n\u2113 = 1 2 y \u2022 \u03b4 2 + (1 \u2212 y) \u2022 max(0, m \u2212 \u03b4) 2 (4)\nwhere we adopt a margin m = 0.5. We use as default distance \u03b4 the cosine distance between the encoded representations of s 1 and s 2 , i.e. \u03b4 = cos(s enc 1 , s enc 2 ). The main advantage of XL-LEXEME concerning models based on the crossencoder architecture is efficiency. The time cost can be directly derived from the different architectures that exploit XL-LEXEME and the crossencoder baseline. The self-attention time complexity O(N 2 * d) depends on the vector dimension d and the sequence length, which is N for the cross-encoder and N 2 for XL-LEXEME. For XL-LEXEME, the time complexity is reduced to O((N^2)^2 * 2d).\n4 Experimental setting\n\nLexical Semantic Change Detection\nSemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection (Schlechtweg et al., 2020) is the first task on Unsupervised Lexical Semantic Change Detection in English, German, Swedish, and Latin languages. For each language, two corpora represent two different periods (T0, T1). Moreover, a set of target words, annotated using the DUREL framework (Schlechtweg et al., 2018) , are provided. SemEval-2020 Task 1 involves two subtasks. The binary classification task requires assigning a label (changed/stable) to each target word. The ranking task sorts the target words according to their degree of semantic change. In this work, we focus on Subtask 2, and for the sake of simplicity, we refer to SemEval-2020 Task 1 Subtask 2 as SemEval-2020 Task 1. RuShiftEval, different from SemEval-2020 Task 1, involves three sub-corpora extracted from the Russian National Corpus spanning three periods. Models are evaluated on the resulting three test sets, namely RuShiftEval1 (pre-Soviet and Soviet), RuShiftEval2 (Soviet and post-Soviet), and RuShiftEval3 (pre-Soviet and post-Soviet). RuShiftEval provides participants with development data that can be used for tuning models. RuShiftEval aims to corroborate if training data can improve LSC Detection models. The development data rely on the RuSemShift dataset (Rodina and Kutuzov, 2020) , which includes two sets of 70 target words for the pre-Soviet to Soviet period and Soviet to post-Soviet period, respectively. The dataset also includes annotated pairwise sentences, which can be used for training the models.\n\nTraining details\nXL-LEXEME and the cross-encoder are trained using XLM-RoBERTa (XLM-R) (Conneau et al., 2020a) large as the underlying Language Model 2 and using an NVIDIA GeForce RTX 3090. As for training data, the model uses the training data of MCL-WiC (Martelli et al., 2021) , AM 2 ICO (Liu et al., 2021) , and XL-WiC datasets (Raganato et al., 2020) merged with the randomly sampled 75% of the respective development data of each dataset. The remaining 25% of the development data is used to fine-tune hyper-parameters. Moreover, we augment training data for the cross-encoder by swapping the order of sentences in the training set (Martelli et al., 2021) .\nWe use AdamW optimizer and linear learning warm-up over the 10% of training data. We perform a grid search for the hyper-parameters optimization, tuning the learning rate in {1e-6, 2e-6, 5e-6, 1e-5, 2e-5} and the weight decay {0.0, 0.01}. Table 3 (Appendix A) shows the selected hyperparameters. We sample 200 sentences containing the target word for each language and each period. The sampling is repeated ten times, and the results are averaged over the ten iterations. We use the same methodology of Rachinskiy and Arefyev (2021) for sampling sentences from the RuShiftEval corpora. We sample sentences in which we find the exact match with the target words with no pre-processing of the SemEval dataset. The LSC score is computed as the average distance between the vectors over the two different periods:\nLSC(s t 0 , s t 1 ) = 1 N \u2022 M N i=0 M j=0 \u03b4(s t 0 i , s t 1 j ) (5)\nwhere \u03b4 is the distance measure, i.e. \u03b4 = 1 \u2212 log \u03c3(W s enc ) for the cross-encoder baseline and \u03b4 = cos(s enc 1 , s enc 2 ) for XL-LEXEME.\n\nResults\nTable 1 and Table 2 report the results on the SemEval-2020 Task 1 Subtask 2 and the results on the RuShiftEval test set. The results of the best systems are in bold. XL-LEXEME achieve the best score for English, German, Swedish, RuShiftEval1, RuShiftEval2, and RuShiftEval3. XL-LEXEME achieves a strong Spearman correlation for English and Swedish languages and a solid correlation on the German dataset, obtaining a significative correlation (p < 0.001). XL-LEXEME obtains no significant results in the Latin language since the predicted scores for the target words are not correlated with the test set. Latin is underrepresented in the training data of XLM-R, and there are no similar languages in the WiC dataset that we use for training XL-LEXEME. Moreover, the Latin dataset is more challenging as it involves the first corpus written in ancient Latin, which differs in many aspects from modern Latin. For this reason, XL-LEXEME could be ineffective in ancient languages and, in general, in languages that are not widely covered by the WiC dataset. We report the statistical significance of the difference between the performance of XL-LEXEME concerning the other models. The statistical significance of the difference is computed using Fisher's z-transformation (Press, 2002) . XL-LEXEME obtains stronger correlations than the cross-encoder, but the differences are not significant. The correlations obtained on the English and the German datasets are significantly different (p < 0.05) for all the systems that participated in the SemEval-2020 Task 1 but not for TempoBERT and Temporal Attention. On the other side, TempoBERT and Temporal Attention obtain a Spearman correlation on English and German that is not statistically different from the systems on the SemEval-2020 Task 1 leaderboard. In the Swedish language, XL-LEXEME is the only one obtaining a significantly different correlation from the Count baseline results. XL-LEXEME showed its effectiveness also in Swedish, although the WiC dataset does not cover this language. Presumably, Swedish benefits from the presence of other languages descending from the Old Norse language, namely Danish and Norwegian.\nXL-LEXEME obtains competitive results for the Russian language in the RuShiftEval leaderboard. Contrary to XL-LEXEME, Deep Mistake and Gloss Reader are fine-tuned on the RuSemShift dataset. The differences between XL-LEXEME and the best two systems in the leaderboard are not statically significant. Moreover, in Table 2 , the results of XL-LEXEME fine-tuned on the RuSemShift are shown. Although the fine-tuned model achieves the best correlation scores in the three datasets, the difference between DeepMistake and GlossReader is not significant.\n\nConclusion\nIn this work, we introduced XL-LEXEME, a model for LSC Detection. XL-LEXEME is pre-trained on a large WiC dataset to mirror sentence-level encoders focusing on specific words in contexts. We evaluated our model on two Lexical Semantic Change Detection datasets: SemEval-2020 Task 1 and RuShiftEval. XL-LEXEME outperforms stateof-the-art models for LSC Detection in English, German, Swedish, and Russian datasets, with significant differences from the baselines. The XL-LEXEME effectiveness and efficiency make it reliable for LSC Detection on large diachronic corpora.\n"}
{"question": "What is the groundbreaking point of this article's work\uff1f", "evidence": "   Historically, defense methods tend to be compute intensive (Abadi et al., 2016; Dupuy et al., 2021)  Second, we explore a defense setting where we create prompts that reduce extraction rates and achieve different privacy-utility trade-offs, via a user-specified hyperparameter. To the best of our knowledge, our work is the first to adapt the use of instructive prompts for the analysis and mitigation of privacy in LLMs.  ", "options": ["A. The cost of training models is very low", "B. They are the first one to adapt the use of instructive prompts for the analysis.", "C. The defense methods of this article is compute intensive.", "D. They explores a kind of new attack setting."], "answer": "B", "content": "\nIntroduction\nPretrained large language models (LLMs; Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2020; Soltan et al., 2022) , commonly trained on massive crowd-sourced corpora, have been of much interest in the recent past due to their usage as backbones in state-of-the-art models across multiple downstream NLU tasks. However, they have been shown to memorize significant portions of their training data that can be extracted using appropriately-crafted prompts (Carlini et al., 2020 (Carlini et al., , 2022;; Zhang et al., 2021) . Such extractions pose a privacy risk to the contributors of the training data.\nIn this context, methods that allow developers to control the extractability of memorized examples from LLMs are of much value. For example, methods that increase extraction rates correspond to attacks in an adversarial setting, and provide developers with the ability to analyze privacy-risk.\nMethods that decrease extraction rates, referred to as defenses, are useful for protecting against such attacks. Historically, defense methods tend to be compute intensive (Abadi et al., 2016; Dupuy et al., 2021) .\nIn this work, we train continuous soft-prompts (Lester et al. 2021 ; hereafter referred to simply as prompts) and leverage them as a way of passing an external signal into an LLM, to control the extraction of memorized data. We freeze the model weights, and only use the trained prompt to control the generation. First, we train prompts in an attack setting and study the extent of extractable memorized content in our models. Second, we explore a defense setting where we create prompts that reduce extraction rates and achieve different privacy-utility trade-offs, via a user-specified hyperparameter. Since the original model weights are frozen in both these settings, our methods are compute efficient across the board.\nTo the best of our knowledge, our work is the first to adapt the use of instructive prompts for the analysis and mitigation of privacy in LLMs. We have released the code developed for our experiments 1 .\n\nBackground and Related Work\nPrevious work has shown that LLMs display memorization and has explored a range of methods that quantify extractability (Carlini et al., 2018 (Carlini et al., , 2020 (Carlini et al., , 2022)) . Differentially-private training (Dwork, 2006; Abadi et al., 2016) is a popular method that has been used to mitigate this risk. However, it tends to reduce model utility and requires retraining of the LLM, which might not be feasible due to heavy computational burden. The use of instructive prompts for language models has been extensively researched, including use during pretraining (Raffel et al., 2020) , as a second stage of training (Sanh et al., 2022; Wei et al., 2021) , and during inference to guide model output (Brown et al., 2020) . Within the third category, in order to improve upon manual prompt engineering researchers have implemented methods to learn discrete natural language prompts (Shin et al., 2020) , to mine them (Jiang et al., 2020) , or, neglecting natural language, to learn continuous prompts (Li and Liang, 2021; Lester et al., 2021) .\nOur work leverages continuous prompts as a way of passing an external signal to a model to trigger a desired model behavior (i.e., less or more memorized data in open language generation, which map to an extraction attack and defense, respectively).\n\nMethod\nPrompt-tuning requires the prepending of a prompt to the prefix embedding and access to the training loss (see Figure 1 ). Given these constraints, we explore a white-box attack where the adversary has access to the target model parameters, and a blackbox defense where the adversary interacts with the target model via an API. We therefore do not test our defense against our own attack.\nLet [prefix || suffix] be a sequence in the training set where the prefix is of length k tokens. Carlini et al. (2022) defined a suffix to be k-extractable if the model generates the suffix exactly, after being prompted with its the corresponding lengthk prefix. Our white-box attack aims to increase the number of k-extractable sequences, while our black-box defense aims to reduce the number of k-extractable sequences that can be extracted by an adversary who submits prefixes via an API.\n\nAttack\nIn the attack setting, we assume that the adversary has a set of [ prefix || suffix ] sequences S train , sampled from the training set of the target model. Their goal is to extract the suffixes corresponding to a disjoint set of prefixes, denoted by S test 2 . To do so, the adversary first initializes a prompt: a continuous set of l \u00d7 e parameters where e is the embedding size of the model, and l is the length of the prompt, a hyperparameter decided by the adversary. The prompt is trained over S train to facilitate the correct generation of suffixes. To do this, we first prepend the prompt to the embedding of the prefix and pass the joint embedding through the model for generation. We then minimize the loss objective (see below) with respect to the prompt while keeping the parameters of the model frozen.\nWe explore two loss objectives. The first is causal language modeling (hereafter referred to as CLM), where we minimize the cross-entropy loss over the entire sequence (Radford et al., 2019) . In the second, the prompt is optimized by minimizing the cross entropy loss of only the suffixes, given the prefixes. Here, the training is aligned with our inference task such that during training the model is penalized only on the suffix tokens; hence we refer to it as aligned CLM. During inference, the learned prompt is prepended to each embedding of the prefixes in S test , and the joint embedding is passed to the model for generation (see Figure 1 ).\n\nDefense\nIn the defense setting, the defender (API owner) trains the prompt, and prepends it to the incoming prefixes before passing them to the model. Our algorithm is inspired by machine-unlearning literature (Halimi et al., 2022) , and defenses against membership inference and backdoor attacks (Chen et al., 2022; Ozdayi et al., 2021) . We introduce a hyperparameter named learning threshold denoted by \u03b8. During prompt training (see Section 3.1), when loss is less than \u03b8 we do gradient ascent to penalize the prompt. If the loss is greater than \u03b8, we perform gradient descent with respect to the prompt as usual. Training is stopped once the average epoch loss is equal or above \u03b8. This allows us to increase training loss in a controlled manner and stabilize it around \u03b8. Through this process, we can achieve various privacy-utility trade-offs efficiently without re-training any part of the model. To explore \u03b8, we set the initial value to be slightly above the model training loss and increase in steps of 0.25 until desired performance is achieved.\n\nExperiments\nFor our experiments, we use the 125M and 1.3B parameter variants of the GPT-Neo models (Black et al., 2021) . These are public, decoder-only transformer models (Vaswani et al., 2017) trained using CLM on the Pile dataset (Gao et al., 2020) . We extract S train and S test from the Language Model Extraction Benchmark dataset (Google-Research). This dataset contains 15k sequences sampled from the training split of the Pile where each sequence is partitioned into a prefix and suffix. In the default evaluation setting, both prefix and suffix consist of 50 tokens. We ensure a random train/test split of 14k/1k samples.\nOur evaluation metric of choice is Exact extraction rate which is the fraction of correctly generated suffixes (i.e., all tokens of the generated suffix match with ground-truth suffix) over the test set. We additionally discuss fractional extraction rate and present results in Appendix A. As a baseline, we use the attack analyzed in Carlini et al. (2022) , which consists of feeding the prefixes to the model, and generating suffixes with greedy decoding. This is the only extraction attack for this setting apart from our work, to the best of our knowledge. Our training setup is discussed in Appendix B. All experiments are repeated over 5 runs with a new random train/test split in each run.\n\nAttack\nWe explore the performance of our attack across several dimensions: prompt length, suffix size, prefix size, and beam size. We use greedy-decoding in all cases, except the beam size experiments.\nPrompt Length First, we explore prompt length in the context of the default setting (prefix and suf-fix consist of 50 tokens; Figures 2-A1 and 2-A2 ). We note that prompts tuned with both CLM and aligned CLM provide improvements over the baseline in all cases, with aligned CLM providing the best performance. Given this, we train prompts using the aligned CLM objective for all other experiments, including our defense.\nWith aligned CLM, we achieve the highest extraction rates of 25.8% and 54.3% for the 125M and 1.3B models, respectively (an improvement of 8.9 and 9.3 percentage points, respectively), with a 100 token prompt (blue line). We observe that extraction rates increase with prompt length and tend to saturate after prompt length 100. Over-fitting was ruled out as a potential cause of saturation as there is no increase in test loss observed during training. This suggests that there is a max limit on the parameter count in the prompt that might add value for extraction purposes given our objective. We note that more sophisticated training strategies (designing better loss functions, better prompt initialization etc.) might yield better extraction rates.\nSuffix Size Next, we fix the prefix size to 50 and vary the suffix size. As shown in Figures 2-B1 and 2-B2, extraction rates decrease roughly exponentially with suffix size. We note that as suffix size increases, longer prompts (\u2265 20) provide greater improvements over the baseline. For example, with a prompt length of 100 (blue line) using the 1.3B model, at suffix size 5 we observe an extraction rate increase of 5.3 percentage points. Whereas at suffix size 50, the increase is 9.3 percentage points.\nPrefix Size Next, we fix the suffix size to 50 and vary the prefix size. As shown in Figures 2-C1 and 2-C2, extraction rates increase roughly logarithmically (as in Carlini et al. 2022) . Contrary to suffix size, we observe that the gaps between baseline and attacks decrease with increasing prefix size. This suggests that our attack stands to benefit a less informed adversary (small prefix sizes) when compared to the baseline.\nBeam Decoding Finally, we utilize the default setting with prefix and suffix sizes at 50 tokens and vary the beam size (beam size=1 corresponds to greedy decoding). The results are shown in Figures 2-D1 and 2-D2. We observe that extraction rates increase across the board when increasing beam size from 1 to 5. However, improvements tend to plateau or oscillate when beam size is greater than 5. The 1.3B model benefits more from increasing beam size achieving the highest extraction rate of 61.4%, at a beam size of 20 (with a prompt length of 150). The highest extraction rate achieved for the 125M model was 28.3% at a beam size of 15 (with a prompt length of 100).\n\nDefense\nFinally, we evaluate the privacy-utility trade-off of our black-box defense. As mentioned in Section 3, our defense is designed for a black-box adversary, and cannot be tested against our white-box attack.\nTherefore, we utilize the baseline attack (Section 4) to quantify privacy. We note that longer prompts did not add value in a defense setting, so we resort to using a prompt of length 1. We utilize perplexity (PPL) on generated suffixes, to quantify the utility of the model in addition to using exact extraction rate as in Section 3.1. To measure PPL, we use a random subset of 1k sequences sampled from the test split of the Pile, ensuring that PPL is measured on data unseen by the model. We also compare our metrics with those of similar sized models that were not trained on the Pile dataset (GPT2 models). Our premise here is that better performance in terms of privacy and utility, when compared to an out-ofdomain model of similar size, would mean that our defense mechanism is of value to an API owner.\nIn Table 1 , we display our results obtained using the default evaluation setting (prefix and suffix comprise of 50 tokens). Our defense achieves lower extraction rates with competitive PPL values. For the 125M model, we achieve an exact extraction rate reduction of 99.4% relative to baseline with a PPL increase of 25.3% at \u03b8 = 1.75. For the 1.3B model, the extraction rate is reduced by 97.7% relative to baseline with a PPL increase of 16.9% at \u03b8 = 1. The ability to achieve lower extraction rates with lower PPL values as measured against the GPT2 models of the corresponding size, provides evidence that our defense is effective.\n\nConclusion\nWe present the first known effort to leverage prompt-tuning to control the extractability of memorized data from LLMs in an open language generation task. We develop a novel data extraction attack and defense, and illustrate their performance under various settings. Our attack consistently outperforms the baseline in terms of exact extraction rate. Our defense provides competitive privacy-utility trade-offs and would prove beneficial to API owners with model trained on sensitive content. These results are achieved efficiently, without any change to the original model weights. We details avenues of future work in Appendix C\n"}
{"question": "Which intervention consistently enhances both sample efficiency and average robustness in low-data scenarios for BERT BASE models in NLI and sentiment analysis?", "evidence": "  Our results show that models with higher sample efficiency may not necessarily have higher average OOD robustness-different tasks and modeling interventions affect robustness in different ways.  prompt-based fine-tuning consistently improves both sample efficiency and average robustness, but only in low-data settings These results suggest that general-purpose methods for improving sample efficiency are unlikely to yield universal OOD robustness improvements, since such improvements are highly dataset-and task-dependent.  ", "options": ["A. Increasing model size", "B. Gerneral-purpose methods", "C. Zero-shot prompting of text-davinci-001", "D. Prompt-based fine-tuning"], "answer": "D", "content": "\nIntroduction\nNLP models perform well when evaluated on data drawn from their training distribution (indistribution / ID), but they typically suffer large drops in performance when evaluated on data distributions unseen during training (out-of-distribution / OOD; Blitzer, 2008) .\nHow does exposure to ID training examples affect the ID-OOD gap? If two models have the same ID performance, will models trained on fewer ID examples (higher sample efficiency) also have higher OOD performance (higher robustness)? At one extreme, zero-shot models will not learn IDspecific patterns because they are not exposed to any labeled ID examples. Similarly, few-shot models trained on very few ID examples may also rely less on ID-specific patterns; if a model never sees the token \"cat\" while training on SNLI, then it will not learn that its presence is spuriously predictive of the contradiction label (Gururangan et al., 2018; Utama et al., 2021) . Supporting this intuition, recent work in image classification (Radford et al., 2021) and extractive question answering (Awadalla et al., 2022) show that zero-shot inference and fewshot fine-tuning improve average robustness across a range of OOD test sets. However, it is unclear how universal these trends are across various tasks and methods for reducing exposure to ID examples, or how predictive they are for any individual test set of interest. Figure 1 illustrates this central question.\nWe conduct a broad empirical study over 14 datasets across three tasks to investigate the relationship between exposure to ID training examples (sample efficiency) and robustness. We experiment with three modeling interventions that improve sample efficiency: (1) using natural language prompts for zero-shot prediction and during finetuning (Brown et al., 2020; Schick and Sch\u00fctze, 2021; Gao et al., 2021) ; (2) fine-tuning models of increasing size; (3) fine-tuning models pre-trained on increasing amounts of data.\nWe find that higher sample efficiency is only sometimes correlated with better robustness, and the effect of specific modeling interventions varies by task. For example, increasing pre-trained model size substantially improves sample efficiency and results in higher average robustness in sentiment experiments, but these sample efficiency gains do not translate to higher average robustness in NLI and extractive QA experiments. On individual datasets, models with better sample efficiency can even be less robust (e.g., increasing model size when training on SST-2 and evaluating OOD on IMDb).\nOverall, these results indicate that general- purpose methods for improving sample efficiency are far from guaranteed to yield significant OOD robustness improvements-their success is highly dataset-and task-dependent. Furthermore, even in this era of large, multi-purpose pre-trained language models, task-specific decisions are often necessary to achieve OOD generalization.\n2 Measuring Sample Efficiency and Robustness. (3) M 's performance on examples from D ood (i.e., the OOD performance).\nLet M 1 and M 2 be two models with equivalent performance on held-out ID data. If M 1 was trained on fewer ID examples than M 2 , then it has higher sample efficiency. If M 1 has higher OOD performance than M 2 , it has higher effective robustness (henceforth \"robustness\"; Taori et al., 2020) . Comparing models with equivalent ID performance controls for its effect on OOD performance, since improving ID performance usually yields commensurate improvements on OOD performance-in this study, we focus on OOD performance improvements beyond what is expected from ID gains.\nSatisfying this equivalent-ID constraint is often difficult in practice; given an arbitrary model M 1 and its corresponding ID performance, it is difficult to produce a different model M 2 with identical ID performance. Rather than explicitly training models to identical ID performance, we train models on varying-size subsamples of a given ID dataset and interpolate between the results to estimate (1) the number of labeled ID training examples necessary to achieve a particular ID performance (sample efficiency) and (2) OOD performance, given ID performance (robustness). These interpolated curves approximate the ideal setting of training a model for every possible ID value. Figure 1 provides a schematized example, with model B having better sample efficiency and robustness than model A.\n\nExperimental Setup\nWe study three modeling interventions-using natural language prompts, increasing pre-trained model size, and pre-training on more data-on 14 total datasets spanning natural language inference (NLI), sentiment analysis, and extractive question answering (QA). See Appendix A for further details about experimental settings.\nTasks and Datasets. In our natural language inference (NLI) experiments, we use MultiNLI (Williams et al., 2018) , SNLI (Bowman et al., 2015), and MedNLI (Romanov and Shivade, 2018) . For sentiment analysis, we use IMDb reviews Maas et al. (2011) , SST-2 (Socher et al., 2013) , and reviews from the \"Movies and TV\" subsection of the Amazon Reviews corpus (Ni et al., 2019) . Lastly, for extractive question answering, we use SQuAD (Rajpurkar et al., 2016 ), NaturalQuestions (Kwiatkowski et al., 2019) , TriviaQA, BioASQ (Tsatsaronis et al., 2015) , and the four SQuAD-Shifts test sets (Miller et al., 2020) .\nModeling Interventions. To understand the effect of a particular modeling intervention on sample efficiency and robustness, we evaluate pre-trained models that differ only along the axis of interest (e.g., model size or fine-tuning method). Since the optimal fine-tuning hyperparameters depend on the ID training dataset size, we separately tune hyperparameters for each model on each training dataset subsample size, taking the models that achieve the best held-out ID performance for each setting. See \n\nResults and Discussion\nOur results show that models with higher sample efficiency may not necessarily have higher average OOD robustness-different tasks and modeling interventions affect robustness in different ways . For example, prompt-based fine-tuning consistently improves both sample efficiency and average robustness, but only in low-data settings (Figure 2 ). In contrast, increasing model size improves sample efficiency across the range of training dataset sizes and tasks, but only improves average robustness on sentiment analysis (Figure 3 ). On individual datasets, we even observe cases where models with lower sample efficiency have higher robustness (Figure 3d ). See Appendix C for full results on every ID-OOD setting.\nNatural Language Prompting. We compare BERT BASE models using (1) standard fine-tuning, (2) prompt-based fine-tuning, and (3) zero-shot prompting. We also compare these results with zero-shot prompting of text-davinci-001, a much larger model trained on substantially more data. We run experiments on NLI and sentiment analysis, since extractive QA is not amenable to prompt-based fine-tuning with masked language models.\nFigures 2a and 2b plot the average performance on all OOD datasets as a function of ID performance and the ID performance as a function of the number of labeled training examples. Sample efficiency improvements from prompt-based finetuning also translate to higher average robustness. However these improvements only apply in the few-shot setting. As the size of the training dataset increases, the improvements in sample efficiency and average robustness steadily diminish. When using sufficiently large training datasets, models trained with prompt-based fine-tuning yield essentially the same sample efficiency and robustness results as standard fine-tuning (\u223c1K examples for NLI, \u223c130 examples for sentiment).\nHowever, results on individual OOD test sets can significantly differ from averaged-OOD trends. For example, Figure 2c shows that prompt-based fine-tuning on MNLI and evaluating on SNLI improves sample efficiency in the few-shot setting but without any robustness improvements.\nSurprisingly, we also find that zero-shot inference does not necessarily improve average robustness over prompt-based fine-tuning-zero-shot performance lies on or below the trend line formed by prompt-based fine-tuning, despite not using any ID-specific data at all. See Appendix C.1 for full results of increasing pre-trained model size for every ID-OOD setting.\nIncreasing Pre-Trained Model Size. We run experiments with the checkpoints of Turc et al. (2019) , who pre-train BERT models with various numbers of transformer layers (L) and hidden embedding sizes (H). We run experiments on NLI, sentiment analysis, and extractive QA to compare pre-trained models of five sizes: (1) Large (L=24, H=1024), ( 2) Base (L=12, H=768), (3) Medium (L=8, H=512), (4) Small (L=4, H=512), and\n(5) Tiny (L=2, H=128). Although increasing the pre-trained model size improves sample efficiency on every task, it does not always improve average robustness (Figure 3 ). In particular, increasing model size minimally affects average robustness in NLI and extractive QA (Figure 3a ,3c), but substantially improves average robustness on sentiment analysis (Figure 3b ). 4a,b ). In extractive QA experiments, varying the amount of pre-training data does not significantly change average robustness (Figure 4c ). Again, we find that results on average OOD performance are not predictive of results on individual test sets-despite unchanged average OOD robustness when pre-training on more data, OOD performance can be higher on individual extractive QA test sets (e.g., SQuAD \u2192 BioASQ; Figure 4d ). See Appendix C.3 for full results of pre-training on Figure 4 : Pre-training on more data is an effective method for improving sample efficiency, but these sample efficiency improvements are not always accompanied by robustness improvements. In NLI and sentiment analysis experiments, these sample efficiency gains correlate with improved average robustness (a,b). However, there are no average robustness gains in extractive QA (c). Despite no average robustness improvement in extractive QA, pre-training on more data can still improve robustness on particular test sets (e.g., BioASQ; d). more data for every ID-OOD setting.\n\nConclusion\nWe study the relationship between sample efficiency and robustness across three tasks and three modeling interventions, finding that sample efficiency improvements often fail to translate to improved robustness. As larger models quickly become more sample efficient, our results caution that sample efficiency and robustness are different axes of improvement and that optimizing for sample efficiency will not necessarily always yield robustness gains.\n"}
{"question": "What method is used to minimize the decoding distribution between the student and the teacher models in the proposed uccST framework?", "evidence": " For the cross-modal unified or MT decoding path, it requires the transcription from an additional ASR, which is commonly a pre-training step for ST. Further, we impose a representation regularization on the encoder output. Particularly, we apply the MSE loss. where we concatenate the encoder outputs of ST and MT such that it results in the same length as the unified model.  We employ Kullback-Leibler divergence (KL) to minimize the decoding distribution between the student and the teacher model.  ", "options": ["A. Mean Squared Error (MSE) loss", "B. Transcription from ASR", "C. Concatenation of encoder outputs", "D. Kullback-Leibler divergence (KL)"], "answer": "D", "content": "\nIntroduction\nSpeech translation (ST) is the task that automatically translates a source acoustic speech signal into a text sequence in a target language. With the advance of Transformer, recent works on end-to-end speech translation (E2E ST) can alleviate many problems usually occurred in the cascade system and achieve comparable performance (Bahar et al., 2021; Bentivogli et al., 2021; Fang et al., 2022) .\nFor the E2E ST model, MT is often used as the teacher of ST, and methods such as knowledge distillation or contrastive learning are used to bridge the modality gap. The MT teacher only uses the source text (transcription) information. The speech and text modalities are consumed individually by ST model. There are two main drawbacks. One is the teacher MT model can not use speech information, which limits the overall model perfor-mance. The other is MT uses text input, ST uses the speech input, then close the two individual modalities. There is no unified module can simultaneously use cross-modal information.\nHere, we take a further step towards more effective use of both speech and transcription text in ST. Inspired by the related works of video Transformer (Kim et al., 2021) , when processing video, concatenating video information and text embedding information can better model the cross-modal information of the video. We concatenate the preprocessed speech and the transcription text jointly, and encode the two-modal information simultaneously. Following the recent popular advance in E2E ST with knowledge distillation (KD) (Tang et al., 2021; Zhao et al., 2021) , it provides a practical paradigm for transferring knowledge from rich-resource MT task to limited resource ST task. However, we re-define the role of teacher in our framework, because the information of the two modalities can further improve the upper bound of model performance than the single modality. Our proposed model, a unified cross-modal concatenate ST structure (uccST) introduces the teacher-student learning with Kullback-Leibler divergence (KL) regularization to transfer knowledge from cross-modal translation model to two subtasks -ST and MT.\nOur main contributions can be summarized.\n(1) Compared with the previous ST frameworks which can only utilize one single modality text in MT teacher, we design a unified framework that can use both input information of the two modalities simultaneously by concatenating speech and text.\n(2) Our cross-modal framework has three diverse inputs when inference, containing three end-toend and cascade decoding paths. Our multi-task learning framework allows sub-tasks to collaborate, showing promising performance on both end-toend and cascade ST.\n(3) We conduct various experiments on the MuST-C corpus. When using the limited ternary ST data, our E2E ST model can achieve state-ofthe-art performance. When adding the external data, our method significantly improves over the strong baselines.\n2 Unified Cross-modal Concatenate ST\n\nBackground\nGiven the source acoustic speech sequence s, the corresponding transcription x and the text sequence y in target language, speech translation usually model the conditional distribution as follows.\nEQUATION\nIn most works, the assumption p(y|x) = p(y|x, s) is usually adopted as the source transcription can deterministicially infer the final translation. However, we prefer to leverage the original conditional probability for our modeling.\n\nCross-modal Concatenate Framework\nInspired by video Transformer, the unified model can take as input the concatenation of the features of two modalities along the temporal dimension. As shown in Figure 1 (b), the speech preprocessing module usually includes CNN down-sampling and a speech encoder, such as the encoder of the pre-trained ASR or the pre-trained audio encoder wav2vec2.0. For the text sequence, we simply process each token with an embedding layer. After the concatenation, we add the position embedding and segment embedding in the fashion of BERT.\n\nMulti-task Training\nConcretely, given a ternary ST example (s, x, y).\nWe optimize three translation tasks in parallel, including MT, ST and our introduced unified crossmodal translation.\nL M T = log p(y|x) + log p(y|s) + log p(y|[x, s]) (2) where [\u2022, \u2022] indicates the concatenation operation.\n\nRegularization\nUnlike other ST frameworks, the unified crossmodal decoder output provides the teacher signal, and the ST and MT models are two students. We employ Kullback-Leibler divergence (KL) to minimize the decoding distribution between the student and the teacher model.\nL KL = KL (p st \u2225p unified ) + KL (p mt \u2225p unified ) (3)\nFurther, we impose a representation regularization on the encoder output. Particularly, we apply the MSE loss. where we concatenate the encoder outputs of ST and MT such that it results in the same length as the unified model.\nL M SE = MSE ([Z ST , Z M T ] , Z Unified ) (4)\n\nTraining and Inference\nIn summary, the final loss of the proposed uccST can be written as follows.\nEQUATION\nwhere \u03bb and \u03b8 are hyper-parameters. During inference, we have 3 optional decoding paths. If only audio is available, we can actually choose any decoding path. For the cross-modal unified or MT decoding path, it requires the transcription from an additional ASR, which is commonly a pre-training step for ST.\n3 Experiments Settings\n\nDatasets and Settings\nData For a fair comparison with previous works, we conduct our experiments on the widely used MuST-C V1: English-German (En-De), English- French (En-Fr) and English-Spanish (En-Es) corpus (Gangi et al., 2019) . On En-De and En-Fr, we also verify to what extent the auxiliary MT data can improve our multitask training. Specifically, we extract about 20M sentence pairs for the WMT14 En-Fr, 4.5M for WMT14 En-De, and 18M for Opensubtitle2018 En-De. Settings We implement all our experiments on Fairseq 1 . We experiment with two architectures 2 . One is the transformer model with 512 hidden units 2048 feed-forward size, which is same as Tang et al. (2021) , in purpose for constrained ST data. The other one is to leverage pre-trained wav2vec2.0 (Baevski et al., 2020) as the speech preprocessing module. Since wav2vec2.0 has been already pre-trained with the audio data of Librispeech (Panayotov et al., 2015) , we only compare this setup to other works with same architecture. During training, the text input is ground truth transcript of MuST-C. Note that the transcription data in Librispeech is not used in our case. We select the alternative batches between ST and MT with sampling ratios 1.0 and 0.25, respectively.\n\nResults on the Constrained ST Data\nAs shown in Table 1 , our method achieves an appealing performance on the three language pairs in the restricted ternary MuST-C data.\nCompared with the direct E2E ST baseline, our method has enhanced 0.7 to 1.7 BLEU on the three language directions, with an average gain of 1.13 BLEU. In a word, our approach can achieve the SOTA translation performance among all end-toend ST methods.\nCompared with the cascade method that we have reproduced, our E2E ST decoding path surpasses the cascade on the language pairs En-Fr, and reaches a comparable level on En-De and En-Es. The results of the MT decoding path with the transcription exceed the cascade method on all language pairs. Our cross-modal unified decoding method has enhanced 0.8 to 1.9 BLEU than the cascade method, with an average gain of 1.17 BLEU. In summary, our E2E ST method has matched or surpassed the cascade method on the constrained triple ST data, and our cross-modal unified decoding method has exceeded the traditional cascade baseline.\n\nResults on the External Data\nSince our model is a multitask learning method that includes the MT subtask, we add additional MT data for comparison experiments. As shown in Table 2 , we compare different baselines with similar data usage. Our E2E method (i.e., ST decoding path) and the corresponding baselines are presented in the bottom two rows. The first two rows in the table are the baselines without wav2vec2.0, and the middle part of the table represents the methods with wav2vec2.0 architecture. It is concluded that the pre-trained audio encoder model is indeed helpful for downstream ST task. By introducing more auxiliary MT data, our model with pre-trained wav2vec2.0 improves 1.5 and 2.3 BLEU on the two language pairs En-De and En-Fr, respectively. In shot, our approach outperforms existing state-ofthe-art models, especially on En-Fr.\n\nAblation Analysis of Concatenation\nIn order to analyze whether our concatenation is effective, we have done comparative experiments on different input models. As shown in loss is calculated between ST and MT. Unified simple model only concatenates the speech and text sequence from each corresponding encoder output.\nIn accordance to the result, no concatenation or the concatenation method in Unified simple model is inferior to our proposal.\n\nAblation Study on Loss\nTo analyze the importance of each component of the overall uccST loss, we conduct an ablation study by removing each loss step by step. Table 4 summarizes the results of the ablation study. We first remove the KL loss but reserve the unified structure. It concludes that the KL terms contribute to an improvement of 0.42 BLEU score. After further removing the MSE loss, the model becomes a standard multi-task ST Transformer. When removing multi-task, it reduces to a standard E2E ST model.\n\nComparison with the Cascaded Model\nAs shown in Table 5 , our proposed E2E ST has reached a comparable level to cascaded methods, both in data-constrained and non-constrained cases.\nAs to the two decoding methods that require transcription text, our method can outperform the cascade baseline. Meanwhile, we can observe that with the additional external data, the gap between two inference setups S|X and S is narrowed.\n\nRelated Works\nCascade ST. Cascade ST system concatenates the individual ASR and MT components (Stentiford and Steer, 1988; Waibel et al., 1991) , and represents an intuitive solution to achieve reasonable performance and high intelligibility. At the same time, this cascade method also faces some thorny problems: the traditional cascade method suffers from error propagation and the loss of acoustic information that might be useful to improve final translations. To alleviate the aforementioned problems, some tight integration methods have been proposed (Sperber et al., 2019; Bahar et al., 2020) . End-to-end ST. To overcome the weakness of cascade models, Berard et al. (2016) proposed the first direct neural network model of an encoder decoder architecture without the intermediate transcription.\nCurrently, more effective solutions are used in endto-end ST models (Park et al., 2019; Dong et al., 2021) . To alleviate the cross-modal difficulty in end-to-end models, two-pass (Kano et al., 2017; Anastasopoulos and Chiang, 2018) methods are proposed. Curriculum learning (Kano et al., 2017; Wang et al., 2020) is proposed to improve performance of ST models.\n\nConclusion\nIn this paper, we designed a unified ST framework.\nCompared with the previous ST frameworks which can only utilize one single modality text in MT teacher, our method can use both information of the two modalities simultaneously by concatenating speech and text. Our ST method can better utilize the cross-modal information. Experiments show that our method can significantly improve ST performance regardless of using the limited ternary data or adding auxiliary external data.\n"}
{"question": "What is the primary goal of the study mentioned in the paper?", "evidence": "  This study revisits the fastest pattern-based NLP methods to make them as accurate as possible... I take an orthogonal approach toward absolutely efficient NLP by seeking to boost the accuracy of the fastest methods. ", "options": ["A. To improve the speed of neural models for NLP.", "B. To develop an efficient neural model for Japanese morphological analysis.", "C. To boost the accuracy of pattern-based NLP methods.", "D. To compare the efficiency of classical methods with neural models. "], "answer": "C", "content": "\nIntroduction\nThe amount of text data being processed has greatly increased since the advent of communication platforms such as Twitter, Zoom, and Slack, and NLP services such as DeepL and Grammarly have millions of users. Some users analyze textual big data for marketing, linguistics, or sociology, while others deploy NLP services on their own devices because of privacy concerns. It is therefore becoming important to develop highly efficient methods to process massive text data and user queries with limited computational resources.\nHowever, the recent campaign for efficient NLP does not focus on literally efficient methods that scale to increasing data sizes and run on resourceconstrained devices. Instead, most \"efficient\" NLP studies (Treviso et al., 2022) focus on neural methods, which are too slow to handle billions of social media posts and too large to deploy on edge devices. Those studies seek to make model training or inference relatively efficient within the deep learning framework. Thus, the large efficiency gap with respect to classical methods has never been filled. !\"#$ %&'()*+,+*(-. %/00+#1 !!\"\"\" \"#!#$%& $!#$%' %!\" #&!!#$%& '! ()!\"\"\"\"\"\"\"\"\"\"\"\" \" ! \"# $ % #& ' ()\"\"\"\"\"\"\"\"\"\"\" \" $%& $%' ()*( $%& +,-. &*(/0 ()*( In this study, I take an orthogonal approach toward absolutely efficient NLP by seeking to boost the accuracy of the fastest methods. Specifically, I have developed a remarkably simple yet accurate method for Japanese morphological analysis, which is a joint task of word segmentation, part-of-speech (POS) tagging, and lemmatization. This method revisits the classical longest matching method; it greedily applies patterns that determine the next position to segment and then identifies the POS tag for the segmented word, as illustrated in Figure 1 . To obtain reliable patterns, starting from words in a morphological dictionary and training data, patterns are extended with posterior surface contexts and previous POS tags, and the patterns' segmentation offsets and tags are determined by frequency. The extracted patterns are then stored in an efficient double-array trie (Aoe, 1989) .\n!(&\"'(%0& !\"#$% ! &' &(% \"%)' *( %+# \"#$%#&' () ! !!\" #$%#&\nThe proposed method was evaluated on two standard corpora (Kurohashi and Nagao, 2003; Hangyo et al., 2012) . The experimental results confirmed that this simple method can process 1,000,000 sentences per second on an M2 MacBook Air, with comparable accuracy to learning-based baselines (Kudo et al., 2004; Neubig et al., 2011) . This section describes the method of Japanese morphological analysis used here, which performs word segmentation, POS tagging, and lemmatization. To maximize the tagging efficiency, I return to a pattern-based algorithm that is similar to the longest matching algorithm (Nagata, 1994) .\nThe longest matching algorithm performs deterministic word segmentation by using a dictionary. Starting from the beginning of the input, it greedily finds the longest dictionary words to segment the input. Although this simple algorithm exhibits moderate accuracy in Chinese and Japanese with transformation rules (Palmer, 1997; Hockenmaier and Brew, 1998; Sassano, 2014) , there is a gap in accuracy from search-and classification-based approaches (Kudo et al., 2004; Neubig et al., 2011) . To make search-based morphological analysis partially deterministic, Morita and Iwakura (2019) extracted surface patterns from tagging results; however, the speed-up factor was at most 1.5.\n\nBasic algorithm\nAlgorithm 1 is a simple, deterministic algorithm for joint word segmentation, POS tagging, and lemmatization. It repeatedly applies the longest-matching patterns in a trie P to a given sequence of characters, c, and a start position i to segment and tag the next word (w j = c i+ \u015dhift i and tj ). As will be shown later in \u00a7 3, this simple algorithm works as well as learning-based approaches.\nThis algorithm is inspired by the longest matching algorithm but differs in that the segmentation offset shift can be smaller than the surface length matched with patterns, k (see Line 7 in Algorithm 2). A running example is shown in Figure 1 .\nThe algorithm is also inspired by the precomputation of feature weights in sequence labeling (Kaji et al., 2010) and classification with conjunctive features (Yoshinaga and Kitsuregawa, 2009 , 2010 , 2014) weights in advance and retrieve those partial results by using simple keys such as word unigrams, POS bigrams, and primitive feature sequences to compute the final results (labels) by an argmax operation on the weights. The proposed method regards word segmentation and tagging as a joint, multiclass classification problem and directly obtains the label (i.e., where to segment and what to tag) by using the feature sequence as a pattern, thus skipping the expensive argmax operation over a number of labels. The longest matching thus implies classification with as many features as possible.\n\nPattern extraction from data\nFollowing the feature templates of learning-based methods (Kudo et al., 2004; Neubig et al., 2011) , the algorithm's pattern template was designed as a sequence of characters, c, followed by the previous word's POS tag t j\u22121 , thus giving c; t j\u22121 , where ';' represents string concatenation.\nAlgorithm 2 is the procedure to extract patterns for word segmentation and POS tagging from the annotated data and a dictionary. Given training data D with annotation of (word) segmentations and (POS) tags and a dictionary V compiling words and their possible tags, the algorithm iteratively extracts possible patterns from D. It first enumerates surface patterns c i+k i from all starting positions of words in D, and it then concatenates them with tag t j\u22121 for the preceding words to form pattern candidates (Lines 3-10 in Algorithm 2). Patterns are added for dictionary words that are unseen in the training data (Lines 11-12). The segmentation offset (shift) and tag t for a pattern are determined by the frequency (Lines 14-15). To avoid extra matching to the posterior contexts and previous tag, we only keep patterns whose segmentation offsets and tags differ from those of the longest prefix patterns that share prefixes of posterior contexts (Lines 16-18). This not only reduces the number and length of patterns but also minimizes the longest matching method's overhead for word segmentation. 1\n\nExperiments\nThis section describes an experimental evaluation of the pattern-based morphological analyzer on two annotated corpora in different domains (Kurohashi and Nagao, 2003; Hangyo et al., 2012) . The method was compared with two learning-based baselines (Kudo et al., 2004; Neubig et al., 2011) in terms of efficiency and accuracy. Note that all language resources and software used in the experiments are publicly available and free for academic use.\n\nSetup\nData The experiments used the Kyoto-University Text Corpus 2 (KYOTO) (Kurohashi and Nagao, 2003) , compiled from newspaper articles, and the Kyoto-University Web Document Leads Corpus 3 (KWDLC) (Hangyo et al., 2012) , compiled from the first three sentences of various Web pages. I adopted the split of development and test sets given in the corpora's github repositories and used the remaining portions as training sets. The datasets' statistics are listed in Table 1 .\nMethods The three methods below were compared. To prevent overfitting, the hyperparameter C in the underlying model was tuned for the two learning-based baseline methods 4 by using the development set to maximize the F 1 of the POS tags.\n1 In preliminary experiments, a variant of backtracking-free search (Maruyama, 1994) did not improve the throughput.\nVaporetto (ver. 0.6.2) is a Rust 6 implementation of a classification-based method (Neubig et al., 2011) . 7 It first performs word segmentation by classifying whether to segment after each character in the input, and it then identifies the resulting words' POS tags. It also trains classifiers for the possible POS tag sets of individual words, and it assigns the POSs of its first dictionary entries for words that are unseen in the training data. 8 A morphological dictionary was used to extract word features.\nJagger is a C++ implementation of the proposed algorithm. It greedily applies patterns extracted from the training data and a dictionary to jointly segment words and assign tags. Appendices A and B respectively describe the method to handle unknown words and the implementation details. Jagger is more similar to Vaporetto than to MeCab but differs in that it jointly performs segmentation and tagging instead of using a two-step cascaded pipeline, and it uses patterns instead of classifiers to find labels (i.e., where to segment and what to tag). Appendix C compares Jagger with the other implementations.\nDictionaries As listed in Table 2 , the experiments used two morphological dictionaries imported to MeCab from a manually tailored morphological analyzer, JUMAN. 9 Specifically, mecabjumandic-5.1-20070304 and mecab-jumandic-7.0-20130310 were compared to examine the impact of the dictionary's quality and size. The jumandic-5 https://taku910.github.io/mecab/ 6 Rust exhibits comparable efficiency to C++ on program benchmarks: https://github.com/kostya/benchmarks/.\n(2) minor POS (e.g., common noun); (3) conjugation type (e.g., ichidan verb); and (4) conjugation form (e.g., irrealis). For example, the POS tags of shumi and iru in Figure 1 are noun-common_noun-*-* and verb-*-ichidan_verb-terminal, respectively.\nEvaluation procedure The precision, recall, and F 1 of the segmentation with various levels of POS tags (Kudo et al., 2004) were used as metrics. As Vaporetto does not output lemmas, lemmatization was evaluated via the tagging results of the full POS tag set (\"all (levels 1-4)\" in Tables 3 and 4 ), which included conjugation types and forms, given that Japanese words can be mapped to their lemmas according to their conjugation types and forms. I processed 1000 copies of the test data and measured the time, speed, and maximum memory consumption three times with the /usr/bin/time -l command. The median values are reported here. All experiments were done on an M2 MacBook Air with a 3.5-GHz CPU and 24-GB main memory.\n\nResults\nTables 3 and 4 summarize the morphological analysis results on the KYOTO and KWDLC datasets.\nThe pattern-based method here, Jagger, was 16 and 7 times faster than MeCab and Vaporetto with 1/2 and 1/20 as much memory consumption, respectively, while achieving comparable accuracy.\nJagger is efficient because it does not have massive floating-point parameters, unlike other methods, and because it minimizes the number and length of patterns by pruning (Lines 16-18 in Algorithm 2). As a result, the training took less than six seconds. MeCab's accuracy depends on the dictionary: with jumandic-7.0, it worked best on KWDLC and worst on KYOTO. In contrast, Vaporetto's accuracy depends on the training data size. It worked best on KYOTO but was just as good as Jagger on KWDLC. Below are the detailed results for Jagger with the jumandic-7.0 dictionary. Comparison to neural methods Jagger was compared to a state-of-the-art neural method (Tolmachev et al., 2018) , JUMAN++-V2, 10 which was trained on the same data with the official script and hyperparameters. 11 Note that this comparison was unfair to Jagger in terms of accuracy and to JUMAN++-V2 in terms of efficiency, because JUMAN++-V2 uses 0.8 million additional dictionary entries from Wikipedia and a neural language model trained on 10 million sentences from the Web. Table 5 summarizes the comparison between Jagger and JUMAN++-V2. Although JUMAN++-V2 was reported to speed up JUMAN++ (Morita et al., 2015) by a factor of 250, Jagger was faster than JUMAN++-V2 by a factor of 180 with 1/7 as much of a memory footprint. JUMAN++-V2 was more accurate than Jagger, but the gain was less than 1% for word segmentation. If external text could be used, this gap could be reduced with a technique called structure compilation (Liang et al., 2008) , which runs JUMAN++-V2 on external text to extract patterns. That idea is beyond this paper's scope but important for future work.\nWord segmentation efficiency Because of different approaches to handling unknown words and supporting lemmatization, it is difficult to compare Vaporetto with Jagger and MeCab as a morphological analyzer in a strictly fair manner. Instead, the word segmentation efficiency was compared, as summarized in Table 6 . Here, Vaporetto was trained to perform only word segmentation by using the dictionary and the training data without POS tags. Jagger was faster and more space-efficient than Vaporetto, even taking the overhead of loading large models (1.7 seconds) into account. enjoys the benefits of the dictionary and training data: it can change its behavior by adding not only dictionary entries but also patterns.\n\nConclusions\nThis study sought to improve the accuracy of speedoriented, pattern-based methods for Japanese morphological analysis, rather than improving the speed of accuracy-oriented neural models. The proposed method extracts POS-augmented patterns from a morphological dictionary and annotated data. Experimental results on two standard datasets confirmed that this method achieves accuracy comparable to that of learning-based methods, with a very fast throughput of over 1,000,000 sentences per second on a laptop. I plan to apply this approach to other languages and even to other NLP tasks by discretizing the continuous representations induced by neural models to obtain patterns. The source code is released with GPL, LGPL, and 2-clause BSD licenses.\nMessage to researchers Because the accuracies on NLP benchmark datasets are becoming saturated with a larger foundation model, researchers may want to set diverse goals based on underrepresented metrics besides accuracy (e.g., efficiency). I hope that this study will initiate serious research on speed-intensive approaches to NLP that can meet industry demands and enable researchers with limited computational resources to exert their ability.\n"}
{"question": "What\u2019s the result of our experiment?", "evidence": "  For all but the least powerful model, Ada, the non-ellipsis accuracy is substantially higher than ellipsis accuracy, supporting the hypothesis that ellipsis-dependent reasoning presents a difficult challenge for these models. While the Ada model actually performs somewhat better for ellipsis than non-ellipsis, this is not because the Ada model does well with ellipsis cases; rather, the model has great difficulty with both the ellipsis and non-ellipsis cases, and is close to a random guessing baseline of .50.\nIn figures 2 through 6, we present results for each model. We show the accuracy for each structure, for both the ellipsis version and the non-ellipsis version. Consider the most powerful models, Davinci-003 and Davinci-002. In figures 2 and 3, we can see that ellipsis is not difficult in the first two structures: 2Sent (Separate Sentence) and 1Sent (Conjoined Sentence). Here the accuracy is nearly perfect for the ellipsis as well as the non-ellipsis condition. However, in all the other structures, there is a large divergence in accuracy between ellipsis and nonellipsis, for both the Davinci-003 and Davinci-002 models. Subordination for either antecedent or ellipsis is quite challenging, with accuracies ranging from 48.8 to 85.8. The Backwards and Two Actions structures are even more difficult for ellipsis.\n ", "options": ["A. Except for the Ada model, the accuracy of non-ellipsis examples is significantly higher than that of examples involving ellipsis.", "B.  The Ada model is powerful for both the ellipsis and non-ellipsis cases, and is close to a random guessing baseline of 0.60.", "C. The least powerful models are Davinci-003 and Davinci-002. ", "D. In all the other structures, there is no divergence in accuracy between ellipsis and nonellipsis, for both the Davinci-003 and Davinci-002 models. "], "answer": "A", "content": "\nIntroduction\nEllipsis is a fundamental feature of human language, occurring in all registers, where parts of sentences are omitted, although the missing parts are essential for understanding the meaning. The following is an example of Verb Phrase Ellipsis (VPE) (Bos and Spenader, 2011) :\n(1) William went running. Harold did too.\n(1) is understood as asserting that Harold went running; that is, the hearer or reader naturally fills in the missing material. This is done by identifying the antecedent VP, went running, in the first sentence. The following is the non-elliptical counterpart of (1):\n(2) William went running. Harold went running too.\nWith such examples, we can test understanding of ellipsis by targeting the ellipsis phrase with a simple Yes/No question:\n(3) Did Harold go running?\nIf a system answers the question incorrectly for (1), the ellipsis example, but answers correctly for (2), the non-elliptical counterpart of (1), we can ascribe the result specifically to the challenge of ellipsis, since the examples are otherwise identical.\nAs with pronominal anaphora and other discourse processes, there is great flexibility in the way ellipsis can occur in discourse. Example (1) involves two simple adjacent sentences. It is also possible for ellipsis or the antecedent to occur in embedded clauses. Furthermore, ellipsis can occur either before or after the antecedent. Finally, an arbitrary amount of material can intervene between the antecedent and the ellipsis occurrence.\nIn this paper, we propose the challenge of ellipsis-dependent reasoning. This challenge consists of examples involving an ellipsis clause, the target. Each ellipsis example is paired with its nonelliptical counterpart, where the target clause is overt rather than elliptical. We then pose a question whose answer is dependent on the target clause. A key aspect of the challenge is that ellipsis occurrences are possible in a variety of diverse structural configurations. We test a series of GPT-3 models (GPT) on several such ellipsis structures.\n\nRelated Work\nThere is a large literature concerning the probing of language models from a variety of perspectives. Furthermore, there has been substantial work specifically addressing ellipsis in NLP. In this paper, we are proposing the challenge of ellipsisdependent reasoning. This proposal builds on various strands of prior research; below we consider some particularly relevant aspects of this literature.\n\nProbing Models for Knowledge\nThe Winograd Schema (Kocijan et al. (2022) ; Levesque et al. (2012)) involves test examples that use the linguistic problem of pronoun resolution to gain insight into the commonsense reasoning abilities of an AI system. To do this, the Winograd Schema requires pairs of examples that differ only in one specific, small way, as in ( 4):\n(4)\nThe city councilmen refused the demonstrators a permit because they feared/advocated violence.\nWith \"feared\", the pronoun \"they\" refers to the city councilmen, while with \"advocated\", it refers to the demonstrators. 2019): here, examples are constructed which test specific aspects of linguistic knowledge of a system, namely, whether BERT embeddings \"encode hierarchical information\". For example, a task is defined to identify the main auxiliary verb in a sentence, even in cases where the main auxiliary is not the first auxiliary verb to appear. Training and testing datasets are automatically generated using a context-free grammar for several such tasks involving hierarchical syntactic information.\n\nAnaphora and Question Answering\nQuoref (Dasigi et al. (2019) ; Zhang and Zhao (2022) ) is a question-answer dataset designed so that correct answers cannot be given unless a coreference relationship is correctly identified; that is, the reasoning involved in question answering is dependent on resolving coreference. This is, in a sense, the inverse of the Winograd schema, where resolving coreference is dependent upon reasoning. Just as with the Winograd schema, it is difficult to ensure that resolving this dependency is required for system success. (Dasigi et al., 2019)[p. 1] note that this is \"challenging, because it is hard to avoid lexical cues that shortcut complex reasoning\", and based on a random sample, found that coreference resolution was required for 78% of questions.\n\nEllipsis as a Task\nThere has been substantial work on ellipsis as a discrete NLP task (Khullar (2020) 2021) frame ellipsis as a question-answering task, i.e., a task of locating an antecedent, understood as a span of tokens in context. Aralikatte et al. (2021) report token F1 scores of 78.66 for VPE and 86.01 for sluicing, an-other form of ellipsis. It's important to note that the task here, of antecedent identification, is a sub-part of the ellipsis challenge. Before the antecedent is identified, an ellipsis occurrence must be identified, and after the antecedent is identified, it must be interpreted, or \"reconstructed\", at the ellipsis site.\n\nRelevance for Ellipsis-Dependent Reasoning\nThe specific task of ellipsis is addressed in work like that of Aralikatte et al. (2021) , but the key difference here is that we are probing for a complete solution to the ellipsis problem. The proposed ellipsis-dependent reasoning task involves a question that can only be answered correctly if the ellipsis is properly identified and interpreted. This combines aspects of the preceding works in a novel way: like the Winograd schema and the syntactic work by Lin et al. (2019) , it probes for what we see as a specific type of psychologically-defined knowledge: namely, a representation of context that supports the resolution of ellipsis. Similarly to the work on Quoref, we use targeted questions to probe for discourse-related knowledge.\nThere is an extensive literature on the contextual interpretation of natural language, resting on the idea of a dynamic, ongoing model of discourse. For example, Discourse Representation Theory (Kamp, 1981 ) describes a semantic model supporting discourse phenomena such as pronominal and temporal anaphora, and Sag and Hankamer (1984) argue explicitly that ellipsis and other such phenomena are interpreted with respect to a discourse model (Garnham, 2010) . As one study puts it, \"Interpreting a verb-phrase ellipsis (VP ellipsis) requires accessing an antecedent in memory, and then integrating a representation of this antecedent into the local context\" (Martin and McElree, 2008) . In this paper, we seek to determine whether a large language model is capable of such an interpretive process.\n\nData\nThere is a great deal of variety in the structural configurations in which ellipsis can occur. In tables 1 and 2 we define structures for ellipsis and antecedent occurrences.\nIn all structures, there is an ellipsis occurrence, and the question targets the ellipsis occurrence. Furthermore, each ellipsis example is paired with a non-ellipsis version. We generate large numbers of examples of each structure by performing random substitutions for both the subject and verb. The substitution lists are given in the appendix, along with samples of each structure and the size of the resulting sets. 1\n\nTest\nFor each instantiation of a given structure, we produce paired ellipsis and non-ellipsis examples, with an associated Yes/No question. We randomly select 1000 examples for each structure, including 500 ellipsis examples and 500 examples which are their non-elliptical counterparts. Each example is presented to the system, preceded by the text, \"Please give a Yes or No answer:\". We test five GPT-3 models on these structures: Davinci-003, Davinci-002, Curie-001, Babbage-001, and Ada-001. According to the GPT-3 documentation, Davinci-003 is the most powerful model and Ada-001, the least.\n\nResults\nFigure 1 gives the accuracy for ellipsis and nonellipsis, for each of the five models. We have set up the test examples so that an ellipsis example is paired with a non-ellipsis example that is otherwise identical. Because of this, we claim that the difference in accuracy of the non-ellipsis case vs. the ellipsis case provides a measurement of the difficulty specifically posed by ellipsis. For all but the least powerful model, Ada, the non-ellipsis accuracy is substantially higher than ellipsis accuracy, supporting the hypothesis that ellipsis-dependent reasoning presents a difficult challenge for these models. While the Ada model actually performs somewhat better for ellipsis than non-ellipsis, this is not because the Ada model does well with ellipsis cases; rather, the model has great difficulty with both the ellipsis and non-ellipsis cases, and is close to a random guessing baseline of .50.\nIn figures 2 through 6, we present results for each model. We show the accuracy for each structure, for both the ellipsis version and the non-ellipsis version. Consider the most powerful models, Davinci-003 and Davinci-002. In figures 2 and 3, we can see that ellipsis is not difficult in the first two structures: 2Sent (Separate Sentence) and 1Sent (Conjoined Sentence). Here the accuracy is nearly perfect for the ellipsis as well as the non-ellipsis condition. However, in all the other structures, there is a large divergence in accuracy between ellipsis and nonellipsis, for both the Davinci-003 and Davinci-002 models. Subordination for either antecedent or ellipsis is quite challenging, with accuracies ranging from 48.8 to 85.8. The Backwards and Two Actions structures are even more difficult for ellipsis.\n\nAnalysis\nFor the two most powerful models, it is clear that ellipsis poses a difficult challenge, except in the two simplest ellipsis structures. For the less powerful models, the picture is mixed. For these models, the non-ellipsis examples are themselves a difficult challenge, so we are not able to observe the specific difficulties posed by ellipsis.\nAs we can see in figure 1 , the Davinci-002 model performs somewhat better overall than Davinci- 003, on both ellipsis and non-ellipsis. However, figures 2 and 3 show that the advantage of Davinci-002 on ellipsis is exclusively due to the subordinate antecedent construction. In every other ellipsis structure, Davinci-003 performs better than Davinci-002.\nThere are striking differences in the distribution of errors. For both the Davinci-003 and Davinci-002 models, errors are nearly always false negatives -that is, incorrect \"No\" answers. There are virtually no false positives, either for the ellipsis case or nonellipsis case. For the other three models, there are many errors of each type, with a much higher ratio of false positives.\n\nConclusion\nMost of the current rapid progress in NLP is due to pre-trained large language models. GPT-3 is an impressive publicly available collection of such models, and is able to perform in a way that suggests human-level understanding. Because of this, it is important to explore areas in which it might still differ from human language understanding. In this paper we have argued that ellipsis is one such area. For many simple ellipsis structures, the most powerful GPT-3 models struggle, with accuracies far lower on ellipsis examples than on non-elliptical counterparts.\nIn many ways, GPT-3 appears to understand the texts that it processes, often being able to answer questions that appear to rely on sophisticated reasoning. However, the challenge of ellipsisdependent reasoning provides evidence that GPT-3 is not able to understand in anything like the way humans do. \n"}
{"question": "What is the primary algorithm that inspired the proposed method in the paper?", "evidence": "  This algorithm is inspired by the longest matching algorithm... The longest matching algorithm performs deterministic word segmentation by using a dictionary... ", "options": ["A. Longest Matching Algorithm", "B. Backtracking-Free Search", "C. Sequence Labeling Algorithm", "D. Feature Weight Computation "], "answer": "A", "content": "\nIntroduction\nThe amount of text data being processed has greatly increased since the advent of communication platforms such as Twitter, Zoom, and Slack, and NLP services such as DeepL and Grammarly have millions of users. Some users analyze textual big data for marketing, linguistics, or sociology, while others deploy NLP services on their own devices because of privacy concerns. It is therefore becoming important to develop highly efficient methods to process massive text data and user queries with limited computational resources.\nHowever, the recent campaign for efficient NLP does not focus on literally efficient methods that scale to increasing data sizes and run on resourceconstrained devices. Instead, most \"efficient\" NLP studies (Treviso et al., 2022) focus on neural methods, which are too slow to handle billions of social media posts and too large to deploy on edge devices. Those studies seek to make model training or inference relatively efficient within the deep learning framework. Thus, the large efficiency gap with respect to classical methods has never been filled. !\"#$ %&'()*+,+*(-. %/00+#1 !!\"\"\" \"#!#$%& $!#$%' %!\" #&!!#$%& '! ()!\"\"\"\"\"\"\"\"\"\"\"\" \" ! \"# $ % #& ' ()\"\"\"\"\"\"\"\"\"\"\" \" $%& $%' ()*( $%& +,-. &*(/0 ()*( In this study, I take an orthogonal approach toward absolutely efficient NLP by seeking to boost the accuracy of the fastest methods. Specifically, I have developed a remarkably simple yet accurate method for Japanese morphological analysis, which is a joint task of word segmentation, part-of-speech (POS) tagging, and lemmatization. This method revisits the classical longest matching method; it greedily applies patterns that determine the next position to segment and then identifies the POS tag for the segmented word, as illustrated in Figure 1 . To obtain reliable patterns, starting from words in a morphological dictionary and training data, patterns are extended with posterior surface contexts and previous POS tags, and the patterns' segmentation offsets and tags are determined by frequency. The extracted patterns are then stored in an efficient double-array trie (Aoe, 1989) .\n!(&\"'(%0& !\"#$% ! &' &(% \"%)' *( %+# \"#$%#&' () ! !!\" #$%#&\nThe proposed method was evaluated on two standard corpora (Kurohashi and Nagao, 2003; Hangyo et al., 2012) . The experimental results confirmed that this simple method can process 1,000,000 sentences per second on an M2 MacBook Air, with comparable accuracy to learning-based baselines (Kudo et al., 2004; Neubig et al., 2011) . This section describes the method of Japanese morphological analysis used here, which performs word segmentation, POS tagging, and lemmatization. To maximize the tagging efficiency, I return to a pattern-based algorithm that is similar to the longest matching algorithm (Nagata, 1994) .\nThe longest matching algorithm performs deterministic word segmentation by using a dictionary. Starting from the beginning of the input, it greedily finds the longest dictionary words to segment the input. Although this simple algorithm exhibits moderate accuracy in Chinese and Japanese with transformation rules (Palmer, 1997; Hockenmaier and Brew, 1998; Sassano, 2014) , there is a gap in accuracy from search-and classification-based approaches (Kudo et al., 2004; Neubig et al., 2011) . To make search-based morphological analysis partially deterministic, Morita and Iwakura (2019) extracted surface patterns from tagging results; however, the speed-up factor was at most 1.5.\n\nBasic algorithm\nAlgorithm 1 is a simple, deterministic algorithm for joint word segmentation, POS tagging, and lemmatization. It repeatedly applies the longest-matching patterns in a trie P to a given sequence of characters, c, and a start position i to segment and tag the next word (w j = c i+ \u015dhift i and tj ). As will be shown later in \u00a7 3, this simple algorithm works as well as learning-based approaches.\nThis algorithm is inspired by the longest matching algorithm but differs in that the segmentation offset shift can be smaller than the surface length matched with patterns, k (see Line 7 in Algorithm 2). A running example is shown in Figure 1 .\nThe algorithm is also inspired by the precomputation of feature weights in sequence labeling (Kaji et al., 2010) and classification with conjunctive features (Yoshinaga and Kitsuregawa, 2009 , 2010 , 2014) weights in advance and retrieve those partial results by using simple keys such as word unigrams, POS bigrams, and primitive feature sequences to compute the final results (labels) by an argmax operation on the weights. The proposed method regards word segmentation and tagging as a joint, multiclass classification problem and directly obtains the label (i.e., where to segment and what to tag) by using the feature sequence as a pattern, thus skipping the expensive argmax operation over a number of labels. The longest matching thus implies classification with as many features as possible.\n\nPattern extraction from data\nFollowing the feature templates of learning-based methods (Kudo et al., 2004; Neubig et al., 2011) , the algorithm's pattern template was designed as a sequence of characters, c, followed by the previous word's POS tag t j\u22121 , thus giving c; t j\u22121 , where ';' represents string concatenation.\nAlgorithm 2 is the procedure to extract patterns for word segmentation and POS tagging from the annotated data and a dictionary. Given training data D with annotation of (word) segmentations and (POS) tags and a dictionary V compiling words and their possible tags, the algorithm iteratively extracts possible patterns from D. It first enumerates surface patterns c i+k i from all starting positions of words in D, and it then concatenates them with tag t j\u22121 for the preceding words to form pattern candidates (Lines 3-10 in Algorithm 2). Patterns are added for dictionary words that are unseen in the training data (Lines 11-12). The segmentation offset (shift) and tag t for a pattern are determined by the frequency (Lines 14-15). To avoid extra matching to the posterior contexts and previous tag, we only keep patterns whose segmentation offsets and tags differ from those of the longest prefix patterns that share prefixes of posterior contexts (Lines 16-18). This not only reduces the number and length of patterns but also minimizes the longest matching method's overhead for word segmentation. 1\n\nExperiments\nThis section describes an experimental evaluation of the pattern-based morphological analyzer on two annotated corpora in different domains (Kurohashi and Nagao, 2003; Hangyo et al., 2012) . The method was compared with two learning-based baselines (Kudo et al., 2004; Neubig et al., 2011) in terms of efficiency and accuracy. Note that all language resources and software used in the experiments are publicly available and free for academic use.\n\nSetup\nData The experiments used the Kyoto-University Text Corpus 2 (KYOTO) (Kurohashi and Nagao, 2003) , compiled from newspaper articles, and the Kyoto-University Web Document Leads Corpus 3 (KWDLC) (Hangyo et al., 2012) , compiled from the first three sentences of various Web pages. I adopted the split of development and test sets given in the corpora's github repositories and used the remaining portions as training sets. The datasets' statistics are listed in Table 1 .\nMethods The three methods below were compared. To prevent overfitting, the hyperparameter C in the underlying model was tuned for the two learning-based baseline methods 4 by using the development set to maximize the F 1 of the POS tags.\n1 In preliminary experiments, a variant of backtracking-free search (Maruyama, 1994) did not improve the throughput.\nVaporetto (ver. 0.6.2) is a Rust 6 implementation of a classification-based method (Neubig et al., 2011) . 7 It first performs word segmentation by classifying whether to segment after each character in the input, and it then identifies the resulting words' POS tags. It also trains classifiers for the possible POS tag sets of individual words, and it assigns the POSs of its first dictionary entries for words that are unseen in the training data. 8 A morphological dictionary was used to extract word features.\nJagger is a C++ implementation of the proposed algorithm. It greedily applies patterns extracted from the training data and a dictionary to jointly segment words and assign tags. Appendices A and B respectively describe the method to handle unknown words and the implementation details. Jagger is more similar to Vaporetto than to MeCab but differs in that it jointly performs segmentation and tagging instead of using a two-step cascaded pipeline, and it uses patterns instead of classifiers to find labels (i.e., where to segment and what to tag). Appendix C compares Jagger with the other implementations.\nDictionaries As listed in Table 2 , the experiments used two morphological dictionaries imported to MeCab from a manually tailored morphological analyzer, JUMAN. 9 Specifically, mecabjumandic-5.1-20070304 and mecab-jumandic-7.0-20130310 were compared to examine the impact of the dictionary's quality and size. The jumandic-5 https://taku910.github.io/mecab/ 6 Rust exhibits comparable efficiency to C++ on program benchmarks: https://github.com/kostya/benchmarks/.\n(2) minor POS (e.g., common noun); (3) conjugation type (e.g., ichidan verb); and (4) conjugation form (e.g., irrealis). For example, the POS tags of shumi and iru in Figure 1 are noun-common_noun-*-* and verb-*-ichidan_verb-terminal, respectively.\nEvaluation procedure The precision, recall, and F 1 of the segmentation with various levels of POS tags (Kudo et al., 2004) were used as metrics. As Vaporetto does not output lemmas, lemmatization was evaluated via the tagging results of the full POS tag set (\"all (levels 1-4)\" in Tables 3 and 4 ), which included conjugation types and forms, given that Japanese words can be mapped to their lemmas according to their conjugation types and forms. I processed 1000 copies of the test data and measured the time, speed, and maximum memory consumption three times with the /usr/bin/time -l command. The median values are reported here. All experiments were done on an M2 MacBook Air with a 3.5-GHz CPU and 24-GB main memory.\n\nResults\nTables 3 and 4 summarize the morphological analysis results on the KYOTO and KWDLC datasets.\nThe pattern-based method here, Jagger, was 16 and 7 times faster than MeCab and Vaporetto with 1/2 and 1/20 as much memory consumption, respectively, while achieving comparable accuracy.\nJagger is efficient because it does not have massive floating-point parameters, unlike other methods, and because it minimizes the number and length of patterns by pruning (Lines 16-18 in Algorithm 2). As a result, the training took less than six seconds. MeCab's accuracy depends on the dictionary: with jumandic-7.0, it worked best on KWDLC and worst on KYOTO. In contrast, Vaporetto's accuracy depends on the training data size. It worked best on KYOTO but was just as good as Jagger on KWDLC. Below are the detailed results for Jagger with the jumandic-7.0 dictionary. Comparison to neural methods Jagger was compared to a state-of-the-art neural method (Tolmachev et al., 2018) , JUMAN++-V2, 10 which was trained on the same data with the official script and hyperparameters. 11 Note that this comparison was unfair to Jagger in terms of accuracy and to JUMAN++-V2 in terms of efficiency, because JUMAN++-V2 uses 0.8 million additional dictionary entries from Wikipedia and a neural language model trained on 10 million sentences from the Web. Table 5 summarizes the comparison between Jagger and JUMAN++-V2. Although JUMAN++-V2 was reported to speed up JUMAN++ (Morita et al., 2015) by a factor of 250, Jagger was faster than JUMAN++-V2 by a factor of 180 with 1/7 as much of a memory footprint. JUMAN++-V2 was more accurate than Jagger, but the gain was less than 1% for word segmentation. If external text could be used, this gap could be reduced with a technique called structure compilation (Liang et al., 2008) , which runs JUMAN++-V2 on external text to extract patterns. That idea is beyond this paper's scope but important for future work.\nWord segmentation efficiency Because of different approaches to handling unknown words and supporting lemmatization, it is difficult to compare Vaporetto with Jagger and MeCab as a morphological analyzer in a strictly fair manner. Instead, the word segmentation efficiency was compared, as summarized in Table 6 . Here, Vaporetto was trained to perform only word segmentation by using the dictionary and the training data without POS tags. Jagger was faster and more space-efficient than Vaporetto, even taking the overhead of loading large models (1.7 seconds) into account. enjoys the benefits of the dictionary and training data: it can change its behavior by adding not only dictionary entries but also patterns.\n\nConclusions\nThis study sought to improve the accuracy of speedoriented, pattern-based methods for Japanese morphological analysis, rather than improving the speed of accuracy-oriented neural models. The proposed method extracts POS-augmented patterns from a morphological dictionary and annotated data. Experimental results on two standard datasets confirmed that this method achieves accuracy comparable to that of learning-based methods, with a very fast throughput of over 1,000,000 sentences per second on a laptop. I plan to apply this approach to other languages and even to other NLP tasks by discretizing the continuous representations induced by neural models to obtain patterns. The source code is released with GPL, LGPL, and 2-clause BSD licenses.\nMessage to researchers Because the accuracies on NLP benchmark datasets are becoming saturated with a larger foundation model, researchers may want to set diverse goals based on underrepresented metrics besides accuracy (e.g., efficiency). I hope that this study will initiate serious research on speed-intensive approaches to NLP that can meet industry demands and enable researchers with limited computational resources to exert their ability.\n"}
{"question": "In the defense setting, what is the purpose of introducing the hyperparameter \u03b8 in the algorithm described in the passage?", "evidence": "  We introduce a hyperparameter named learning threshold denoted by \u03b8.   ", "options": ["A. To control the length of the prompt during training.", "B. To increase the embedding size of the model.", "C. To optimize the model for generation during inference.", "D. To control the level of punishment for prompts during training", "During prompt training (see Section 3.1), when loss is less than \u03b8 we do gradient ascent to penalize the prompt. If the loss is greater than \u03b8, we perform gradient descent with respect to the prompt as usual. Training is stopped once the average epoch loss is equal or above \u03b8. This allows us to increase training loss in a controlled manner and stabilize it around \u03b8."], "answer": "D", "content": "\nIntroduction\nPretrained large language models (LLMs; Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2020; Soltan et al., 2022) , commonly trained on massive crowd-sourced corpora, have been of much interest in the recent past due to their usage as backbones in state-of-the-art models across multiple downstream NLU tasks. However, they have been shown to memorize significant portions of their training data that can be extracted using appropriately-crafted prompts (Carlini et al., 2020 (Carlini et al., , 2022;; Zhang et al., 2021) . Such extractions pose a privacy risk to the contributors of the training data.\nIn this context, methods that allow developers to control the extractability of memorized examples from LLMs are of much value. For example, methods that increase extraction rates correspond to attacks in an adversarial setting, and provide developers with the ability to analyze privacy-risk.\nMethods that decrease extraction rates, referred to as defenses, are useful for protecting against such attacks. Historically, defense methods tend to be compute intensive (Abadi et al., 2016; Dupuy et al., 2021) .\nIn this work, we train continuous soft-prompts (Lester et al. 2021 ; hereafter referred to simply as prompts) and leverage them as a way of passing an external signal into an LLM, to control the extraction of memorized data. We freeze the model weights, and only use the trained prompt to control the generation. First, we train prompts in an attack setting and study the extent of extractable memorized content in our models. Second, we explore a defense setting where we create prompts that reduce extraction rates and achieve different privacy-utility trade-offs, via a user-specified hyperparameter. Since the original model weights are frozen in both these settings, our methods are compute efficient across the board.\nTo the best of our knowledge, our work is the first to adapt the use of instructive prompts for the analysis and mitigation of privacy in LLMs. We have released the code developed for our experiments 1 .\n\nBackground and Related Work\nPrevious work has shown that LLMs display memorization and has explored a range of methods that quantify extractability (Carlini et al., 2018 (Carlini et al., , 2020 (Carlini et al., , 2022)) . Differentially-private training (Dwork, 2006; Abadi et al., 2016) is a popular method that has been used to mitigate this risk. However, it tends to reduce model utility and requires retraining of the LLM, which might not be feasible due to heavy computational burden. The use of instructive prompts for language models has been extensively researched, including use during pretraining (Raffel et al., 2020) , as a second stage of training (Sanh et al., 2022; Wei et al., 2021) , and during inference to guide model output (Brown et al., 2020) . Within the third category, in order to improve upon manual prompt engineering researchers have implemented methods to learn discrete natural language prompts (Shin et al., 2020) , to mine them (Jiang et al., 2020) , or, neglecting natural language, to learn continuous prompts (Li and Liang, 2021; Lester et al., 2021) .\nOur work leverages continuous prompts as a way of passing an external signal to a model to trigger a desired model behavior (i.e., less or more memorized data in open language generation, which map to an extraction attack and defense, respectively).\n\nMethod\nPrompt-tuning requires the prepending of a prompt to the prefix embedding and access to the training loss (see Figure 1 ). Given these constraints, we explore a white-box attack where the adversary has access to the target model parameters, and a blackbox defense where the adversary interacts with the target model via an API. We therefore do not test our defense against our own attack.\nLet [prefix || suffix] be a sequence in the training set where the prefix is of length k tokens. Carlini et al. (2022) defined a suffix to be k-extractable if the model generates the suffix exactly, after being prompted with its the corresponding lengthk prefix. Our white-box attack aims to increase the number of k-extractable sequences, while our black-box defense aims to reduce the number of k-extractable sequences that can be extracted by an adversary who submits prefixes via an API.\n\nAttack\nIn the attack setting, we assume that the adversary has a set of [ prefix || suffix ] sequences S train , sampled from the training set of the target model. Their goal is to extract the suffixes corresponding to a disjoint set of prefixes, denoted by S test 2 . To do so, the adversary first initializes a prompt: a continuous set of l \u00d7 e parameters where e is the embedding size of the model, and l is the length of the prompt, a hyperparameter decided by the adversary. The prompt is trained over S train to facilitate the correct generation of suffixes. To do this, we first prepend the prompt to the embedding of the prefix and pass the joint embedding through the model for generation. We then minimize the loss objective (see below) with respect to the prompt while keeping the parameters of the model frozen.\nWe explore two loss objectives. The first is causal language modeling (hereafter referred to as CLM), where we minimize the cross-entropy loss over the entire sequence (Radford et al., 2019) . In the second, the prompt is optimized by minimizing the cross entropy loss of only the suffixes, given the prefixes. Here, the training is aligned with our inference task such that during training the model is penalized only on the suffix tokens; hence we refer to it as aligned CLM. During inference, the learned prompt is prepended to each embedding of the prefixes in S test , and the joint embedding is passed to the model for generation (see Figure 1 ).\n\nDefense\nIn the defense setting, the defender (API owner) trains the prompt, and prepends it to the incoming prefixes before passing them to the model. Our algorithm is inspired by machine-unlearning literature (Halimi et al., 2022) , and defenses against membership inference and backdoor attacks (Chen et al., 2022; Ozdayi et al., 2021) . We introduce a hyperparameter named learning threshold denoted by \u03b8. During prompt training (see Section 3.1), when loss is less than \u03b8 we do gradient ascent to penalize the prompt. If the loss is greater than \u03b8, we perform gradient descent with respect to the prompt as usual. Training is stopped once the average epoch loss is equal or above \u03b8. This allows us to increase training loss in a controlled manner and stabilize it around \u03b8. Through this process, we can achieve various privacy-utility trade-offs efficiently without re-training any part of the model. To explore \u03b8, we set the initial value to be slightly above the model training loss and increase in steps of 0.25 until desired performance is achieved.\n\nExperiments\nFor our experiments, we use the 125M and 1.3B parameter variants of the GPT-Neo models (Black et al., 2021) . These are public, decoder-only transformer models (Vaswani et al., 2017) trained using CLM on the Pile dataset (Gao et al., 2020) . We extract S train and S test from the Language Model Extraction Benchmark dataset (Google-Research). This dataset contains 15k sequences sampled from the training split of the Pile where each sequence is partitioned into a prefix and suffix. In the default evaluation setting, both prefix and suffix consist of 50 tokens. We ensure a random train/test split of 14k/1k samples.\nOur evaluation metric of choice is Exact extraction rate which is the fraction of correctly generated suffixes (i.e., all tokens of the generated suffix match with ground-truth suffix) over the test set. We additionally discuss fractional extraction rate and present results in Appendix A. As a baseline, we use the attack analyzed in Carlini et al. (2022) , which consists of feeding the prefixes to the model, and generating suffixes with greedy decoding. This is the only extraction attack for this setting apart from our work, to the best of our knowledge. Our training setup is discussed in Appendix B. All experiments are repeated over 5 runs with a new random train/test split in each run.\n\nAttack\nWe explore the performance of our attack across several dimensions: prompt length, suffix size, prefix size, and beam size. We use greedy-decoding in all cases, except the beam size experiments.\nPrompt Length First, we explore prompt length in the context of the default setting (prefix and suf-fix consist of 50 tokens; Figures 2-A1 and 2-A2 ). We note that prompts tuned with both CLM and aligned CLM provide improvements over the baseline in all cases, with aligned CLM providing the best performance. Given this, we train prompts using the aligned CLM objective for all other experiments, including our defense.\nWith aligned CLM, we achieve the highest extraction rates of 25.8% and 54.3% for the 125M and 1.3B models, respectively (an improvement of 8.9 and 9.3 percentage points, respectively), with a 100 token prompt (blue line). We observe that extraction rates increase with prompt length and tend to saturate after prompt length 100. Over-fitting was ruled out as a potential cause of saturation as there is no increase in test loss observed during training. This suggests that there is a max limit on the parameter count in the prompt that might add value for extraction purposes given our objective. We note that more sophisticated training strategies (designing better loss functions, better prompt initialization etc.) might yield better extraction rates.\nSuffix Size Next, we fix the prefix size to 50 and vary the suffix size. As shown in Figures 2-B1 and 2-B2, extraction rates decrease roughly exponentially with suffix size. We note that as suffix size increases, longer prompts (\u2265 20) provide greater improvements over the baseline. For example, with a prompt length of 100 (blue line) using the 1.3B model, at suffix size 5 we observe an extraction rate increase of 5.3 percentage points. Whereas at suffix size 50, the increase is 9.3 percentage points.\nPrefix Size Next, we fix the suffix size to 50 and vary the prefix size. As shown in Figures 2-C1 and 2-C2, extraction rates increase roughly logarithmically (as in Carlini et al. 2022) . Contrary to suffix size, we observe that the gaps between baseline and attacks decrease with increasing prefix size. This suggests that our attack stands to benefit a less informed adversary (small prefix sizes) when compared to the baseline.\nBeam Decoding Finally, we utilize the default setting with prefix and suffix sizes at 50 tokens and vary the beam size (beam size=1 corresponds to greedy decoding). The results are shown in Figures 2-D1 and 2-D2. We observe that extraction rates increase across the board when increasing beam size from 1 to 5. However, improvements tend to plateau or oscillate when beam size is greater than 5. The 1.3B model benefits more from increasing beam size achieving the highest extraction rate of 61.4%, at a beam size of 20 (with a prompt length of 150). The highest extraction rate achieved for the 125M model was 28.3% at a beam size of 15 (with a prompt length of 100).\n\nDefense\nFinally, we evaluate the privacy-utility trade-off of our black-box defense. As mentioned in Section 3, our defense is designed for a black-box adversary, and cannot be tested against our white-box attack.\nTherefore, we utilize the baseline attack (Section 4) to quantify privacy. We note that longer prompts did not add value in a defense setting, so we resort to using a prompt of length 1. We utilize perplexity (PPL) on generated suffixes, to quantify the utility of the model in addition to using exact extraction rate as in Section 3.1. To measure PPL, we use a random subset of 1k sequences sampled from the test split of the Pile, ensuring that PPL is measured on data unseen by the model. We also compare our metrics with those of similar sized models that were not trained on the Pile dataset (GPT2 models). Our premise here is that better performance in terms of privacy and utility, when compared to an out-ofdomain model of similar size, would mean that our defense mechanism is of value to an API owner.\nIn Table 1 , we display our results obtained using the default evaluation setting (prefix and suffix comprise of 50 tokens). Our defense achieves lower extraction rates with competitive PPL values. For the 125M model, we achieve an exact extraction rate reduction of 99.4% relative to baseline with a PPL increase of 25.3% at \u03b8 = 1.75. For the 1.3B model, the extraction rate is reduced by 97.7% relative to baseline with a PPL increase of 16.9% at \u03b8 = 1. The ability to achieve lower extraction rates with lower PPL values as measured against the GPT2 models of the corresponding size, provides evidence that our defense is effective.\n\nConclusion\nWe present the first known effort to leverage prompt-tuning to control the extractability of memorized data from LLMs in an open language generation task. We develop a novel data extraction attack and defense, and illustrate their performance under various settings. Our attack consistently outperforms the baseline in terms of exact extraction rate. Our defense provides competitive privacy-utility trade-offs and would prove beneficial to API owners with model trained on sensitive content. These results are achieved efficiently, without any change to the original model weights. We details avenues of future work in Appendix C\n"}
{"question": "Which problem that the article clearly points out in the existing model  does S3HQA solved?", "evidence": "  Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S 3 HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem.  ", "options": ["A. It eliminates the need for a retriever module.", "B. It relies solely on reading comprehension for answers.", "C. It improves the utilization of heterogeneous information.", "D. It avoids the noisy problem."], "answer": "D", "content": "\nIntroduction\nQuestion answering systems devote to answering various questions with the evidence located in the structured knowledge base (e.g., table) (Pasupat and Liang, 2015; Yu et al., 2018) or unstructured texts (Rajpurkar et al., 2016) . Considering that many questions need to utilize multiple sources of knowledge jointly in real-world applications, the hybrid form of question answering over texts and tables (TextTableQA) has been proposed and attracted more and more attention (Chen et al., Walnut Q1: Who is the athlete in a city located on the Mississippi River? A1: Philip Mulkey Q2: In which year did Walnut-born athletes participate in the Rome Olympics? A2: 1960 Q3: Who is the higher scoring athlete from the cities of Eugene and Walnut? Comparison A3: Rafer Johnson 2020b,a; Zhu et al., 2021; Chen et al., 2021; Zhao et al., 2022; Wang et al., 2022a) . Fact reasoning (Chen et al., 2020a,b) is a critical question type of TextTableQA. It requires jointly using multiple evidence from tables and texts to reasoning the answers with different operations, such as correlation (e.g., multi-hop) and aggregation (e.g., comparison) . Hyperlinks among some table cells and linked passages are essential resources to establish their relationship and support the retrieval and reasoning for multi-hop questions. As shown in Figure 1 , answering a complex question Q1 requires jointly reasoning from textual evidence (P1) to table evidence ([R2, Place] ) and then to other table evidence ([R2, Athlete]).\nExisting methods consist of two main stages: retriever and reader (Chen et al., 2020b; Feng et al., 2022) . The retriever filters out the cells and passages with high relevance to the question, and then the reader extracts a span from the retrieval results as the final answer. However, current methods with two stages still have three limitations as follows.\n1) Noisy labeling for training retriever. Existing retrieval methods usually ignore the weakly supervised answer annotation (Chen et al., 2020b; Wang et al., 2022b; Feng et al., 2022) . For the Q2 of Figure 1 , we cannot know the specific location of the hybrid evidence, only given the final answer \"1960\". Therefore, there is a lot of pseudo-true evidence labeled (Marked in green) automatically by string matching, which introduces a lot of evidence noise.\n2) Insufficient utilization of heterogeneous information. After retrieval, existing methods selected a particular cell or passage for reading to extract the final answer (Chen et al., 2020b; Wang et al., 2022b) . As for Q1 in Figure 1 , previous models were more likely to choose P1 or the coordinates [R2, Place] to extract the answer. However, these methods seldomly used the hybrid information of table schema and cell-passage hyperlinks, which is the key factor in answering multi-hop questions.\n3) Deficient ability for different reasoning operations. Previous methods (Eisenschlos et al., 2021; Kumar et al., 2021; Wang et al., 2022b) mainly used an extraction module to obtain answers, which cannot support knowledge reasoning that requires comparison, calculation, and other operations.\nIn this paper, we propose a three-stage approach S 3 HQA to solve the above problems. (1) Retriever with Refinement Training, we propose a two-step training method, splitting the training data into two parts, so that the noise in the retrieval phase can be alleviated. (2) Hybrid Selector has been proposed and selects supporting facts with different granularity and resources depending on the question type. By considering the hybrid data of tables and text, this paper proposes a hybrid selection algorithm that can effectively utilize the heterogeneous information of tables and passages. (3) Generationbased reasoner utilizes a generation-based model for addressing different question types. The model allows better aggregation of information on the input side, which not only have better multi-hop reasoning capabilities but also be able to handle comparison and counting questions. Furthermore, we are the first to use the LLM in-context learning approach for table-text hybrid question-answering tasks.\nWe evaluate our proposed model on the challenging TextTableQA benchmark HybridQA. The empirical results show that our approach outperforms all the existing models 2 .\n\nGiven a natural language question\nQ = {q i } |Q| i=1\nand a table T with \u27e8H, R\u27e9, H indicates the table headers, and\nR = {r i } |R| i=1 indicates the rows with number |R|. Each row r i is consists of N cells r i = {c ij } N j=1\n. The header's number is also N . Some cells have a linked passage P ij . Our goal aims to generate the answer A with model \u0398, which is a span from table cells or linked passage or a derivation result of counting questions.\n\nRetriever with Refinement Training\nThe retriever aims to perform initial filtering of heterogeneous resources. However, accurately labeling the location of answers consumes high labeling costs. For TextTableQA data, the answer A usually appears in multiple locations, which makes it difficult for us to generate precise retrieval la-bels. We use a two-step training method, with a row-based retriever and a passage-based retriever for each step.\nInspired by (Kumar et al., 2021) , the retrieval has two steps. First, we divide the data D into two folds according to the string matching labels G i . Specifically, for a question-answer instance, the answer A appears one time as D 1 , and the instance whose answer A appears multiple times as D 2 . Take the example in Figure 1 , Q1, Q3 belongs to D 1 while Q2 belongs to D 2 . The data is organized in the form of\n[CLS]q 1 q 2 ...q |Q| [SEP]c i1 c i2 ...c iN [SEP] or [CLS]q 1 q 2 ...q |Q| [SEP]p ij [SEP].\nIn the first step, we only use D 1 to train a model \u0398 1 , which data are noiseless. Then in the second step, we use the trained weight \u0398 1 to train the model \u0398 2 . For the input x, the loss function is:\nL(\u0398 2 , x, R) = z\u2208R \u2212q(z) log p \u0398 1 (z|x)\nwhere q(z) = p \u0398 1 (z|x, z \u2208 R) is the probability distribution given by the model restricted to candidate rows R containing the answer span, taken here as a constant with zero gradients (Eisenschlos et al., 2021) .\nMeanwhile, we use a passage-based retriever to enhance the performance of a row-based retriever (PassageFilter). Specifically, we use the passage-based retriever to obtain a prediction score of passage relevance. Based on this score, we reorder the input of the row-based retriever. It avoids the limitation on input sequence length imposed by the pre-trained model.\n\nHybrid Selector\nThis module needs to combine the results of the two granularity retrievers. As for this task, we consider the question type and the relationships between the table and linked passages essential. As shown in Figure 2 , the hybrid selector chooses the appropriate data source from the two retrieval results depending on question types.\nSpecifically, for general bridge multi-hop questions, we use a single row and its linked passage. While for comparison/count questions, we consider multiple rows and further filter the related sentences, delete the linked paragraphs with the low scores. This not only enables the generation module to obtain accurate information, but also prevents the introduction of a large amount of unrelated information. The selector algorithm outputs a mixed sequence with high relevance based on the relationship between the question, the table, and the passages. The algorithm is shown in Algorithm 1.\nAlgorithm 1 Hybrid Selector Algorithm.\nInput: question Q, table rows R, linked passages P, rowbased retriever \u0398R, passage-based retriever \u0398P , selector target row count NS Output: generator input S Get the row/passage ordered list by relevant scores\n1: OR \u2190 sort(\u0398R(Q, R)) 2: OP \u2190 sort(\u0398P (Q, P)) 3: p type \u2190 Classif ication(Q) 4: if p type = bridge then 5: if OP [0] in OR[0] then 6: S \u2190 Q + OR[0] 7: else 8: S \u2190 Q + OR[0] + OP [0] 9:\nend if 10: else 11:\nOPC \u2190 P[len(OP )//2 :] 12:\nS \u2190 Q + OR[0 : NS] \u2212 OPC 13: end if 14: return S\n\nGeneration-based Reasoner\nThe results of the selector take into account both two granularity. Unlike the previous approaches, which were based on a span extraction module, we use a generation-based model for answer prediction.\n\nRow-wise generator\nTo generate an accurate answer string A = (a 1 , a 2 , ..., a n ) given the question Q and selection evidence S, we perform lexical analysis to identify the question type, such as counting or comparison, by looking for certain keywords or comparative adjectives. We utilize two special tags \u27e8Count\u27e9 and \u27e8Compare\u27e9, which indicates the question types.\nWe then use the results of the passage retriever to rank the passages in order of their relevance, eliminating the impact of model input length limitations. Finally, we train a Seq2Seq language model with parameters \u0398, using the input sequence Q, S and the previous outputs a <i to optimize the product of the probabilities of the output sequence a 1 , a 2 , ..., a n :\nA = argmax n i=1 P (a i |a <i , Q, S; \u0398)\n\nLLM prompting generator\nWith the emergence of large language models, In-Context Learning (Dong et al., 2022) and Chain-of-Thought prompting (Wei et al., 2022) have become two particularly popular research topics in this field.\nIn this paper, we introduce a prompting strategy for multi-hop TextTableQA.\nWe utilize selection evidence S and apply LLMbased prompting. We conducted experiments on both vanilla prompting and chain-of-thought prompting in zero-shot and few-shot scenarios.\n\nExperiment Setup\nDatasets We conduct experiments on Hy-bridQA (Chen et al., 2020b) . The detailed statistics are shown in Appendix A. For evaluation, we followed the official evaluation to report exact match accuracy and F1 score. Implementation details The implementation details are shown in Appendix B. The experimental results are the average of five times results.\n\nFully-supervised Results\nTable 1 shows the comparison results between our models with previous typical approaches on both development and test sets. It shows that our proposed S 3 HQA works significantly better than the baselines in terms of EM and F1 on HybridQA. The results indicate that S 3 HQA is an effective model for multi-hop question answering over tabular and textual data. Specifically, it can effectively handle multi-hop reasoning and make full use of heterogeneous information.\nHowever, we found that our approach was outperformed by the DEHG model (Feng et al., 2022) in terms of F1 score on the Dev set. We speculate that this might be because the DEHG approach uses their own Open Information Extraction (OIE) tool.\n\nModel\nDev EM F1 Zero-shot prompt GPT3.5 direct 33.1 50.5 GPT3.5 CoT 52.9 66.6 Few-shot prompt (2-shot) GPT3.5 direct 57.1 68.8 GPT3.5 CoT 60.3 72.1 \n\nLLM-prompting Results\nWe present our zero-shot and few-shot results in Table 2 . \"Direct\" refers to a simple prompting method where only the question, context, and answer are provided to the model without any additional reasoning process. In contrast, \"CoT\" involves a human-authored Chain-of-Thought reasoning process that provides a more structured and logical way of prompting the model. The experiments demonstrate that in-context learning used to prompt large language models can achieve promising results. Specifically, utilizing the Chain-of-Thought prompt method can significantly enhance the model's performance.\nHowever, it's worth noting that there is still a performance gap compared to fine-tuning the model on the full dataset (Table 1 ). Fine-tuning allows the model to learn more specific information about the TextTableQA task, resulting in better performance. Nevertheless, our results show that the LLM-prompting method can be a useful alternative to fine-tuning, especially when there is a limited amount of labeled data available.\n\nAblation Studies\nWe conduct ablation studies on the test set. We validate the effects of three modules: retriever with refinement training, hybrid selector, and generation-based reasoner. The retriever performs initial filtering of heterogeneous resources; Selectors combined with hyperlinks further identify the exact evidence needed to answer multi-hop questions; and the reasoner uses the selection evidence to obtain the final answer. Effect of proposed retriever. As shown in the Table 3 , under the setting of using the BERTbase-uncased model, sing the BERT-base-uncased model setting, the retriever with refinement training achieved 87.2. When we use Deberta-base, the top1 retrieval performance improved by 0.8%. For w/o refinement training, we use the entire data directly for training, the top1 recall drops about 3.2%. For w/o PassageFilter, we remove the mechanism, the top1 recall drops about 3.2%. For Vanilla-Retriever, we use the row-based retriever (Kumar et al., 2021) and remove all our mechanisms, the top1 score drops about 5.3%. This shows that our model can solve the weakly supervised data noise problem well.\n\nModel\nEffect of hybrid selector. As shown in the Table 4, we removed the selector of S 3 HQA and replaced it with the previous cell-based selector (Wang et al., 2022b) . This method directly uses the top1 result of the row retriever as input to the generator. w/o hybrid selector shows that the EM drops 2.9% and F1 drops 1.6%, which proves the effectiveness of our selector approach.\nEffect of reasoner. As shown in the Table 4 , we design two baselines. BERT-large reader (Chen et al., 2020b; Wang et al., 2022b) uses BERT (Devlin et al., 2018) as encoder and solves this task by predicting the start/end tokens. w/o special tags deletes the special tags. Both the two experiments demonstrate our S 3 HQA reasoner performs the best for HybridQA task.\n\nRelated Work\nThe TextTableQA task (Wang et al., 2022a) has attracted more and more attention. As for multi-hop type dataset, previous work used pipeline approach (Chen et al., 2020b) , unsupervised approach (Pan et al., 2021) , multigranularity (Wang et al., 2022b) , table pre-trained language model (Eisenschlos et al., 2021) , multiinstance learning (Kumar et al., 2021) and graph neural network (Feng et al., 2022) to solve this task. As for numerical reasoning task, which is quite different from multi-hop type dataset, there is also a lot of work (Zhu et al., 2021; Zhao et al., 2022; Zhou et al., 2022; Lei et al., 2022; Li et al., 2022; Wei et al., 2023) to look at these types of questions. Unlike these methods, our proposed three-stage model S 3 HQA can alleviate noises from weakly supervised and solve different types of multi-hop TextTableQA questions by handling the relationship between tables and text.\n\nConclusion\nThis paper proposes a three-stage model consisting of retriever, selector, and reasoner, which can effectively address multi-hop TextTableQA. The proposed method solves three drawbacks of the previous methods: noisy labeling for training retriever, insufficient utilization of heterogeneous information, and deficient ability for reasoning. It achieves new state-of-the-art performance on the widely used benchmark HybridQA. In future work, we will design more interpretable TextTableQA models to predict the explicit reasoning path.\n"}
{"question": "What problem do conditional language models face, as mentioned in the paper?", "evidence": "  This raises a need for automatic faithfulness metrics.  ", "options": ["A. They generate unfaithful output that is supported by their input.", "B. They generate faithful output consistently.", "C. They require additional machinery for inference.", "D. They have too many parameters. ", "Conditional language models suffer from a tendency to hallucinate information, resulting in generations that are not faithful to their input documents."], "answer": "A", "content": "\nIntroduction\nConditional language models suffer from a tendency to hallucinate information (Maynez et al., 2020) , resulting in generations that are not faithful to their input documents, which limits the trustworthiness of such models. This raises a need for automatic faithfulness metrics. In this context, models trained on natural language inference (NLI) (Bowman et al., 2015) are attractive since, intuitively, a generation being faithful implies it must be entailed by the source (Falke et al., 2019) . However, pure NLI models have seen mixed success in faithfulness evaluation (Falke et al., 2019; Kryscinski et al., 2020; Wang et al., 2020; Maynez et al., 2020) . While in recent evaluation on the TRUE benchmark (Honovich et al., 2022) , which contains datasets from knowledge-grounded dialogue, summarization and paraphrasing, NLIderived metrics perform best overall, they require impractically large models, or costly additional machinery such as question generation and answering models at inference, while still showing robustness issues. Thus we ask: What is still needed for pure NLI models to perform robustly across faithfulness datasets -while remaining cheap enough to serve as a lean and practical evaluation tool?\nWe enhance a relatively small NLI model to make it work robustly across tasks in three ways:\nTask-Adaptive Data Augmentation. In NLI, a hypothesis must be fully entailed by its supporting premise. However, in faithfulness, not all parts of the generation always need to be grounded. We identify an instance of this phenomenon in dialogue where parts of a turn can fulfill communicative functions such as hedging or establishing emotional connection and are often disregarded in faithfulness annotation. Hence, when applying NLI models to complete dialogue turns that may include statements irrelevant for grounding, we run a risk of producing incorrect unfaithfulness predictions.\nTo alleviate this issue, we propose a simple data augmentation method to adapt NLI models to genres where they need to be aware of statements that must be exempt from NLI-based faithfulness evaluation. Our approach is computationally attractive, as it avoids an increase of cost at inference time.\nIntegration of NLI Contradiction Scores. Existing NLI faithfulness metrics typically use the entailment score for their predictions (Honovich et al., 2022; Falke et al., 2019; Kryscinski et al., 2020) . However, Chen and Eger (2022) show that subtracting the contradiction score from the entail-ment score (referred to as e-c ) can improve NLI performance in certain evaluation tasks. We show that there also is a strong positive effect of e-c for faithfulness prediction, and demonstrate that this is due to a high contradiction probability being a more reliable predictor of unfaithfulness than low entailment probability.\nMonte-Carlo Dropout Inference. Applying NLI models to faithfulness prediction involves a domain shift from largely human-written data to automatically generated text. To make NLI model scores more robust under this shift, we propose to use Monte-Carlo dropout during inference (Srivastava et al., 2014) . This essentially creates a cheap ensemble and has been shown to deal better with noisy labels (Goel and Chen, 2021) . This approach leads to consistent score improvements in our tasks.\nThe combination of all modifications not only strongly improves over a baseline NLI model, but also outperforms all other metrics on TRUE, on average, while being cheaper and smaller. 1 2 Method Details\n\nTask-adaptive Data Augmentation\nTo illustrate that task requirements can be incompatible between faithfulness and NLI, consider the following instance from the Q2 dialogue corpus (Honovich et al., 2021) that is labelled as faithful:\nGrounding: American pancakes are similar to Scotch pancakes or drop scones. Generation: yes , i love american pancakes , they are like scotch pancakes From an NLI perspective, the generation is clearly not entailed, since the statement \"I love american pancakes\" is not supported by the input.\nTo better prepare an NLI system for such genre or task-specific cases, we manually curate a small list of statements that should not influence the faithfulness prediction. We augment NLI data from the ANLI corpus (Nie et al., 2020) by adding a randomly chosen phrase from this set to each instance, while preserving the label. We then train an already fine-tuned NLI model on a concatenation of these augmented samples and original ANLI data. For training details see Appendix A.\n1 All code is available at https://github.com/julmaxi/ with_a_little_push\n\nMonte-Carlo Dropout\nTo compute scores under Monte-Carlo dropout, we randomly sample k dropout masks and compute the average of the model predictions. We set k = 15, since preliminary experiments showed that performance did not profit from additional samples.\n\nExperimental Setup\nWe run experiments on TRUE (Honovich et al., 2022) , a benchmark that compiles a wide variety of faithfulness tasks in a standardized format. It contains summarization (Pagnoni et al., 2021; Maynez et al., 2020; Wang et al., 2020; Fabbri et al., 2021) , knowledge-grounded dialog (Honovich et al., 2021; Gupta et al., 2022; Dziri et al., 2022) 2 and paraphrasing (Zhang et al., 2019) datasets. 3 Following recommendations in TRUE, we evaluate using Area under the ROC Curve (AUC).\nAs our BASE model, we use the DeBERTa-large (He et al., 2020) model of Laurer et al. (2022) , trained on MultiNLI (Williams et al., 2018) , Fever-NLI (Thorne et al., 2018) , ANLI (Nie et al., 2020) , LingNLI (Parrish et al., 2021) and WANLI (Liu et al., 2022) . The metric All uses all three of our proposed modifications to Base. We also investigate a variant without MC dropout inference (-MC) as a more cost efficient alternative.\nWe compare to the strongest models on TRUE: T5 ANLI (Honovich et al., 2022 ) is a T5-11B (Raffel et al., 2020) model trained on ANLI. 4 SummacZS (Laban et al., 2022 ) evaluates an NLI model on all pairs of input and generated sentences and then averages maximum entailment probabilities for each generated sentence.\nQ2 (Honovich et al., 2021) combines a question generation/answering pipeline with an NLI score.\nFinally, Honovich et al. (2022) introduce a strong ensemble of these 3 methods (Eorig). To further verify our approach, we construct a new ensemble (Eour) by replacing T5 with All.\n\nResults\nTable 1 shows the AUC scores for each metric. Base on six out of nine corpora, but also significantly outperforms all other competitors on average, while being more computationally efficient.\nAs expected, we find the biggest gains in dialogue, where the All model even outperforms Eorig on 2 out of 3 corpora. We do not improve on BEGIN, which is likely due to bias in the dataset construction, which we elaborate on in Section 5.1. On the summarization part, All improves significantly over Base on 3 out of 5 corpora, while not significantly harming performance on any corpus. However, it still falls short of the best models in TRUE. The strong showing of T5 on these corpora suggests that this might be alleviated with a stronger base model.\nOverall, a very similar behaviour is exhibited by -MC, presenting an attractive option when the added overhead of multiple samples is undesirable.\nEour is on par with Eorig, despite massively reduced costs; it even significantly outperforms it on two dialog and the paraphrasing corpora.\nWe also investigate the performance of each individual modification to our model (Table 2 ). They all improve average scores, while only leading to a notable decrease on BEGIN for both e-c and dialogue augmentations and on MNBM for e-c .\nOutside of dialogue, we find that the augmentation methods have a positive impact on PAWS, as well as all summarization corpora that are at least partially based on summaries for the CNN/DM dataset (Hermann et al., 2015) (Frank, QAGS-C, and SummEval). While we do not have a definitive explanation for this phenomenon, we hypothesize that on these datasets our augmentations aid in making the model robust in the presence of noise or irrelevant context since our augmentations are label-neutral and must similarly be 'ignored' during training.\n\nEffect of Dialogue Adaptation\nWe investigate whether the improvements via our augmentation approach are indeed due to them improving the handling of personal statements.\nWe use the occurrences of the pronoun I in a generation as a proxy measure 5 and compute its correlation with human labels and metrics (see Table 3 ). On both Q2 and Dialfact, our proxy measure, while uncorrelated with human labels, is strongly correlated with the scores of both Base and T5. This indicates these metrics indeed tend to incorrectly reject generations with personal statements. All on the other hand reduces this dependency.\nOur results also help explain why negatively correlated with first person pronouns. This is likely due to a bias in dataset construction:\nThe BEGIN dataset used in TRUE has generations from two models, one of which is both more likely to generate pronouns and more likely to generate unfaithful output (see Appendix B).\n\nEffect of integrating contradiction scores\nTo isolate the effect of e-c we compare score distributions of Base and Base+e-c in Figure 1 . The lefthand side of the figure shows that in Base ca. 2700 faithful instances are predicted as non-entailed (i.e., e-score near 0), which implies they are labelled as contradictory or neutral. e-c , on the other hand, further differentiates these instances into instances with high contradiction (negative e-c score) and high neutral probability (e-c score near 0). We observe that almost all low-scoring faithful generations are classified as neutral, whereas nearly all instances that are classified as contradictory are indeed unfaithful. Where Base has no way to make use of this information, e-c allows to reliably label contradictory instances as unfaithful.\n\nCost comparison to other approaches\nThere is increasing awareness of the resource-hungriness of deep learning (Strubell et al., 2019) . Especially for faithfulness, cheap and reliable metrics are critical, given rising demands for NLG in research and industry. Table 5 : Results of our phrase selection robustness analysis. For each run, we sample five phrases, recreated our dataset and retrain our model. We repeat this process ten times and report the average, as well as the standard deviation, minimum and maximum scores of the runs.\nSmall numbers indicate difference to the original scores. All results were computed using e-c and MC dropout.\nFor better comparison, we also report the scores of a model without any augmentation (i.e. without any additional training) with e-c and MC dropout.\nrequires fewer parameters than any other metric, including a more than 30x reduction compared to T5. During inference our model always requires a constant number of calls which can be reduced to a single call when ablating MC dropout. On the other hand, the number of calls in SummacZS scales with the number of input and output sentences. Q2 needs to generate questions by calling an auto-regressive QG model n times, where n factors in the amount and length of questions (#Q\u00d7Ql), answer #Q questions with the QA model and finally check #Q answers with an NLI model (#Q \u00d7 2).\nIn sum, our model compares favourably with other approaches, while also allowing for a performance/cost tradeoff by forgoing MC dropout.\n\nPhrase Selection Robustness\nTo ensure that our augmentation is robust and not overly reliant on any particular choice of phrases, we repeat our dataset augmentation process multiple times with five randomly chosen augmentation phrases out of the original ten. We sample ten such datasets and retrain our model for each. Table 5 shows the average score, minimum and maxi-mum score, as well as the standard deviation of the scores. We also report results of a model with both MC dropout and e-c but without any additional training and augmentations to directly quantify whether the augmentations are still helpful in their reduced form. This corresponds to applying MC dropout and e-c to Base.\nAs expected, we find that reducing the variety of available phrases leads to a drop in performance across almost all datasets, compared to All. The only exception is BEGIN, where we instead see a slight improvement. This is likely to be related to the construction of BEGIN (see the discussion in Section 5.1).\nWhen comparing our limited augmentation models to the non-augmented model, we find that they still outperform the non-augmented model in almost all cases. In particular for Q2 and DialFact, for which we expect the strongest impact of our augmentations, we find that even the worst run still outperforms non-augmented model. This suggests that our augmentations can robustly adapt the model to the dialogue task.\nFinally, we observe a relatively large drop in scores for all datasets that are at (least partially) derived from CNN/DM (Frank, SummEval and QAGS-C). This mirrors our earlier observation in Section 4 that these datasets profit from our augmentation procedure.\n\nRelated Work\nPrevious work on the utility of NLI for faithfulness led to mixed conclusions. In summarization, Falke et al. (2019) and Kryscinski et al. (2020) find out-of-the-box models have only limited utility in a faithfulness setting. In Wang et al. (2020) , an NLI model is outperformed by a question generation/answering (QA/QG)-based method. In contrast, Maynez et al. (2020) find that a similar NLI model vastly outperforms a QA/QG metric on their data. In knowledge-grounded dialogue, Dziri et al. (2022) , Gupta et al. (2022) and Honovich et al. (2021) find out-of-the-box models underperform.\nTo improve NLI models for faithfulness in summarization, Kryscinski et al. (2020) propose FactCC, which is trained on artificially noised summaries. Utama et al. (2022) propose a controllable generation model to generate artificial faithfulness data. In knowledge-grounded dialogue, Dziri et al. (2022) and Gupta et al. (2022) combine noising techniques to generate additional training data for NLI-based faithfulness models. In contrast to our work, these approaches a) generate training data from external sources, instead of directly augmenting NLI data, and b) do not explicitly focus on reconciling differences between NLI and faithfulness with their augmentation. Outside of augmentationbased approaches, Goyal and Durrett (2020) propose to train NLI models to label faithfulness at the dependency arc level.\n\nConclusion\nWe have demonstrated that with a small number of focused adaptations, even a relatively small NLI model can robustly predict faithfulness. We have:\n1. Shown that NLI-based metrics can be incompatible with task-specific requirements and identified and fixed one such incompatibility in dialogue with an augmentation strategy.\n2. Demonstrated the importance of contradiction probability for scoring and that the underlying mechanism is the high reliability of NLI contradiction scores for detecting unfaithfulness 3. Shown that using Monte-Carlo dropout improves metric performance.\nOur improved NLI model significantly improves over its baseline across many corpora and outperforms all competitors in average score on TRUE, while being much more efficient at inference. Our work suggests that strong improvements are possible for NLI-based faithfulness metrics, by combining data augmentation with adapted NLI score computation. We hope this finding will spurn advances in cheap and robust NLI for faithfulness. unclear whether our results would still provide advantages when applied to larger models such as T5-11B, whose parameter count makes experimentation infeasible on the hardware available to us.\n"}
{"question": "What is the primary purpose of using a two-step training method for the retriever in TextTableQA?", "evidence": "  For TextTableQA data, the answer A usually appears in multiple locations, which makes it difficult for us to generate precise retrieval labels.  We use a two-step training method, with a row-based retriever and a passage-based retriever for each step.  ", "options": ["A. To reduce labeling costs for answer locations.", "B. To improve the accuracy of passage-based retriever.", "C. To minimize the use of passage-based retriever.", "D. To generate precise retrieval labels for every question."], "answer": "D", "content": "\nIntroduction\nQuestion answering systems devote to answering various questions with the evidence located in the structured knowledge base (e.g., table) (Pasupat and Liang, 2015; Yu et al., 2018) or unstructured texts (Rajpurkar et al., 2016) . Considering that many questions need to utilize multiple sources of knowledge jointly in real-world applications, the hybrid form of question answering over texts and tables (TextTableQA) has been proposed and attracted more and more attention (Chen et al., Walnut Q1: Who is the athlete in a city located on the Mississippi River? A1: Philip Mulkey Q2: In which year did Walnut-born athletes participate in the Rome Olympics? A2: 1960 Q3: Who is the higher scoring athlete from the cities of Eugene and Walnut? Comparison A3: Rafer Johnson 2020b,a; Zhu et al., 2021; Chen et al., 2021; Zhao et al., 2022; Wang et al., 2022a) . Fact reasoning (Chen et al., 2020a,b) is a critical question type of TextTableQA. It requires jointly using multiple evidence from tables and texts to reasoning the answers with different operations, such as correlation (e.g., multi-hop) and aggregation (e.g., comparison) . Hyperlinks among some table cells and linked passages are essential resources to establish their relationship and support the retrieval and reasoning for multi-hop questions. As shown in Figure 1 , answering a complex question Q1 requires jointly reasoning from textual evidence (P1) to table evidence ([R2, Place] ) and then to other table evidence ([R2, Athlete]).\nExisting methods consist of two main stages: retriever and reader (Chen et al., 2020b; Feng et al., 2022) . The retriever filters out the cells and passages with high relevance to the question, and then the reader extracts a span from the retrieval results as the final answer. However, current methods with two stages still have three limitations as follows.\n1) Noisy labeling for training retriever. Existing retrieval methods usually ignore the weakly supervised answer annotation (Chen et al., 2020b; Wang et al., 2022b; Feng et al., 2022) . For the Q2 of Figure 1 , we cannot know the specific location of the hybrid evidence, only given the final answer \"1960\". Therefore, there is a lot of pseudo-true evidence labeled (Marked in green) automatically by string matching, which introduces a lot of evidence noise.\n2) Insufficient utilization of heterogeneous information. After retrieval, existing methods selected a particular cell or passage for reading to extract the final answer (Chen et al., 2020b; Wang et al., 2022b) . As for Q1 in Figure 1 , previous models were more likely to choose P1 or the coordinates [R2, Place] to extract the answer. However, these methods seldomly used the hybrid information of table schema and cell-passage hyperlinks, which is the key factor in answering multi-hop questions.\n3) Deficient ability for different reasoning operations. Previous methods (Eisenschlos et al., 2021; Kumar et al., 2021; Wang et al., 2022b) mainly used an extraction module to obtain answers, which cannot support knowledge reasoning that requires comparison, calculation, and other operations.\nIn this paper, we propose a three-stage approach S 3 HQA to solve the above problems. (1) Retriever with Refinement Training, we propose a two-step training method, splitting the training data into two parts, so that the noise in the retrieval phase can be alleviated. (2) Hybrid Selector has been proposed and selects supporting facts with different granularity and resources depending on the question type. By considering the hybrid data of tables and text, this paper proposes a hybrid selection algorithm that can effectively utilize the heterogeneous information of tables and passages. (3) Generationbased reasoner utilizes a generation-based model for addressing different question types. The model allows better aggregation of information on the input side, which not only have better multi-hop reasoning capabilities but also be able to handle comparison and counting questions. Furthermore, we are the first to use the LLM in-context learning approach for table-text hybrid question-answering tasks.\nWe evaluate our proposed model on the challenging TextTableQA benchmark HybridQA. The empirical results show that our approach outperforms all the existing models 2 .\n\nGiven a natural language question\nQ = {q i } |Q| i=1\nand a table T with \u27e8H, R\u27e9, H indicates the table headers, and\nR = {r i } |R| i=1 indicates the rows with number |R|. Each row r i is consists of N cells r i = {c ij } N j=1\n. The header's number is also N . Some cells have a linked passage P ij . Our goal aims to generate the answer A with model \u0398, which is a span from table cells or linked passage or a derivation result of counting questions.\n\nRetriever with Refinement Training\nThe retriever aims to perform initial filtering of heterogeneous resources. However, accurately labeling the location of answers consumes high labeling costs. For TextTableQA data, the answer A usually appears in multiple locations, which makes it difficult for us to generate precise retrieval la-bels. We use a two-step training method, with a row-based retriever and a passage-based retriever for each step.\nInspired by (Kumar et al., 2021) , the retrieval has two steps. First, we divide the data D into two folds according to the string matching labels G i . Specifically, for a question-answer instance, the answer A appears one time as D 1 , and the instance whose answer A appears multiple times as D 2 . Take the example in Figure 1 , Q1, Q3 belongs to D 1 while Q2 belongs to D 2 . The data is organized in the form of\n[CLS]q 1 q 2 ...q |Q| [SEP]c i1 c i2 ...c iN [SEP] or [CLS]q 1 q 2 ...q |Q| [SEP]p ij [SEP].\nIn the first step, we only use D 1 to train a model \u0398 1 , which data are noiseless. Then in the second step, we use the trained weight \u0398 1 to train the model \u0398 2 . For the input x, the loss function is:\nL(\u0398 2 , x, R) = z\u2208R \u2212q(z) log p \u0398 1 (z|x)\nwhere q(z) = p \u0398 1 (z|x, z \u2208 R) is the probability distribution given by the model restricted to candidate rows R containing the answer span, taken here as a constant with zero gradients (Eisenschlos et al., 2021) .\nMeanwhile, we use a passage-based retriever to enhance the performance of a row-based retriever (PassageFilter). Specifically, we use the passage-based retriever to obtain a prediction score of passage relevance. Based on this score, we reorder the input of the row-based retriever. It avoids the limitation on input sequence length imposed by the pre-trained model.\n\nHybrid Selector\nThis module needs to combine the results of the two granularity retrievers. As for this task, we consider the question type and the relationships between the table and linked passages essential. As shown in Figure 2 , the hybrid selector chooses the appropriate data source from the two retrieval results depending on question types.\nSpecifically, for general bridge multi-hop questions, we use a single row and its linked passage. While for comparison/count questions, we consider multiple rows and further filter the related sentences, delete the linked paragraphs with the low scores. This not only enables the generation module to obtain accurate information, but also prevents the introduction of a large amount of unrelated information. The selector algorithm outputs a mixed sequence with high relevance based on the relationship between the question, the table, and the passages. The algorithm is shown in Algorithm 1.\nAlgorithm 1 Hybrid Selector Algorithm.\nInput: question Q, table rows R, linked passages P, rowbased retriever \u0398R, passage-based retriever \u0398P , selector target row count NS Output: generator input S Get the row/passage ordered list by relevant scores\n1: OR \u2190 sort(\u0398R(Q, R)) 2: OP \u2190 sort(\u0398P (Q, P)) 3: p type \u2190 Classif ication(Q) 4: if p type = bridge then 5: if OP [0] in OR[0] then 6: S \u2190 Q + OR[0] 7: else 8: S \u2190 Q + OR[0] + OP [0] 9:\nend if 10: else 11:\nOPC \u2190 P[len(OP )//2 :] 12:\nS \u2190 Q + OR[0 : NS] \u2212 OPC 13: end if 14: return S\n\nGeneration-based Reasoner\nThe results of the selector take into account both two granularity. Unlike the previous approaches, which were based on a span extraction module, we use a generation-based model for answer prediction.\n\nRow-wise generator\nTo generate an accurate answer string A = (a 1 , a 2 , ..., a n ) given the question Q and selection evidence S, we perform lexical analysis to identify the question type, such as counting or comparison, by looking for certain keywords or comparative adjectives. We utilize two special tags \u27e8Count\u27e9 and \u27e8Compare\u27e9, which indicates the question types.\nWe then use the results of the passage retriever to rank the passages in order of their relevance, eliminating the impact of model input length limitations. Finally, we train a Seq2Seq language model with parameters \u0398, using the input sequence Q, S and the previous outputs a <i to optimize the product of the probabilities of the output sequence a 1 , a 2 , ..., a n :\nA = argmax n i=1 P (a i |a <i , Q, S; \u0398)\n\nLLM prompting generator\nWith the emergence of large language models, In-Context Learning (Dong et al., 2022) and Chain-of-Thought prompting (Wei et al., 2022) have become two particularly popular research topics in this field.\nIn this paper, we introduce a prompting strategy for multi-hop TextTableQA.\nWe utilize selection evidence S and apply LLMbased prompting. We conducted experiments on both vanilla prompting and chain-of-thought prompting in zero-shot and few-shot scenarios.\n\nExperiment Setup\nDatasets We conduct experiments on Hy-bridQA (Chen et al., 2020b) . The detailed statistics are shown in Appendix A. For evaluation, we followed the official evaluation to report exact match accuracy and F1 score. Implementation details The implementation details are shown in Appendix B. The experimental results are the average of five times results.\n\nFully-supervised Results\nTable 1 shows the comparison results between our models with previous typical approaches on both development and test sets. It shows that our proposed S 3 HQA works significantly better than the baselines in terms of EM and F1 on HybridQA. The results indicate that S 3 HQA is an effective model for multi-hop question answering over tabular and textual data. Specifically, it can effectively handle multi-hop reasoning and make full use of heterogeneous information.\nHowever, we found that our approach was outperformed by the DEHG model (Feng et al., 2022) in terms of F1 score on the Dev set. We speculate that this might be because the DEHG approach uses their own Open Information Extraction (OIE) tool.\n\nModel\nDev EM F1 Zero-shot prompt GPT3.5 direct 33.1 50.5 GPT3.5 CoT 52.9 66.6 Few-shot prompt (2-shot) GPT3.5 direct 57.1 68.8 GPT3.5 CoT 60.3 72.1 \n\nLLM-prompting Results\nWe present our zero-shot and few-shot results in Table 2 . \"Direct\" refers to a simple prompting method where only the question, context, and answer are provided to the model without any additional reasoning process. In contrast, \"CoT\" involves a human-authored Chain-of-Thought reasoning process that provides a more structured and logical way of prompting the model. The experiments demonstrate that in-context learning used to prompt large language models can achieve promising results. Specifically, utilizing the Chain-of-Thought prompt method can significantly enhance the model's performance.\nHowever, it's worth noting that there is still a performance gap compared to fine-tuning the model on the full dataset (Table 1 ). Fine-tuning allows the model to learn more specific information about the TextTableQA task, resulting in better performance. Nevertheless, our results show that the LLM-prompting method can be a useful alternative to fine-tuning, especially when there is a limited amount of labeled data available.\n\nAblation Studies\nWe conduct ablation studies on the test set. We validate the effects of three modules: retriever with refinement training, hybrid selector, and generation-based reasoner. The retriever performs initial filtering of heterogeneous resources; Selectors combined with hyperlinks further identify the exact evidence needed to answer multi-hop questions; and the reasoner uses the selection evidence to obtain the final answer. Effect of proposed retriever. As shown in the Table 3 , under the setting of using the BERTbase-uncased model, sing the BERT-base-uncased model setting, the retriever with refinement training achieved 87.2. When we use Deberta-base, the top1 retrieval performance improved by 0.8%. For w/o refinement training, we use the entire data directly for training, the top1 recall drops about 3.2%. For w/o PassageFilter, we remove the mechanism, the top1 recall drops about 3.2%. For Vanilla-Retriever, we use the row-based retriever (Kumar et al., 2021) and remove all our mechanisms, the top1 score drops about 5.3%. This shows that our model can solve the weakly supervised data noise problem well.\n\nModel\nEffect of hybrid selector. As shown in the Table 4, we removed the selector of S 3 HQA and replaced it with the previous cell-based selector (Wang et al., 2022b) . This method directly uses the top1 result of the row retriever as input to the generator. w/o hybrid selector shows that the EM drops 2.9% and F1 drops 1.6%, which proves the effectiveness of our selector approach.\nEffect of reasoner. As shown in the Table 4 , we design two baselines. BERT-large reader (Chen et al., 2020b; Wang et al., 2022b) uses BERT (Devlin et al., 2018) as encoder and solves this task by predicting the start/end tokens. w/o special tags deletes the special tags. Both the two experiments demonstrate our S 3 HQA reasoner performs the best for HybridQA task.\n\nRelated Work\nThe TextTableQA task (Wang et al., 2022a) has attracted more and more attention. As for multi-hop type dataset, previous work used pipeline approach (Chen et al., 2020b) , unsupervised approach (Pan et al., 2021) , multigranularity (Wang et al., 2022b) , table pre-trained language model (Eisenschlos et al., 2021) , multiinstance learning (Kumar et al., 2021) and graph neural network (Feng et al., 2022) to solve this task. As for numerical reasoning task, which is quite different from multi-hop type dataset, there is also a lot of work (Zhu et al., 2021; Zhao et al., 2022; Zhou et al., 2022; Lei et al., 2022; Li et al., 2022; Wei et al., 2023) to look at these types of questions. Unlike these methods, our proposed three-stage model S 3 HQA can alleviate noises from weakly supervised and solve different types of multi-hop TextTableQA questions by handling the relationship between tables and text.\n\nConclusion\nThis paper proposes a three-stage model consisting of retriever, selector, and reasoner, which can effectively address multi-hop TextTableQA. The proposed method solves three drawbacks of the previous methods: noisy labeling for training retriever, insufficient utilization of heterogeneous information, and deficient ability for reasoning. It achieves new state-of-the-art performance on the widely used benchmark HybridQA. In future work, we will design more interpretable TextTableQA models to predict the explicit reasoning path.\n"}
{"question": "Which factor significantly contributes to the improved performance of the model in the ST task when using pre-trained wav2vec2.0 architecture?", "evidence": "  We first remove the KL loss but reserve the unified structure. It concludes that the KL terms contribute to an improvement of 0.42 BLEU score. It is concluded that the pre-trained audio encoder model is indeed helpful for downstream ST task.   ", "options": ["A. The removal of KL loss", "B. The addition of auxiliary MT data", "C. The use of unified structure", "D. The concatenation of speech and text sequences", "By introducing more auxiliary MT data, our model with pre-trained wav2vec2.0 improves 1.5 and 2.3 BLEU on the two language pairs En-De and En-Fr, respectively."], "answer": "B", "content": "\nIntroduction\nSpeech translation (ST) is the task that automatically translates a source acoustic speech signal into a text sequence in a target language. With the advance of Transformer, recent works on end-to-end speech translation (E2E ST) can alleviate many problems usually occurred in the cascade system and achieve comparable performance (Bahar et al., 2021; Bentivogli et al., 2021; Fang et al., 2022) .\nFor the E2E ST model, MT is often used as the teacher of ST, and methods such as knowledge distillation or contrastive learning are used to bridge the modality gap. The MT teacher only uses the source text (transcription) information. The speech and text modalities are consumed individually by ST model. There are two main drawbacks. One is the teacher MT model can not use speech information, which limits the overall model perfor-mance. The other is MT uses text input, ST uses the speech input, then close the two individual modalities. There is no unified module can simultaneously use cross-modal information.\nHere, we take a further step towards more effective use of both speech and transcription text in ST. Inspired by the related works of video Transformer (Kim et al., 2021) , when processing video, concatenating video information and text embedding information can better model the cross-modal information of the video. We concatenate the preprocessed speech and the transcription text jointly, and encode the two-modal information simultaneously. Following the recent popular advance in E2E ST with knowledge distillation (KD) (Tang et al., 2021; Zhao et al., 2021) , it provides a practical paradigm for transferring knowledge from rich-resource MT task to limited resource ST task. However, we re-define the role of teacher in our framework, because the information of the two modalities can further improve the upper bound of model performance than the single modality. Our proposed model, a unified cross-modal concatenate ST structure (uccST) introduces the teacher-student learning with Kullback-Leibler divergence (KL) regularization to transfer knowledge from cross-modal translation model to two subtasks -ST and MT.\nOur main contributions can be summarized.\n(1) Compared with the previous ST frameworks which can only utilize one single modality text in MT teacher, we design a unified framework that can use both input information of the two modalities simultaneously by concatenating speech and text.\n(2) Our cross-modal framework has three diverse inputs when inference, containing three end-toend and cascade decoding paths. Our multi-task learning framework allows sub-tasks to collaborate, showing promising performance on both end-toend and cascade ST.\n(3) We conduct various experiments on the MuST-C corpus. When using the limited ternary ST data, our E2E ST model can achieve state-ofthe-art performance. When adding the external data, our method significantly improves over the strong baselines.\n2 Unified Cross-modal Concatenate ST\n\nBackground\nGiven the source acoustic speech sequence s, the corresponding transcription x and the text sequence y in target language, speech translation usually model the conditional distribution as follows.\nEQUATION\nIn most works, the assumption p(y|x) = p(y|x, s) is usually adopted as the source transcription can deterministicially infer the final translation. However, we prefer to leverage the original conditional probability for our modeling.\n\nCross-modal Concatenate Framework\nInspired by video Transformer, the unified model can take as input the concatenation of the features of two modalities along the temporal dimension. As shown in Figure 1 (b), the speech preprocessing module usually includes CNN down-sampling and a speech encoder, such as the encoder of the pre-trained ASR or the pre-trained audio encoder wav2vec2.0. For the text sequence, we simply process each token with an embedding layer. After the concatenation, we add the position embedding and segment embedding in the fashion of BERT.\n\nMulti-task Training\nConcretely, given a ternary ST example (s, x, y).\nWe optimize three translation tasks in parallel, including MT, ST and our introduced unified crossmodal translation.\nL M T = log p(y|x) + log p(y|s) + log p(y|[x, s]) (2) where [\u2022, \u2022] indicates the concatenation operation.\n\nRegularization\nUnlike other ST frameworks, the unified crossmodal decoder output provides the teacher signal, and the ST and MT models are two students. We employ Kullback-Leibler divergence (KL) to minimize the decoding distribution between the student and the teacher model.\nL KL = KL (p st \u2225p unified ) + KL (p mt \u2225p unified ) (3)\nFurther, we impose a representation regularization on the encoder output. Particularly, we apply the MSE loss. where we concatenate the encoder outputs of ST and MT such that it results in the same length as the unified model.\nL M SE = MSE ([Z ST , Z M T ] , Z Unified ) (4)\n\nTraining and Inference\nIn summary, the final loss of the proposed uccST can be written as follows.\nEQUATION\nwhere \u03bb and \u03b8 are hyper-parameters. During inference, we have 3 optional decoding paths. If only audio is available, we can actually choose any decoding path. For the cross-modal unified or MT decoding path, it requires the transcription from an additional ASR, which is commonly a pre-training step for ST.\n3 Experiments Settings\n\nDatasets and Settings\nData For a fair comparison with previous works, we conduct our experiments on the widely used MuST-C V1: English-German (En-De), English- French (En-Fr) and English-Spanish (En-Es) corpus (Gangi et al., 2019) . On En-De and En-Fr, we also verify to what extent the auxiliary MT data can improve our multitask training. Specifically, we extract about 20M sentence pairs for the WMT14 En-Fr, 4.5M for WMT14 En-De, and 18M for Opensubtitle2018 En-De. Settings We implement all our experiments on Fairseq 1 . We experiment with two architectures 2 . One is the transformer model with 512 hidden units 2048 feed-forward size, which is same as Tang et al. (2021) , in purpose for constrained ST data. The other one is to leverage pre-trained wav2vec2.0 (Baevski et al., 2020) as the speech preprocessing module. Since wav2vec2.0 has been already pre-trained with the audio data of Librispeech (Panayotov et al., 2015) , we only compare this setup to other works with same architecture. During training, the text input is ground truth transcript of MuST-C. Note that the transcription data in Librispeech is not used in our case. We select the alternative batches between ST and MT with sampling ratios 1.0 and 0.25, respectively.\n\nResults on the Constrained ST Data\nAs shown in Table 1 , our method achieves an appealing performance on the three language pairs in the restricted ternary MuST-C data.\nCompared with the direct E2E ST baseline, our method has enhanced 0.7 to 1.7 BLEU on the three language directions, with an average gain of 1.13 BLEU. In a word, our approach can achieve the SOTA translation performance among all end-toend ST methods.\nCompared with the cascade method that we have reproduced, our E2E ST decoding path surpasses the cascade on the language pairs En-Fr, and reaches a comparable level on En-De and En-Es. The results of the MT decoding path with the transcription exceed the cascade method on all language pairs. Our cross-modal unified decoding method has enhanced 0.8 to 1.9 BLEU than the cascade method, with an average gain of 1.17 BLEU. In summary, our E2E ST method has matched or surpassed the cascade method on the constrained triple ST data, and our cross-modal unified decoding method has exceeded the traditional cascade baseline.\n\nResults on the External Data\nSince our model is a multitask learning method that includes the MT subtask, we add additional MT data for comparison experiments. As shown in Table 2 , we compare different baselines with similar data usage. Our E2E method (i.e., ST decoding path) and the corresponding baselines are presented in the bottom two rows. The first two rows in the table are the baselines without wav2vec2.0, and the middle part of the table represents the methods with wav2vec2.0 architecture. It is concluded that the pre-trained audio encoder model is indeed helpful for downstream ST task. By introducing more auxiliary MT data, our model with pre-trained wav2vec2.0 improves 1.5 and 2.3 BLEU on the two language pairs En-De and En-Fr, respectively. In shot, our approach outperforms existing state-ofthe-art models, especially on En-Fr.\n\nAblation Analysis of Concatenation\nIn order to analyze whether our concatenation is effective, we have done comparative experiments on different input models. As shown in loss is calculated between ST and MT. Unified simple model only concatenates the speech and text sequence from each corresponding encoder output.\nIn accordance to the result, no concatenation or the concatenation method in Unified simple model is inferior to our proposal.\n\nAblation Study on Loss\nTo analyze the importance of each component of the overall uccST loss, we conduct an ablation study by removing each loss step by step. Table 4 summarizes the results of the ablation study. We first remove the KL loss but reserve the unified structure. It concludes that the KL terms contribute to an improvement of 0.42 BLEU score. After further removing the MSE loss, the model becomes a standard multi-task ST Transformer. When removing multi-task, it reduces to a standard E2E ST model.\n\nComparison with the Cascaded Model\nAs shown in Table 5 , our proposed E2E ST has reached a comparable level to cascaded methods, both in data-constrained and non-constrained cases.\nAs to the two decoding methods that require transcription text, our method can outperform the cascade baseline. Meanwhile, we can observe that with the additional external data, the gap between two inference setups S|X and S is narrowed.\n\nRelated Works\nCascade ST. Cascade ST system concatenates the individual ASR and MT components (Stentiford and Steer, 1988; Waibel et al., 1991) , and represents an intuitive solution to achieve reasonable performance and high intelligibility. At the same time, this cascade method also faces some thorny problems: the traditional cascade method suffers from error propagation and the loss of acoustic information that might be useful to improve final translations. To alleviate the aforementioned problems, some tight integration methods have been proposed (Sperber et al., 2019; Bahar et al., 2020) . End-to-end ST. To overcome the weakness of cascade models, Berard et al. (2016) proposed the first direct neural network model of an encoder decoder architecture without the intermediate transcription.\nCurrently, more effective solutions are used in endto-end ST models (Park et al., 2019; Dong et al., 2021) . To alleviate the cross-modal difficulty in end-to-end models, two-pass (Kano et al., 2017; Anastasopoulos and Chiang, 2018) methods are proposed. Curriculum learning (Kano et al., 2017; Wang et al., 2020) is proposed to improve performance of ST models.\n\nConclusion\nIn this paper, we designed a unified ST framework.\nCompared with the previous ST frameworks which can only utilize one single modality text in MT teacher, our method can use both information of the two modalities simultaneously by concatenating speech and text. Our ST method can better utilize the cross-modal information. Experiments show that our method can significantly improve ST performance regardless of using the limited ternary data or adding auxiliary external data.\n"}
{"question": "why the authors choose to re-investigate the existing work by Chalkidis et al in 2022?", "evidence": "  For our illustration, we re-investigate an existing work (Chalkidis et al., 2022) that evaluates both linear SVM and pre-trained language models, but the authors pay more attention to the latter. The linear method is somewhat ignored even though the performance is competitive on some problems. ", "options": ["A. Because it only considers large models like pre-trained language models.", "B. Because it only considers simple models like linear SVM.", "C. Because it considers both linear SVM and pre-trained language models.", "D. Because it considers both linear SVM and pre-trained language models, but its authors pay more attention to the latter and somewhat ignore the linear method even though the performance of linear model is competitive on some problems. "], "answer": "D", "content": "\nIntroduction\nText classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems.\nWhile various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. "}
{"question": "What does the research mainlt do?", "evidence": "  Most of the current rapid progress in NLP is due to pre-trained large language models. GPT-3 is an impressive publicly available collection of such models, and is able to perform in a way that suggests human-level understanding. Because of this, it is important to explore areas in which it might still differ from human language understanding. In this paper we have argued that ellipsis is one such area. For many simple ellipsis structures, the most powerful GPT-3 models struggle, with accuracies far lower on ellipsis examples than on non-elliptical counterparts.\nIn many ways, GPT-3 appears to understand the texts that it processes, often being able to answer questions that appear to rely on sophisticated reasoning. However, the challenge of ellipsisdependent reasoning provides evidence that GPT-3 is not able to understand in anything like the way humans do. \n ", "options": ["A. Explore areas in which GPT-3 might still differ from human language understanding.", "B. Explore how to use ellipsis.", "C. Prove that GPT-3 is perfect.", "D. Promote the rapid progress in NLP "], "answer": "A", "content": "\nIntroduction\nEllipsis is a fundamental feature of human language, occurring in all registers, where parts of sentences are omitted, although the missing parts are essential for understanding the meaning. The following is an example of Verb Phrase Ellipsis (VPE) (Bos and Spenader, 2011) :\n(1) William went running. Harold did too.\n(1) is understood as asserting that Harold went running; that is, the hearer or reader naturally fills in the missing material. This is done by identifying the antecedent VP, went running, in the first sentence. The following is the non-elliptical counterpart of (1):\n(2) William went running. Harold went running too.\nWith such examples, we can test understanding of ellipsis by targeting the ellipsis phrase with a simple Yes/No question:\n(3) Did Harold go running?\nIf a system answers the question incorrectly for (1), the ellipsis example, but answers correctly for (2), the non-elliptical counterpart of (1), we can ascribe the result specifically to the challenge of ellipsis, since the examples are otherwise identical.\nAs with pronominal anaphora and other discourse processes, there is great flexibility in the way ellipsis can occur in discourse. Example (1) involves two simple adjacent sentences. It is also possible for ellipsis or the antecedent to occur in embedded clauses. Furthermore, ellipsis can occur either before or after the antecedent. Finally, an arbitrary amount of material can intervene between the antecedent and the ellipsis occurrence.\nIn this paper, we propose the challenge of ellipsis-dependent reasoning. This challenge consists of examples involving an ellipsis clause, the target. Each ellipsis example is paired with its nonelliptical counterpart, where the target clause is overt rather than elliptical. We then pose a question whose answer is dependent on the target clause. A key aspect of the challenge is that ellipsis occurrences are possible in a variety of diverse structural configurations. We test a series of GPT-3 models (GPT) on several such ellipsis structures.\n\nRelated Work\nThere is a large literature concerning the probing of language models from a variety of perspectives. Furthermore, there has been substantial work specifically addressing ellipsis in NLP. In this paper, we are proposing the challenge of ellipsisdependent reasoning. This proposal builds on various strands of prior research; below we consider some particularly relevant aspects of this literature.\n\nProbing Models for Knowledge\nThe Winograd Schema (Kocijan et al. (2022) ; Levesque et al. (2012)) involves test examples that use the linguistic problem of pronoun resolution to gain insight into the commonsense reasoning abilities of an AI system. To do this, the Winograd Schema requires pairs of examples that differ only in one specific, small way, as in ( 4):\n(4)\nThe city councilmen refused the demonstrators a permit because they feared/advocated violence.\nWith \"feared\", the pronoun \"they\" refers to the city councilmen, while with \"advocated\", it refers to the demonstrators. 2019): here, examples are constructed which test specific aspects of linguistic knowledge of a system, namely, whether BERT embeddings \"encode hierarchical information\". For example, a task is defined to identify the main auxiliary verb in a sentence, even in cases where the main auxiliary is not the first auxiliary verb to appear. Training and testing datasets are automatically generated using a context-free grammar for several such tasks involving hierarchical syntactic information.\n\nAnaphora and Question Answering\nQuoref (Dasigi et al. (2019) ; Zhang and Zhao (2022) ) is a question-answer dataset designed so that correct answers cannot be given unless a coreference relationship is correctly identified; that is, the reasoning involved in question answering is dependent on resolving coreference. This is, in a sense, the inverse of the Winograd schema, where resolving coreference is dependent upon reasoning. Just as with the Winograd schema, it is difficult to ensure that resolving this dependency is required for system success. (Dasigi et al., 2019)[p. 1] note that this is \"challenging, because it is hard to avoid lexical cues that shortcut complex reasoning\", and based on a random sample, found that coreference resolution was required for 78% of questions.\n\nEllipsis as a Task\nThere has been substantial work on ellipsis as a discrete NLP task (Khullar (2020) 2021) frame ellipsis as a question-answering task, i.e., a task of locating an antecedent, understood as a span of tokens in context. Aralikatte et al. (2021) report token F1 scores of 78.66 for VPE and 86.01 for sluicing, an-other form of ellipsis. It's important to note that the task here, of antecedent identification, is a sub-part of the ellipsis challenge. Before the antecedent is identified, an ellipsis occurrence must be identified, and after the antecedent is identified, it must be interpreted, or \"reconstructed\", at the ellipsis site.\n\nRelevance for Ellipsis-Dependent Reasoning\nThe specific task of ellipsis is addressed in work like that of Aralikatte et al. (2021) , but the key difference here is that we are probing for a complete solution to the ellipsis problem. The proposed ellipsis-dependent reasoning task involves a question that can only be answered correctly if the ellipsis is properly identified and interpreted. This combines aspects of the preceding works in a novel way: like the Winograd schema and the syntactic work by Lin et al. (2019) , it probes for what we see as a specific type of psychologically-defined knowledge: namely, a representation of context that supports the resolution of ellipsis. Similarly to the work on Quoref, we use targeted questions to probe for discourse-related knowledge.\nThere is an extensive literature on the contextual interpretation of natural language, resting on the idea of a dynamic, ongoing model of discourse. For example, Discourse Representation Theory (Kamp, 1981 ) describes a semantic model supporting discourse phenomena such as pronominal and temporal anaphora, and Sag and Hankamer (1984) argue explicitly that ellipsis and other such phenomena are interpreted with respect to a discourse model (Garnham, 2010) . As one study puts it, \"Interpreting a verb-phrase ellipsis (VP ellipsis) requires accessing an antecedent in memory, and then integrating a representation of this antecedent into the local context\" (Martin and McElree, 2008) . In this paper, we seek to determine whether a large language model is capable of such an interpretive process.\n\nData\nThere is a great deal of variety in the structural configurations in which ellipsis can occur. In tables 1 and 2 we define structures for ellipsis and antecedent occurrences.\nIn all structures, there is an ellipsis occurrence, and the question targets the ellipsis occurrence. Furthermore, each ellipsis example is paired with a non-ellipsis version. We generate large numbers of examples of each structure by performing random substitutions for both the subject and verb. The substitution lists are given in the appendix, along with samples of each structure and the size of the resulting sets. 1\n\nTest\nFor each instantiation of a given structure, we produce paired ellipsis and non-ellipsis examples, with an associated Yes/No question. We randomly select 1000 examples for each structure, including 500 ellipsis examples and 500 examples which are their non-elliptical counterparts. Each example is presented to the system, preceded by the text, \"Please give a Yes or No answer:\". We test five GPT-3 models on these structures: Davinci-003, Davinci-002, Curie-001, Babbage-001, and Ada-001. According to the GPT-3 documentation, Davinci-003 is the most powerful model and Ada-001, the least.\n\nResults\nFigure 1 gives the accuracy for ellipsis and nonellipsis, for each of the five models. We have set up the test examples so that an ellipsis example is paired with a non-ellipsis example that is otherwise identical. Because of this, we claim that the difference in accuracy of the non-ellipsis case vs. the ellipsis case provides a measurement of the difficulty specifically posed by ellipsis. For all but the least powerful model, Ada, the non-ellipsis accuracy is substantially higher than ellipsis accuracy, supporting the hypothesis that ellipsis-dependent reasoning presents a difficult challenge for these models. While the Ada model actually performs somewhat better for ellipsis than non-ellipsis, this is not because the Ada model does well with ellipsis cases; rather, the model has great difficulty with both the ellipsis and non-ellipsis cases, and is close to a random guessing baseline of .50.\nIn figures 2 through 6, we present results for each model. We show the accuracy for each structure, for both the ellipsis version and the non-ellipsis version. Consider the most powerful models, Davinci-003 and Davinci-002. In figures 2 and 3, we can see that ellipsis is not difficult in the first two structures: 2Sent (Separate Sentence) and 1Sent (Conjoined Sentence). Here the accuracy is nearly perfect for the ellipsis as well as the non-ellipsis condition. However, in all the other structures, there is a large divergence in accuracy between ellipsis and nonellipsis, for both the Davinci-003 and Davinci-002 models. Subordination for either antecedent or ellipsis is quite challenging, with accuracies ranging from 48.8 to 85.8. The Backwards and Two Actions structures are even more difficult for ellipsis.\n\nAnalysis\nFor the two most powerful models, it is clear that ellipsis poses a difficult challenge, except in the two simplest ellipsis structures. For the less powerful models, the picture is mixed. For these models, the non-ellipsis examples are themselves a difficult challenge, so we are not able to observe the specific difficulties posed by ellipsis.\nAs we can see in figure 1 , the Davinci-002 model performs somewhat better overall than Davinci- 003, on both ellipsis and non-ellipsis. However, figures 2 and 3 show that the advantage of Davinci-002 on ellipsis is exclusively due to the subordinate antecedent construction. In every other ellipsis structure, Davinci-003 performs better than Davinci-002.\nThere are striking differences in the distribution of errors. For both the Davinci-003 and Davinci-002 models, errors are nearly always false negatives -that is, incorrect \"No\" answers. There are virtually no false positives, either for the ellipsis case or nonellipsis case. For the other three models, there are many errors of each type, with a much higher ratio of false positives.\n\nConclusion\nMost of the current rapid progress in NLP is due to pre-trained large language models. GPT-3 is an impressive publicly available collection of such models, and is able to perform in a way that suggests human-level understanding. Because of this, it is important to explore areas in which it might still differ from human language understanding. In this paper we have argued that ellipsis is one such area. For many simple ellipsis structures, the most powerful GPT-3 models struggle, with accuracies far lower on ellipsis examples than on non-elliptical counterparts.\nIn many ways, GPT-3 appears to understand the texts that it processes, often being able to answer questions that appear to rely on sophisticated reasoning. However, the challenge of ellipsisdependent reasoning provides evidence that GPT-3 is not able to understand in anything like the way humans do. \n"}
{"question": "Which method or model is the most related to our model?", "evidence": "  Long Sequence Models Numerous methods have been proposed to reduce the complexity of attention from O(n 2 ) to O(n) such as kernel approximations (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021) and fixed (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) or learned (Kitaev et al., 2020) sparse attention patterns. For a broader summary, please refer to Tay et al. (2022) . In this work, we use Longformer (Beltagy et al., 2020) . To linearize attention complexity, Longformer employs sliding window attention while globally attending to relatively few special tokens. ", "options": ["A. Kernel approximations.", "B. Fixed sparse attention patterns.", "C. Learned sparse attention patterns.", "D. Longformer"], "answer": "D", "content": "\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017) has changed the landscape of recent natural language processing approaches by enabling the pretraining of state-of-the-art large language models (LLM) (Devlin et al., 2019; He et al., 2020; Brown et al., 2020) . However, fine-tuning and storing full copies of LLMs can consume prohibitively large quantities of resources. Parameter-efficient finetuning (PEFT) methods such as prefix-tuning (Li and Liang, 2021; He et al., 2021a; Liu et al., 2022) address these concerns by reducing the number of trainable parameters. Prefix-tuning can tune 0.01% of parameters and still match the performance of regular fine-tuning (updating all model parameters). PEFT has been investigated for tasks with inputs consisting of sentences, sentence-pair, or sequences that fit within the typical LLM maximum tokens. However, the performance of PEFT for tasks with longer textual sequences has been overlooked. In this work, we investigate this oversight and provide evidence suggesting that the gap between PEFT and regular fine-tuning is substantial when modelling long sequences. As shown in Table 1, prefix-tuning underperforms fine-tuning on long sequence classification tasks, Hyperpartisan (Kiesel et al., 2019) and 20-newsgroups (Lang, 1995) , when used with the popular long-document model Longformer (Beltagy et al., 2020) .\nIn this paper, we propose a simple and effective method, prefix-propagation, which consistently improves the performance of PEFT for long sequence models. Unlike prefix-tuning, prefix-propagation propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer.\nTo further understand prefix propagation, we investigate the reliability of the model's predictions by performing analyses on calibration. Lastly, we conduct study on prefix-based methods in terms of kernel attention to strengthen their theoretical value.\nIn summary, our contributions are as follows:\n... Figure 1 : Illustration of the differences between (a) prefix-propagation (ours) (b) and prefix-tuning (Liu et al., 2022; Li and Liang, 2021) . Blue blocks denote trainable prompts, and \"Transformer Layer\" represents the computation done in a layer of the pre-trained LLM. Note that in prefix-propagation (a), the summation of prefixes continues for layers beyond 3, up to n. This operation is encapsulated by the ellipses. In prefix-tuning (b), prefixes in subsequent layers do not depend on hidden states from past layers (they are simply overwritten).\n.\n\u2022 We study PEFT for long documents and show that prefix-tuning is significantly inferior to fine-tuning in this scenario. To the best of our knowledge, this is the first work to focus on PEFT for long documents.\n\u2022 We introduce prefix-propagation, which consistently improves the performance over prefix turning on the different long document datasets, while using 50% fewer parameters.\n\u2022 We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated.\n\u2022 We elucidate the relationship between prefixpropagation and kernel attention and perform an ablation study that utilizes this insight.\n\nRelated Works\nLong Sequence Models Numerous methods have been proposed to reduce the complexity of attention from O(n 2 ) to O(n) such as kernel approximations (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021) and fixed (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) or learned (Kitaev et al., 2020) sparse attention patterns. For a broader summary, please refer to Tay et al. (2022) . In this work, we use Longformer (Beltagy et al., 2020) . To linearize attention complexity, Longformer employs sliding window attention while globally attending to relatively few special tokens.\nParameter-Efficient Tuning Inspired by the success of manual prompting (Brown et al., 2020), prefix-tuning (Li and Liang, 2021; Liu et al., 2022) prepends trainable \"soft\" prompts to an input sequence. Although further PEFT methods have since been introduced (He et al., 2021a; Hu et al., 2021; Ben Zaken et al., 2022) , we focus on adapting prefix-tuning. We note that our adaptation does not violate orthogonality and thus prefixpropagation can still be compounded with other PEFT methods as proposed in the UnifiedPET framework (He et al., 2021a) , likely yielding similar performance gains. We leave the empirical validation of this hypothesis for future work.\nOut work also adheres to the key motivation of the recent PEFT method, inducer-tuning (Chen et al., 2022) , which is that optimal prefixes should be close to queries within their latent space. We derive queries, keys, and values from the same prefix token, limiting the distance that separates them.\n\nMethodology\nIn this section we introduce prefix-propagation, which, unlike prefix-tuning, propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer. Prefix-propagation and its predecessor, prefix-tuning are depicted in Figure 1a embeddings) to the input sequence (blue blocks in top left of Figure 1a ). Then, before every subsequent layer, we sum new trainable matrices onto the first j embeddings corresponding to the prefixes (denoted by the sum operators in Figure 1a ). By propagating instead of overwriting, we halve the number of parameters trained while simultaneously improving performance on long-document tasks.\nWe now formalize prefix-propagation. Multiheaded attention processes query, key, and value matrices derived from a sequence C \u2208 R m\u00d7d with length m and embeddings of size d. Our method modifies traditional attention by concatenating a prefix P \u2208 R j\u00d7d of length j to the sequence:\nH l,i = Attn(D (l) W (l,i) q , D (l) W (l,i) k , D (l) W (l,i) v ) (1) D (l) = cat(P (l) , C) if l = 1 cat(P (l) + C[:j, :], C[j:, :]) if l > 1 where inputs C are projected through pre-trained weight matrices W (l,i) q , W (l,i) k , W (l,i) v\n\u2208 R d\u00d7d h per layer l and head i yielding the output of the attention head, H \u2208 R (j+m)\u00d7d h . The prefixes are concatenated for the first layer (l = 1) and summed to their corresponding hidden states for the remaining layers (l > 1). We do not continually concatenate new prefixes to the sequence to avoid increasing the sequence length after each layer.\nFor both prefix-tuning and prefix-propagation, prefixes (keys and values) are globally attended to by all queries. Unlike prefix-tuning however, our method concatenates additional hidden states before the hidden states C are projected by\nW (i) k and W (i)\nv . By doing so, prefix-propagation modifies query matrices, allowing prefixes to attend to other hidden states globally, thereby increasing representation capability. This approach is somewhat analogous to the external global tokens inserted in the BigBird-ETC model (Zaheer et al., 2020) . By attending to other tokens, the prefixes can act as special storage tokens, which is particularly useful in the restricted regime of long-document modelling where relatively few tokens have global context. Conversely, prefix-tuning only concatenates trained key and value matrices, P k , P v \u2208 R j\u00d7d h , statically to the sequence:\nH l,i = Attn(CW (l,i) q , cat(P (l,i) k , CW (l,i) k ), cat(P (l,i) v , CW (l,i) v ))\n(2)\nSince our method has a single prefix matrix, P instead of separate P k and P v matrices, we reduce the number of trained parameters by 50%.\n\nCalibration\nWe further study the proposed prefix-propagation method to understand the reliability of model's predictions through calibration. Well-calibrated models output confidence scores that closely match the models' accuracy. Either over-confident or underconfident models are undesirable. Calibration has widely been overlooked in PEFT methods. To quantify calibration in our work, we use expected calibration error (ECE), which bins predictions based on model confidence and compares them to accuracy (Pakdaman Naeini et al., 2015; Guo et al., 2017) .\n\nKernel Decomposition\nTraditional attention is analogous to applying a kernel smoother over inputs (Tsai et al., 2019) .\nMotivated by this insight, we reformulate prefixpropagation as a sum of kernelized attention modules. Separating the modules introduces flexibility in two ways: (1) Their individual kernel forms can be mixed and matched and (2) A hyperparameter scale factor \u03b1 can be applied to the prefix component to increase or decrease its weighting. Equation 3 defines kernel decomposition for prefixpropagation 2 :\nH = Kern(cat(P, C)W q , CW k , CW v ) + (\u03b1)Kern(cat(P, C)W q , P W k , P W v ) (3)\nwhere Kern refers to kernel attention as formulated in (Tsai et al., 2019) . The first term results from attending to the original sequence, C, and the second comes from attending to the prefixes, P . We provide the derivation of Equation 3 and the full definition of kernel attention in Appendix A.\nOur main motivation for presenting prefix decomposition is to establish foundational knowledge and guide future research. Ergo, we restrict experiments in this initial presentation to using just the default exponential kernel (Appendix A).\n\nExperiments and Results\nDatasets We evaluate our approach on three long-document classification tasks: ArXiv (He et al., 2019) , an 11-class classification task composed of academic research papers, the 20-newsgroups (Lang, 1995) classification task consisting of mailing lists that fall into one of 20 classes, and the Hyperpartisan dataset, a binary classification task for extremist news classification (Kiesel et al., 2019) . We also run experiments on WikiHop (Welbl et al., 2018) , a long-document reading comprehension task requiring multi-step reasoning.\nDue to compute limitations inherent to working with long documents, with the exception of Hyperpartisan, we only report a single run for each task. This mimics the original Longformer reporting scheme (Beltagy et al., 2020) . For Hyperpartisan, the smallest of the datasets, we report mean metrics averaged over five seeds.\nBaselines As a baseline, we fine-tune Longformer-base (approx.\n149M parameters) as closely as possible to Beltagy et al. (2020) . For PEFT, we evaluate prefix-tuning on Longformer-base and RoBERTa-base (approx. 125M parameters) (Liu et al., 2019) . 2 We omit layer, l and head, i for brevity.\n\nMethod\nArXiv HY. NG. More details on dataset sizes, pre-processing, and hyperparameters are in Appendix B.\n\nResults and Discussion\nAcross all tasks, our results in Table 2 verify that prefix-tuning is inferior to fine-tuning long sequences. Conversely, prefix-propagation consistently outperforms prefix-tuning and is comparable to fine-tuning on most tasks. Prefix propagation also performs competitively on Hyperpartisan, a relatively small dataset with only 625 samples. This is in contrast to prefix-tuning, which is known to underperform in low-data settings (Gu et al., 2022) . Because we ran multiple seeds on Hyperpartisan, we also found that prefix-propagation's better performance relative to prefix-tuning is statistically significant (p < 0.05, using a single-tailed t-test). We do not have multiple samples to run these tests for larger datasets, but we emphasize that Hyperpartisan likely has the most variance and yet it is still statistically significant. We suspect that prefixpropagation's performance exceeds prefix-tuning because propagated prefixes can transmit global context across multiple layers, possibly modelling more expressive abstractions.\nWe note one exception where prefix-based methods still leave room for improvement: multiplechoice question answering on WikiHop. We hypothesize that prefix methods have insufficient capacity to properly model complex long-document multi-step question answering.\nWe also observe that prefix-based methods, and especially prefix-propagation, achieve better calibration than fine-tuning, as shown in Table 3 . Unlike prefix-tuning however, prefix-propagation effectively balances calibration with accuracy metrics. The calibration of fine-tuning deteriorates as training progresses (Figure 4 \n\nMicro F1\nFigure 2 : Violin plot of Micro F1 Score for five different seeds on the Hyperpartisan task. White dots, gray boxes, and gray lines are the medians, interquartile ranges, and ranges respectively. Width of the five violin shapes show the probability densities for the corresponding F1score. All methods tune Longformer-base except \"R Prefix\", which is prefix-tuning on RoBERTa-base.\nforgetting (Jagielski et al., 2022) .\nAs an initial test for our ongoing prefixpropagation kernel study, we show results on Hyperpartisan in Figure 2 . The kernelized version of prefix-propagation achieves the best single-run performance, but has higher variance than fine-tuning and prefix-propagation which necessitates further research.\n\nConclusion\nOur research focuses on parameter efficient tuning for long documents tasks. We introduce prefix-propagation, which consistently improves performance over prefix-turning on long document datasets, while using 50% fewer parameters. We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated. We lastly explicate prefix-propagation from a kernel perspective, uncovering insights for future PEFT research.\n"}
{"question": "What is the primary purpose of RuShiftEval in comparison to SemEval-2020 Task 1?", "evidence": "  We evaluate XL-LEXEME on the multilingual benchmarks for SemEval-2020 Task 1 -Lexical Semantic Change (LSC) RuShiftEval aims to corroborate if training data can improve LSC Detection models.   ", "options": ["A. To evaluate models on Russian, German, and Latin languages.", "B. To determine the degree of semantic change in the English language.", "C. To assess if training data can improve Lexical Semantic Change Detection models.", "D. To provide annotated pairwise sentences for training purposes."], "answer": "C", "content": "\nIntroduction and Motivation\nLexical Semantic Change (LSC) Detection is the task of automatically identifying words that change their meaning over time. The LSC Detection task implicitly aims to disambiguate synchronic word sense occurrences and then find differences in the word sense frequencies in different periods. Word Sense Disambiguation (WSD) is a longstudied task in Natural Language Processing (Navigli, 2009) , which consists of associating the correct sense to a word occurring in a specific context. WSD involves some crucial issues, such as relying on a fixed sense inventory. Fixed sense inventories ignore the diachronic aspect of language because they can miss older unused senses or be outdated and missing new senses.\nThe Word in Context task (WiC) (Pilehvar and Camacho-Collados, 2019) aims to overcome these issues. In this work, we train a model on the WiC task and then use it to perform LSC Detection. In the WiC task, given the word w and two different contexts C1, C2, the systems have to determine whether the meaning of w is the same in the two contexts or not. Our approach is grounded on the assumption that models trained on the WiC tasks are robust enough to transfer the knowledge learned in a synchronic setting to a diachronic one. We summarise the main contribution of this work as follows: (i) We propose a pre-trained biencoder model, called XL-LEXEME, on a largescale dataset for the WiC task, which allows us to obtain comparable lexical-based representations; (ii) We assert the effectiveness of XL-LEXEME despite the computational limitation compared to the cross-encoder architecture for the LSC Detection task; (iii) Experiments on the LSC Detection task show that XL-LEXEME outperforms state-ofthe-art LSC Detection models for English, German, Swedish, and Russian.\n\nRelated Work\nLSC Detection systems can be categorized based on the distributional embeddings used to tackle the LSC Detection task. One category is represented by those approaches that adopt type-base (i.e., static) embeddings. UWB (Praz\u00e1k et al., 2020; Praz\u00e1k et al., 2021) represents an example of this category of systems. First, it employs word2vec Skip-gram with Negative Sampling (Mikolov et al., 2013) to compute a semantic space for each corpus. It uses techniques like the Canonical Correlation Analysis (Hardoon et al., 2004) and the Orthogonal Transformation (Hamilton et al., 2016) to align the abovementioned spaces. Therefore, the cosine similarity between the vectors representing the word in two different spaces is used to detect the semantic shift.\nWith the increasing use of contextualized word embeddings, numerous approaches employing BERT-base models have been developed for LSC Detection (Montanelli and Periti, 2023; Laicher et al., 2021) . In TempoBERT (Rosin et al., 2022) , the authors exploit the concept of Masked Language Modeling (MLM), where the goal is to train a language model to predict a masked portion of text given the remaining part. In particular, they employ this technique to encode the concept of time into a BERT model. This is done by concatenating a specific token representing time to the text sequence. At inference time, TempoBERT can be used to predict the year of a sentence, masking the time reference, or to predict a masked token of the sentence conditioned by the time reference. In the same line of research, in Temporal Attention (Rosin and Radinsky, 2022) , the authors investigate the effect of modifying the model instead of the input sentence like in TempoBERT. This is done by extending the model's attention mechanism to consider the time when computing the weight of each word. The time dimension is encoded using a different query embedding matrix for each timestamp.\nAnother kind of approach exploits the information coming from other tasks to perform LSC Detection. GlossReader represents an example (Rachinskiy and Arefyev, 2021) , where a model based on XML-R (Conneau et al., 2020b) is first trained on English SemCor (Miller et al., 1994) with glosses from WordNet 3.0 (Miller, 1992) to perform WSD. Exploiting the zero-shot crosslingual characteristics of XML-R, the authors used the same model to perform LSC Detection in the Russian language. With DeepMistake (Arefyev et al., 2021) , the authors take advantage of the WiC task instead of WSD. They train a cross-encoder with XML-R as an underlying Language Model on the MCL-WiC training and development set and fine-tune on the RuSemShift dataset (Rodina and Kutuzov, 2020) . DeepMistake, differently from XL-LEXEME, relies on the cross-encoder architecture and exploits only the MCL-WiC training dataset.\n\nXL-LEXEME\nGenerally, for pairwise sentence similarity tasks, BERT models use a cross-encoder, in which the pairwise sequences are jointly encoded, and the overall vectors are used for the classification. However, in several tasks, the cross-encoder is not suitable since it cannot provide a distinct meaningful representation for each sentence. An approach to overcome this issue involves pooling the BERT out-put encoded vectors, which often results in worse performance. Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) overcomes the limitation of cross-encoders using a Siamese Network, i.e., the weights of the underlying networks are shared. SBERT encodes the two sequences separately in the BERT model exploiting the Siamese architecture. The sequence-level representation is obtained by averaging the output encoded vectors, which are directly compared using similarity measures such as cosine similarity.\nMeanwhile, cross-encoders perform better since they are trained to profit from the attention over the whole input. In this work, we introduce XL-LEXEME 1 which mirrors models for pairwise sequence similarity tasks and adapts them to the WiC task, giving prominence to the target word, i.e. the word for which we want to detect the LSC. The model takes as input two sequences s 1 and s 2 . The sequences are tokenized using subwords tokenizer, such as Sentence Piece (Kudo and Richardson, 2018) , and the special tokens <t> and </t> are used as target word delimiters (Xie et al., 2021) :\nEQUATION\nwhere N and M represent the number of subwords of the sequence s 1 and s 2 respectively, while w t i , ..., w t i+k and w t j , ..., w t j+p are the subwords of the target words. In the following, we describe the baseline cross-encoder and XL-LEXEME based on a bi-encoder. For the crossencoder, the two input sequences are concatenated by the special token [SEP ] in an overall sequence\ns = [CLS] s 1 [SEP ] s 2 [SEP ].\nIf the length of s, i.e. N + M + 3, is greater than the maximum sequence length \u03bb, then the sequence s is cut such that the length of s 1 and s 2 is less than \u03bb * = \u03bb\u22123 2 . To comply with the maximum length, the left and right contexts of the sequence are truncated. For instance, s 1 is truncated as follows:\ns 1 = w n 0 , ..., <t>, w t i , ..., w t i+k , </t>, ..., w n 1 (2) where n 0 = max(0, i \u2212 1 \u2212 \u03bb * \u2212k\u22122 2 ) and n 1 = min(N, i + k + 1 + \u03bb * \u2212k\u22122 2\n). The truncated sequence has a length \u03b3 < \u03bb. The encoded representations of each subword (v 1 , v 2 , ..., v \u03b3 ) are summed to get the encoded representation of the overall sequence, i.e. s enc = \u03b3 i v i . Finally, the vector s enc is used to compute the logits:\nlogit = log \u03c3(W s enc ) (3)\nwhere W \u2208 IR 1\u00d7d . The model is trained to minimize the Binary Cross-entropy loss function. XL-LEXEME is a bi-encoder that encodes the input sequences using a Siamese Network into two different vector representations. Each sequence is tokenized and truncated according to the maximum length \u03bb * , using Equation (2). We thus obtain the new lengths \u03b3 1 , \u03b3 2 . The vector representation is computed as the sum of the encoded subwords\n(v 1 , v 2 , ..., v \u03b3 ), i.e. s enc 1 = \u03b3 1 i v i and s enc 2 = \u03b3 2 j v j .\nXL-LEXEME is trained to minimize the Contrastive loss (Hadsell et al., 2006) :\n\u2113 = 1 2 y \u2022 \u03b4 2 + (1 \u2212 y) \u2022 max(0, m \u2212 \u03b4) 2 (4)\nwhere we adopt a margin m = 0.5. We use as default distance \u03b4 the cosine distance between the encoded representations of s 1 and s 2 , i.e. \u03b4 = cos(s enc 1 , s enc 2 ). The main advantage of XL-LEXEME concerning models based on the crossencoder architecture is efficiency. The time cost can be directly derived from the different architectures that exploit XL-LEXEME and the crossencoder baseline. The self-attention time complexity O(N 2 * d) depends on the vector dimension d and the sequence length, which is N for the cross-encoder and N 2 for XL-LEXEME. For XL-LEXEME, the time complexity is reduced to O((N^2)^2 * 2d).\n4 Experimental setting\n\nLexical Semantic Change Detection\nSemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection (Schlechtweg et al., 2020) is the first task on Unsupervised Lexical Semantic Change Detection in English, German, Swedish, and Latin languages. For each language, two corpora represent two different periods (T0, T1). Moreover, a set of target words, annotated using the DUREL framework (Schlechtweg et al., 2018) , are provided. SemEval-2020 Task 1 involves two subtasks. The binary classification task requires assigning a label (changed/stable) to each target word. The ranking task sorts the target words according to their degree of semantic change. In this work, we focus on Subtask 2, and for the sake of simplicity, we refer to SemEval-2020 Task 1 Subtask 2 as SemEval-2020 Task 1. RuShiftEval, different from SemEval-2020 Task 1, involves three sub-corpora extracted from the Russian National Corpus spanning three periods. Models are evaluated on the resulting three test sets, namely RuShiftEval1 (pre-Soviet and Soviet), RuShiftEval2 (Soviet and post-Soviet), and RuShiftEval3 (pre-Soviet and post-Soviet). RuShiftEval provides participants with development data that can be used for tuning models. RuShiftEval aims to corroborate if training data can improve LSC Detection models. The development data rely on the RuSemShift dataset (Rodina and Kutuzov, 2020) , which includes two sets of 70 target words for the pre-Soviet to Soviet period and Soviet to post-Soviet period, respectively. The dataset also includes annotated pairwise sentences, which can be used for training the models.\n\nTraining details\nXL-LEXEME and the cross-encoder are trained using XLM-RoBERTa (XLM-R) (Conneau et al., 2020a) large as the underlying Language Model 2 and using an NVIDIA GeForce RTX 3090. As for training data, the model uses the training data of MCL-WiC (Martelli et al., 2021) , AM 2 ICO (Liu et al., 2021) , and XL-WiC datasets (Raganato et al., 2020) merged with the randomly sampled 75% of the respective development data of each dataset. The remaining 25% of the development data is used to fine-tune hyper-parameters. Moreover, we augment training data for the cross-encoder by swapping the order of sentences in the training set (Martelli et al., 2021) .\nWe use AdamW optimizer and linear learning warm-up over the 10% of training data. We perform a grid search for the hyper-parameters optimization, tuning the learning rate in {1e-6, 2e-6, 5e-6, 1e-5, 2e-5} and the weight decay {0.0, 0.01}. Table 3 (Appendix A) shows the selected hyperparameters. We sample 200 sentences containing the target word for each language and each period. The sampling is repeated ten times, and the results are averaged over the ten iterations. We use the same methodology of Rachinskiy and Arefyev (2021) for sampling sentences from the RuShiftEval corpora. We sample sentences in which we find the exact match with the target words with no pre-processing of the SemEval dataset. The LSC score is computed as the average distance between the vectors over the two different periods:\nLSC(s t 0 , s t 1 ) = 1 N \u2022 M N i=0 M j=0 \u03b4(s t 0 i , s t 1 j ) (5)\nwhere \u03b4 is the distance measure, i.e. \u03b4 = 1 \u2212 log \u03c3(W s enc ) for the cross-encoder baseline and \u03b4 = cos(s enc 1 , s enc 2 ) for XL-LEXEME.\n\nResults\nTable 1 and Table 2 report the results on the SemEval-2020 Task 1 Subtask 2 and the results on the RuShiftEval test set. The results of the best systems are in bold. XL-LEXEME achieve the best score for English, German, Swedish, RuShiftEval1, RuShiftEval2, and RuShiftEval3. XL-LEXEME achieves a strong Spearman correlation for English and Swedish languages and a solid correlation on the German dataset, obtaining a significative correlation (p < 0.001). XL-LEXEME obtains no significant results in the Latin language since the predicted scores for the target words are not correlated with the test set. Latin is underrepresented in the training data of XLM-R, and there are no similar languages in the WiC dataset that we use for training XL-LEXEME. Moreover, the Latin dataset is more challenging as it involves the first corpus written in ancient Latin, which differs in many aspects from modern Latin. For this reason, XL-LEXEME could be ineffective in ancient languages and, in general, in languages that are not widely covered by the WiC dataset. We report the statistical significance of the difference between the performance of XL-LEXEME concerning the other models. The statistical significance of the difference is computed using Fisher's z-transformation (Press, 2002) . XL-LEXEME obtains stronger correlations than the cross-encoder, but the differences are not significant. The correlations obtained on the English and the German datasets are significantly different (p < 0.05) for all the systems that participated in the SemEval-2020 Task 1 but not for TempoBERT and Temporal Attention. On the other side, TempoBERT and Temporal Attention obtain a Spearman correlation on English and German that is not statistically different from the systems on the SemEval-2020 Task 1 leaderboard. In the Swedish language, XL-LEXEME is the only one obtaining a significantly different correlation from the Count baseline results. XL-LEXEME showed its effectiveness also in Swedish, although the WiC dataset does not cover this language. Presumably, Swedish benefits from the presence of other languages descending from the Old Norse language, namely Danish and Norwegian.\nXL-LEXEME obtains competitive results for the Russian language in the RuShiftEval leaderboard. Contrary to XL-LEXEME, Deep Mistake and Gloss Reader are fine-tuned on the RuSemShift dataset. The differences between XL-LEXEME and the best two systems in the leaderboard are not statically significant. Moreover, in Table 2 , the results of XL-LEXEME fine-tuned on the RuSemShift are shown. Although the fine-tuned model achieves the best correlation scores in the three datasets, the difference between DeepMistake and GlossReader is not significant.\n\nConclusion\nIn this work, we introduced XL-LEXEME, a model for LSC Detection. XL-LEXEME is pre-trained on a large WiC dataset to mirror sentence-level encoders focusing on specific words in contexts. We evaluated our model on two Lexical Semantic Change Detection datasets: SemEval-2020 Task 1 and RuShiftEval. XL-LEXEME outperforms stateof-the-art models for LSC Detection in English, German, Swedish, and Russian datasets, with significant differences from the baselines. The XL-LEXEME effectiveness and efficiency make it reliable for LSC Detection on large diachronic corpora.\n"}
{"question": "Which one among the following experimental Setup  contains a mistake?", "evidence": "We use the official training and testing split for GSM8K, taking the last 10% of the training split for validation, and the 5-fold cross validation splits available for MAWPS and ASDiv. As a testing split is not available, we do not shuffle the dataset to allow reproducing our split of taking the first 80% as training data, We select PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) as teacher models.  We establish T5 XXL model finetuned on the original target as the baseline. ", "options": ["A:We use the official training and testing split for GSM8K, taking the last 10% of the training split for validation, and the 5-fold cross validation splits available for MAWPS and ASDiv.", "B:As a testing split is not available, we do not shuffle the dataset to allow reproducing our split of taking the first 80% as training data,", "C:We select PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) as teacher models. ", "D:We establish T5 XXL model finetuned on the original target as the baseline. "], "answer": "A", "content": "\nIntroduction\nChain of thought (CoT) prompting encourages language models (LMs) to break down a reasoning task into a series of intermediate steps (Wei et al., 2022) . They demonstrate that this prompting significantly increases the task accuracy of large language models (LLMs) across commonsense, symbolic and mathematical reasoning datasets. Here, LLMs are models with at least tens of billions of parameters, such as PaLM 540B (Chowdhery et al., 2022) , GPT-3 175B (Brown et al., 2020) , or UL2 20B (Tay et al., 2022) . However, the reasoning capabilities of smaller LMs do not improve with CoT prompting, mostly producing illogical CoT. Notably, CoT prompting even reduces the accuracy of models with less than 10 billion parameters. Wei et al. (2022) attribute this to abilities, such as semantic understanding and symbolic mapping, only emerging at larger scales. This leads us to our re- * Research conducted during an internship at Google. search question: can the reasoning capabilities of LLMs be transferred to smaller LMs via finetuning?\nThis work explores CoT knowledge distillation (Hinton et al., 2015) from PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) to different sizes of the smaller language model T5 (Raffel et al., 2020) , such as T5 XXL, XL and base, which have 11 billion, 3 billion and 220 million parameters, respectively. As a result of our work, we make two recommendations: (1) perform knowledge distillation by finetuning the student model on the CoT generated by a large teacher model; and (2) generate the CoT from an LLM, as proposed by Wei et al. (2022) , but crucially provide the solution to the task in the few-shot prompt. We demonstrate that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets irrespective of the teacher model used. For example, we show an accuracy increase from 8.11% to 21.99% and 18.42% on the GSM8K (Cobbe et al., 2021) dataset when finetuning T5 XXL on PaLM 540B and GPT-3 175B generated CoT data, respectively.\n\nRelated Work\nThis work is inspired by the seminal work of Wei et al. (2022) on CoT prompting. They demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021) . Wang et al. (2022) show that task accuracy can be further improved by using self-consistency in CoT prompting. Selfconsistency samples CoT reasoning paths from a model's decoder and returns the most consistent path by taking the majority vote. Subsequently, Chung et al. (2022) explore finetuning a FLANbased (Wei et al., 2021) version of PaLM on manually generated CoT data.\nConcurrent to our work, a small number of other works propose methods focused on CoT student-teacher knowledge distillation. Ho et al. (2022) and Li et al. (2022) also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. We also achieve a higher accuracy on common datasets, such as GSM8K, than Ho et al. (2022) . In contrast to our work, Shridhar et al. (2022) focus on training two models, one for problem decomposition and one for solving. Yet differently, the focus of Eisenstein et al. (2022) relies on producing markupand-mask explanations for open-book question answering. Lastly, Huang et al. (2022) present one related experiment, however, we present a more indepth exploration on more datasets. To the best of our knowledge, our work is the first to extensively explore the improvement of the reasoning ability of small LMs via knowledge distillation across multiple model architectures, and observing the effects of student model size and dataset size on accuracy.\n\nMethod\nWe propose a two-step pipeline for CoT knowledge distillation. The first step comprises annotating an existing supervised dataset with CoT reasoning generated by a teacher model. To generate high quality data, we propose using LLMs, such as PaLM 540B or GPT-3 175B, as teachers, based on the finding that CoT reasoning improves with model scale (Wei et al., 2022) . Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by Wei et al. (2022) . We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT. This is based on the observation that providing this guidance allows LLMs to correct small mistakes in the CoT. Lastly, we remove all incorrect CoT based on the target answer to prevent the student to learn from bad examples. The second step comprises finetuning a student model via teacher forcing (Williams and Zipser, 1989) . The student is provided with the question as input, and the CoT and answer as the target. As the model is trained on producing a CoT during finetuning, prompting is not required. \n\nExperimental Setup\nWe follow a similar experimental setup to Wei et al. (2022) , focusing on tasks covering arithmetic, commonsense and symbolic reasoning.\n\nArithmetic Reasoning\nWe benchmark the proposed method on the following math word problem datasets: (1) GSM8K (Cobbe et al., 2021) , ( 2) MAWPS (Koncel-Kedziorski et al., 2016) and (3) ASDiv (Miao et al., 2021) . We use the official training and testing split for GSM8K, taking the last 10% of the training split for validation, and the 5-fold cross validation splits available for MAWPS and ASDiv. We evaluate task accuracy by checking for the target answer as the final answer in the CoT. In addition, we compute the task accuracy given an external calculator, to account for arithmetic mistakes made by the model, despite the CoT being correct. The external calculator moves through the generated output, recalculating the left hand-side of equations. It then replaces the right-hand side with the calculated output, to avoid arithmetic mistakes being carried forward. For example, if a model outputted '5 + 5 = 11. 11 * 2 = 22', then the external calculator would first calculate '5+5' and replace the '11' with a '10'. In the subsequent equation, it would also replace the '11' with a '10' and arrive at the final result of '20'.\n\nCommonsense Reasoning\nWe benchmark the model's ability to perform commonsense reasoning on the StrategyQA dataset (Geva et al., 2021a) . As a testing split is not available, we do not shuffle the dataset to allow reproducing our split of taking the first 80% as training data, the following 10% as validation data, and the final 10% as testing data. We compute task accuracy in the same manner as previously mentioned.\n\nSymbolic Reasoning\nLastly, we benchmark the model on two synthetic tasks for symbolic reasoning: (1) last letter concatenation and (2) coinflip (Wei et al., 2022) . Last letter concatenation prompts the model to concatenate the last letter of each word in a string. Coinflip prompts the model to perform state tracking of the coin being flipped. We evaluate task accuracy in the same manner as before. Due to the rigid structure of the datasets, we focus on evaluating the model's generalizability to out-of-distribution (OOD) examples. We finetune the models on examples of length two and evaluate on sequences of length three and four. We initially infer the CoT using PaLM 540B, however, find that the LLM is able to perfectly replicate the desired CoT bar one example due to the rigidness of the template. We therefore decide to use the template generated CoT in our experiments.\n\nBaselines and setup\nWe select PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) as teacher models. We select PaLM 540B based on the stateof-the-art results on the benchmarking datasets reported by Wei et al. (2022) , and confirm the observed trends with GPT-3 175B. The publicly accessible teacher models are prompted as described in Section 3.\nWe select different sizes of T5 (Raffel et al., 2020) as student models, as T5 is publicly available in many sizes. The student models are trained on the PaLM 540B or GPT-3 175B generated CoT data as described in Section 3. We establish T5 XXL model finetuned on the original target as the baseline. We refrain from shuffling the datasets to allow for reproducibility.For the MAWPS and ASDiv dataset, we perform 5-fold cross validation. For all remaining datasets, we take 10% of the Input: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nOutput:\nRoger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. We refer the reader to Appendix A for an overview of the dataset licenses. We also refer the reader to Appendix B for an overview of the computatinal resources.\n\nArithmetic reasoning\nTable 1 details the task accuracy with and without an external calculator for the arithmetic reasoning benchmarks. Our results show that the proposed method improves task accuracy across all datasets. Most notably, the task accuracy of MAWPS is significantly improved. The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. 1 : Task accuracy across arithmetic reasoning datasets for T5 XXL without finetuning (baseline) and finetuned on PaLM 540B generated chain-of-thought (CoT). We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on generating chain-of-thought data\nWe perform an ablation study to confirm that providing a LLM with the target during CoT generation is beneficial. We found that for the GSM8K dataset, PaLM 540B only achieves a 59.98% accuracy if prompted without the target. In comparison, when including the target in the prompt the accuracy is 79.37%. A superficial explanation would be that when the model is conditioned on the expected answer, it produces the same CoT but copies the answer. However, an analysis of a subset of the differences between CoT produced with and without this conditioning shows that most of the benefits actually come from the model correcting CoT that had a single step missing or was wrong.\n\nCommonsense reasoning\nFor the StrategyQA dataset (Table 3 ), we found that using CoT finetuning improves accuracy from 68.12% to 71.98%, using only 1319 of the original 1648 examples. Compared to the arithmetic reasoning datasets, the improvement is not as significant. This can be explained by the model lacking factual knowledge that the dataset requires. The task is heavily focused on the model reasoning on such knowledge, however, a smaller LM is most likely not in possession of this knowledge compared to a larger model with higher memorisation capacity.\n\nSymbolic reasoning\nTable 2 shows the results obtained for the synthetic symbolic reasoning datasets, focusing on OOD generalization. Focusing on Last Letter Concatenation, it can be stated that both traditional finetuning and the suggested method fail at generalizing to a longer sequence length. In comparison, the proposed method significantly increases accuracy for the Coinflip dataset with regard to generalizing to three coinflips. In contrast, generalisation to four coinflips is slightly weaker than the baseline, which performs very strongly. This may be related to the task length being twice that of the training task.\n\nReplicating Results using different Teacher Models\nWe demonstrate the robustness of our method using a different teacher model, namely GPT-3 175B. Table 3 shows the results for GSM8K and Strat-egyQA when T5 XXL is finetuned on CoT data generated by GPT-3. The results show that the proposed method elicits improvements also with other Table 3 : Task accuracy for T5 XXL finetuned on chainof-thought (CoT) data generated by PaLM 540B and GPT-3 175B. We also finetune on the reasoning steps provided by the datasets. We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on model size\nWe investigate the performance gain achieved via finetuning student models of different sizes. Figure 3 shows the performance gain achieved when finetuning T5 of different sizes on the GSM8K dataset.\nOur results show that T5 base, with 44 times fewer parameters than T5 XXL, matches the performance of the baseline T5 XXL when trained on CoT data. Moreover, given an external calculator, even T5 small outperforms the baseline T5 XXL.\n\nAblation study on dataset size\nWe also investigate the trade-off between the performance gain from CoT finetuning and dataset size. \n\nDiscussion\nWe demonstrate that finetuning larger LMs on the CoT data generated by LLMs of over 100 billion parameters can significantly improve task accuracy. Even a small number of CoT examples appear to suffice for this. However, such improvements appear to be task dependent. For example, the effects are limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller LMs may not have memorised due to their limited capacity. Nevertheless, there is some performance improvement, which may be attributed to the model learning how to approach such tasks. Moreover, the CoT knowledge distillation pipeline presented allows to trade-off model and dataset size with accuracy. Future work could explore improving the reasoning of small models in multi-task settings, as well as the generation of new training data using LLMs, rather than annotating existing datasets.\n\nConclusion\nThis work explores CoT knowledge distillation from LLMs of over 100 billion parameters to smaller LMs. We propose a knowledge distillation pipeline consisting of two keys steps: (1) generate CoT for existing datasets using LLMs and\n(2) finetune smaller LMs on the CoT. Our results demonstrate that finetuning on CoT improves task accuracy across a range of benchmarking datasets.\n"}
{"question": "In related work, what is the shortest corpus metioned?", "evidence": "  Related Work\nThe SDS-200 corpus (Pl\u00fcss et al., 2022) contains 200 hours of speech by around 4,000 speakers with Standard German transcripts. The recordings cover a large part of the Swiss German dialect landscape. The number of recordings per speaker follows a long-tail distribution. For example, the top 3 speak-ers account for 23% of recordings. The Swiss Parliaments Corpus or SPC (Pl\u00fcss et al., 2021a) contains 299 hours of speech in the Bernese dialect. The text is Standard German, taken from parliament minutes, and is not a fully accurate transcription. Text and audio are automatically aligned. The SwissDial corpus (Dogan-Sch\u00f6nberger et al., 2021) contains 26 hours of studio-quality recordings by 8 speakers, each speaking a different dialect, with both Standard German and Swiss German transcripts. The Radio Rottu Oberwallis corpus (Garner et al., 2014) contains 8 hours of speech transcribed in Swiss German, of which 2 are also transcribed in Standard German. The ArchiMob corpus (Samard\u017ei\u0107 et al., 2016) contains 69 hours of speech with Swiss German transcripts. ", "options": ["A. ArchiMob corpus", "B. Swiss Parliaments Corpus", "C. Radio Rottu Oberwallis corpus", "D. SwissDial corpus"], "answer": "C", "content": "\nIntroduction\nWe present STT4SG-350, a corpus of Swiss German speech, annotated with Standard German text at the sentence level. The corpus represents all Swiss German dialect regions and contains 343 hours of speech.\nSwiss German is a family of German dialects spoken by around 5 million people in Switzerland. It differs from Standard German regarding phonology, vocabulary, morphology, and syntax. There are significant differences among the Swiss German dialects as well, particularly regarding phonology and vocabulary. Swiss German is primarily a spoken language. It is also used in writing, but mainly in informal text messages. In most other contexts, including formal letters, laws, and newspapers, Standard German is used instead. One important reason for this is Swiss German's lack of a standardized orthography.\nThe diversity among dialects, exacerbated by the lack of a standardized orthography, leads to a large number of written variants for each word. This, together with the small amount of text resources compared to Standard German, makes automated processing of Swiss German text challenging.\nSTT4SG-350 is, to the best of our knowledge, the largest public speech corpus for Swiss German. While the primary use case is automatic speech recognition (ASR), it is also a useful resource for text-to-speech (TTS), dialect identification, and speaker recognition. By providing roughly the same amount of data per dialect region, irrespective of its population size, the corpus contributes to improving speech technology for underrepresented dialects. In addition, the test set, which contains the same spoken sentences in each dialect, allows a fair evaluation of the quality of speech technologies in different dialects. Furthermore, it contributes to more inclusive speech technology by keeping a balanced gender ratio and featuring speakers of all ages.\n\nRelated Work\nThe SDS-200 corpus (Pl\u00fcss et al., 2022) contains 200 hours of speech by around 4,000 speakers with Standard German transcripts. The recordings cover a large part of the Swiss German dialect landscape. The number of recordings per speaker follows a long-tail distribution. For example, the top 3 speak-ers account for 23% of recordings. The Swiss Parliaments Corpus or SPC (Pl\u00fcss et al., 2021a) contains 299 hours of speech in the Bernese dialect. The text is Standard German, taken from parliament minutes, and is not a fully accurate transcription. Text and audio are automatically aligned. The SwissDial corpus (Dogan-Sch\u00f6nberger et al., 2021) contains 26 hours of studio-quality recordings by 8 speakers, each speaking a different dialect, with both Standard German and Swiss German transcripts. The Radio Rottu Oberwallis corpus (Garner et al., 2014) contains 8 hours of speech transcribed in Swiss German, of which 2 are also transcribed in Standard German. The ArchiMob corpus (Samard\u017ei\u0107 et al., 2016) contains 69 hours of speech with Swiss German transcripts.\nFor Swiss German ASR, the desired output text language is Standard German for the vast majority of use cases. Tackling speech-to-text translation with an end-to-end approach is feasible as shown by Weiss et al. (2017) . Applying a similar approach to Swiss German ASR and therefore avoiding Swiss German text and its challenges altogether lead to promising results in recent years, see (Pl\u00fcss et al., 2023; Khosravani et al., 2021; Pl\u00fcss et al., 2022 Pl\u00fcss et al., , 2021a)) . Dogan-Sch\u00f6nberger et al. (2021) experiment with TTS for Swiss German. Their models achieve a 5-scale mean opinion score of 2.9 to 4.1. Importantly, their approach requires Swiss German input text.\n\nData Collection\nData for STT4SG-350 was collected in two phases: 1) the test set with 76 participants from December 2021 until March 2022, and 2) the train and validation sets with 240 participants from May until November 2022.\n\nRecording\nSpeech was recorded using a web app based on the code 1 by Pl\u00fcss et al. (2022) . Recordings are made sentence by sentence. The app displays a Standard German sentence, which the participant is asked to translate to Swiss German and speak aloud. A screenshot of the recording functionality can be found in Appendix A. The goal of the translation step is to get a correct, natural-sounding Swiss German sentence in the participant's dialect. We display a popup with examples before the first 1 MPL-2.0 license recording to explain this to participants. We also display a short explanation below the sentence to be recorded. We manually validated the correctness of at least 10 randomly sampled recordings per participant at collection time. In contrast to Pl\u00fcss et al. (2022) , for phase 2, we recorded 44.1 kHz lossless FLAC audio rather than 32 kHz lossy MP3 audio. The recording quality depends on the microphones used by participants, which range from studio microphones to headsets and laptop microphones. Depending on the microphone, mouse clicks can be audible in recordings.\n\nDialect Regions\nFor this work, we divided the Swiss German dialect continuum into 7 dialect regions, listed in Table 1 , based on the clustering method by Scherrer and Stoeckle (2016) 2 . The cluster analysis was carried out on 350 phonological, lexical, morphological, and syntactic phenomena. We slightly adjusted the resulting clusters to match the dialect regions commonly used in public discourse more closely. The goal of these adjustments was to make it more intuitive for participants to choose their dialect region. The borders are intentionally fuzzy to give participants the freedom to choose the region that fits their dialect best.\n\nSentence Selection\nSentences were randomly selected from Swiss newspapers and from parliament minutes of 2 Swiss parliaments. Sentence filtering for newspapers follows Pl\u00fcss et al. (2022) . The goal of the filtering is to limit sentence complexity to reduce errors in the translation task. For example, only sentences of 5 to 12 words are kept. The newspaper sentences cover a broad range of topics, including culture, finance, science, sports, and technology. They also cover content and named entities particularly relevant for Switzerland. Parliament sentences are not filtered. They bring additional diversity to the corpus with longer sentences on average and a distinct vocabulary. For the test set, 3,515 sentences were selected (67% newspapers, and 33% parliaments). To allow a fair comparison among the dialects, each sentence was recorded in each of the 7 dialects. For the training and validation data, 94% news and 6% parliament sentences were selected, and we dropped the requirement to record each sentence in all dialect regions to in-crease vocabulary and phrase diversity.\n\nMetadata\nParticipants self-reported the following metadata:\n\u2022 The dialect region that best fits the participant's dialect. \u2022 The zip code of the place where the participant grew up or went to school. \u2022 Age group (< 19, 19-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89 , > 89) \u2022 Gender (female, male, non-binary) We manually checked the correspondence of reported metadata and recordings for each participant. Collecting the dialect provenance as a zip code allows us to investigate dialects and the performance of speech technologies for them at different granularity levels. Collecting age group and gender helps to make sure that speech technology is inclusive and works across different demographic groups.\n\nRecruitment\nFor the test set, all participants were recruited via the crowdsourcing platform TestingTime 3 . For the train set, half the participants were recruited via TestingTime, whereas the other half were recruited via universities, high schools, newspaper ads, personal contacts, and the crowdsourcing platform seniors@work 4 (for details refer to Appendix F and 6). Only native Swiss German speakers able to correctly translate Standard German to Swiss German were recruited. The goal was to collect the same amount of recordings in each dialect region and we recruited accordingly. The number of recordings per participant was limited to 368 for the test set 5 and 1,112 for the train data. Recruiting the 316 participants required a considerable effort, especially in the low-population regions GR and VS.\n\nCorpus\nThe corpus is publicly available 6 \n\nData Cleaning\nFiltering. Recordings with a duration of less than 2 seconds were removed. Silent recordings were also removed. For the test set, we applied heuristics to flag incomplete sentences, which were removed after double-checking them. We only kept sentences with a recording in all dialect regions in the test set. In total, we filtered out 1.5% of recordings.\nValidation. We validated each speaker manually.\nFor this, we randomly sampled 10 recordings from each speaker, and checked whether the dialect is correct, the recording is in Swiss German, the translation is correct, and whether the sound quality is high enough. All of the participants passed the manual check.\n\nStatistics\nThe provided by 316 different speakers, of which 51% identified as female and 49% as male. No speaker identified as non-binary. Figure 1 shows the distribution of the recordings over the age groups, as well as the gender distributions per age group. The age groups from the thirties to the sixties are well represented, while the twenties are overrepresented and the teens as well as seventies are underrepresented. The age groups eighties and above are not represented at all. Table 1 shows the corpus statistics per dialect region. While the German-speaking population differs by a factor of up to 16 between regions, the number of recordings per region is a lot more balanced, differing by a factor of not more than 1.2.\n\nSplits\nTable 2 shows the different corpus splits. We provide training, validation, and test splits. There is no speaker overlap between training, validation, and test. There are no common sentences between test and either training or validation. There is, however, an intersection of 835 sentences between training and validation. There are 2 different training splits. train_all contains all training data, 276 hours of speech. train_balanced is a subset of train_all with 239 hours of speech that is balanced in the number of recordings per dialect region. For GR, the region with the fewest recordings, the recordings of all speakers are included in train_balanced. For the other regions, we randomly chose speakers and added their recordings until the number of GR recordings was reached. train_balanced includes 33-35 hours of speech, 24,088-25,183 recordings, and 25-32 speakers per region.\nLike train_balanced, the validation split, with 34 hours of speech, is balanced in the number of recordings per dialect region. We randomly chose 3 speakers per region with at least 1,000 recordings. The test set comprises 34 hours of speech. Importantly, the same 3,515 sentences were recorded in all 7 dialect regions to allow a fair comparison between different dialects. equate speaker diversity in each region. For this reason, the mean number of recordings per speaker is markedly lower than in the other splits.\n\nAutomatic Speech Recognition Baseline\nWe train a baseline model to demonstrate the use of the STT4SG-350 corpus for Swiss German ASR. We fine-tune XLS-R (1B) 8 (Babu et al., 2021) on the train_balanced split. XLS-R is a model based on wav2vec 2.0 (Baevski et al., 2020) with 965 million parameters pretrained on 436K hours of unlabeled speech data covering more than 128 languages. Swiss German was not part of the training data. We provide the fine-tuning details and experimental setup in appendix C. We report the results of our fine-tuned model on three publicly available Swiss German datasets and the STT4SG-350 validation and test sets in Table 3 . The model achieves state-of-the-art results on the All Swiss German Dialects Test Set (ASGDTS) (Pl\u00fcss et al., 2021b) and SDS-200 (Pl\u00fcss et al., 2022) , and improves the best reported BLEU scores on the test sets by 43% and 9%, respectively. Our model is 6% behind the best reported BLEU score on the SPC test set (Pl\u00fcss et al., 2021a) . These results highlight the benefit of the STT4SG-350 dataset on test data from different domains.\n\nConclusion\nWe have described STT4SG-350, which is, to the best of our knowledge, the largest public speech corpus for Swiss German with 343 hours of speech. Our ASR baseline model trained on the corpus achieves a BLEU score of 74.7 on the test set. In addition, it beats the best published BLEU scores on 2 other test sets, demonstrating the quality of the corpus. STT4SG-350 is balanced across the 7 dialect regions, and the test set allows a fair comparison of ASR performance on different dialects. We intend to take advantage of these properties in future work and conduct in-depth experiments to explore differences in ASR quality between dialects. Subsequently, we want to find ways to improve performance for underrepresented dialects.\n"}
{"question": "What do we do to deal with the collected data?", "evidence": "There is a great deal of variety in the structural configurations in which ellipsis can occur. In tables 1 and 2 we define structures for ellipsis and antecedent occurrences.In all structures, there is an ellipsis occurrence, and the question targets the ellipsis occurrence. Furthermore, each ellipsis example is paired with a non-ellipsis version. We generate large numbers of examples of each structure by performing random substitutions for both the subject and verb. The substitution lists are given in the appendix, along with samples of each structure and the size of the resulting sets. ", "options": ["A. we only define structures for ellipsis.", "B. In all structures ,an ellipsis occur randomly.", "C. Ensure that each example with ellipsis is paired with a corresponding version that does not contain ellipsis.", "D. We generate large numbers of examples of each structure by manually pairing the subject and verb. ", "Data\nThere is a great deal of variety in the structural configurations in which ellipsis can occur. In tables 1 and 2 we define structures for ellipsis and antecedent occurrences.\nIn all structures, there is an ellipsis occurrence, and the question targets the ellipsis occurrence. Furthermore, each ellipsis example is paired with a non-ellipsis version. We generate large numbers of examples of each structure by performing random substitutions for both the subject and verb. The substitution lists are given in the appendix, along with samples of each structure and the size of the resulting sets. \n"], "answer": "C", "content": "\nIntroduction\nEllipsis is a fundamental feature of human language, occurring in all registers, where parts of sentences are omitted, although the missing parts are essential for understanding the meaning. The following is an example of Verb Phrase Ellipsis (VPE) (Bos and Spenader, 2011) :\n(1) William went running. Harold did too.\n(1) is understood as asserting that Harold went running; that is, the hearer or reader naturally fills in the missing material. This is done by identifying the antecedent VP, went running, in the first sentence. The following is the non-elliptical counterpart of (1):\n(2) William went running. Harold went running too.\nWith such examples, we can test understanding of ellipsis by targeting the ellipsis phrase with a simple Yes/No question:\n(3) Did Harold go running?\nIf a system answers the question incorrectly for (1), the ellipsis example, but answers correctly for (2), the non-elliptical counterpart of (1), we can ascribe the result specifically to the challenge of ellipsis, since the examples are otherwise identical.\nAs with pronominal anaphora and other discourse processes, there is great flexibility in the way ellipsis can occur in discourse. Example (1) involves two simple adjacent sentences. It is also possible for ellipsis or the antecedent to occur in embedded clauses. Furthermore, ellipsis can occur either before or after the antecedent. Finally, an arbitrary amount of material can intervene between the antecedent and the ellipsis occurrence.\nIn this paper, we propose the challenge of ellipsis-dependent reasoning. This challenge consists of examples involving an ellipsis clause, the target. Each ellipsis example is paired with its nonelliptical counterpart, where the target clause is overt rather than elliptical. We then pose a question whose answer is dependent on the target clause. A key aspect of the challenge is that ellipsis occurrences are possible in a variety of diverse structural configurations. We test a series of GPT-3 models (GPT) on several such ellipsis structures.\n\nRelated Work\nThere is a large literature concerning the probing of language models from a variety of perspectives. Furthermore, there has been substantial work specifically addressing ellipsis in NLP. In this paper, we are proposing the challenge of ellipsisdependent reasoning. This proposal builds on various strands of prior research; below we consider some particularly relevant aspects of this literature.\n\nProbing Models for Knowledge\nThe Winograd Schema (Kocijan et al. (2022) ; Levesque et al. (2012)) involves test examples that use the linguistic problem of pronoun resolution to gain insight into the commonsense reasoning abilities of an AI system. To do this, the Winograd Schema requires pairs of examples that differ only in one specific, small way, as in ( 4):\n(4)\nThe city councilmen refused the demonstrators a permit because they feared/advocated violence.\nWith \"feared\", the pronoun \"they\" refers to the city councilmen, while with \"advocated\", it refers to the demonstrators. 2019): here, examples are constructed which test specific aspects of linguistic knowledge of a system, namely, whether BERT embeddings \"encode hierarchical information\". For example, a task is defined to identify the main auxiliary verb in a sentence, even in cases where the main auxiliary is not the first auxiliary verb to appear. Training and testing datasets are automatically generated using a context-free grammar for several such tasks involving hierarchical syntactic information.\n\nAnaphora and Question Answering\nQuoref (Dasigi et al. (2019) ; Zhang and Zhao (2022) ) is a question-answer dataset designed so that correct answers cannot be given unless a coreference relationship is correctly identified; that is, the reasoning involved in question answering is dependent on resolving coreference. This is, in a sense, the inverse of the Winograd schema, where resolving coreference is dependent upon reasoning. Just as with the Winograd schema, it is difficult to ensure that resolving this dependency is required for system success. (Dasigi et al., 2019)[p. 1] note that this is \"challenging, because it is hard to avoid lexical cues that shortcut complex reasoning\", and based on a random sample, found that coreference resolution was required for 78% of questions.\n\nEllipsis as a Task\nThere has been substantial work on ellipsis as a discrete NLP task (Khullar (2020) 2021) frame ellipsis as a question-answering task, i.e., a task of locating an antecedent, understood as a span of tokens in context. Aralikatte et al. (2021) report token F1 scores of 78.66 for VPE and 86.01 for sluicing, an-other form of ellipsis. It's important to note that the task here, of antecedent identification, is a sub-part of the ellipsis challenge. Before the antecedent is identified, an ellipsis occurrence must be identified, and after the antecedent is identified, it must be interpreted, or \"reconstructed\", at the ellipsis site.\n\nRelevance for Ellipsis-Dependent Reasoning\nThe specific task of ellipsis is addressed in work like that of Aralikatte et al. (2021) , but the key difference here is that we are probing for a complete solution to the ellipsis problem. The proposed ellipsis-dependent reasoning task involves a question that can only be answered correctly if the ellipsis is properly identified and interpreted. This combines aspects of the preceding works in a novel way: like the Winograd schema and the syntactic work by Lin et al. (2019) , it probes for what we see as a specific type of psychologically-defined knowledge: namely, a representation of context that supports the resolution of ellipsis. Similarly to the work on Quoref, we use targeted questions to probe for discourse-related knowledge.\nThere is an extensive literature on the contextual interpretation of natural language, resting on the idea of a dynamic, ongoing model of discourse. For example, Discourse Representation Theory (Kamp, 1981 ) describes a semantic model supporting discourse phenomena such as pronominal and temporal anaphora, and Sag and Hankamer (1984) argue explicitly that ellipsis and other such phenomena are interpreted with respect to a discourse model (Garnham, 2010) . As one study puts it, \"Interpreting a verb-phrase ellipsis (VP ellipsis) requires accessing an antecedent in memory, and then integrating a representation of this antecedent into the local context\" (Martin and McElree, 2008) . In this paper, we seek to determine whether a large language model is capable of such an interpretive process.\n\nData\nThere is a great deal of variety in the structural configurations in which ellipsis can occur. In tables 1 and 2 we define structures for ellipsis and antecedent occurrences.\nIn all structures, there is an ellipsis occurrence, and the question targets the ellipsis occurrence. Furthermore, each ellipsis example is paired with a non-ellipsis version. We generate large numbers of examples of each structure by performing random substitutions for both the subject and verb. The substitution lists are given in the appendix, along with samples of each structure and the size of the resulting sets. 1\n\nTest\nFor each instantiation of a given structure, we produce paired ellipsis and non-ellipsis examples, with an associated Yes/No question. We randomly select 1000 examples for each structure, including 500 ellipsis examples and 500 examples which are their non-elliptical counterparts. Each example is presented to the system, preceded by the text, \"Please give a Yes or No answer:\". We test five GPT-3 models on these structures: Davinci-003, Davinci-002, Curie-001, Babbage-001, and Ada-001. According to the GPT-3 documentation, Davinci-003 is the most powerful model and Ada-001, the least.\n\nResults\nFigure 1 gives the accuracy for ellipsis and nonellipsis, for each of the five models. We have set up the test examples so that an ellipsis example is paired with a non-ellipsis example that is otherwise identical. Because of this, we claim that the difference in accuracy of the non-ellipsis case vs. the ellipsis case provides a measurement of the difficulty specifically posed by ellipsis. For all but the least powerful model, Ada, the non-ellipsis accuracy is substantially higher than ellipsis accuracy, supporting the hypothesis that ellipsis-dependent reasoning presents a difficult challenge for these models. While the Ada model actually performs somewhat better for ellipsis than non-ellipsis, this is not because the Ada model does well with ellipsis cases; rather, the model has great difficulty with both the ellipsis and non-ellipsis cases, and is close to a random guessing baseline of .50.\nIn figures 2 through 6, we present results for each model. We show the accuracy for each structure, for both the ellipsis version and the non-ellipsis version. Consider the most powerful models, Davinci-003 and Davinci-002. In figures 2 and 3, we can see that ellipsis is not difficult in the first two structures: 2Sent (Separate Sentence) and 1Sent (Conjoined Sentence). Here the accuracy is nearly perfect for the ellipsis as well as the non-ellipsis condition. However, in all the other structures, there is a large divergence in accuracy between ellipsis and nonellipsis, for both the Davinci-003 and Davinci-002 models. Subordination for either antecedent or ellipsis is quite challenging, with accuracies ranging from 48.8 to 85.8. The Backwards and Two Actions structures are even more difficult for ellipsis.\n\nAnalysis\nFor the two most powerful models, it is clear that ellipsis poses a difficult challenge, except in the two simplest ellipsis structures. For the less powerful models, the picture is mixed. For these models, the non-ellipsis examples are themselves a difficult challenge, so we are not able to observe the specific difficulties posed by ellipsis.\nAs we can see in figure 1 , the Davinci-002 model performs somewhat better overall than Davinci- 003, on both ellipsis and non-ellipsis. However, figures 2 and 3 show that the advantage of Davinci-002 on ellipsis is exclusively due to the subordinate antecedent construction. In every other ellipsis structure, Davinci-003 performs better than Davinci-002.\nThere are striking differences in the distribution of errors. For both the Davinci-003 and Davinci-002 models, errors are nearly always false negatives -that is, incorrect \"No\" answers. There are virtually no false positives, either for the ellipsis case or nonellipsis case. For the other three models, there are many errors of each type, with a much higher ratio of false positives.\n\nConclusion\nMost of the current rapid progress in NLP is due to pre-trained large language models. GPT-3 is an impressive publicly available collection of such models, and is able to perform in a way that suggests human-level understanding. Because of this, it is important to explore areas in which it might still differ from human language understanding. In this paper we have argued that ellipsis is one such area. For many simple ellipsis structures, the most powerful GPT-3 models struggle, with accuracies far lower on ellipsis examples than on non-elliptical counterparts.\nIn many ways, GPT-3 appears to understand the texts that it processes, often being able to answer questions that appear to rely on sophisticated reasoning. However, the challenge of ellipsisdependent reasoning provides evidence that GPT-3 is not able to understand in anything like the way humans do. \n"}
{"question": "What are the possible reasons for social scientists to using the tools of computer science?", "evidence": "•	Computational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.", "options": ["A. Because the numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. ", "B. Social scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.", "C.  For the speed, scale, granularity, and consistency of computer science.", "D. Because computational text analysis methods combine natural language processing with social science", "Computational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis."], "answer": "C", "content": "\nIntroduction\nComputational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period (Lazer and Radford, 2017; van Atteveldt and Peng, 2018) . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" (Boydstun et al., 2014) . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.\nThe conclusion of this reasoning, and the dominant stance in computational social science, is a reliance on machines alone to answer questions in the field, surrendering to their supposed objectivity * Equal contribution.\nor impartiality. Can a machine's output go beyond descriptive catalogs of evidence, accelerating understanding of processes and motivations? From our experience, computers are nowhere near supplanting humans in interpreting social science results.\n1 An interdisciplinary inquiry must go farther than matching computational techniques to social science questions (O'Connor et al., 2011; Nguyen et al., 2020) . It embraces synergistic methodology and connects the norms and standards of evidence from both. This means partnering computer science's preference for the structured, generalizable, and objective with the unstructured, critical, and contextual which the social sciences champion. This level of interdisciplinarity addresses the question raised by descriptive findings: So what?\nWe see theory as the solution, empowering rather than shackling investigations. What this paper advocates is not one particular theory-certainly these are myriad, and \"even subject matter which has been under intensive and prolonged study remains at the unsettled periphery of research\" (Nagel, 1963) . Instead, we expand on our prior work (Dore and McCarthy, 2022) to clarify calls echoed for decades by computational and social science (McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . Underlying each, we find, is the urge to return to theory, which we espouse herein.\n\nDescription vs. Integration\nWe contrast descriptive findings and theoretical analysis. An example of a descriptive finding is that an apple falls, or that it falls faster when pushed than dropped, or even that it falls at a particular rate estimated with some standard error by a complex interpolation. A theoretical analysis of the same phenomenon, credited to Newton, is that a fundamental force acts upon the apple, and that this same force governs the motion of the heavens. The theoretical analysis links the finding about the world critically to a broader body of knowledge and context.\nDespite advances in causal inference in NLP, the descriptive is all that a machine can provide to the social sciences (Feder et al., 2021) . Certainly the methods of computational text analysis have advanced since the General Inquirer (Stone and Hunt, 1963) and Mosteller and Wallace's statistical inference of text authorship (1963) . But methods are means, not ends. They uncover more descriptive findings in data: the rate of an apple's fall, the topics of refugees' tweets (Walk et al., 2022) , the space given to marginalized groups in textbooks (Lucy et al., 2020) , or patterns of state censorship (Bamman et al., 2012; King et al., 2013) .\nThe foils to descriptive findings are integrative findings (Hofman et al., 2021) , which offer causal explanations that enable future predictions-a theory, or as a 'model' in the sense of the Standard Model, rather than of a statistical model. Integrative findings can either offer new theories or couch their explanations in existing theories-but the theory is essential either way.\n\nWe Don't Integrate\nTo contrast descriptive and integrative findings, we reviewed approximately 60 papers in computational text analysis published in *ACL venues. In Table 1 , we describe several of these in terms of their descriptive or theory-grounded contributions.\n2 Descriptive papers may refer to social science theories or make generalizable claims, as when Demszky et al. (2019) write, \"The shooter's race appears to play a role in topic preference: if the shooter is white, Democrats become more likely to focus on shooter's identity,\" but they do not link to the two to each other. An excellent theory-grounded quantitative work is Nelson (2021) ; she confirms some of the most compelling features of identity theory, specifically that identities based on race were most distinguished by cultural discourse, whereas those based on gender by the domestic and the economic discourse. Similarly, we conducted theory-grounded quantitative work to investigate the application of the protest paradigm and thematic framing in how westernand Hong Kong based newspapers portray protests in Hong Kong (McCarthy et al., 2021; McCarthy and Dore, 2022) . Generally, it remains challenging to find computational social science papers in *ACL venues that go beyond description and prediction, advancing theory. Why is this? We believe it stemmed from the field's \"empirical turn\".\n3 Few remember when the meetings of ACL offered a few dozen papers, all entrenched in formalisms and linguistic theories. Arguably, 1996 was a turning point when the founders of SIGDAT held the first EMNLP at Penn under the auspices of the ACL. 4 This gave a spotlight to the few but growing empiricists in the field and drew in more.\nEMNLP began a half-decade of measurable reorganization the field (Anderson et al., 2012) . That EMNLP remains affiliated with ACL keeps the language-focused machine learning practitioners in our tent. The slow blurring of boundaries between each *ACL conference's expectations (Church, 2020) increases this unity. Both groups belong under this tent. But without a doubt, one group's voice is becoming less heard.\nPublication venues within the ACL focus on methods over theory. 5 Techniques are taken off the shelf without critical examination because these are \"the best\" (often \"state of the art\") for their purposes (Ethayarajh and Jurafsky, 2020) . This widens the gap between theoretical and empirical work. 6 Hopkins and King (2010) claim, \"computer scientists may be interested in finding the needle in the haystack. . . social scientists are more commonly interested in characterizing the haystack\"-evincing the value of broader context. 7 Wallach (2018), quoting Hopkins and King, explains that the two groups 3 A lesser reason is the challenge of serving two masters: adequately covering both the theoretical and methodological components within 8 pages. We recently received two reviews for an *ACL submission: one advocating for more of the social science context in the main text by eschewing methods to the appendix, and the other instructing us to do the opposite. 4 And its predecessor the Workshop on Very Large Corpora. 5 This is due to the outsized influence of computer science, often seen as the science of method (Hoare and Jones, 1989; Shapiro, 2001) , when not instead seen as an engineering discipline (Rapaport, 2005).\n6 A related criticism is that empirical research has narrowed to focus on 'easy' questions that its tools can address (Coleman, 1986; Baden et al., 2021) , especially when research questions are baked into the design of the task. 7 As evidence, see Siegel (2018) : \"We usually don't know about causation, and we often don't necessarily care. . . the objective is more to predict than it is to understand the world. . . It just needs to work; prediction trumps explanation.\" Descriptive Chang et al. (2009) The article presents new quantitative methods to measure semantic meaning in inferred topics. The authors emphasize the qualitative relevance of their findings as it validates the use of topics for corpus exploration and information retrieval. However, their working hypothesis and empirical findings are not connected to the extremely relevant field of communication theory. Bamman et al. (2012) The article presents the first large-scale analysis of political content censorship in social media. The authors miss the opportunity to relate their hypothesis and findings to censorship theory, a natural theoretical context for the research, which would strengthen the relevance and generalizability of the findings. Field et al. (2018) The article discusses media manipulation in Russia in the context of agenda-setting and framing, the tools that Russian state-owned (or heavily influenced) media outlets use to distract public attention from domestic economic politics. The authors implicitly refer to propaganda theory and autocratic theory throughout the article even though their findings are not discussed in relation to these theories. Demszky et al. (2019) The article applies \"a more comprhensive NLP framework to study linguistic aspects of polarization in social media\". While the article implicitly refer to theories of social conformity and social conflict, the findings are not linked or discussed (either explicitly or implicitly) to the theoretical frameworks that the authors touch on in their \u00a71.\n\nIntegrative\nDiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.are interested in very different research  questions, and that computational social science must be more than computer science with social data; it must strive for valid explanatory models. In the same vein, at ACL 2022, ACL fellow Eduard Hovy remarked that NLP must be more than \"just machine learning on corpora\". Social scientists are also coming to terms with the meaning of computational techniques applied more often in social science (Bail, 2014; Biernacki, 2015; Lee and Martin, 2015; Spillman, 2015) . The focus of the debates, however, is on which methods are best suited to extract meaning from text, without addressing any theoretical considerations related to the methods or whether a theoretical framework for those methods even exists. The discussions on whether computational methods make social science research more efficient, reliable, and reproducible overtake attempts at theory-building.\n\nMoving Forward\nWe are not denying the value of computational approaches to analyzing text. Certainly, comput-ing can be an instrumental approach for modeling and understanding social complexity. This does not mean that other approaches, such as historical, ethnographic, or mathematical, become irrelevant. On the contrary, computational methods necessarily (whether awarely or not) rely on these earlier approaches to add value, in terms of improving our explanations and understanding (Radford and Joseph, 2020) .\nAs we are a field that prioritizes methods, consider the seminal book on methods in science: Abbott ( 2004) taxonomizes scientific ways of knowing. Its five broad categories are ethnography, historical narration, standard causal analysis, small-N comparison, and formal modeling. We in NLP myopically choose the third and fifth of these, ignoring the value of the others. But the broader point of Methods of Discovery is not methods. It is the research question. Any methodology should be grounded in the question, not incremental tweaks and reviewers' comfort (Church, 2020) . This admits even qualitative or mixed-method approaches to text analysis.\nThe role of humans in scientific inquiry is nothing new. Using qualitative analysis to complement quantitative techniques has its roots in Achen and Snidal (1989)'s recommendation to use historical case studies as a complement to statistical research.\n8\nTheir plea was strengthened by Verba's work in the early 1990s (Verba et al., 1993 (Verba et al., , 1995;; Verba, 1996) and Tarrow (1995) , who openly called for bridging qualitative and quantitative modes of research in social science. In doing so, they have enriched the field with critical methodological innovations (Gerring, 2004) , benefiting from the recognition that \"quantitative methods must augment humans, not replace them\" (Grimmer and Stewart, 2013, 4) .\nThe field can draw more from social science's rich tradition of inductive theory-building and interpretation to develop its theoretical approach-to prize either induction or deduction alone is a myth of scientific procedure (Thagard, 1988) , but the melding of the two opens new doors. Rather than eschewing the complexity (a criticism leveled by Baden et al., 2021) , it should put complexity at the center of its ontology on the basis that there are no immutable laws in social life or optimal solutions to social problems.\nSkepticism can linger toward findings not drawn from the standard practices of one's own field; indeed, social science was long skeptical of computational contributions (Armstrong, 1967) . We believe that this drives the hyperfocus on improving a few accepted methods instead of exploring more broadly. If the doorway between disciplines is only narrowly open, this reflects a lack of appreciation for each field's ways of knowing. The disciplinary divide keeps computational researchers from embracing methods beyond standard causal analysis or formal modeling, so the interpreter-centric richness allowed by histories, ethnographies, and small-N exploration are precluded.\n\nConclusion\nWe have explained the distinction between descriptive and theoretical findings as it pertains to computational text analysis. The bulk of work we found provided vast descriptive findings, often of high quality, but not giving back to questions of theory. We offer several suggestions on how to 'push the pendulum back' by prioritizing theory-building or theory-affirming research questions and accepting whichever methods are best suited toward answering it-not only the familiar and entrenched ones.\nWe are not the first to advocate for a shift in the patterns of applying computational techniques to real-world problems. There is a steady drumbeat from voices in the field advocating careful approaches (Nagel, 1963; McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . What we see underlying all of thesethose writing against 'mathiness' and speculation, advocating for clear evaluation over anecdotes, criticizing textual researchers' dilution of conceptual standards, highlighting work that ties linguistic information into complex models-is an unspoken, perhaps unrealized, call for a return to theory.\nNot only do we aver that incorporating theory is essential; but also, other fields have strengthened themselves when espousing organizing principles beyond those of their progenitors. Behavioral economics is a success story here. It transcended the neat (but psychosocially stripped) mathematics it draws from to acknowledge deviations from rationality and blend economics with cognitive science (Kahneman and Tversky, 1979; Thaler, 1980; Thaler and Sunstein, 2009) .\nFor scientific-not simply engineeringadvances to arise from the *ACL community, authors and reviewers alike must resist the temptation toward incremental, 'safe' research and follow Church (2005) : \"Controversial papers are great; boring unobjectionable incremental papers are not.\" In reviewing new research, we should privilege not only work that presents new and unusual computational methods, but also interactions between computational and humanistic approaches to answering research questions. EMNLP was founded because of reviewing biases at ACL against groundbreaking methodological advances, and since then the two have homogenized; \"EMNLP reviewing is no longer much of a differentiator\" (Church, 2020) . We found that theoretically grounded findings in text analysis are often published in non-*ACL venues (Table 1 ), but ACL sets the standard for work involving computational text analysis and NLP. Is there no home for groundbreaking integrative or interdisciplinary work in *ACL, such that a new venue is required? Or can we adapt our standards to invite deeper connections to theory and new ways of knowing?\n"}
{"question": "What does the most related previous work intend to do?", "evidence": "	Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. ", "options": ["A. Quantify bias", "B. Remove bias algorithmically from the models", "C.  Find thchniques to modify the dataset and further fine-tune the model", "D. Addressing gender bias is possible even with a limited amount of training data samples.", "Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. "], "answer": "C", "content": "\nIntroduction\nRecently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019 ; * Equal Contribution Barocas et al., 2017; Kurita et al., 2019) . Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\nPrevious work (Nadeem et al., 2021) , (Nangia et al., 2020a) , (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021) , we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop bias mitigation strategies feasible.\n\u2022 We introduce a novel data sampling technique that utilises LLMs to mine for the most biased samples from a dataset and can benefit existing state-of-the-art debiasing methods. When used for debiasing a model, these few samples serve as exemplars and induce large reductions in gender bias.\n\nGender Bias in BERT Predictions\nMost-biased data samples ____ is very good at cooking but not great at work. \n\nReduced Gender Bias in BERT Predictions\nFigure 1 : Our method can be summarized as a combination of bias discovery and mitigation. First, we use a pre-trained LLM to find the most gender-biased samples. Then, we apply our data intervention techniques and use these modified training samples to fine-tune the model. Experiments show that our method is very effective at reducing gender bias, outperforming three state-of-the-art baselines and being comparable to two other baselines.\n\nRelated Work\nIn recent years, there has been growing concern about the bias/stereotypical discriminatory behavior by NLP models, particularly concerning gender. Several studies have investigated the presence of gender bias in various NLP tasks and proposed methods for mitigating it.\nOne line of research has focused on analyzing the extent of gender bias in pre-trained language models such as BERT and GPT-2. These studies have found that these models exhibit a significant amount of gender bias in their word embeddings for BERT (Jentzsch and Turan, 2022) and for GPT-2 (Kirk et al., 2021) and are prone to making stereotypical gender-based predictions (e.g., assuming that a doctor is male and a nurse is female). A standard evaluation metric used in this line of research is Stereotype metrics such as StereoSet (Nadeem et al., 2021) , which evaluates the model's ability to predict gender stereotypes and CrowS pairs (Nangia et al., 2020b) which measure whether a model generally prefers more stereotypical sentences. A similar line of work is gender bias tests proposed in BIG-bench (Srivastava et al., 2022) . The tests assess the language model's gender biases, stereotypes, and ability to infer gender information. It evaluates gender bias and stereotype between male and female, and gender minority bias and stereotype between majority and minority. It also examines the model's language modeling performance, which can be affected during de-biasing.\nAnother line of research has proposed methods for debiasing these models. These methods can be broadly categorized into two groups: data-based and algorithm-based. Data-based methods aim to reduce bias by removing or altering biased words from the training set. In contrast, algorithm-based methods aim to modify the model's architecture or training procedure to reduce bias. One popular databased method is \"uncertainty sampling\" (Lewis and Gale, 1994) , where the model is trained on the instances that it is most uncertain about, which can help to reduce bias by forcing the model to learn from a diverse set of examples. A popular algorithmbased method is \"Adversarial Debiasing\" proposed by Zhang et al. (2018) , which fine-tunes the model using an adversarial loss to make it less sensitive to sensitive attributes such as gender. OSCar proposed by Dev et al. (2021) , is another algorithm based method that utilizes the idea of disentangling \"problematic concepts\" like occupation and gender relationship instead of removing them altogether. MABEL (He et al., 2022) has both algorithm and data-based components, as it first augments the training data by swapping gender words and then applies a contrastive learning objective and alignment via entailment pairs. Their data augmentation strategy is similar in spirit to the data intervention techniques we propose, however our analysis does not require training auxiliary models and uses significantly lesser data.\nData-based methods include the \"Equalization\" technique proposed by Bolukbasi et al. (2016) , which aims to equalize the representation of genderspecific words in the embedding space, the \"Counterfactual Data Augmentation\" (CDA) method proposed by Zimmermann and Hoffmann (2022) Pre-trained LLMs are biased towards different genders, as seen in a simple mask-fill experiment using BERT. (Here, and in the rest of the paper, we assume a binary treatment of gender for simplicity.) The task is then to mask out the gender-related nouns and pronouns (such as he, she, her, woman, etc.) and get BERT to predict the masked words for the affected sequences in the dataset. Here, we consider a fixed list of gender-specific words curated from previous work (Lu et al., 2018; Zmigrod et al., 2019) and neutral words list 1 . We finally compute the \"total confidence difference\" as the sum of differences in the model's prediction confidence for each gender-word pair (such as confidence of predicting he \u2212 she, man \u2212 woman, etc.). Formally, we define total confidence difference as\nEQUATION\n))| where f (x) represent the confidence of model's prediction, N is the total number of tokens in the dataset and x is the tokenized gender word. The higher this number, the more biased the model is concluded to be. We compute the metric at token level and ensure that each of the gender word gets tokenized into exactly one token by initially extending the tokenizer with our gender word list. The top 3 biased gender-word pairs in StereoSet are shown in Table 1 . Intuitively, our technique for gauging bias in LLMs is sensitive to the fixed word list used to represent the sensitive attributes (here, gender). In Table 2 , we show the number of words covered by the word list used for both WikiText-2 and StereoSet datasets.\n\nData Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vocabulary of the model and try to balance the confidence of prediction on opposite-gender words by using phrases instead. Thus, we call our final approach random-phrase-masking as we instead substitute words with phrases that reflect the equality of gender. This approach not only reduces gender bias but also preserves the original meaning of the sentence in most cases. In our approach, we chose the phrases and order of gender words at random with equal probability. Additionally, we hypothesize that the choice of the dataset for fine-tuning is also essential. We choose two datasets: the WikiText-2 (Merity et al., 2017) dataset, which has implicit gender bias since its sources from Wikipedia articles, and the Stere-oSet dataset (Nadeem et al., 2021) , which has explicit/more gender bias as it has been designed to evaluate gender bias. WikiText-2 2 has 600 train articles and roughly 2M tokens while StereoSet 3 (dev) has 2123 samples out of which we only consider 800 samples which are not unrelated. Naturally, our data intervention method should work better on a dataset with training examples with gender bias while being devoid of meaningful gender associations like \"She needs a gynecologist,\" where the gender of the person is important. By testing our method on both datasets, we can understand the sensitivity of our approach to the quality of training samples used.\n\nBias Evaluation Metrics\nWe focus on evaluating the bias of a model while also measuring its language modeling capability. The ideal model would not just be one with the least bias but also one which does not compromise its language modeling performance. The dual estimation of bias and performance of a model was proposed in the StereoSet benchmark (Nadeem et al., 2021) , with the Language Modeling Score (LMS) measuring the percentage of times a meaningful token is predicted for the mask as opposed to a meaningless token, the Stereotype Score (SS) measuring the percentage of times the model predicted a stereotypical word as compared to an anti-stereotypical word, and an idealized CAT score (ICAT) combining the LMS and SS score into a single metric. An ideal model has an ICAT score of 100, while the worst biased model has an ICAT score of 0. We additionally evaluate the CrowS-Pairs benchmark (Nangia et al., 2020a) , which captures data with greater diversity in both the stereotypes expressed and the structure of sentences (50 is ideal). However, we note that the Crow-S benchmark is much more limited compared to StereoSet (Nadeem et al., 2021) in terms of both the volume and variety of linguistic phenomenon relating to gender bias it covers.\n\nExperiments\nWe compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work. For a fairer comparison, we run the baselines with the same size (100) of the training set as our method. For all of our experiments, we consider the \"bert-base-uncased\" pre-trained model available from HuggingFace. For fine-tuning our model, we select a varying number of most-biased training samples (10, 50, and 100) from the WikiText-2 and StereoSet (we only use the dev set) datasets, as discussed in section 4. We also compare this to a random selection of data points as an ablation study. On the selected dataset, we apply our interventions and obtain the modified dataset, which is then used to fine-tune our pre-trained model using masked language modeling (MLM) loss. The key point is that we only fine-tune the model on the gender words conditioned on the remaining text, significantly reducing the fine-tuning time. We perform ablations on various types of interventions as discussed in Table 7 . The model is trained for 30 epochs, with a learning rate of 0.001 and AdamW optimizer. We ran all of our experiments on NVIDIA Tesla T4 GPU on Google Colab for roughly 48 hours. For all experiments, we report the numbers as the mean and standard deviations (6) of 3 different runs. Our experiment code can be found here. 4 \n\nResults\nTable 4 shows the StereoSet and Crow-S scores for our baselines and our best-performing interventions on the WikiText-2 Dataset. In the StereoSet benchmark, we observe that random-phrase-masking obtains lower SS than all other baselines. On the Crow-S benchmark, random-phrase-masking does better than thre of the baselines except Sen-tenceDebias which achieves slightly better scores. While random-phrase-masking results in lower SS scores than neutral-masking, it also obtained [MASK] is very good at cooking but not great at [MASK] work. she is very good at cooking but not great at her work.\nhe is very good at cooking but not great at farm work. Being a mother is not easy since she will have to stay home and take care of the child.\nBeing a father is not easy since one will have to stay home and take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\n\nShe is very caring and kind but not good at what she does.\nHe is very caring and kind but not good at what he does.\n\nInput Sentence:\nOutput of Biased Model:\n\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to modify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the same pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the our method is successfully able to reduced biased substitutions. very low LMS scores. We attribute this performance degradation to the blunt substitution of phrases that our method uses, which might lead to odd-sounding sentences. In the Crow-S benchmarks, we see similar behavior and find that random-phrase-masking does better than neutral-masking. Since we believe that our method is sensitive to the choice of the dataset, we also present results on the StereoSet (dev) dataset 6. In Figure 2 , we perform a qualitative analysis of our proposed approach and find that random-phrase-masking is able to flip the predictions on fill-mask tasks for stereotypical sentences.\n\nConclusion\nIn this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs. We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples. Evaluation of our methods on state-of-the-art bias benchmarks empirically suggests that our methods effectively reduce gender bias. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n"}
{"question": "Which of the following is not an example of a descriptive finding?", "evidence": "  We contrast descriptive findings and theoretical analysis. An example of a descriptive finding is that an apple falls, or that it falls faster when pushed than dropped, or even that it falls at a particular rate estimated with some standard error by a complex interpolation. A theoretical analysis of the same phenomenon, credited to Newton, is that a fundamental force acts upon the apple, and that this same force governs the motion of the heavens. The theoretical analysis links the finding about the world critically to a broader body of knowledge and context.  ", "options": ["Apple falls at a particular rate estimated with some standard error by a complex interpolation."], "answer": "A", "content": "\nIntroduction\nComputational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period (Lazer and Radford, 2017; van Atteveldt and Peng, 2018) . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" (Boydstun et al., 2014) . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.\nThe conclusion of this reasoning, and the dominant stance in computational social science, is a reliance on machines alone to answer questions in the field, surrendering to their supposed objectivity * Equal contribution.\nor impartiality. Can a machine's output go beyond descriptive catalogs of evidence, accelerating understanding of processes and motivations? From our experience, computers are nowhere near supplanting humans in interpreting social science results.\n1 An interdisciplinary inquiry must go farther than matching computational techniques to social science questions (O'Connor et al., 2011; Nguyen et al., 2020) . It embraces synergistic methodology and connects the norms and standards of evidence from both. This means partnering computer science's preference for the structured, generalizable, and objective with the unstructured, critical, and contextual which the social sciences champion. This level of interdisciplinarity addresses the question raised by descriptive findings: So what?\nWe see theory as the solution, empowering rather than shackling investigations. What this paper advocates is not one particular theory-certainly these are myriad, and \"even subject matter which has been under intensive and prolonged study remains at the unsettled periphery of research\" (Nagel, 1963) . Instead, we expand on our prior work (Dore and McCarthy, 2022) to clarify calls echoed for decades by computational and social science (McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . Underlying each, we find, is the urge to return to theory, which we espouse herein.\n\nDescription vs. Integration\nWe contrast descriptive findings and theoretical analysis. An example of a descriptive finding is that an apple falls, or that it falls faster when pushed than dropped, or even that it falls at a particular rate estimated with some standard error by a complex interpolation. A theoretical analysis of the same phenomenon, credited to Newton, is that a fundamental force acts upon the apple, and that this same force governs the motion of the heavens. The theoretical analysis links the finding about the world critically to a broader body of knowledge and context.\nDespite advances in causal inference in NLP, the descriptive is all that a machine can provide to the social sciences (Feder et al., 2021) . Certainly the methods of computational text analysis have advanced since the General Inquirer (Stone and Hunt, 1963) and Mosteller and Wallace's statistical inference of text authorship (1963) . But methods are means, not ends. They uncover more descriptive findings in data: the rate of an apple's fall, the topics of refugees' tweets (Walk et al., 2022) , the space given to marginalized groups in textbooks (Lucy et al., 2020) , or patterns of state censorship (Bamman et al., 2012; King et al., 2013) .\nThe foils to descriptive findings are integrative findings (Hofman et al., 2021) , which offer causal explanations that enable future predictions-a theory, or as a 'model' in the sense of the Standard Model, rather than of a statistical model. Integrative findings can either offer new theories or couch their explanations in existing theories-but the theory is essential either way.\n\nWe Don't Integrate\nTo contrast descriptive and integrative findings, we reviewed approximately 60 papers in computational text analysis published in *ACL venues. In Table 1 , we describe several of these in terms of their descriptive or theory-grounded contributions.\n2 Descriptive papers may refer to social science theories or make generalizable claims, as when Demszky et al. (2019) write, \"The shooter's race appears to play a role in topic preference: if the shooter is white, Democrats become more likely to focus on shooter's identity,\" but they do not link to the two to each other. An excellent theory-grounded quantitative work is Nelson (2021) ; she confirms some of the most compelling features of identity theory, specifically that identities based on race were most distinguished by cultural discourse, whereas those based on gender by the domestic and the economic discourse. Similarly, we conducted theory-grounded quantitative work to investigate the application of the protest paradigm and thematic framing in how westernand Hong Kong based newspapers portray protests in Hong Kong (McCarthy et al., 2021; McCarthy and Dore, 2022) . Generally, it remains challenging to find computational social science papers in *ACL venues that go beyond description and prediction, advancing theory. Why is this? We believe it stemmed from the field's \"empirical turn\".\n3 Few remember when the meetings of ACL offered a few dozen papers, all entrenched in formalisms and linguistic theories. Arguably, 1996 was a turning point when the founders of SIGDAT held the first EMNLP at Penn under the auspices of the ACL. 4 This gave a spotlight to the few but growing empiricists in the field and drew in more.\nEMNLP began a half-decade of measurable reorganization the field (Anderson et al., 2012) . That EMNLP remains affiliated with ACL keeps the language-focused machine learning practitioners in our tent. The slow blurring of boundaries between each *ACL conference's expectations (Church, 2020) increases this unity. Both groups belong under this tent. But without a doubt, one group's voice is becoming less heard.\nPublication venues within the ACL focus on methods over theory. 5 Techniques are taken off the shelf without critical examination because these are \"the best\" (often \"state of the art\") for their purposes (Ethayarajh and Jurafsky, 2020) . This widens the gap between theoretical and empirical work. 6 Hopkins and King (2010) claim, \"computer scientists may be interested in finding the needle in the haystack. . . social scientists are more commonly interested in characterizing the haystack\"-evincing the value of broader context. 7 Wallach (2018), quoting Hopkins and King, explains that the two groups 3 A lesser reason is the challenge of serving two masters: adequately covering both the theoretical and methodological components within 8 pages. We recently received two reviews for an *ACL submission: one advocating for more of the social science context in the main text by eschewing methods to the appendix, and the other instructing us to do the opposite. 4 And its predecessor the Workshop on Very Large Corpora. 5 This is due to the outsized influence of computer science, often seen as the science of method (Hoare and Jones, 1989; Shapiro, 2001) , when not instead seen as an engineering discipline (Rapaport, 2005).\n6 A related criticism is that empirical research has narrowed to focus on 'easy' questions that its tools can address (Coleman, 1986; Baden et al., 2021) , especially when research questions are baked into the design of the task. 7 As evidence, see Siegel (2018) : \"We usually don't know about causation, and we often don't necessarily care. . . the objective is more to predict than it is to understand the world. . . It just needs to work; prediction trumps explanation.\" Descriptive Chang et al. (2009) The article presents new quantitative methods to measure semantic meaning in inferred topics. The authors emphasize the qualitative relevance of their findings as it validates the use of topics for corpus exploration and information retrieval. However, their working hypothesis and empirical findings are not connected to the extremely relevant field of communication theory. Bamman et al. (2012) The article presents the first large-scale analysis of political content censorship in social media. The authors miss the opportunity to relate their hypothesis and findings to censorship theory, a natural theoretical context for the research, which would strengthen the relevance and generalizability of the findings. Field et al. (2018) The article discusses media manipulation in Russia in the context of agenda-setting and framing, the tools that Russian state-owned (or heavily influenced) media outlets use to distract public attention from domestic economic politics. The authors implicitly refer to propaganda theory and autocratic theory throughout the article even though their findings are not discussed in relation to these theories. Demszky et al. (2019) The article applies \"a more comprhensive NLP framework to study linguistic aspects of polarization in social media\". While the article implicitly refer to theories of social conformity and social conflict, the findings are not linked or discussed (either explicitly or implicitly) to the theoretical frameworks that the authors touch on in their \u00a71.\n\nIntegrative\nDiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.are interested in very different research  questions, and that computational social science must be more than computer science with social data; it must strive for valid explanatory models. In the same vein, at ACL 2022, ACL fellow Eduard Hovy remarked that NLP must be more than \"just machine learning on corpora\". Social scientists are also coming to terms with the meaning of computational techniques applied more often in social science (Bail, 2014; Biernacki, 2015; Lee and Martin, 2015; Spillman, 2015) . The focus of the debates, however, is on which methods are best suited to extract meaning from text, without addressing any theoretical considerations related to the methods or whether a theoretical framework for those methods even exists. The discussions on whether computational methods make social science research more efficient, reliable, and reproducible overtake attempts at theory-building.\n\nMoving Forward\nWe are not denying the value of computational approaches to analyzing text. Certainly, comput-ing can be an instrumental approach for modeling and understanding social complexity. This does not mean that other approaches, such as historical, ethnographic, or mathematical, become irrelevant. On the contrary, computational methods necessarily (whether awarely or not) rely on these earlier approaches to add value, in terms of improving our explanations and understanding (Radford and Joseph, 2020) .\nAs we are a field that prioritizes methods, consider the seminal book on methods in science: Abbott ( 2004) taxonomizes scientific ways of knowing. Its five broad categories are ethnography, historical narration, standard causal analysis, small-N comparison, and formal modeling. We in NLP myopically choose the third and fifth of these, ignoring the value of the others. But the broader point of Methods of Discovery is not methods. It is the research question. Any methodology should be grounded in the question, not incremental tweaks and reviewers' comfort (Church, 2020) . This admits even qualitative or mixed-method approaches to text analysis.\nThe role of humans in scientific inquiry is nothing new. Using qualitative analysis to complement quantitative techniques has its roots in Achen and Snidal (1989)'s recommendation to use historical case studies as a complement to statistical research.\n8\nTheir plea was strengthened by Verba's work in the early 1990s (Verba et al., 1993 (Verba et al., , 1995;; Verba, 1996) and Tarrow (1995) , who openly called for bridging qualitative and quantitative modes of research in social science. In doing so, they have enriched the field with critical methodological innovations (Gerring, 2004) , benefiting from the recognition that \"quantitative methods must augment humans, not replace them\" (Grimmer and Stewart, 2013, 4) .\nThe field can draw more from social science's rich tradition of inductive theory-building and interpretation to develop its theoretical approach-to prize either induction or deduction alone is a myth of scientific procedure (Thagard, 1988) , but the melding of the two opens new doors. Rather than eschewing the complexity (a criticism leveled by Baden et al., 2021) , it should put complexity at the center of its ontology on the basis that there are no immutable laws in social life or optimal solutions to social problems.\nSkepticism can linger toward findings not drawn from the standard practices of one's own field; indeed, social science was long skeptical of computational contributions (Armstrong, 1967) . We believe that this drives the hyperfocus on improving a few accepted methods instead of exploring more broadly. If the doorway between disciplines is only narrowly open, this reflects a lack of appreciation for each field's ways of knowing. The disciplinary divide keeps computational researchers from embracing methods beyond standard causal analysis or formal modeling, so the interpreter-centric richness allowed by histories, ethnographies, and small-N exploration are precluded.\n\nConclusion\nWe have explained the distinction between descriptive and theoretical findings as it pertains to computational text analysis. The bulk of work we found provided vast descriptive findings, often of high quality, but not giving back to questions of theory. We offer several suggestions on how to 'push the pendulum back' by prioritizing theory-building or theory-affirming research questions and accepting whichever methods are best suited toward answering it-not only the familiar and entrenched ones.\nWe are not the first to advocate for a shift in the patterns of applying computational techniques to real-world problems. There is a steady drumbeat from voices in the field advocating careful approaches (Nagel, 1963; McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . What we see underlying all of thesethose writing against 'mathiness' and speculation, advocating for clear evaluation over anecdotes, criticizing textual researchers' dilution of conceptual standards, highlighting work that ties linguistic information into complex models-is an unspoken, perhaps unrealized, call for a return to theory.\nNot only do we aver that incorporating theory is essential; but also, other fields have strengthened themselves when espousing organizing principles beyond those of their progenitors. Behavioral economics is a success story here. It transcended the neat (but psychosocially stripped) mathematics it draws from to acknowledge deviations from rationality and blend economics with cognitive science (Kahneman and Tversky, 1979; Thaler, 1980; Thaler and Sunstein, 2009) .\nFor scientific-not simply engineeringadvances to arise from the *ACL community, authors and reviewers alike must resist the temptation toward incremental, 'safe' research and follow Church (2005) : \"Controversial papers are great; boring unobjectionable incremental papers are not.\" In reviewing new research, we should privilege not only work that presents new and unusual computational methods, but also interactions between computational and humanistic approaches to answering research questions. EMNLP was founded because of reviewing biases at ACL against groundbreaking methodological advances, and since then the two have homogenized; \"EMNLP reviewing is no longer much of a differentiator\" (Church, 2020) . We found that theoretically grounded findings in text analysis are often published in non-*ACL venues (Table 1 ), but ACL sets the standard for work involving computational text analysis and NLP. Is there no home for groundbreaking integrative or interdisciplinary work in *ACL, such that a new venue is required? Or can we adapt our standards to invite deeper connections to theory and new ways of knowing?\n"}
{"question": "What are the three text adversarial attacks evaluated in this paper?", "evidence": "  Our experiments evaluate the robustness of text classification models under three state-of-the-art text adversarial attacks TextFooler (black-box), BAE (black-box) and SemAttack (white-box), described below. The attacks considered are word-level, i.e. they replace words in a clean text with their synonyms to maintain the meaning of the clean text, but change the prediction of the victim models.  ", "options": ["A. TextFooler, Word2Vec, GPT-3", "B. TextFooler, BAE, SemAttack", "C. ImageNet, BERT, LSTM", "D. BERT, GPT-2, GloVe"], "answer": "B", "content": "\nIntroduction\nNeural networks are vulnerable to adversarial attacks: small perturbations to the input ,which do not fool humans (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2017) . In NLP tasks, previous studies (Alzantot et al., 2018; Jin et al., 2019; Li et al., 2020; Garg and Ramakrishnan, 2020) demonstrate that simple word-level text attacks (synonym substitution, word insertion/deletion) easily fool state-of-the-art models, including pre-trained transformers like BERT (Devlin et al., 2019; Wolf et al., 2020) . Further, it has recently been shown models are overconfident 1 on examples which are easy to attack (Qin et al., 2021) and indeed, such over-confident predictions plague much of modern deep learning (Kong et al., 2020; Guo et al., 2017; Nguyen et al., 2015; Rahimi et al., 2020) . Label smoothing is a regularization method that has been proven effective in a variety of applications, and modalities (Szegedy et al., 2016; Chorowski and Jaitly, 2017; Vaswani et al., 2017) . Importantly, it has been shown to reduce overconfident predictions and produce better confidence calibrated classifiers (Muller et al., 2019; Zhang et al., 2021; Dan and Roth, 2021; Desai and Durrett, 2020; Huang et al., 2021; Liu and JaJa, 2020) .\nIn this work, we focus on the question: does label smoothing also implicitly help in adversarial robustness? While there has been some investigation in this direction for adversarial attacks in computer vision, (Fu et al., 2020; Goibert and Dohmatob, 2019; Shafahi et al., 2019) , there is a gap in understanding of whether it helps with discrete, text adversarial attacks used against NLP systems. With the increasing need for robust NLP models in safety-critical applications and a lack of generic robustness strategies, 2 there is a need to understand inherent robustness properties of popular label smoothing strategies, and the interplay between confidence and robustness of a model.\nIn this paper, we extensively study standard label smoothing and its adversarial variant, covering robustness, prediction confidence, and domain transfer properties. We observe that label smoothing provides implicit robustness against adversarial examples. Particularly, we focus on pre-trained transformer models and test robustness under various kinds of black-box and white-box word-level adversarial attacks, in both in-domain and out-ofdomain scenarios. Our experiments show that label smoothing (1) improves robustness to text adversarial attacks (both black-box and white-box), and (2) mitigates over-confident errors on adversarial textual examples. Analysing the adversarial exam-ples along various quality dimensions reveals the remarkable efficacy of label smoothing as a simple add-on robustness and calibration tool.\n\nText Adversarial Attacks\nOur experiments evaluate the robustness of text classification models under three state-of-the-art text adversarial attacks TextFooler (black-box), BAE (black-box) and SemAttack (white-box), described below. 3 For a particular victim NLP model and a raw text input, the attack produces semantically-similar adversarial text as output. Importantly, only those examples are attacked, which are originally correctly predicted by the victim model. The attacks considered are word-level, i.e. they replace words in a clean text with their synonyms to maintain the meaning of the clean text, but change the prediction of the victim models.\n\u2022 TextFooler (TF): (Jin et al., 2019) proposes an attack which determines the word importance in a sentence, and then replaces the important words with qualified synonyms.\n\u2022 BAE: (Garg and Ramakrishnan, 2020) uses masked pre-trained language models to generate replacements for the important words until the victim model's prediction is incorrect.\n\u2022 SemAttack (SemAtt): (Wang et al., 2022) introduces an attack to search perturbations in the contextualized embedding space by formulating an optimization problem as in (Carlini and Wagner, 2016) . We specifically use the white-box word-level version of this attack.\n\nLabel Smoothing\nLabel Smoothing is a modified fine-tuning procedure to address overconfident predictions. It introduces uncertainty to smoothen the posterior distribution over the target labels. Label smoothing has been shown to implicitly calibrate neural networks on out-of-distribution data, where calibration measures how well the model confidences are aligned with the empirical likelihoods (Guo et al., 2017) .\n\u2022 Standard Label Smoothing (LS) (Szegedy et al., 2013; Muller et al., 2019 ) constructs 3 The black-box attacks keep querying the model with its attempts until the victim model is fooled while the white-box attack has access to the gradients to the model. Further details of the attacks are in (Jin et al., 2019; Garg and Ramakrishnan, 2020; Wang et al., 2022) . a new target vector (y LS i ) from the one-hot target vector (y i ), where y LS i = (1 \u2212 \u03b1)y i + \u03b1/K for a K class classification problem. \u03b1 is a hyperparameter selection and its range is from 0 to 1. ) with a probability of 1 \u2212 \u03b1 on the target label and \u03b1 on the label to which the classification model assigns the minimum softmax scores, thus introducing uncertainty.\nFor both LS and ALS, the cross entropy loss is subsequently minimized between the model predictions and the modified target vectors y LS i , y ALS i .\n\nExperiments\nIn this section, we present a thorough empirical evaluation on the effect of label smoothing on adversarial robustness for two pre-trained transformer models: BERT and its distilled variant, dBERT, which are the victim models. 4 We attack the victim models using TF, BAE, and SemAttack. For each attack, we present results on both the standard models and the label-smoothed models on various classification tasks: text classification and natural language inference. For each dataset we evaluate on a randomly sampled subset of the test set (1000 examples), as done in prior work (Li et al., 2021; Jin et al., 2019; Garg and Ramakrishnan, 2020) . We evaluate on the following tasks, and other details about the setting is in Appendix A.8:\n\u2022 Text Classification: We evaluate on movie review classification using Movie Review (MR) (Pang and Lee, 2005) and Stanford Sentiment Treebank (SST2) (Socher et al., 2013) against various attacks for in-domain data. We show clean accuracy, attack success rate and average confidence on successful adversarial texts. For each dataset, the left column are the results for standard model, and the right column are for LS models where \u03b1 denotes the label smoothing factor (\u03b1=0: no LS). \u2191 (\u2193) denotes higher (lower) is better respectively. dBERT denotes the distilBERT model.\non the matched genre test-set in the OOD setting presented in subsection 3.2 .\n\nIn-domain Setting\nIn the in-domain setting (iD), the pre-trained transformer models are fine-tuned on the train-set for each task and evaluated on the corresponding testset. For each case, we report the clean accuracy, the adversarial attack success rate (percentage of misclassified examples after an attack) and the average confidence on successfully attacked examples (on which the model makes a wrong prediction). 5 Table 1 shows the performance of BERT and dBERT, with and without label-smoothing. We choose label smoothing factor \u03b1 = 0.45 for standard labelsmoothed models in our experiments.\nWe see that label-smoothed models are more robust for every adversarial attack across different datasets in terms of the attack success rate, which is a standard metric in this area (Li et al., 2021; Lee et al., 2022) . Additionally, the higher confidence of the standard models on the successfully attacked examples indicates that label smoothing helps mitigate overconfident mistakes in the adversarial setting. Importantly, the clean accuracy remains almost unchanged in all the cases. Moreover, we observe that the models gain much more robustness from LS under white-box attack, compared 5 Details of each metric are presented in Appendix A.2.\nto the black-box setting. We perform hyperparameter sweeping for the label smoothing factor \u03b1 to investigate their impact to model accuracy and adversarial robustness. Figure 1 shows that the attack success rate gets lower as we increase the label smooth factor when fine-tuning the model while the test accuracy is comparable 6 . However, when the label smoothing factor is larger than 0.45, there is no further improvement on adversarial robustness in terms of attack success rate. Automatic search for an optimal label smoothing factor and its theoretical analysis is important future work. We also investigate the impact of adversarial label smoothing (ALS) and show that the adversarial label smoothed methods also improves model's robustness in Table 2 . \n\nOut-of-Domain setting\nWe now evaluate the benefits of label smoothing for robustness in the out-of-domain (OOD) setting, where the pre-trained model is fine-tuned on a particular dataset and is then evaluated directly on a different dataset, which has a matching label space. Three examples of these that we evaluate on are the Movie Reviews to SST-2 transfer, the SST-2 to Yelp transfer, and the SNLI to MNLI transfer.\nIn Table 3 helps produce more robust models in the OOD setting although with less gain compared to iD setting. This is a challenging setting, as evidenced by the significant performance drop in the clean accuracy as compared to the in-domain setting. We also see that the standard models make over-confident errors on successfully attacked adversarial examples, when compared to label-smoothed models.\n\nQualitative Results\nIn this section, we try to understand how the generated adversarial examples differ for label smoothed and standard models. First we look at some qualitative examples: in We also performed automatic evaluation of the quality of the adversarial examples for standard and label smoothed models, adopting standard metrics from previous studies (Jin et al., 2019; Li et al., 2021) . Ideally, we want the adversarial sentences to be free of grammar errors, fluent, and semantically similar to the clean text. This can be quantified using metrics such as grammar errors, perplexity, and similarity scores (compared to the clean text). Table 5 shows that the quality of generated adversarial examples on label smoothed models is worse than those on standard models for different metrics, suggesting that the adversarial sentences generated by standard models are easier to perceive. This further demonstrates that label smoothing makes it harder to find adversarial vulnerabilities.\n\nConclusion\nWe presented an extensive empirical study to investigate the effect of label smoothing techniques on adversarial robustness for various NLP tasks, for various victim models and adversarial attacks. Our results demonstrate that label smoothing imparts implicit robustness to models, even under domain shifts. This first work on the effects of LS for text adversarial attacks, complemented with prior work on LS and implicit calibration (Desai and Durrett, 2020; Dan and Roth, 2021) , is an important step towards developing robust, reliable models. In the future, it would be interesting to explore the combination of label smoothing with other regularization and adversarial training techniques to further enhance the adversarial robustness of NLP models.\n"}
{"question": "In the experiments conducted on the HybridQA dataset, what method significantly outperformed the S3HQA model in terms of F1 score on the Dev set?", "evidence": "  Table 1 shows the comparison results between our models with previous typical approaches on both development and test sets. It shows that our proposed S 3 HQA works significantly better than the baselines in terms of EM and F1 on HybridQA. However, we found that our approach was outperformed by the DEHG model (Feng et al., 2022) in terms of F1 score on the Dev set.  ", "options": ["A. The \"Direct\" prompting method", "B. The \"CoT\" prompting method", "C. The DEHG model", "D. The Vanilla-Retriever model"], "answer": "C", "content": "\nIntroduction\nQuestion answering systems devote to answering various questions with the evidence located in the structured knowledge base (e.g., table) (Pasupat and Liang, 2015; Yu et al., 2018) or unstructured texts (Rajpurkar et al., 2016) . Considering that many questions need to utilize multiple sources of knowledge jointly in real-world applications, the hybrid form of question answering over texts and tables (TextTableQA) has been proposed and attracted more and more attention (Chen et al., Walnut Q1: Who is the athlete in a city located on the Mississippi River? A1: Philip Mulkey Q2: In which year did Walnut-born athletes participate in the Rome Olympics? A2: 1960 Q3: Who is the higher scoring athlete from the cities of Eugene and Walnut? Comparison A3: Rafer Johnson 2020b,a; Zhu et al., 2021; Chen et al., 2021; Zhao et al., 2022; Wang et al., 2022a) . Fact reasoning (Chen et al., 2020a,b) is a critical question type of TextTableQA. It requires jointly using multiple evidence from tables and texts to reasoning the answers with different operations, such as correlation (e.g., multi-hop) and aggregation (e.g., comparison) . Hyperlinks among some table cells and linked passages are essential resources to establish their relationship and support the retrieval and reasoning for multi-hop questions. As shown in Figure 1 , answering a complex question Q1 requires jointly reasoning from textual evidence (P1) to table evidence ([R2, Place] ) and then to other table evidence ([R2, Athlete]).\nExisting methods consist of two main stages: retriever and reader (Chen et al., 2020b; Feng et al., 2022) . The retriever filters out the cells and passages with high relevance to the question, and then the reader extracts a span from the retrieval results as the final answer. However, current methods with two stages still have three limitations as follows.\n1) Noisy labeling for training retriever. Existing retrieval methods usually ignore the weakly supervised answer annotation (Chen et al., 2020b; Wang et al., 2022b; Feng et al., 2022) . For the Q2 of Figure 1 , we cannot know the specific location of the hybrid evidence, only given the final answer \"1960\". Therefore, there is a lot of pseudo-true evidence labeled (Marked in green) automatically by string matching, which introduces a lot of evidence noise.\n2) Insufficient utilization of heterogeneous information. After retrieval, existing methods selected a particular cell or passage for reading to extract the final answer (Chen et al., 2020b; Wang et al., 2022b) . As for Q1 in Figure 1 , previous models were more likely to choose P1 or the coordinates [R2, Place] to extract the answer. However, these methods seldomly used the hybrid information of table schema and cell-passage hyperlinks, which is the key factor in answering multi-hop questions.\n3) Deficient ability for different reasoning operations. Previous methods (Eisenschlos et al., 2021; Kumar et al., 2021; Wang et al., 2022b) mainly used an extraction module to obtain answers, which cannot support knowledge reasoning that requires comparison, calculation, and other operations.\nIn this paper, we propose a three-stage approach S 3 HQA to solve the above problems. (1) Retriever with Refinement Training, we propose a two-step training method, splitting the training data into two parts, so that the noise in the retrieval phase can be alleviated. (2) Hybrid Selector has been proposed and selects supporting facts with different granularity and resources depending on the question type. By considering the hybrid data of tables and text, this paper proposes a hybrid selection algorithm that can effectively utilize the heterogeneous information of tables and passages. (3) Generationbased reasoner utilizes a generation-based model for addressing different question types. The model allows better aggregation of information on the input side, which not only have better multi-hop reasoning capabilities but also be able to handle comparison and counting questions. Furthermore, we are the first to use the LLM in-context learning approach for table-text hybrid question-answering tasks.\nWe evaluate our proposed model on the challenging TextTableQA benchmark HybridQA. The empirical results show that our approach outperforms all the existing models 2 .\n\nGiven a natural language question\nQ = {q i } |Q| i=1\nand a table T with \u27e8H, R\u27e9, H indicates the table headers, and\nR = {r i } |R| i=1 indicates the rows with number |R|. Each row r i is consists of N cells r i = {c ij } N j=1\n. The header's number is also N . Some cells have a linked passage P ij . Our goal aims to generate the answer A with model \u0398, which is a span from table cells or linked passage or a derivation result of counting questions.\n\nRetriever with Refinement Training\nThe retriever aims to perform initial filtering of heterogeneous resources. However, accurately labeling the location of answers consumes high labeling costs. For TextTableQA data, the answer A usually appears in multiple locations, which makes it difficult for us to generate precise retrieval la-bels. We use a two-step training method, with a row-based retriever and a passage-based retriever for each step.\nInspired by (Kumar et al., 2021) , the retrieval has two steps. First, we divide the data D into two folds according to the string matching labels G i . Specifically, for a question-answer instance, the answer A appears one time as D 1 , and the instance whose answer A appears multiple times as D 2 . Take the example in Figure 1 , Q1, Q3 belongs to D 1 while Q2 belongs to D 2 . The data is organized in the form of\n[CLS]q 1 q 2 ...q |Q| [SEP]c i1 c i2 ...c iN [SEP] or [CLS]q 1 q 2 ...q |Q| [SEP]p ij [SEP].\nIn the first step, we only use D 1 to train a model \u0398 1 , which data are noiseless. Then in the second step, we use the trained weight \u0398 1 to train the model \u0398 2 . For the input x, the loss function is:\nL(\u0398 2 , x, R) = z\u2208R \u2212q(z) log p \u0398 1 (z|x)\nwhere q(z) = p \u0398 1 (z|x, z \u2208 R) is the probability distribution given by the model restricted to candidate rows R containing the answer span, taken here as a constant with zero gradients (Eisenschlos et al., 2021) .\nMeanwhile, we use a passage-based retriever to enhance the performance of a row-based retriever (PassageFilter). Specifically, we use the passage-based retriever to obtain a prediction score of passage relevance. Based on this score, we reorder the input of the row-based retriever. It avoids the limitation on input sequence length imposed by the pre-trained model.\n\nHybrid Selector\nThis module needs to combine the results of the two granularity retrievers. As for this task, we consider the question type and the relationships between the table and linked passages essential. As shown in Figure 2 , the hybrid selector chooses the appropriate data source from the two retrieval results depending on question types.\nSpecifically, for general bridge multi-hop questions, we use a single row and its linked passage. While for comparison/count questions, we consider multiple rows and further filter the related sentences, delete the linked paragraphs with the low scores. This not only enables the generation module to obtain accurate information, but also prevents the introduction of a large amount of unrelated information. The selector algorithm outputs a mixed sequence with high relevance based on the relationship between the question, the table, and the passages. The algorithm is shown in Algorithm 1.\nAlgorithm 1 Hybrid Selector Algorithm.\nInput: question Q, table rows R, linked passages P, rowbased retriever \u0398R, passage-based retriever \u0398P , selector target row count NS Output: generator input S Get the row/passage ordered list by relevant scores\n1: OR \u2190 sort(\u0398R(Q, R)) 2: OP \u2190 sort(\u0398P (Q, P)) 3: p type \u2190 Classif ication(Q) 4: if p type = bridge then 5: if OP [0] in OR[0] then 6: S \u2190 Q + OR[0] 7: else 8: S \u2190 Q + OR[0] + OP [0] 9:\nend if 10: else 11:\nOPC \u2190 P[len(OP )//2 :] 12:\nS \u2190 Q + OR[0 : NS] \u2212 OPC 13: end if 14: return S\n\nGeneration-based Reasoner\nThe results of the selector take into account both two granularity. Unlike the previous approaches, which were based on a span extraction module, we use a generation-based model for answer prediction.\n\nRow-wise generator\nTo generate an accurate answer string A = (a 1 , a 2 , ..., a n ) given the question Q and selection evidence S, we perform lexical analysis to identify the question type, such as counting or comparison, by looking for certain keywords or comparative adjectives. We utilize two special tags \u27e8Count\u27e9 and \u27e8Compare\u27e9, which indicates the question types.\nWe then use the results of the passage retriever to rank the passages in order of their relevance, eliminating the impact of model input length limitations. Finally, we train a Seq2Seq language model with parameters \u0398, using the input sequence Q, S and the previous outputs a <i to optimize the product of the probabilities of the output sequence a 1 , a 2 , ..., a n :\nA = argmax n i=1 P (a i |a <i , Q, S; \u0398)\n\nLLM prompting generator\nWith the emergence of large language models, In-Context Learning (Dong et al., 2022) and Chain-of-Thought prompting (Wei et al., 2022) have become two particularly popular research topics in this field.\nIn this paper, we introduce a prompting strategy for multi-hop TextTableQA.\nWe utilize selection evidence S and apply LLMbased prompting. We conducted experiments on both vanilla prompting and chain-of-thought prompting in zero-shot and few-shot scenarios.\n\nExperiment Setup\nDatasets We conduct experiments on Hy-bridQA (Chen et al., 2020b) . The detailed statistics are shown in Appendix A. For evaluation, we followed the official evaluation to report exact match accuracy and F1 score. Implementation details The implementation details are shown in Appendix B. The experimental results are the average of five times results.\n\nFully-supervised Results\nTable 1 shows the comparison results between our models with previous typical approaches on both development and test sets. It shows that our proposed S 3 HQA works significantly better than the baselines in terms of EM and F1 on HybridQA. The results indicate that S 3 HQA is an effective model for multi-hop question answering over tabular and textual data. Specifically, it can effectively handle multi-hop reasoning and make full use of heterogeneous information.\nHowever, we found that our approach was outperformed by the DEHG model (Feng et al., 2022) in terms of F1 score on the Dev set. We speculate that this might be because the DEHG approach uses their own Open Information Extraction (OIE) tool.\n\nModel\nDev EM F1 Zero-shot prompt GPT3.5 direct 33.1 50.5 GPT3.5 CoT 52.9 66.6 Few-shot prompt (2-shot) GPT3.5 direct 57.1 68.8 GPT3.5 CoT 60.3 72.1 \n\nLLM-prompting Results\nWe present our zero-shot and few-shot results in Table 2 . \"Direct\" refers to a simple prompting method where only the question, context, and answer are provided to the model without any additional reasoning process. In contrast, \"CoT\" involves a human-authored Chain-of-Thought reasoning process that provides a more structured and logical way of prompting the model. The experiments demonstrate that in-context learning used to prompt large language models can achieve promising results. Specifically, utilizing the Chain-of-Thought prompt method can significantly enhance the model's performance.\nHowever, it's worth noting that there is still a performance gap compared to fine-tuning the model on the full dataset (Table 1 ). Fine-tuning allows the model to learn more specific information about the TextTableQA task, resulting in better performance. Nevertheless, our results show that the LLM-prompting method can be a useful alternative to fine-tuning, especially when there is a limited amount of labeled data available.\n\nAblation Studies\nWe conduct ablation studies on the test set. We validate the effects of three modules: retriever with refinement training, hybrid selector, and generation-based reasoner. The retriever performs initial filtering of heterogeneous resources; Selectors combined with hyperlinks further identify the exact evidence needed to answer multi-hop questions; and the reasoner uses the selection evidence to obtain the final answer. Effect of proposed retriever. As shown in the Table 3 , under the setting of using the BERTbase-uncased model, sing the BERT-base-uncased model setting, the retriever with refinement training achieved 87.2. When we use Deberta-base, the top1 retrieval performance improved by 0.8%. For w/o refinement training, we use the entire data directly for training, the top1 recall drops about 3.2%. For w/o PassageFilter, we remove the mechanism, the top1 recall drops about 3.2%. For Vanilla-Retriever, we use the row-based retriever (Kumar et al., 2021) and remove all our mechanisms, the top1 score drops about 5.3%. This shows that our model can solve the weakly supervised data noise problem well.\n\nModel\nEffect of hybrid selector. As shown in the Table 4, we removed the selector of S 3 HQA and replaced it with the previous cell-based selector (Wang et al., 2022b) . This method directly uses the top1 result of the row retriever as input to the generator. w/o hybrid selector shows that the EM drops 2.9% and F1 drops 1.6%, which proves the effectiveness of our selector approach.\nEffect of reasoner. As shown in the Table 4 , we design two baselines. BERT-large reader (Chen et al., 2020b; Wang et al., 2022b) uses BERT (Devlin et al., 2018) as encoder and solves this task by predicting the start/end tokens. w/o special tags deletes the special tags. Both the two experiments demonstrate our S 3 HQA reasoner performs the best for HybridQA task.\n\nRelated Work\nThe TextTableQA task (Wang et al., 2022a) has attracted more and more attention. As for multi-hop type dataset, previous work used pipeline approach (Chen et al., 2020b) , unsupervised approach (Pan et al., 2021) , multigranularity (Wang et al., 2022b) , table pre-trained language model (Eisenschlos et al., 2021) , multiinstance learning (Kumar et al., 2021) and graph neural network (Feng et al., 2022) to solve this task. As for numerical reasoning task, which is quite different from multi-hop type dataset, there is also a lot of work (Zhu et al., 2021; Zhao et al., 2022; Zhou et al., 2022; Lei et al., 2022; Li et al., 2022; Wei et al., 2023) to look at these types of questions. Unlike these methods, our proposed three-stage model S 3 HQA can alleviate noises from weakly supervised and solve different types of multi-hop TextTableQA questions by handling the relationship between tables and text.\n\nConclusion\nThis paper proposes a three-stage model consisting of retriever, selector, and reasoner, which can effectively address multi-hop TextTableQA. The proposed method solves three drawbacks of the previous methods: noisy labeling for training retriever, insufficient utilization of heterogeneous information, and deficient ability for reasoning. It achieves new state-of-the-art performance on the widely used benchmark HybridQA. In future work, we will design more interpretable TextTableQA models to predict the explicit reasoning path.\n"}
{"question": "Which is the the difference between prefix-tuning  and prefix-propagation\uff1f", "evidence": "  In this paper, we propose a simple and effective method, prefix-propagation, which consistently improves the performance of PEFT for long sequence models. Unlike prefix-tuning, prefix-propagation propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer. However, fine-tuning and storing full copies of LLMs can consume prohibitively large quantities of resources. Parameter-efficient finetuning (PEFT) methods such as prefix-tuning address these concerns by reducing the number of trainable parameters. Prefix-tuning underperforms fine-tuning on long sequence classification tasks, Hyperpartisan and 20-newsgroups , when used with the popular long-document model Longformer  . ", "options": ["A. Prefix-propagation propagates the hidden states corresponding to prefixes through the attention computation. ", "B. prefix-tuning address concerns that fine-tuning and storing full copies of LLMs can consume prohibitively large quantities of resources by reducing the number of trainable parameters. ", "C. Prefix-tuning allows for the prefixes hidden states to dynamically change as the input propagates through each layer.", "D. Prefix-tuning performs well fine-tuning on long sequence classification tasks and 20-newsgroups , when used with the popular long-document model Longformer  ."], "answer": "A", "content": "\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017) has changed the landscape of recent natural language processing approaches by enabling the pretraining of state-of-the-art large language models (LLM) (Devlin et al., 2019; He et al., 2020; Brown et al., 2020) . However, fine-tuning and storing full copies of LLMs can consume prohibitively large quantities of resources. Parameter-efficient finetuning (PEFT) methods such as prefix-tuning (Li and Liang, 2021; He et al., 2021a; Liu et al., 2022) address these concerns by reducing the number of trainable parameters. Prefix-tuning can tune 0.01% of parameters and still match the performance of regular fine-tuning (updating all model parameters). PEFT has been investigated for tasks with inputs consisting of sentences, sentence-pair, or sequences that fit within the typical LLM maximum tokens. However, the performance of PEFT for tasks with longer textual sequences has been overlooked. In this work, we investigate this oversight and provide evidence suggesting that the gap between PEFT and regular fine-tuning is substantial when modelling long sequences. As shown in Table 1, prefix-tuning underperforms fine-tuning on long sequence classification tasks, Hyperpartisan (Kiesel et al., 2019) and 20-newsgroups (Lang, 1995) , when used with the popular long-document model Longformer (Beltagy et al., 2020) .\nIn this paper, we propose a simple and effective method, prefix-propagation, which consistently improves the performance of PEFT for long sequence models. Unlike prefix-tuning, prefix-propagation propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer.\nTo further understand prefix propagation, we investigate the reliability of the model's predictions by performing analyses on calibration. Lastly, we conduct study on prefix-based methods in terms of kernel attention to strengthen their theoretical value.\nIn summary, our contributions are as follows:\n... Figure 1 : Illustration of the differences between (a) prefix-propagation (ours) (b) and prefix-tuning (Liu et al., 2022; Li and Liang, 2021) . Blue blocks denote trainable prompts, and \"Transformer Layer\" represents the computation done in a layer of the pre-trained LLM. Note that in prefix-propagation (a), the summation of prefixes continues for layers beyond 3, up to n. This operation is encapsulated by the ellipses. In prefix-tuning (b), prefixes in subsequent layers do not depend on hidden states from past layers (they are simply overwritten).\n.\n\u2022 We study PEFT for long documents and show that prefix-tuning is significantly inferior to fine-tuning in this scenario. To the best of our knowledge, this is the first work to focus on PEFT for long documents.\n\u2022 We introduce prefix-propagation, which consistently improves the performance over prefix turning on the different long document datasets, while using 50% fewer parameters.\n\u2022 We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated.\n\u2022 We elucidate the relationship between prefixpropagation and kernel attention and perform an ablation study that utilizes this insight.\n\nRelated Works\nLong Sequence Models Numerous methods have been proposed to reduce the complexity of attention from O(n 2 ) to O(n) such as kernel approximations (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021) and fixed (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) or learned (Kitaev et al., 2020) sparse attention patterns. For a broader summary, please refer to Tay et al. (2022) . In this work, we use Longformer (Beltagy et al., 2020) . To linearize attention complexity, Longformer employs sliding window attention while globally attending to relatively few special tokens.\nParameter-Efficient Tuning Inspired by the success of manual prompting (Brown et al., 2020), prefix-tuning (Li and Liang, 2021; Liu et al., 2022) prepends trainable \"soft\" prompts to an input sequence. Although further PEFT methods have since been introduced (He et al., 2021a; Hu et al., 2021; Ben Zaken et al., 2022) , we focus on adapting prefix-tuning. We note that our adaptation does not violate orthogonality and thus prefixpropagation can still be compounded with other PEFT methods as proposed in the UnifiedPET framework (He et al., 2021a) , likely yielding similar performance gains. We leave the empirical validation of this hypothesis for future work.\nOut work also adheres to the key motivation of the recent PEFT method, inducer-tuning (Chen et al., 2022) , which is that optimal prefixes should be close to queries within their latent space. We derive queries, keys, and values from the same prefix token, limiting the distance that separates them.\n\nMethodology\nIn this section we introduce prefix-propagation, which, unlike prefix-tuning, propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer. Prefix-propagation and its predecessor, prefix-tuning are depicted in Figure 1a embeddings) to the input sequence (blue blocks in top left of Figure 1a ). Then, before every subsequent layer, we sum new trainable matrices onto the first j embeddings corresponding to the prefixes (denoted by the sum operators in Figure 1a ). By propagating instead of overwriting, we halve the number of parameters trained while simultaneously improving performance on long-document tasks.\nWe now formalize prefix-propagation. Multiheaded attention processes query, key, and value matrices derived from a sequence C \u2208 R m\u00d7d with length m and embeddings of size d. Our method modifies traditional attention by concatenating a prefix P \u2208 R j\u00d7d of length j to the sequence:\nH l,i = Attn(D (l) W (l,i) q , D (l) W (l,i) k , D (l) W (l,i) v ) (1) D (l) = cat(P (l) , C) if l = 1 cat(P (l) + C[:j, :], C[j:, :]) if l > 1 where inputs C are projected through pre-trained weight matrices W (l,i) q , W (l,i) k , W (l,i) v\n\u2208 R d\u00d7d h per layer l and head i yielding the output of the attention head, H \u2208 R (j+m)\u00d7d h . The prefixes are concatenated for the first layer (l = 1) and summed to their corresponding hidden states for the remaining layers (l > 1). We do not continually concatenate new prefixes to the sequence to avoid increasing the sequence length after each layer.\nFor both prefix-tuning and prefix-propagation, prefixes (keys and values) are globally attended to by all queries. Unlike prefix-tuning however, our method concatenates additional hidden states before the hidden states C are projected by\nW (i) k and W (i)\nv . By doing so, prefix-propagation modifies query matrices, allowing prefixes to attend to other hidden states globally, thereby increasing representation capability. This approach is somewhat analogous to the external global tokens inserted in the BigBird-ETC model (Zaheer et al., 2020) . By attending to other tokens, the prefixes can act as special storage tokens, which is particularly useful in the restricted regime of long-document modelling where relatively few tokens have global context. Conversely, prefix-tuning only concatenates trained key and value matrices, P k , P v \u2208 R j\u00d7d h , statically to the sequence:\nH l,i = Attn(CW (l,i) q , cat(P (l,i) k , CW (l,i) k ), cat(P (l,i) v , CW (l,i) v ))\n(2)\nSince our method has a single prefix matrix, P instead of separate P k and P v matrices, we reduce the number of trained parameters by 50%.\n\nCalibration\nWe further study the proposed prefix-propagation method to understand the reliability of model's predictions through calibration. Well-calibrated models output confidence scores that closely match the models' accuracy. Either over-confident or underconfident models are undesirable. Calibration has widely been overlooked in PEFT methods. To quantify calibration in our work, we use expected calibration error (ECE), which bins predictions based on model confidence and compares them to accuracy (Pakdaman Naeini et al., 2015; Guo et al., 2017) .\n\nKernel Decomposition\nTraditional attention is analogous to applying a kernel smoother over inputs (Tsai et al., 2019) .\nMotivated by this insight, we reformulate prefixpropagation as a sum of kernelized attention modules. Separating the modules introduces flexibility in two ways: (1) Their individual kernel forms can be mixed and matched and (2) A hyperparameter scale factor \u03b1 can be applied to the prefix component to increase or decrease its weighting. Equation 3 defines kernel decomposition for prefixpropagation 2 :\nH = Kern(cat(P, C)W q , CW k , CW v ) + (\u03b1)Kern(cat(P, C)W q , P W k , P W v ) (3)\nwhere Kern refers to kernel attention as formulated in (Tsai et al., 2019) . The first term results from attending to the original sequence, C, and the second comes from attending to the prefixes, P . We provide the derivation of Equation 3 and the full definition of kernel attention in Appendix A.\nOur main motivation for presenting prefix decomposition is to establish foundational knowledge and guide future research. Ergo, we restrict experiments in this initial presentation to using just the default exponential kernel (Appendix A).\n\nExperiments and Results\nDatasets We evaluate our approach on three long-document classification tasks: ArXiv (He et al., 2019) , an 11-class classification task composed of academic research papers, the 20-newsgroups (Lang, 1995) classification task consisting of mailing lists that fall into one of 20 classes, and the Hyperpartisan dataset, a binary classification task for extremist news classification (Kiesel et al., 2019) . We also run experiments on WikiHop (Welbl et al., 2018) , a long-document reading comprehension task requiring multi-step reasoning.\nDue to compute limitations inherent to working with long documents, with the exception of Hyperpartisan, we only report a single run for each task. This mimics the original Longformer reporting scheme (Beltagy et al., 2020) . For Hyperpartisan, the smallest of the datasets, we report mean metrics averaged over five seeds.\nBaselines As a baseline, we fine-tune Longformer-base (approx.\n149M parameters) as closely as possible to Beltagy et al. (2020) . For PEFT, we evaluate prefix-tuning on Longformer-base and RoBERTa-base (approx. 125M parameters) (Liu et al., 2019) . 2 We omit layer, l and head, i for brevity.\n\nMethod\nArXiv HY. NG. More details on dataset sizes, pre-processing, and hyperparameters are in Appendix B.\n\nResults and Discussion\nAcross all tasks, our results in Table 2 verify that prefix-tuning is inferior to fine-tuning long sequences. Conversely, prefix-propagation consistently outperforms prefix-tuning and is comparable to fine-tuning on most tasks. Prefix propagation also performs competitively on Hyperpartisan, a relatively small dataset with only 625 samples. This is in contrast to prefix-tuning, which is known to underperform in low-data settings (Gu et al., 2022) . Because we ran multiple seeds on Hyperpartisan, we also found that prefix-propagation's better performance relative to prefix-tuning is statistically significant (p < 0.05, using a single-tailed t-test). We do not have multiple samples to run these tests for larger datasets, but we emphasize that Hyperpartisan likely has the most variance and yet it is still statistically significant. We suspect that prefixpropagation's performance exceeds prefix-tuning because propagated prefixes can transmit global context across multiple layers, possibly modelling more expressive abstractions.\nWe note one exception where prefix-based methods still leave room for improvement: multiplechoice question answering on WikiHop. We hypothesize that prefix methods have insufficient capacity to properly model complex long-document multi-step question answering.\nWe also observe that prefix-based methods, and especially prefix-propagation, achieve better calibration than fine-tuning, as shown in Table 3 . Unlike prefix-tuning however, prefix-propagation effectively balances calibration with accuracy metrics. The calibration of fine-tuning deteriorates as training progresses (Figure 4 \n\nMicro F1\nFigure 2 : Violin plot of Micro F1 Score for five different seeds on the Hyperpartisan task. White dots, gray boxes, and gray lines are the medians, interquartile ranges, and ranges respectively. Width of the five violin shapes show the probability densities for the corresponding F1score. All methods tune Longformer-base except \"R Prefix\", which is prefix-tuning on RoBERTa-base.\nforgetting (Jagielski et al., 2022) .\nAs an initial test for our ongoing prefixpropagation kernel study, we show results on Hyperpartisan in Figure 2 . The kernelized version of prefix-propagation achieves the best single-run performance, but has higher variance than fine-tuning and prefix-propagation which necessitates further research.\n\nConclusion\nOur research focuses on parameter efficient tuning for long documents tasks. We introduce prefix-propagation, which consistently improves performance over prefix-turning on long document datasets, while using 50% fewer parameters. We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated. We lastly explicate prefix-propagation from a kernel perspective, uncovering insights for future PEFT research.\n"}
{"question": "What is the main reason behind the performance improvement observed in the TK-Instruct model when compared to T0 in CLS tasks on the NatInst-V2 dataset?", "evidence": "  The TK is the TK-Instruct(770M) model trained with 10 instances per task. To ensure a fairer comparison between these models, we employ constrained decoding techniques to align the model's predictions with the label space.   ", "options": ["A. The TK-Instruct model has a better overall Rouge-L score.", "B. T0's format correctness is significantly higher than the Random baseline.", "C. The TK-Instruct model is trained with 10 instances per task.", "D. T0's classification score improves after employing constrained decoding techniques.", "By adopting this approach, we observe a substantial performance improvement for T0 in CLS tasks (34.03 to 51.31)."], "answer": "D", "content": "\nIntroduction\nRecently, instruction tuning(IT) has drawn much attention in the NLP communities, with the rapid growth of new models (Sanh et al., 2021; Wei et al., 2021; Ouyang et al., 2022) and datasets (Wang et al., 2022; Gupta et al., 2022; Finlayson et al., 2022; Mishra et al., 2021; Ye et al., 2021; Bach et al., 2022) . Models trained with task instructions demonstrate impressive zero-shot cross-task generalization ability. Despite the remarkable results, 1 : Comparison between two types of instruction tuning models. Noted that we reported an estimated number of instructions for T0 during training and testing since they have 5 to 10 instructions for each task. Our analysis focuses on the \"generalize to unseen task\" type.\nhow models utilize the instructions during training and inference time remains an open question.\nPrior works have raised the question of whether models really learn to follow the instructions or just capture spurious correlations. Jang et al. (2022) , Webson and Pavlick (2021) showed that the current large language models (LLMs) can achieve similar performance with misleading instructions(prompts) in in-context learning(ICL) and few-shot learning scenarios. Min et al. (2022) analyze how model utilize examples in ICL. They observed that (1) Input-output mapping in examples is not important and(2) Output space information is crucial.\nBesides ICL and few-shot prompt-tuning, some works raise concerns about instruction following in the instruction tuning field (Finlayson et al., 2022; Gupta et al., 2022; Gu et al., 2022) , with a focus on test-time analysis. In contrast, we focus on analyzing how the models utilize instructions during the training process. We compare our analyzing methods and observation with prior works in Appendix A.1.\nIn this work, we conduct controlled experiments on NatInst-V2 (Wang et al., 2022) , the largest opensource instruction learning dataset includes 800+ English tasks with diverse task types, to study how models utilize instructions during IT. Note that existing research on IT can be categorized into two major camps: generalize to unseen tasks and generalize to unseen instructions, based on their objectives. Table 1 shows the comparison. Our analysis focuses on the former with more background and justifications provided in section 2. We strategically alter the instructions and compare them with original instructions for IT. Specifically, for task definition, we create simplified versions by removing all semantic components in the instructions and only leaving the output space information. For task examples, we create delusive examples with incorrect input-output mapping, where the examples' input and output spaces are correct, but the inputoutput mappings are wrong. Figure 1 demonstrates specific examples of these altered instructions.\nOur experiments show that models trained with simplified task definitions achieve performances on par with the original IT models with different numbers of training examples ranging from 10 to 800 per task. We also observe that instructiontuned models are sensitive to input-output mapping during the testing ICL stage, but not during the instruction-tuning (training) stage, especially in low resource settings (i.e., \u2264 50 training instance per task). To further understand why instruction tuning improves performance for zero-shot test tasks, we establish a random baseline that only knows the correct output format (label space) for classification and multi-choice tasks. We discover that the random baseline can get 30% absolute exact-match score improvement over an untuned model, almost comparable to some IT models in low resource settings.\nOur results suggest that the impressive performance gains of IT may just come from models learning superficial patterns, such as the output space and format. We suggest future research on IT more carefully analyze their performance gains and benchmark against trivial baselines.\n\nBackground\nRecently, many instruction tuning work train and test the models with instructions to achieve better zero-shot generalizability toward unseen tasks/instructions. We categorize these works by their objectives: generalize to unseen tasks and generalize to unseen instructions, and show the comparison in Table 1 . Instruction tuning to generalize to unseen tasks. Figure 1 illustrates a two-stage instruction tuning pipeline used in many IT models, such as T0 (Sanh et al., 2021) , FLAN (Wei et al., 2021) , and TK-Instruct (Wang et al., 2022) . In the first stage, the models are trained on a set of training tasks with instructions (task-definition and task-examples). After training, the models are evaluated on a set of unseen testing tasks for zero-shot generalizability. By incorporating instructions during training, the models are shown to significantly improve performance over untuned models. The impressive performance gains led people to believe that models learned to follow instructions via instruction tuning. The goal of our analysis is to verify this belief. Instruction tuning to generalize to unseen instructions. Different from T0, FLAN, and TK-Instruct training and testing the model with clear task boundaries and focusing on cross-task generalizability, Instruct-GPT (Ouyang et al., 2022) , Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) focus more on instruction generalizability, which they train their model without clear task boundary but with diverse instructions, and further test on user-oriented instructions. These models show very different behavior compared with instruction tuning models that aim to generalize to unseen tasks.\nSince Instruct-GPT is not open-sourced and distilled IT models such as Alpaca and Vicuna come up after our submission, we focus our analysis on the first category using the TK-instruct model and NatInst-V2 dataset. However, we also conduct additional experiments and discuss the Alpaca model's instruction following ability in Table 2 .\n\nAnalysis Method\nTask definition manipulation.\nTo analyze whether models really \"understand\" and utilize the semantic meaning of task definitions, we conduct controlled experiments to remove semantic information in task definitions. Specifically, we conduct instruction-tuning with task definitions at 3 levels of granularity: Original, Simplified, and Empty. The Original version uses human-crafted human-readable task definitions provided in NatInst-V2 (Wang et al., 2022) . The Simplified task definitions remove all semantic components in the original task definition and only leave the output space information. Specifically, we only provide possible output labels as task definitions for classification tasks, and completely remove task definitions for other tasks (mostly generative tasks) during IT. Figure 1 shows an example of Simplified task definition. More details can be found in Appendix A.2. For Empty, we don't provide task definition during instruction-tuning.\nTask example manipulation. Finlayson et al. (2022) show that by providing a few task examples, both humans and models can guess and perform a task. We thus design a controlled experiment to study whether models learn the input-output mapping from task examples. Specifically, we compare models trained with 3 types of task examples: Original, Delusive, and Empty. For the Original setup, we provide one positive example in NatInst-V2 (Wang et al., 2022) \n\nExperimental Setup\nDataset. We conduct experiments on the NatInst-V2 (Wang et al., 2022) , the largest open-source instruction learning dataset, including over 800+ English tasks with diverse task types. The instructions include human-crafted human-readable Task Definition, Positive Task Examples, Negative Task Examples, and Explanation. We focus on studying task definition and task examples, which were shown to be most useful in the original paper.\nModel. we conduct experiments on TK-Instruct, the current SOTA model provided in NatInst-V2 paper. The model significantly outperformed previous SOTA models, such as T0 (62.0 v.s. 32.3 rouge-L for 11B model). We follow the seq-to-seq instruction-tuning method used in TK-Instruct, and train a T5-large-lm-adapt (770M parameters) model (Raffel et al., 2020) with performance comparable to the larger model (3B parameters) reported in Wang et al. (2022) . 1 Evaluation Metrics. For task definition, we separately evaluate Classification and Generative tasks using exact match and rouge-L respectively. For , 10, 20, 50, 200, 800) .\n\nResults\nTask Definition Experiments. Figure 2 shows experimental results for task definitions. In the top sub-figures, we can see that the models trained with Simplified instructions achieve almost the same results as models trained with Original definitions both on Classification and Generative tasks. Note that Simplified task definitions remove all semantic components in task definitions and only retain output space information for Classification tasks and remove task definitions altogether for Generative tasks. This indicates that models may only utilize output space information during instruction tuning. The bottom-left sub-figure in Figure 2 shows the overall rouge-L score for classification tasks, where models trained on the Original task definition slightly outperform the Simplified ones. A closer examination reveals that models trained on the Original task definitions are more likely to predict partially correct answers that help with the ROUGE-L score in some tasks. We provide further details in Appendix A.5. In addition, we also observe that training with Simplified prompts can yield comparable performance to the T0 model trained with Original prompts on T0 dataset. Please refer to Appendix A.6 for details. Task Examples Experiments. Combined with the previous results for task definition, we observe that comparing to the untuned models(T5 w/o IT), the IT models may achieve significant performance gain (Rouge-L from 22 to 46) with (1)Simplified task definition and (2)Delusive task example, indicating that the current impressive improvement of IT models can come from the models learning superficial patterns without utilizing (following) the instructions like human do.\nFor the right sub-figure, we show the results using Delusive task examples during test time via in-context learning. We see the performance drops for all three models, indicating that the inputoutput mapping matters for in-context learning on instruction-tuned models. This observation seems to misalign with previous work (Min et al., 2022) , which they found input-output mapping is unimportant for in context learning for classification tasks. However, a closer investigation found that most tasks suffer from significant performance drop are analogical tasks rather than classification tasks as studied in Min et al. (2022) . 2\n\nAdditional Analysis\nRandom baseline. While our experiments suggest that models do not utilize most information in the instructions, we still observe huge performance gains via instruction tuning. To understand where the gains come from, we introduce a Random baseline that simply guesses within the cor- 200) can improve exact-match score to 52%. However, while the performance gains seem impressive, the Random Guessing baseline can also achieve 42.6% exact-match score, on par with TK-Instruct trained in low resource setting (less than five instances per task). This suggests that the majority of score improvement from IT may come from model learning the output format, especially in low-resource settings.\nFair comparison for IT models. Existing studies on instruction tuning often introduce changes to both models and datasets simultaneously, which can obscure fair comparisons. To address this issue, we conduct experiments comparing different models (T0, TK-Instruct) on the same dataset (NatInst-V2) and emphasize the importance of careful evaluation. In Table 3 , when evaluating using the NatInst-V2 evaluation method and considering only the overall Rouge-L score, the TK-Instruct model appears to outperform T0 significantly. However, upon closer examination of the classification (CLS) and generative (GEN) tasks separately, we observe that T0's classification score is even lower than the Random baseline, primarily due to its format correctness being only 64%. To ensure a fairer comparison between these models, we employ constrained decoding techniques to align the model's predictions with the label space. By adopting this approach, we observe a substantial performance improvement for T0 in CLS tasks (34.03 to 51.31). T0 surpasses both the TK-Instruct model and the random baseline, indicating that it Table 3 : Careful evaluation of the NatInst-V2 dataset. The Format metric is the same as the format correctness in Figure 4 . The w/ CD indicates that the model's decoding is constrained to match the label choices for CLS tasks. The TK is the TK-Instruct(770M) model trained with 10 instances per task.\nis indeed superior to these models in CLS tasks.\n\nDiscussion\nDo Alpaca better follow the instruction on NatInst-V2 dataset? After our submission, new instruction tuning models, like Alpaca and Vicuna, are trained on distilled data from Chat-GPT and exhibit behavior closer to it. To investigate their instruction utilization, we conduct the \"Altered Task Definition\" experiment on LLaMA-7B (Touvron et al., 2023) and Alpaca-7B models using the NatInst-V2 test set. In Table 2 , training the LLaMA model on the NatInst-V2 dataset using the Original task definition leads to substantial performance enhancements than zeroshot. However, the Simplified task definition also achieves comparable performance, with a minimal decrease of 3 (EM/Rouge-L)scores. This finding is consistent with our previous observations on the TK-Instruct and T0 models. Even without tuning on NatInst-V2, the Alpaca model demonstrates strong performance on the NatInst-V2 test set. However, when the model is tested using a simplified task definition, there is a significant decrease in performance for generative tasks (but not for classification tasks). This highlights the importance of a well-written task definition for the Alpaca model to effectively perform generative tasks.\n\nConclusion\nWe constructed controlled experiments on NatInst-V2 to compare model training with altered vs. original instructions (task definitions and examples). Our findings indicate that some current IT models do not fully utilize instructions, and the impressive performance gains of IT may come from models learning superficial patterns, such as the output space and format. We suggest future research on instruction tuning to analyze their performance gains with more comprehensive evaluation and benchmark against trivial baselines. 1321\n"}
{"question": "What is one reason why the class-based IFs are proposed for error detection?", "evidence": "  Influence score on the true class is a stronger indicator of the harmfulness of a data point and is better at differentiating erroneous and correct data points.  ", "options": ["A mislabeled/correct data point often has a very negative/positive influence on data points of the same (true) class."], "answer": "C", "content": "\nIntroduction\nDeep learning models are data hungry. Large models such as transformers (Vaswani et al., 2017) , BERT (Devlin et al., 2019) , and GPT-3 (Brown et al., 2020) require millions to billions of training data points. However, data labeling is an expensive, time consuming, and error prone process. Popular datasets such as the ImageNet (Deng et al., 2009) contain a significant amount of errors -data points with incorrect or ambiguous labels (Beyer et al., 2020) . The need for automatic error detection tools is increasing as the sizes of modern datasets grow.\nInfluence function (IF) (Koh and Liang, 2017) and its variants (Charpiat et al., 2019; Khanna et al., 2019; Barshan et al., 2020; Pruthi et al., 2020) are a powerful tool for estimating the influence of a data point on another data point. Researchers leveraged this capability of IFs to design or detect adversarial (Cohen et al., 2020) , poisonous (Koh et al., 2022; Koh and Liang, 2017) , and erroneous (Dau et al., 2022) examples in large scale datasets. The intuition is that these harmful data points usually have a negative influence on other data points and this influence can be estimated with IFs. Basu et al. (2021) empirically observed that IFs are unstable when they are applied to deep neu- * Joint first authors ral networks (DNNs). The quality of influence estimation deteriorates as networks become more complex. In this paper, we provide empirical and theoretical explanations for the instability of IFs. We show that IFs scores are very noisy when the two data points belong to two different classes but IFs scores are much more stable when the two data points are in the same class (Sec. 3). Based on that finding, we propose IFs-class, variants of IFs that use class information to improve the stability while introducing no additional computational cost. IFs-class can replace IFs in anomalous data detection algorithms. In Sec. 4, we compare IFs-class and IFs on the error detection problem. Experiments on various NLP tasks and datasets confirm the advantages of IFs-class over IFs.\n\nBackground and Related work\nWe define the notations used in this paper. Let z = (x, y) be a data point, where x \u2208 X is the input, y \u2208 Y is the target output; Z = z (i) n i=1 be a dataset of n data points; Z \u2212i = Z\\z (i) be the dataset Z with z (i) removed; f \u03b8 : X \u2192 Y be a model with parameter \u03b8; L Z,\u03b8 = 1 n n i=1 \u2113(f \u03b8 (x (i) ), y (i) ) = 1 n n i=1 \u2113(z (i) ; \u03b8) be the empirical risk of f \u03b8 measured on Z, where \u2113 : Y \u00d7 Y \u2192 R + is the loss function; \u03b8 = arg min \u03b8 L Z,\u03b8 and \u03b8\u2212i = arg min \u03b8 L Z \u2212i ,\u03b8 be the optimal parameters of the model f \u03b8 trained on Z and Z \u2212i . In this paper, f \u03b8 is a deep network and \u03b8 is found by training f \u03b8 with gradient descent on the training set Z.\n\nInfluence function and variants\nThe influence of a data point z (i) on another data point z (j) is defined as the change in loss at z (j) when z (i) is removed from the training set\nEQUATION\nThe absolute value of s (ij) measures the strength of the influence of z (i) on z (j) . The sign of s (ij) show the direction of influence. A negative s (ij) means that removing z (i) decreases the loss at z (j) , i.e. z (i) is harmful to z (j) . s (ij) has high variance because it depends on a single (arbitrary) data point z (j) .\nTo better estimate the influence of z (i) on the entire data distribution, researchers average the influence scores of z (i) over a reference set Z \u2032\ns (i) = 1 |Z \u2032 | z (j) \u2208Z \u2032 s (ij) = L Z \u2032 , \u03b8\u2212i \u2212 L Z \u2032 , \u03b8 (2)\ns (i) is the influence of z (i) on the reference set Z \u2032 . Z \u2032 can be a random subset of the training set or a held-out dataset. Naive computation of s (ij) requires retraining f \u03b8 on Z \u2212i . Koh and Liang (2017) proposed the influence function (IF) to quickly estimate s (ij) without retraining\ns (ij) \u2248 IF (z (i) , z (j) ) \u2248 1 n \u2207 \u03b8\u2113(z (i) ; \u03b8) \u22a4 H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) (3)\nwhere\nH \u03b8 = \u2202 2 L Z, \u03b8/\u2202\u03b8 2\nis the Hessian at \u03b8. Exact computation of H \u22121 \u03b8 is intractable for modern networks. Koh and Liang (2017) developed a fast algorithm for estimating H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) and used only the derivatives w.r.t. the last layer's parameters to improve the algorithm's speed. Charpiat et al. (2019) proposed gradient dot product (GD) and gradient cosine similarity (GC) as faster alternatives to IF. Pruthi et al. (2020) argued that the influence can be better approximated by accumulating it through out the training process (TracIn). The formula for IFs are summarized in Tab. 3 in Appx. A.\nIFs can be viewed as measures of the similarity between the gradients of two data points. Intuitively, gradients of harmful examples are dissimilar from that of normal examples (Fig. 1 ).\n\nInfluence functions for error detection\nIn the error detection problem, we have to detect data points with wrong labels. Given a (potentially noisy) dataset Z, we have to rank data points in Z by how likely they are erroneous. Removing or correcting errors improves the performance and robustness of models trained on that dataset.\nTraditional error detection algorithms that use hand designed rules (Chu et al., 2013) or simple statistics (Huang and He, 2018) , do not scale well to deep learning datasets. Cohen et al. (2020) 2021) empirically showed that IFs with last layer gradient perform as well as or better than IFs with all layers' gradient and variants of IF behave similarly. Therefore, we analyze the behavior of GD with last layer's gradient and generalize our results to other IFs. Fig. 1 shows the last layer's gradient of an MLP on a 3-class classification problem. In the figure, gradients of mislabeled data points have large magnitudes and are opposite to gradients of correct data points in the true class. However, gradients of mislabeled data points are not necessarily opposite to that of correct data points from other classes. Furthermore, gradients of two data points from two different classes are almost perpendicular. We make the following observation. A mislabeled/correct data point often has a very negative/positive influence on data points of the same (true) class, but its influence on other classes is noisy and small.\nWe verify the observation on real-world datasets. (Fig. 2 ). We compute GD scores of pairs of clean data points from 2 different classes and plot the score's distribution. We repeat the procedure for pairs of data points from each class. In the 2-class case, GD scores are almost normally distributed with a very sharp peak at 0. That means, in many cases, a clean data point from one class has no significant influence on data points from the other class. And when it has a significant effect, the effect could be positive or negative with equal probability. In contrast, GD scores of pairs of data points from the same class are almost always positive. A clean data point almost certainly has a positive influence on clean data points of the same class.\nOur theoretical analysis shows that when the two data points have different labels, then the sign of GD depends on two random variables, the sign of inner product of the features and the sign of inner product of gradients of the losses w.r.t. the logits. And as the model becomes more confident about the labels of the two data points, the magnitude of GD becomes smaller very quickly. Small perturbations to the logits or the features can flip the sign of GD. In contrast, if the two data points have the same label, then the sign of GD depends on only one random variable, the sign of the inner product of the feature, and the GD's magnitude remains large when the model becomes more confident. Mathematical details are deferred to Appx. D.\n\nClass based IFs for error detection\nOur class based IFs for error detection is shown in Alg. 1. In Sec. 3.1, we see that an error has a very Algorithm 1 Class based influence function for error detection.\n\nRequire:\n1: Z = z (i) n i=1 : a big noisy dataset 2: C: number of classes 3: for k = 1, ..., C do 9:\nZ \u2032 k = z \u2032(j k ) m k j k =1 : clean data from class k 4: Z \u2032 = C k=1 Z \u2032 k : a clean\ns (i) k = m k j=1 sim(\u2207 \u03b8 \u2113(z (i) ),\u2207 \u03b8 \u2113(z \u2032(j k ) )) m k 10:\nend for 11:\ns (i) = min k (s (i)\nk ) 12: end for 13: \u1e90 = sort(Z, key = s, ascending = True) 14: return \u1e90 strong negative influence on correct data points in the true class, and a correct data point has a positive influence on correct data points in the true class. Influence score on the true class is a stronger indicator of the harmfulness of a data point and is better at differentiating erroneous and correct data points. Because we do not know the true class of z (i) in advance, we compute its influence score on each class in the reference set Z \u2032 and take the minimum of these influence scores as the indicator of the harmfulness of z (i) (line 8-11 create benchmark datasets Z's, we inject random noise into the above datasets. For text classification datasets, we randomly select p% of the data points and randomly change their labels to other classes.\nFor the CoNLL-NER dataset, we randomly select p% of the sentences and change the labels of r% of the phrases in the selected sentences. All tokens in a selected phrase are changed to the same class. The reference set Z \u2032 is created by randomly selecting m k clean data points from each class in Z. To ensure a fair comparison, we use the same reference set Z \u2032 for both IFs and IFs-class algorithms. Models are trained on the noisy dataset Z. To evaluate an error detection algorithm, we select top q% most harmful data points from the sorted dataset \u1e90 and check how many percent of the selected data points are really erroneous. Intuitively, increasing q allows the algorithm to find more errors (increase recall) but may decrease the detection accuracy (decrease precision). Our code is available at https://github.com/Fsoft-AIC/ Class-Based-Influence-Functions.\nResult and Analysis Because results on all datasets share the same patterns, we report representative results here and defer the full results to Appx. C.\nFig. 3(a) shows the error detection accuracy on the SNLI dataset and how the accuracy changes with q. Except for the GC algorithm, our classbased algorithms have higher accuracy and lower variance than the non-class-based versions. When q increases, the performance of IFs-class does not decrease as much as that of IFs. This confirms that IFs-class are less noisy than IFs. Class information fails to improve the performance of GC. To understand this, let's reconsider the similarity measure sim(\u2022, \u2022). Let's assume that there exist some clean data points z \u2032(j) \u2208 Z \u2032 with a very large gradient \u2207 \u03b8\u2113(z \u2032(j) ). If the similarity measure does not normalize the norm of \u2207 \u03b8\u2113(z \u2032(j) ), then z \u2032(j) will have the dominant effect on the influence score. The noise in the influence score is mostly caused by these data points. GC normalizes both gradients, \u2207 \u03b8\u2113(z (i) ) and \u2207 \u03b8\u2113(z \u2032(j) ), and effectively removes such noise. However, gradients of errors tend to be larger than that of normal data points (Fig. 1 ). By normalizing both gradients, GC removes the valuable information about magnitudes of gradients of errors \u2207 \u03b8\u2113(z (i) ). That lowers the detection performance. In Fig. 3 (a), we see that the performance of GC when q \u2265 15% is lower than that of other classbased algorithms. Similar trends are observed on other datasets (Fig. 6 , 7, 8 in Appx. C). Fig. 3(b) shows the change in detection accuracy as the level of noise p goes from 5% to 20%. For each value of p, we set q to be equal to p. Our class-based influence score significantly improves the performance and reduces the variance. We note that when p increases, the error detection problem becomes easier as there are more errors. The detection accuracy, therefore, tends to increase with p as shown in Fig. 3 (b), 9, 10. Fig. 3(c ) shows that GD-class outperforms GD on all entity types in CoNLL2003-NER. The performance difference between GD-class and GD is greater on the MISC and ORG categories. Intuitively, a person's name can likely be an organization's name but the reverse is less likely. Therefore, it is harder to detect that a PER or LOC tag has been changed to ORG or MISC tag than the reverse. The result shows that IFs-class is more effective than IFs in detecting hard erroneous examples.\n\nThe effect of data on error detection algorithms\nWe study the effect of the size and the cleanliness of the reference set on the performance of error detection algorithms.\nThe size of the reference set. We changed the size of classes in the reference set from 10 to 1000 to study the effect of the reference set's size on the detection performance. We report the mean performance of GD and GC algorithms in Tab. 1. We observe no clear trend in the performance as the size of the reference set increases. Our conjecture is that gradients of clean data points from the same class have almost the same direction. Averaging the gradient direction over a small set of data points already gives a very stable gradient direction. Therefore, increasing the size of the reference set does not have much impact on detection performance. \n\nConclusion\nIn this paper, we study influence functions and identify the source of their instability. We give a theoretical explanation for our observations. We introduce a stable variant of IFs and use that to develop a high performance error detection algorithm. Our findings shed light of the development of new influence estimators and on the application of IFs in downstream tasks.\n"}
{"question": "What is the limitation of our model?", "evidence": "Future work could explore improving the reasoning of small models in multi-task settings, as well as the generation of new training data using LLMs, rather than annotating existing datasets.For example, the effects are limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller LMs may not have memorised due to their limited capacity. However, such improvements appear to be task dependent.Even a small number of CoT examples appear to suffice for this. ", "options": ["A:Future work could explore improving the reasoning of small models in multi-task settings, as well as the generation of new training data using LLMs, rather than annotating existing datasets.\nB:For example, the effects are limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller LMs may not have memorised due to their limited capacity. ", "C: However, such improvements appear to be task dependent.", "D: Even a small number of CoT examples appear to suffice for this. "], "answer": "C", "content": "\nIntroduction\nChain of thought (CoT) prompting encourages language models (LMs) to break down a reasoning task into a series of intermediate steps (Wei et al., 2022) . They demonstrate that this prompting significantly increases the task accuracy of large language models (LLMs) across commonsense, symbolic and mathematical reasoning datasets. Here, LLMs are models with at least tens of billions of parameters, such as PaLM 540B (Chowdhery et al., 2022) , GPT-3 175B (Brown et al., 2020) , or UL2 20B (Tay et al., 2022) . However, the reasoning capabilities of smaller LMs do not improve with CoT prompting, mostly producing illogical CoT. Notably, CoT prompting even reduces the accuracy of models with less than 10 billion parameters. Wei et al. (2022) attribute this to abilities, such as semantic understanding and symbolic mapping, only emerging at larger scales. This leads us to our re- * Research conducted during an internship at Google. search question: can the reasoning capabilities of LLMs be transferred to smaller LMs via finetuning?\nThis work explores CoT knowledge distillation (Hinton et al., 2015) from PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) to different sizes of the smaller language model T5 (Raffel et al., 2020) , such as T5 XXL, XL and base, which have 11 billion, 3 billion and 220 million parameters, respectively. As a result of our work, we make two recommendations: (1) perform knowledge distillation by finetuning the student model on the CoT generated by a large teacher model; and (2) generate the CoT from an LLM, as proposed by Wei et al. (2022) , but crucially provide the solution to the task in the few-shot prompt. We demonstrate that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets irrespective of the teacher model used. For example, we show an accuracy increase from 8.11% to 21.99% and 18.42% on the GSM8K (Cobbe et al., 2021) dataset when finetuning T5 XXL on PaLM 540B and GPT-3 175B generated CoT data, respectively.\n\nRelated Work\nThis work is inspired by the seminal work of Wei et al. (2022) on CoT prompting. They demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021) . Wang et al. (2022) show that task accuracy can be further improved by using self-consistency in CoT prompting. Selfconsistency samples CoT reasoning paths from a model's decoder and returns the most consistent path by taking the majority vote. Subsequently, Chung et al. (2022) explore finetuning a FLANbased (Wei et al., 2021) version of PaLM on manually generated CoT data.\nConcurrent to our work, a small number of other works propose methods focused on CoT student-teacher knowledge distillation. Ho et al. (2022) and Li et al. (2022) also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. We also achieve a higher accuracy on common datasets, such as GSM8K, than Ho et al. (2022) . In contrast to our work, Shridhar et al. (2022) focus on training two models, one for problem decomposition and one for solving. Yet differently, the focus of Eisenstein et al. (2022) relies on producing markupand-mask explanations for open-book question answering. Lastly, Huang et al. (2022) present one related experiment, however, we present a more indepth exploration on more datasets. To the best of our knowledge, our work is the first to extensively explore the improvement of the reasoning ability of small LMs via knowledge distillation across multiple model architectures, and observing the effects of student model size and dataset size on accuracy.\n\nMethod\nWe propose a two-step pipeline for CoT knowledge distillation. The first step comprises annotating an existing supervised dataset with CoT reasoning generated by a teacher model. To generate high quality data, we propose using LLMs, such as PaLM 540B or GPT-3 175B, as teachers, based on the finding that CoT reasoning improves with model scale (Wei et al., 2022) . Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by Wei et al. (2022) . We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT. This is based on the observation that providing this guidance allows LLMs to correct small mistakes in the CoT. Lastly, we remove all incorrect CoT based on the target answer to prevent the student to learn from bad examples. The second step comprises finetuning a student model via teacher forcing (Williams and Zipser, 1989) . The student is provided with the question as input, and the CoT and answer as the target. As the model is trained on producing a CoT during finetuning, prompting is not required. \n\nExperimental Setup\nWe follow a similar experimental setup to Wei et al. (2022) , focusing on tasks covering arithmetic, commonsense and symbolic reasoning.\n\nArithmetic Reasoning\nWe benchmark the proposed method on the following math word problem datasets: (1) GSM8K (Cobbe et al., 2021) , ( 2) MAWPS (Koncel-Kedziorski et al., 2016) and (3) ASDiv (Miao et al., 2021) . We use the official training and testing split for GSM8K, taking the last 10% of the training split for validation, and the 5-fold cross validation splits available for MAWPS and ASDiv. We evaluate task accuracy by checking for the target answer as the final answer in the CoT. In addition, we compute the task accuracy given an external calculator, to account for arithmetic mistakes made by the model, despite the CoT being correct. The external calculator moves through the generated output, recalculating the left hand-side of equations. It then replaces the right-hand side with the calculated output, to avoid arithmetic mistakes being carried forward. For example, if a model outputted '5 + 5 = 11. 11 * 2 = 22', then the external calculator would first calculate '5+5' and replace the '11' with a '10'. In the subsequent equation, it would also replace the '11' with a '10' and arrive at the final result of '20'.\n\nCommonsense Reasoning\nWe benchmark the model's ability to perform commonsense reasoning on the StrategyQA dataset (Geva et al., 2021a) . As a testing split is not available, we do not shuffle the dataset to allow reproducing our split of taking the first 80% as training data, the following 10% as validation data, and the final 10% as testing data. We compute task accuracy in the same manner as previously mentioned.\n\nSymbolic Reasoning\nLastly, we benchmark the model on two synthetic tasks for symbolic reasoning: (1) last letter concatenation and (2) coinflip (Wei et al., 2022) . Last letter concatenation prompts the model to concatenate the last letter of each word in a string. Coinflip prompts the model to perform state tracking of the coin being flipped. We evaluate task accuracy in the same manner as before. Due to the rigid structure of the datasets, we focus on evaluating the model's generalizability to out-of-distribution (OOD) examples. We finetune the models on examples of length two and evaluate on sequences of length three and four. We initially infer the CoT using PaLM 540B, however, find that the LLM is able to perfectly replicate the desired CoT bar one example due to the rigidness of the template. We therefore decide to use the template generated CoT in our experiments.\n\nBaselines and setup\nWe select PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) as teacher models. We select PaLM 540B based on the stateof-the-art results on the benchmarking datasets reported by Wei et al. (2022) , and confirm the observed trends with GPT-3 175B. The publicly accessible teacher models are prompted as described in Section 3.\nWe select different sizes of T5 (Raffel et al., 2020) as student models, as T5 is publicly available in many sizes. The student models are trained on the PaLM 540B or GPT-3 175B generated CoT data as described in Section 3. We establish T5 XXL model finetuned on the original target as the baseline. We refrain from shuffling the datasets to allow for reproducibility.For the MAWPS and ASDiv dataset, we perform 5-fold cross validation. For all remaining datasets, we take 10% of the Input: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nOutput:\nRoger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. We refer the reader to Appendix A for an overview of the dataset licenses. We also refer the reader to Appendix B for an overview of the computatinal resources.\n\nArithmetic reasoning\nTable 1 details the task accuracy with and without an external calculator for the arithmetic reasoning benchmarks. Our results show that the proposed method improves task accuracy across all datasets. Most notably, the task accuracy of MAWPS is significantly improved. The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. 1 : Task accuracy across arithmetic reasoning datasets for T5 XXL without finetuning (baseline) and finetuned on PaLM 540B generated chain-of-thought (CoT). We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on generating chain-of-thought data\nWe perform an ablation study to confirm that providing a LLM with the target during CoT generation is beneficial. We found that for the GSM8K dataset, PaLM 540B only achieves a 59.98% accuracy if prompted without the target. In comparison, when including the target in the prompt the accuracy is 79.37%. A superficial explanation would be that when the model is conditioned on the expected answer, it produces the same CoT but copies the answer. However, an analysis of a subset of the differences between CoT produced with and without this conditioning shows that most of the benefits actually come from the model correcting CoT that had a single step missing or was wrong.\n\nCommonsense reasoning\nFor the StrategyQA dataset (Table 3 ), we found that using CoT finetuning improves accuracy from 68.12% to 71.98%, using only 1319 of the original 1648 examples. Compared to the arithmetic reasoning datasets, the improvement is not as significant. This can be explained by the model lacking factual knowledge that the dataset requires. The task is heavily focused on the model reasoning on such knowledge, however, a smaller LM is most likely not in possession of this knowledge compared to a larger model with higher memorisation capacity.\n\nSymbolic reasoning\nTable 2 shows the results obtained for the synthetic symbolic reasoning datasets, focusing on OOD generalization. Focusing on Last Letter Concatenation, it can be stated that both traditional finetuning and the suggested method fail at generalizing to a longer sequence length. In comparison, the proposed method significantly increases accuracy for the Coinflip dataset with regard to generalizing to three coinflips. In contrast, generalisation to four coinflips is slightly weaker than the baseline, which performs very strongly. This may be related to the task length being twice that of the training task.\n\nReplicating Results using different Teacher Models\nWe demonstrate the robustness of our method using a different teacher model, namely GPT-3 175B. Table 3 shows the results for GSM8K and Strat-egyQA when T5 XXL is finetuned on CoT data generated by GPT-3. The results show that the proposed method elicits improvements also with other Table 3 : Task accuracy for T5 XXL finetuned on chainof-thought (CoT) data generated by PaLM 540B and GPT-3 175B. We also finetune on the reasoning steps provided by the datasets. We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on model size\nWe investigate the performance gain achieved via finetuning student models of different sizes. Figure 3 shows the performance gain achieved when finetuning T5 of different sizes on the GSM8K dataset.\nOur results show that T5 base, with 44 times fewer parameters than T5 XXL, matches the performance of the baseline T5 XXL when trained on CoT data. Moreover, given an external calculator, even T5 small outperforms the baseline T5 XXL.\n\nAblation study on dataset size\nWe also investigate the trade-off between the performance gain from CoT finetuning and dataset size. \n\nDiscussion\nWe demonstrate that finetuning larger LMs on the CoT data generated by LLMs of over 100 billion parameters can significantly improve task accuracy. Even a small number of CoT examples appear to suffice for this. However, such improvements appear to be task dependent. For example, the effects are limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller LMs may not have memorised due to their limited capacity. Nevertheless, there is some performance improvement, which may be attributed to the model learning how to approach such tasks. Moreover, the CoT knowledge distillation pipeline presented allows to trade-off model and dataset size with accuracy. Future work could explore improving the reasoning of small models in multi-task settings, as well as the generation of new training data using LLMs, rather than annotating existing datasets.\n\nConclusion\nThis work explores CoT knowledge distillation from LLMs of over 100 billion parameters to smaller LMs. We propose a knowledge distillation pipeline consisting of two keys steps: (1) generate CoT for existing datasets using LLMs and\n(2) finetune smaller LMs on the CoT. Our results demonstrate that finetuning on CoT improves task accuracy across a range of benchmarking datasets.\n"}
{"question": "How does the paper propose to handle multiple edit intentions during text revision?", "evidence": "  Our system is based on prefix-tuning, which first gets prefixes for every edit intent, and then trains a prefix transfer module. This prefix transfer module is configured as two attention units that act respectively on the key states and the value states at each attention layer of the PLM.  ", "options": ["A. By using complex rule-based systems", "B. By fully fine-tuning the language model", "C. By leveraging prefix-tuning and a prefix transfer module", "D. By manually annotating each edit intention in the text"], "answer": "C", "content": "\nIntroduction\nRevision is an essential process to improve the text quality (Vaughan and McDonald, 1986) . During this process, writers perform various editing operations on the text with different editing intentions. As shown in Figure 1 , the writer corrects misspelled words to improve text fluency, deletes redundant words to improve text clarity, adds connective words to improve text coherence, inserts adverbs to convey the writer's writing preferences (style) and modifies data to update text information (meaning-changed).\nLots of recent studies have focused on a text revision task corresponding to a specific edit intention, such as grammatical error correction (Omelianchuk She went to the markt\nThe changes made the paper better than before.\nText Revision She works hard.\nShe is successful.\nEverything was rotten.\n\nShe went to the markt market\nThe changes made the paper better than before improved the paper.\nShe works hard. She; therefore, she is successful.\nEverything was awfully rotten. This method improves the model accuracy from 64% to 7883%. et al., 2020; Kaneko et al., 2020; Liu et al., 2021; Yang et al., 2022 ), text simplification (Dong et al., 2019; Jiang et al., 2020; Omelianchuk et al., 2021; Martin et al., 2022) , and text style transfer (Malmi et al., 2020; Reid and Zhong, 2021) . The work divides text revision into several independent problems. While some methods with strong universality can be applied to multiple tasks (Malmi et al., 2019; Stahlberg and Kumar, 2020; Mallinson et al., 2020) , they train different models on various data sets. Real-world scenarios require addressing multiple types of editing errors at the same time, such as grammatical errors, spelling errors, etc. But these methods failed to integrate knowledge from these tasks into a unified model.\n\nmeaningchanged\nTo solve the problem, Du et al. (2022) attempted to train one model using data with multiple editing intentions and leveraged edit intent information by simply appending it to the input. However, when adding a new intent, the entire model must be re-trained. A more lightweight and scalable approach to multi-intent text revision is still required.\nLi and Liang (2021) proposed a new kind of prompt tuning method to quickly adapt a pretrained model to new tasks, which is called prefixtuning. Prompt tuning can help the pre-trained language model to locate the task learned in pretraining and enable the related knowledge to model text revision with different edit intentions (Reynolds and McDonell, 2021) . This method enables a model to handle multiple edit intentions in a lightweight and scalable way.\nIn this paper, we present our method: a prefixtuning-based model which adapts to text revision with multiple edit intentions. This method involves a two-step training process. In the first step, we initialize a pre-trained language model (PLM) and train multiple prefixes on it. Each edit intention corresponds to a prefix. In the second step, a prefix transfer module is trained at each attention layer of the PLM. The prefix transfer module is configured as two attention units that act respectively on this layer's key states and value states. It enables our model to learn a tailored prefix for the given input with the help of prefix embeddings from the predefined tasks.\nWe conduct experiments on ITERATER (Du et al., 2022) , an iterative text revision dataset. It mainly contains parallel sentences with five edit intentions: fluency, coherence, clarity, style, and meaning-changed. The results show that our approach performs better than the fully fine-tuned BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020) baselines reported in Du et al. (2022) with fewer training parameters.\n\nIterative Text Revision\nFor the first time, Du et al. (2022) systematically studied the iterative revision phenomenon in human writing. They presented the ITERATER, an annotated dataset across multiple domains of formally human-written text, which includes Wikipedia, ArXiv, and Wikinews. And they trained several types of text revision models using ITERATER. Dwivedi-Yu et al. (2022) presented EDITEVAL, an instruction-based benchmark, to evaluate the editing capabilities of models and they also included the test set of ITERATER in it. Based on Du et al. (2022) , our work further explores the method of text revision.\n\nTransfer Learning of Prompt Tuning\nTransfer learning is a common and powerful technique in NLP (Raffel et al., 2020) . Some recent studies have tried to improve prompt tuning performance by leveraging the knowledge of multiple related or unrelated tasks. Asai et al. (2022) used an attention module to make use of the knowledge in exiting soft prompts (Lester et al., 2021) while learning a new task. Chen et al. (2022) improved the few-shot text summarization by multi-task pretraining and prefix-tuning. Specifically, they pretrained a summarization model on a set of popular summarization datasets and then conducted prefixtuning for it on an unseen summarization task. Different from their modeling of a new task through existing tasks, our work aims to achieve the mutual utilization of knowledge between different edit intents in text revision.\n\nMethod\nThe revision task can be defined as the following process: given a source sentence x = [x 1 , . . . , x m ] and an optional edit intent e \u2208 E to generate a revised sentence y = [y 1 , . . . , y n ], where E is the set of all edit intentions. Note that e is optional because it can be inferred from the input x.\nOur method is depicted in Figure 2 . It includes two stages: the multi-prefix tuning stage and the prefix transfer stage.\n\nMulti-Prefix Tuning Stage\nThe prefix is a set of parameters on every attention layer of PLM. For an edit intention e, at each attention layer, the prefix can be described as P e = {P K e , P V e }, where P K e and P V e are parameters added before the key states and value states in this attention layer. After adding these parameters, the calculation of the attention head in this layer becomes:\nH = Attention(Q, [P K e ; K], [P V e ; V ]) (1)\nwhere H is the output vector sequence; Q, K, V are query states, key states, and value states, respectively; Attention means scaled dot-product attention. Only P K e and P V e are updated during the training process. Note that we ignore the layer number information because the operation for each layer is the same.\nAs shown in the left part of Figure 2 , for every edit intention e, we train a prefix P e accordingly. In this way, the model could revise an intentionannotated text by activating the corresponding prefix at inference.\n\nPrefix Transfer Stage\nIdentifying edit intention is always an ambiguous work. At the prefix transfer stage, we aim to build a new prefix for an unannotated input instance by transferring existing prefixes. The new prefix P new is instance-specific.\nThe prefix transfer stage is described in the right part of Figure 2 . At each layer, we rearrange the prefixes {P e | e \u2208 E} obtained in the last stage as\nP K = {P K\ne | e \u2208 E} and P V = {P V e | e \u2208 E} according to whether they are configured before the key states or before the value states. Then a pair of attention units G K and G V are trained for P K and P V .\nTake G K as an example. It calculates the similarity between the key states K and every P K e in P K to get attention scores.\nThe similarity can't be calculated directly, because K and P K e have different lengths. So we perform the max-pool operation for length dimension on K and P K e . After that, we obtain K \u2208 R d and P K e \u2208 R d , where d is the dimension of the hidden states in the PLM.\nTo get attention scores, we train a fully connected layer to extract features from K:\nEQUATION\nwhere W \u2208 R d\u00d7d is a transfer matrix updated during training. Following Asai et al. (2022) , we use SiLU (Elfwing et al., 2018) for the non-linear layer and add a Layer Norm (Ba et al., 2016) layer:\nEQUATION\nThen, we calculate the attention scores for intent e as follows:\nEQUATION\n)\nwhere T is the softmax temperature (Radford et al., 2021) which could avoid making the attention unit over-confident.\nFinally we use them to build P K new as follows:\nEQUATION\nIn the same way, we get P V new by G V . Using the new prefix P new = [P K new , P V new ], our system could revise the unannotated input instance with the knowledge from existing prefixes.\n\nExperimental Setup\nWe choose BART-large as the PLM for our system and use adapter-transformers (Pfeiffer et al., 2020) to implement prefix-tuning. More implementation details are in Appendix A.\n\nDatasets\nWe conduct our experiments on the iterative text revision dataset: ITERATER (Du et al., 2022) . We remove the Other class of the data as it essentially contains a variety of unrecognized edit intentions and accounts for a small proportion (1.44%). The entire dataset consists of two parts: ITERATER-HUMAN and ITERATER-FULL. The former is a smaller dataset with manual annotation of edit intentions, while the latter is a large dataset annotated by a classification model trained on ITERATER-HUMAN. We train our model on both of them. Following Du et al. (2022) , we report the results on the test set of ITERATER-HUMAN in Section 5, which is completely a human-created dataset and is reliable for evaluation. We show more details of the datasets in Appendix B.\n\nEvaluation Metrics\nFollowing previous work, we report three metrics: SARI (Xu et al., 2016) , Rouge-L (Lin, 2004) , and BLEU (Papineni et al., 2002) . Among them, SARI is considered an important metric in situations where input text and output text have a large overlap in words. It also indicates the positive impact of revisions on document quality. The setting of evaluation metrics is the same as Du et al. (2022) . We use the metrics package from Huggingface transformers (Wolf et al., 2020) to calculate the SARI, BLEU, and Rouge-L scores.\n\nModels Setup and Baselines\nUsing our method, we train the models in two ways: the model that only trains the multi-prefix tuning stage and that trains both the multi-prefix tuning stage and the prefix transfer stage.\nWe compare our method with three baselines: full fine-tuning BART (BART-FineTune), full finetuning PEGASUS (PEGASUS-FineTune), and prefixtuning of BART with a single prefix (BART-SinglePrefix). Both BART and PEGASUS are generative models based on the transformer architecture. Compared to the edit-based model FELIX, they perform better. We use the results reported by Du et al. (2022) for these two models. Furthermore, we compare BART-SinglePrefix as a possible technical solution as we choose BART as our backbone model. BART-SinglePrefix trains only one prefix on the entire dataset.\nAll three baselines are trained with two config-urations. The first configuration is using the pure sentence pairs without edit intention annotations to train the model. The second configuration is appending an edit intent token at the beginning of the input text during the training process, which is the same as the approach of Du et al. (2022) .\n5 Results and Analysis\n\nMain Results\nThe main results are shown in Table 1 . Compared to training with a single prefix, the setting of multiple prefixes can improve the results, especially training on ITERATER-HUMAN. Meanwhile, with fewer training parameters, the multi-prefix setting could achieve a comparable SARI score and better average score than the fully fine-tuned BART and PEGASUS baselines. Moreover, prefix transfer could further improve the model's performance. Training on ITERATER-HUMAN, prefix transfer significantly improves the SARI score from 33.12 to 36.01 and gets the highest average score of 67.91. Training on ITERATER-FULL, prefix transfer can also improve the average score from 67.23 to 68.36.\nAn interesting phenomenon is that training on different datasets results in different gains for prefix transfer in evaluation metrics. On ITERATER-HUMAN, prefix transfer improves the SARI score significantly. While on ITERATER-FULL, prefix transfer mainly improves the BLEU score and Rouge-L score. One possible explanation is that in situations when the training data is small, prefix transfer tends to learn more editing operations to improve text quality. In this way, the SARI score related to editing operations will be improved significantly. When the training data is sufficient, pre- fix transfer will model the gold reference in more detail. So the BLEU score and the Rouge-L score will be improved.\n\nAnalysis\nWe further tried to use different training data at different stages of training to conduct experiments.\nThe results are shown in Table 2 . We find that the best practice is to train the model on ITERATER-FULL in the multi-prefix tuning stage and on ITERATER-HUMAN in the prefix transfer stage, which gets the highest SARI score and average score. This may be because of the different distributions of manually annotated edit intent and automatically annotated edit intent. The auto-annotated dataset ITERATER-FULL contains many incorrectly classified sentences, which may cause mismatched knowledge in prefixes. In the prefix transfer stage, due to the existence of mismatched knowledge and incorrectly classified sentences, the continued use of the same training data may finally cause a certain degree of negative transfer. However, if we use ITERATER-HUMAN in the prefix transfer stage, the impact of negative transfer will be mitigated, because ITERATER-HUMAN only contains correctly classified sentences.\nIn Appendix C, we separately provide the performance results on different edit intentions of the best-performing model.\n\nConclusion\nIn this paper, we introduce a new method for multiintent text revision. The system is based on prefixtuning, which first obtains a prefix for every edit intention and then learns to transfer the knowledge in prefixes for every input instance by training a prefix transfer module. This prefix transfer module is configured as two attention units that act respectively on the key states and the value states at each attention layer of the PLM. In this way, our method can make full use of the knowledge of various edit intentions and does not need to anno-tate the intentions of the input. The experimental results show that our method significantly outperforms baselines, and both multi-prefix and prefix transfer settings could improve the performance.\n"}
{"question": "What didn't our study find?", "evidence": "  In this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs.  We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples.  /D. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n ", "options": ["A. By applying straightforward data interventions on a small set of training data, gender bias in Language Models (LLMs) can be effectively mitigated.", "B. A pretrained Language Model (LLM) with biases can be utilized to identify the most impactful examples for training in order to effectively address and mitigate biases.", "C.Our methods can operate with only a few examples and do not necessitate any additional training of auxiliary models.", "D. Our work will for sure  benefit further research in the domain of equaty"], "answer": "D", "content": "\nIntroduction\nRecently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019 ; * Equal Contribution Barocas et al., 2017; Kurita et al., 2019) . Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\nPrevious work (Nadeem et al., 2021) , (Nangia et al., 2020a) , (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021) , we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop bias mitigation strategies feasible.\n\u2022 We introduce a novel data sampling technique that utilises LLMs to mine for the most biased samples from a dataset and can benefit existing state-of-the-art debiasing methods. When used for debiasing a model, these few samples serve as exemplars and induce large reductions in gender bias.\n\nGender Bias in BERT Predictions\nMost-biased data samples ____ is very good at cooking but not great at work. \n\nReduced Gender Bias in BERT Predictions\nFigure 1 : Our method can be summarized as a combination of bias discovery and mitigation. First, we use a pre-trained LLM to find the most gender-biased samples. Then, we apply our data intervention techniques and use these modified training samples to fine-tune the model. Experiments show that our method is very effective at reducing gender bias, outperforming three state-of-the-art baselines and being comparable to two other baselines.\n\nRelated Work\nIn recent years, there has been growing concern about the bias/stereotypical discriminatory behavior by NLP models, particularly concerning gender. Several studies have investigated the presence of gender bias in various NLP tasks and proposed methods for mitigating it.\nOne line of research has focused on analyzing the extent of gender bias in pre-trained language models such as BERT and GPT-2. These studies have found that these models exhibit a significant amount of gender bias in their word embeddings for BERT (Jentzsch and Turan, 2022) and for GPT-2 (Kirk et al., 2021) and are prone to making stereotypical gender-based predictions (e.g., assuming that a doctor is male and a nurse is female). A standard evaluation metric used in this line of research is Stereotype metrics such as StereoSet (Nadeem et al., 2021) , which evaluates the model's ability to predict gender stereotypes and CrowS pairs (Nangia et al., 2020b) which measure whether a model generally prefers more stereotypical sentences. A similar line of work is gender bias tests proposed in BIG-bench (Srivastava et al., 2022) . The tests assess the language model's gender biases, stereotypes, and ability to infer gender information. It evaluates gender bias and stereotype between male and female, and gender minority bias and stereotype between majority and minority. It also examines the model's language modeling performance, which can be affected during de-biasing.\nAnother line of research has proposed methods for debiasing these models. These methods can be broadly categorized into two groups: data-based and algorithm-based. Data-based methods aim to reduce bias by removing or altering biased words from the training set. In contrast, algorithm-based methods aim to modify the model's architecture or training procedure to reduce bias. One popular databased method is \"uncertainty sampling\" (Lewis and Gale, 1994) , where the model is trained on the instances that it is most uncertain about, which can help to reduce bias by forcing the model to learn from a diverse set of examples. A popular algorithmbased method is \"Adversarial Debiasing\" proposed by Zhang et al. (2018) , which fine-tunes the model using an adversarial loss to make it less sensitive to sensitive attributes such as gender. OSCar proposed by Dev et al. (2021) , is another algorithm based method that utilizes the idea of disentangling \"problematic concepts\" like occupation and gender relationship instead of removing them altogether. MABEL (He et al., 2022) has both algorithm and data-based components, as it first augments the training data by swapping gender words and then applies a contrastive learning objective and alignment via entailment pairs. Their data augmentation strategy is similar in spirit to the data intervention techniques we propose, however our analysis does not require training auxiliary models and uses significantly lesser data.\nData-based methods include the \"Equalization\" technique proposed by Bolukbasi et al. (2016) , which aims to equalize the representation of genderspecific words in the embedding space, the \"Counterfactual Data Augmentation\" (CDA) method proposed by Zimmermann and Hoffmann (2022) Pre-trained LLMs are biased towards different genders, as seen in a simple mask-fill experiment using BERT. (Here, and in the rest of the paper, we assume a binary treatment of gender for simplicity.) The task is then to mask out the gender-related nouns and pronouns (such as he, she, her, woman, etc.) and get BERT to predict the masked words for the affected sequences in the dataset. Here, we consider a fixed list of gender-specific words curated from previous work (Lu et al., 2018; Zmigrod et al., 2019) and neutral words list 1 . We finally compute the \"total confidence difference\" as the sum of differences in the model's prediction confidence for each gender-word pair (such as confidence of predicting he \u2212 she, man \u2212 woman, etc.). Formally, we define total confidence difference as\nEQUATION\n))| where f (x) represent the confidence of model's prediction, N is the total number of tokens in the dataset and x is the tokenized gender word. The higher this number, the more biased the model is concluded to be. We compute the metric at token level and ensure that each of the gender word gets tokenized into exactly one token by initially extending the tokenizer with our gender word list. The top 3 biased gender-word pairs in StereoSet are shown in Table 1 . Intuitively, our technique for gauging bias in LLMs is sensitive to the fixed word list used to represent the sensitive attributes (here, gender). In Table 2 , we show the number of words covered by the word list used for both WikiText-2 and StereoSet datasets.\n\nData Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vocabulary of the model and try to balance the confidence of prediction on opposite-gender words by using phrases instead. Thus, we call our final approach random-phrase-masking as we instead substitute words with phrases that reflect the equality of gender. This approach not only reduces gender bias but also preserves the original meaning of the sentence in most cases. In our approach, we chose the phrases and order of gender words at random with equal probability. Additionally, we hypothesize that the choice of the dataset for fine-tuning is also essential. We choose two datasets: the WikiText-2 (Merity et al., 2017) dataset, which has implicit gender bias since its sources from Wikipedia articles, and the Stere-oSet dataset (Nadeem et al., 2021) , which has explicit/more gender bias as it has been designed to evaluate gender bias. WikiText-2 2 has 600 train articles and roughly 2M tokens while StereoSet 3 (dev) has 2123 samples out of which we only consider 800 samples which are not unrelated. Naturally, our data intervention method should work better on a dataset with training examples with gender bias while being devoid of meaningful gender associations like \"She needs a gynecologist,\" where the gender of the person is important. By testing our method on both datasets, we can understand the sensitivity of our approach to the quality of training samples used.\n\nBias Evaluation Metrics\nWe focus on evaluating the bias of a model while also measuring its language modeling capability. The ideal model would not just be one with the least bias but also one which does not compromise its language modeling performance. The dual estimation of bias and performance of a model was proposed in the StereoSet benchmark (Nadeem et al., 2021) , with the Language Modeling Score (LMS) measuring the percentage of times a meaningful token is predicted for the mask as opposed to a meaningless token, the Stereotype Score (SS) measuring the percentage of times the model predicted a stereotypical word as compared to an anti-stereotypical word, and an idealized CAT score (ICAT) combining the LMS and SS score into a single metric. An ideal model has an ICAT score of 100, while the worst biased model has an ICAT score of 0. We additionally evaluate the CrowS-Pairs benchmark (Nangia et al., 2020a) , which captures data with greater diversity in both the stereotypes expressed and the structure of sentences (50 is ideal). However, we note that the Crow-S benchmark is much more limited compared to StereoSet (Nadeem et al., 2021) in terms of both the volume and variety of linguistic phenomenon relating to gender bias it covers.\n\nExperiments\nWe compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work. For a fairer comparison, we run the baselines with the same size (100) of the training set as our method. For all of our experiments, we consider the \"bert-base-uncased\" pre-trained model available from HuggingFace. For fine-tuning our model, we select a varying number of most-biased training samples (10, 50, and 100) from the WikiText-2 and StereoSet (we only use the dev set) datasets, as discussed in section 4. We also compare this to a random selection of data points as an ablation study. On the selected dataset, we apply our interventions and obtain the modified dataset, which is then used to fine-tune our pre-trained model using masked language modeling (MLM) loss. The key point is that we only fine-tune the model on the gender words conditioned on the remaining text, significantly reducing the fine-tuning time. We perform ablations on various types of interventions as discussed in Table 7 . The model is trained for 30 epochs, with a learning rate of 0.001 and AdamW optimizer. We ran all of our experiments on NVIDIA Tesla T4 GPU on Google Colab for roughly 48 hours. For all experiments, we report the numbers as the mean and standard deviations (6) of 3 different runs. Our experiment code can be found here. 4 \n\nResults\nTable 4 shows the StereoSet and Crow-S scores for our baselines and our best-performing interventions on the WikiText-2 Dataset. In the StereoSet benchmark, we observe that random-phrase-masking obtains lower SS than all other baselines. On the Crow-S benchmark, random-phrase-masking does better than thre of the baselines except Sen-tenceDebias which achieves slightly better scores. While random-phrase-masking results in lower SS scores than neutral-masking, it also obtained [MASK] is very good at cooking but not great at [MASK] work. she is very good at cooking but not great at her work.\nhe is very good at cooking but not great at farm work. Being a mother is not easy since she will have to stay home and take care of the child.\nBeing a father is not easy since one will have to stay home and take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\n\nShe is very caring and kind but not good at what she does.\nHe is very caring and kind but not good at what he does.\n\nInput Sentence:\nOutput of Biased Model:\n\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to modify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the same pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the our method is successfully able to reduced biased substitutions. very low LMS scores. We attribute this performance degradation to the blunt substitution of phrases that our method uses, which might lead to odd-sounding sentences. In the Crow-S benchmarks, we see similar behavior and find that random-phrase-masking does better than neutral-masking. Since we believe that our method is sensitive to the choice of the dataset, we also present results on the StereoSet (dev) dataset 6. In Figure 2 , we perform a qualitative analysis of our proposed approach and find that random-phrase-masking is able to flip the predictions on fill-mask tasks for stereotypical sentences.\n\nConclusion\nIn this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs. We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples. Evaluation of our methods on state-of-the-art bias benchmarks empirically suggests that our methods effectively reduce gender bias. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n"}
{"question": "Which is not the baselines we use to make a comparison?", "evidence": "  We compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work.   ", "options": ["A. Original pre-trained model.", "B. The application of dropouts to neural networks", "C. Algorithmic de-biasing technique INLP technique", "D.  Non-data-based approach CDA"], "answer": "D", "content": "\nIntroduction\nRecently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019 ; * Equal Contribution Barocas et al., 2017; Kurita et al., 2019) . Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\nPrevious work (Nadeem et al., 2021) , (Nangia et al., 2020a) , (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021) , we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop bias mitigation strategies feasible.\n\u2022 We introduce a novel data sampling technique that utilises LLMs to mine for the most biased samples from a dataset and can benefit existing state-of-the-art debiasing methods. When used for debiasing a model, these few samples serve as exemplars and induce large reductions in gender bias.\n\nGender Bias in BERT Predictions\nMost-biased data samples ____ is very good at cooking but not great at work. \n\nReduced Gender Bias in BERT Predictions\nFigure 1 : Our method can be summarized as a combination of bias discovery and mitigation. First, we use a pre-trained LLM to find the most gender-biased samples. Then, we apply our data intervention techniques and use these modified training samples to fine-tune the model. Experiments show that our method is very effective at reducing gender bias, outperforming three state-of-the-art baselines and being comparable to two other baselines.\n\nRelated Work\nIn recent years, there has been growing concern about the bias/stereotypical discriminatory behavior by NLP models, particularly concerning gender. Several studies have investigated the presence of gender bias in various NLP tasks and proposed methods for mitigating it.\nOne line of research has focused on analyzing the extent of gender bias in pre-trained language models such as BERT and GPT-2. These studies have found that these models exhibit a significant amount of gender bias in their word embeddings for BERT (Jentzsch and Turan, 2022) and for GPT-2 (Kirk et al., 2021) and are prone to making stereotypical gender-based predictions (e.g., assuming that a doctor is male and a nurse is female). A standard evaluation metric used in this line of research is Stereotype metrics such as StereoSet (Nadeem et al., 2021) , which evaluates the model's ability to predict gender stereotypes and CrowS pairs (Nangia et al., 2020b) which measure whether a model generally prefers more stereotypical sentences. A similar line of work is gender bias tests proposed in BIG-bench (Srivastava et al., 2022) . The tests assess the language model's gender biases, stereotypes, and ability to infer gender information. It evaluates gender bias and stereotype between male and female, and gender minority bias and stereotype between majority and minority. It also examines the model's language modeling performance, which can be affected during de-biasing.\nAnother line of research has proposed methods for debiasing these models. These methods can be broadly categorized into two groups: data-based and algorithm-based. Data-based methods aim to reduce bias by removing or altering biased words from the training set. In contrast, algorithm-based methods aim to modify the model's architecture or training procedure to reduce bias. One popular databased method is \"uncertainty sampling\" (Lewis and Gale, 1994) , where the model is trained on the instances that it is most uncertain about, which can help to reduce bias by forcing the model to learn from a diverse set of examples. A popular algorithmbased method is \"Adversarial Debiasing\" proposed by Zhang et al. (2018) , which fine-tunes the model using an adversarial loss to make it less sensitive to sensitive attributes such as gender. OSCar proposed by Dev et al. (2021) , is another algorithm based method that utilizes the idea of disentangling \"problematic concepts\" like occupation and gender relationship instead of removing them altogether. MABEL (He et al., 2022) has both algorithm and data-based components, as it first augments the training data by swapping gender words and then applies a contrastive learning objective and alignment via entailment pairs. Their data augmentation strategy is similar in spirit to the data intervention techniques we propose, however our analysis does not require training auxiliary models and uses significantly lesser data.\nData-based methods include the \"Equalization\" technique proposed by Bolukbasi et al. (2016) , which aims to equalize the representation of genderspecific words in the embedding space, the \"Counterfactual Data Augmentation\" (CDA) method proposed by Zimmermann and Hoffmann (2022) Pre-trained LLMs are biased towards different genders, as seen in a simple mask-fill experiment using BERT. (Here, and in the rest of the paper, we assume a binary treatment of gender for simplicity.) The task is then to mask out the gender-related nouns and pronouns (such as he, she, her, woman, etc.) and get BERT to predict the masked words for the affected sequences in the dataset. Here, we consider a fixed list of gender-specific words curated from previous work (Lu et al., 2018; Zmigrod et al., 2019) and neutral words list 1 . We finally compute the \"total confidence difference\" as the sum of differences in the model's prediction confidence for each gender-word pair (such as confidence of predicting he \u2212 she, man \u2212 woman, etc.). Formally, we define total confidence difference as\nEQUATION\n))| where f (x) represent the confidence of model's prediction, N is the total number of tokens in the dataset and x is the tokenized gender word. The higher this number, the more biased the model is concluded to be. We compute the metric at token level and ensure that each of the gender word gets tokenized into exactly one token by initially extending the tokenizer with our gender word list. The top 3 biased gender-word pairs in StereoSet are shown in Table 1 . Intuitively, our technique for gauging bias in LLMs is sensitive to the fixed word list used to represent the sensitive attributes (here, gender). In Table 2 , we show the number of words covered by the word list used for both WikiText-2 and StereoSet datasets.\n\nData Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vocabulary of the model and try to balance the confidence of prediction on opposite-gender words by using phrases instead. Thus, we call our final approach random-phrase-masking as we instead substitute words with phrases that reflect the equality of gender. This approach not only reduces gender bias but also preserves the original meaning of the sentence in most cases. In our approach, we chose the phrases and order of gender words at random with equal probability. Additionally, we hypothesize that the choice of the dataset for fine-tuning is also essential. We choose two datasets: the WikiText-2 (Merity et al., 2017) dataset, which has implicit gender bias since its sources from Wikipedia articles, and the Stere-oSet dataset (Nadeem et al., 2021) , which has explicit/more gender bias as it has been designed to evaluate gender bias. WikiText-2 2 has 600 train articles and roughly 2M tokens while StereoSet 3 (dev) has 2123 samples out of which we only consider 800 samples which are not unrelated. Naturally, our data intervention method should work better on a dataset with training examples with gender bias while being devoid of meaningful gender associations like \"She needs a gynecologist,\" where the gender of the person is important. By testing our method on both datasets, we can understand the sensitivity of our approach to the quality of training samples used.\n\nBias Evaluation Metrics\nWe focus on evaluating the bias of a model while also measuring its language modeling capability. The ideal model would not just be one with the least bias but also one which does not compromise its language modeling performance. The dual estimation of bias and performance of a model was proposed in the StereoSet benchmark (Nadeem et al., 2021) , with the Language Modeling Score (LMS) measuring the percentage of times a meaningful token is predicted for the mask as opposed to a meaningless token, the Stereotype Score (SS) measuring the percentage of times the model predicted a stereotypical word as compared to an anti-stereotypical word, and an idealized CAT score (ICAT) combining the LMS and SS score into a single metric. An ideal model has an ICAT score of 100, while the worst biased model has an ICAT score of 0. We additionally evaluate the CrowS-Pairs benchmark (Nangia et al., 2020a) , which captures data with greater diversity in both the stereotypes expressed and the structure of sentences (50 is ideal). However, we note that the Crow-S benchmark is much more limited compared to StereoSet (Nadeem et al., 2021) in terms of both the volume and variety of linguistic phenomenon relating to gender bias it covers.\n\nExperiments\nWe compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work. For a fairer comparison, we run the baselines with the same size (100) of the training set as our method. For all of our experiments, we consider the \"bert-base-uncased\" pre-trained model available from HuggingFace. For fine-tuning our model, we select a varying number of most-biased training samples (10, 50, and 100) from the WikiText-2 and StereoSet (we only use the dev set) datasets, as discussed in section 4. We also compare this to a random selection of data points as an ablation study. On the selected dataset, we apply our interventions and obtain the modified dataset, which is then used to fine-tune our pre-trained model using masked language modeling (MLM) loss. The key point is that we only fine-tune the model on the gender words conditioned on the remaining text, significantly reducing the fine-tuning time. We perform ablations on various types of interventions as discussed in Table 7 . The model is trained for 30 epochs, with a learning rate of 0.001 and AdamW optimizer. We ran all of our experiments on NVIDIA Tesla T4 GPU on Google Colab for roughly 48 hours. For all experiments, we report the numbers as the mean and standard deviations (6) of 3 different runs. Our experiment code can be found here. 4 \n\nResults\nTable 4 shows the StereoSet and Crow-S scores for our baselines and our best-performing interventions on the WikiText-2 Dataset. In the StereoSet benchmark, we observe that random-phrase-masking obtains lower SS than all other baselines. On the Crow-S benchmark, random-phrase-masking does better than thre of the baselines except Sen-tenceDebias which achieves slightly better scores. While random-phrase-masking results in lower SS scores than neutral-masking, it also obtained [MASK] is very good at cooking but not great at [MASK] work. she is very good at cooking but not great at her work.\nhe is very good at cooking but not great at farm work. Being a mother is not easy since she will have to stay home and take care of the child.\nBeing a father is not easy since one will have to stay home and take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\n\nShe is very caring and kind but not good at what she does.\nHe is very caring and kind but not good at what he does.\n\nInput Sentence:\nOutput of Biased Model:\n\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to modify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the same pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the our method is successfully able to reduced biased substitutions. very low LMS scores. We attribute this performance degradation to the blunt substitution of phrases that our method uses, which might lead to odd-sounding sentences. In the Crow-S benchmarks, we see similar behavior and find that random-phrase-masking does better than neutral-masking. Since we believe that our method is sensitive to the choice of the dataset, we also present results on the StereoSet (dev) dataset 6. In Figure 2 , we perform a qualitative analysis of our proposed approach and find that random-phrase-masking is able to flip the predictions on fill-mask tasks for stereotypical sentences.\n\nConclusion\nIn this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs. We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples. Evaluation of our methods on state-of-the-art bias benchmarks empirically suggests that our methods effectively reduce gender bias. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n"}
{"question": "Which of the following reasons is not the reason why XL-LEXEME  is not effective in handling Latin language?", "evidence": "  XL-LEXEME obtains no significant results in the Latin language since the predicted scores for the target words are not correlated with the test set.  Latin is underrepresented in the training data of XLM-R, and there are no similar languages in the WiC dataset that we use for training XL-LEXEME.  Moreover, the Latin dataset is more challenging as it involves the first corpus written in ancient Latin, which differs in many aspects from modern Latin.  ", "options": ["A. The predicted score of the target word is not related to the test set", "B. There is no similar language in the WiC dataset", "C. The Latin language is more challenging.", "D. Latin language is more difficult than any other languages."], "answer": "D", "content": "\nIntroduction and Motivation\nLexical Semantic Change (LSC) Detection is the task of automatically identifying words that change their meaning over time. The LSC Detection task implicitly aims to disambiguate synchronic word sense occurrences and then find differences in the word sense frequencies in different periods. Word Sense Disambiguation (WSD) is a longstudied task in Natural Language Processing (Navigli, 2009) , which consists of associating the correct sense to a word occurring in a specific context. WSD involves some crucial issues, such as relying on a fixed sense inventory. Fixed sense inventories ignore the diachronic aspect of language because they can miss older unused senses or be outdated and missing new senses.\nThe Word in Context task (WiC) (Pilehvar and Camacho-Collados, 2019) aims to overcome these issues. In this work, we train a model on the WiC task and then use it to perform LSC Detection. In the WiC task, given the word w and two different contexts C1, C2, the systems have to determine whether the meaning of w is the same in the two contexts or not. Our approach is grounded on the assumption that models trained on the WiC tasks are robust enough to transfer the knowledge learned in a synchronic setting to a diachronic one. We summarise the main contribution of this work as follows: (i) We propose a pre-trained biencoder model, called XL-LEXEME, on a largescale dataset for the WiC task, which allows us to obtain comparable lexical-based representations; (ii) We assert the effectiveness of XL-LEXEME despite the computational limitation compared to the cross-encoder architecture for the LSC Detection task; (iii) Experiments on the LSC Detection task show that XL-LEXEME outperforms state-ofthe-art LSC Detection models for English, German, Swedish, and Russian.\n\nRelated Work\nLSC Detection systems can be categorized based on the distributional embeddings used to tackle the LSC Detection task. One category is represented by those approaches that adopt type-base (i.e., static) embeddings. UWB (Praz\u00e1k et al., 2020; Praz\u00e1k et al., 2021) represents an example of this category of systems. First, it employs word2vec Skip-gram with Negative Sampling (Mikolov et al., 2013) to compute a semantic space for each corpus. It uses techniques like the Canonical Correlation Analysis (Hardoon et al., 2004) and the Orthogonal Transformation (Hamilton et al., 2016) to align the abovementioned spaces. Therefore, the cosine similarity between the vectors representing the word in two different spaces is used to detect the semantic shift.\nWith the increasing use of contextualized word embeddings, numerous approaches employing BERT-base models have been developed for LSC Detection (Montanelli and Periti, 2023; Laicher et al., 2021) . In TempoBERT (Rosin et al., 2022) , the authors exploit the concept of Masked Language Modeling (MLM), where the goal is to train a language model to predict a masked portion of text given the remaining part. In particular, they employ this technique to encode the concept of time into a BERT model. This is done by concatenating a specific token representing time to the text sequence. At inference time, TempoBERT can be used to predict the year of a sentence, masking the time reference, or to predict a masked token of the sentence conditioned by the time reference. In the same line of research, in Temporal Attention (Rosin and Radinsky, 2022) , the authors investigate the effect of modifying the model instead of the input sentence like in TempoBERT. This is done by extending the model's attention mechanism to consider the time when computing the weight of each word. The time dimension is encoded using a different query embedding matrix for each timestamp.\nAnother kind of approach exploits the information coming from other tasks to perform LSC Detection. GlossReader represents an example (Rachinskiy and Arefyev, 2021) , where a model based on XML-R (Conneau et al., 2020b) is first trained on English SemCor (Miller et al., 1994) with glosses from WordNet 3.0 (Miller, 1992) to perform WSD. Exploiting the zero-shot crosslingual characteristics of XML-R, the authors used the same model to perform LSC Detection in the Russian language. With DeepMistake (Arefyev et al., 2021) , the authors take advantage of the WiC task instead of WSD. They train a cross-encoder with XML-R as an underlying Language Model on the MCL-WiC training and development set and fine-tune on the RuSemShift dataset (Rodina and Kutuzov, 2020) . DeepMistake, differently from XL-LEXEME, relies on the cross-encoder architecture and exploits only the MCL-WiC training dataset.\n\nXL-LEXEME\nGenerally, for pairwise sentence similarity tasks, BERT models use a cross-encoder, in which the pairwise sequences are jointly encoded, and the overall vectors are used for the classification. However, in several tasks, the cross-encoder is not suitable since it cannot provide a distinct meaningful representation for each sentence. An approach to overcome this issue involves pooling the BERT out-put encoded vectors, which often results in worse performance. Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) overcomes the limitation of cross-encoders using a Siamese Network, i.e., the weights of the underlying networks are shared. SBERT encodes the two sequences separately in the BERT model exploiting the Siamese architecture. The sequence-level representation is obtained by averaging the output encoded vectors, which are directly compared using similarity measures such as cosine similarity.\nMeanwhile, cross-encoders perform better since they are trained to profit from the attention over the whole input. In this work, we introduce XL-LEXEME 1 which mirrors models for pairwise sequence similarity tasks and adapts them to the WiC task, giving prominence to the target word, i.e. the word for which we want to detect the LSC. The model takes as input two sequences s 1 and s 2 . The sequences are tokenized using subwords tokenizer, such as Sentence Piece (Kudo and Richardson, 2018) , and the special tokens <t> and </t> are used as target word delimiters (Xie et al., 2021) :\nEQUATION\nwhere N and M represent the number of subwords of the sequence s 1 and s 2 respectively, while w t i , ..., w t i+k and w t j , ..., w t j+p are the subwords of the target words. In the following, we describe the baseline cross-encoder and XL-LEXEME based on a bi-encoder. For the crossencoder, the two input sequences are concatenated by the special token [SEP ] in an overall sequence\ns = [CLS] s 1 [SEP ] s 2 [SEP ].\nIf the length of s, i.e. N + M + 3, is greater than the maximum sequence length \u03bb, then the sequence s is cut such that the length of s 1 and s 2 is less than \u03bb * = \u03bb\u22123 2 . To comply with the maximum length, the left and right contexts of the sequence are truncated. For instance, s 1 is truncated as follows:\ns 1 = w n 0 , ..., <t>, w t i , ..., w t i+k , </t>, ..., w n 1 (2) where n 0 = max(0, i \u2212 1 \u2212 \u03bb * \u2212k\u22122 2 ) and n 1 = min(N, i + k + 1 + \u03bb * \u2212k\u22122 2\n). The truncated sequence has a length \u03b3 < \u03bb. The encoded representations of each subword (v 1 , v 2 , ..., v \u03b3 ) are summed to get the encoded representation of the overall sequence, i.e. s enc = \u03b3 i v i . Finally, the vector s enc is used to compute the logits:\nlogit = log \u03c3(W s enc ) (3)\nwhere W \u2208 IR 1\u00d7d . The model is trained to minimize the Binary Cross-entropy loss function. XL-LEXEME is a bi-encoder that encodes the input sequences using a Siamese Network into two different vector representations. Each sequence is tokenized and truncated according to the maximum length \u03bb * , using Equation (2). We thus obtain the new lengths \u03b3 1 , \u03b3 2 . The vector representation is computed as the sum of the encoded subwords\n(v 1 , v 2 , ..., v \u03b3 ), i.e. s enc 1 = \u03b3 1 i v i and s enc 2 = \u03b3 2 j v j .\nXL-LEXEME is trained to minimize the Contrastive loss (Hadsell et al., 2006) :\n\u2113 = 1 2 y \u2022 \u03b4 2 + (1 \u2212 y) \u2022 max(0, m \u2212 \u03b4) 2 (4)\nwhere we adopt a margin m = 0.5. We use as default distance \u03b4 the cosine distance between the encoded representations of s 1 and s 2 , i.e. \u03b4 = cos(s enc 1 , s enc 2 ). The main advantage of XL-LEXEME concerning models based on the crossencoder architecture is efficiency. The time cost can be directly derived from the different architectures that exploit XL-LEXEME and the crossencoder baseline. The self-attention time complexity O(N 2 * d) depends on the vector dimension d and the sequence length, which is N for the cross-encoder and N 2 for XL-LEXEME. For XL-LEXEME, the time complexity is reduced to O((N^2)^2 * 2d).\n4 Experimental setting\n\nLexical Semantic Change Detection\nSemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection (Schlechtweg et al., 2020) is the first task on Unsupervised Lexical Semantic Change Detection in English, German, Swedish, and Latin languages. For each language, two corpora represent two different periods (T0, T1). Moreover, a set of target words, annotated using the DUREL framework (Schlechtweg et al., 2018) , are provided. SemEval-2020 Task 1 involves two subtasks. The binary classification task requires assigning a label (changed/stable) to each target word. The ranking task sorts the target words according to their degree of semantic change. In this work, we focus on Subtask 2, and for the sake of simplicity, we refer to SemEval-2020 Task 1 Subtask 2 as SemEval-2020 Task 1. RuShiftEval, different from SemEval-2020 Task 1, involves three sub-corpora extracted from the Russian National Corpus spanning three periods. Models are evaluated on the resulting three test sets, namely RuShiftEval1 (pre-Soviet and Soviet), RuShiftEval2 (Soviet and post-Soviet), and RuShiftEval3 (pre-Soviet and post-Soviet). RuShiftEval provides participants with development data that can be used for tuning models. RuShiftEval aims to corroborate if training data can improve LSC Detection models. The development data rely on the RuSemShift dataset (Rodina and Kutuzov, 2020) , which includes two sets of 70 target words for the pre-Soviet to Soviet period and Soviet to post-Soviet period, respectively. The dataset also includes annotated pairwise sentences, which can be used for training the models.\n\nTraining details\nXL-LEXEME and the cross-encoder are trained using XLM-RoBERTa (XLM-R) (Conneau et al., 2020a) large as the underlying Language Model 2 and using an NVIDIA GeForce RTX 3090. As for training data, the model uses the training data of MCL-WiC (Martelli et al., 2021) , AM 2 ICO (Liu et al., 2021) , and XL-WiC datasets (Raganato et al., 2020) merged with the randomly sampled 75% of the respective development data of each dataset. The remaining 25% of the development data is used to fine-tune hyper-parameters. Moreover, we augment training data for the cross-encoder by swapping the order of sentences in the training set (Martelli et al., 2021) .\nWe use AdamW optimizer and linear learning warm-up over the 10% of training data. We perform a grid search for the hyper-parameters optimization, tuning the learning rate in {1e-6, 2e-6, 5e-6, 1e-5, 2e-5} and the weight decay {0.0, 0.01}. Table 3 (Appendix A) shows the selected hyperparameters. We sample 200 sentences containing the target word for each language and each period. The sampling is repeated ten times, and the results are averaged over the ten iterations. We use the same methodology of Rachinskiy and Arefyev (2021) for sampling sentences from the RuShiftEval corpora. We sample sentences in which we find the exact match with the target words with no pre-processing of the SemEval dataset. The LSC score is computed as the average distance between the vectors over the two different periods:\nLSC(s t 0 , s t 1 ) = 1 N \u2022 M N i=0 M j=0 \u03b4(s t 0 i , s t 1 j ) (5)\nwhere \u03b4 is the distance measure, i.e. \u03b4 = 1 \u2212 log \u03c3(W s enc ) for the cross-encoder baseline and \u03b4 = cos(s enc 1 , s enc 2 ) for XL-LEXEME.\n\nResults\nTable 1 and Table 2 report the results on the SemEval-2020 Task 1 Subtask 2 and the results on the RuShiftEval test set. The results of the best systems are in bold. XL-LEXEME achieve the best score for English, German, Swedish, RuShiftEval1, RuShiftEval2, and RuShiftEval3. XL-LEXEME achieves a strong Spearman correlation for English and Swedish languages and a solid correlation on the German dataset, obtaining a significative correlation (p < 0.001). XL-LEXEME obtains no significant results in the Latin language since the predicted scores for the target words are not correlated with the test set. Latin is underrepresented in the training data of XLM-R, and there are no similar languages in the WiC dataset that we use for training XL-LEXEME. Moreover, the Latin dataset is more challenging as it involves the first corpus written in ancient Latin, which differs in many aspects from modern Latin. For this reason, XL-LEXEME could be ineffective in ancient languages and, in general, in languages that are not widely covered by the WiC dataset. We report the statistical significance of the difference between the performance of XL-LEXEME concerning the other models. The statistical significance of the difference is computed using Fisher's z-transformation (Press, 2002) . XL-LEXEME obtains stronger correlations than the cross-encoder, but the differences are not significant. The correlations obtained on the English and the German datasets are significantly different (p < 0.05) for all the systems that participated in the SemEval-2020 Task 1 but not for TempoBERT and Temporal Attention. On the other side, TempoBERT and Temporal Attention obtain a Spearman correlation on English and German that is not statistically different from the systems on the SemEval-2020 Task 1 leaderboard. In the Swedish language, XL-LEXEME is the only one obtaining a significantly different correlation from the Count baseline results. XL-LEXEME showed its effectiveness also in Swedish, although the WiC dataset does not cover this language. Presumably, Swedish benefits from the presence of other languages descending from the Old Norse language, namely Danish and Norwegian.\nXL-LEXEME obtains competitive results for the Russian language in the RuShiftEval leaderboard. Contrary to XL-LEXEME, Deep Mistake and Gloss Reader are fine-tuned on the RuSemShift dataset. The differences between XL-LEXEME and the best two systems in the leaderboard are not statically significant. Moreover, in Table 2 , the results of XL-LEXEME fine-tuned on the RuSemShift are shown. Although the fine-tuned model achieves the best correlation scores in the three datasets, the difference between DeepMistake and GlossReader is not significant.\n\nConclusion\nIn this work, we introduced XL-LEXEME, a model for LSC Detection. XL-LEXEME is pre-trained on a large WiC dataset to mirror sentence-level encoders focusing on specific words in contexts. We evaluated our model on two Lexical Semantic Change Detection datasets: SemEval-2020 Task 1 and RuShiftEval. XL-LEXEME outperforms stateof-the-art models for LSC Detection in English, German, Swedish, and Russian datasets, with significant differences from the baselines. The XL-LEXEME effectiveness and efficiency make it reliable for LSC Detection on large diachronic corpora.\n"}
{"question": "What is the impact of increasing pre-trained model size on average robustness across different tasks?", "evidence": "  Increasing model size minimally affects average robustness in NLI and extractive QA (Figure 3a ,3c), but substantially improves average robustness on sentiment analysis (Figure 3b ). 4a,b ).  ", "options": ["Although increasing the pre-trained model size improves sample efficiency on every task, it does not always improve average robustness (Figure 3 ). "], "answer": "C", "content": "\nIntroduction\nNLP models perform well when evaluated on data drawn from their training distribution (indistribution / ID), but they typically suffer large drops in performance when evaluated on data distributions unseen during training (out-of-distribution / OOD; Blitzer, 2008) .\nHow does exposure to ID training examples affect the ID-OOD gap? If two models have the same ID performance, will models trained on fewer ID examples (higher sample efficiency) also have higher OOD performance (higher robustness)? At one extreme, zero-shot models will not learn IDspecific patterns because they are not exposed to any labeled ID examples. Similarly, few-shot models trained on very few ID examples may also rely less on ID-specific patterns; if a model never sees the token \"cat\" while training on SNLI, then it will not learn that its presence is spuriously predictive of the contradiction label (Gururangan et al., 2018; Utama et al., 2021) . Supporting this intuition, recent work in image classification (Radford et al., 2021) and extractive question answering (Awadalla et al., 2022) show that zero-shot inference and fewshot fine-tuning improve average robustness across a range of OOD test sets. However, it is unclear how universal these trends are across various tasks and methods for reducing exposure to ID examples, or how predictive they are for any individual test set of interest. Figure 1 illustrates this central question.\nWe conduct a broad empirical study over 14 datasets across three tasks to investigate the relationship between exposure to ID training examples (sample efficiency) and robustness. We experiment with three modeling interventions that improve sample efficiency: (1) using natural language prompts for zero-shot prediction and during finetuning (Brown et al., 2020; Schick and Sch\u00fctze, 2021; Gao et al., 2021) ; (2) fine-tuning models of increasing size; (3) fine-tuning models pre-trained on increasing amounts of data.\nWe find that higher sample efficiency is only sometimes correlated with better robustness, and the effect of specific modeling interventions varies by task. For example, increasing pre-trained model size substantially improves sample efficiency and results in higher average robustness in sentiment experiments, but these sample efficiency gains do not translate to higher average robustness in NLI and extractive QA experiments. On individual datasets, models with better sample efficiency can even be less robust (e.g., increasing model size when training on SST-2 and evaluating OOD on IMDb).\nOverall, these results indicate that general- purpose methods for improving sample efficiency are far from guaranteed to yield significant OOD robustness improvements-their success is highly dataset-and task-dependent. Furthermore, even in this era of large, multi-purpose pre-trained language models, task-specific decisions are often necessary to achieve OOD generalization.\n2 Measuring Sample Efficiency and Robustness. (3) M 's performance on examples from D ood (i.e., the OOD performance).\nLet M 1 and M 2 be two models with equivalent performance on held-out ID data. If M 1 was trained on fewer ID examples than M 2 , then it has higher sample efficiency. If M 1 has higher OOD performance than M 2 , it has higher effective robustness (henceforth \"robustness\"; Taori et al., 2020) . Comparing models with equivalent ID performance controls for its effect on OOD performance, since improving ID performance usually yields commensurate improvements on OOD performance-in this study, we focus on OOD performance improvements beyond what is expected from ID gains.\nSatisfying this equivalent-ID constraint is often difficult in practice; given an arbitrary model M 1 and its corresponding ID performance, it is difficult to produce a different model M 2 with identical ID performance. Rather than explicitly training models to identical ID performance, we train models on varying-size subsamples of a given ID dataset and interpolate between the results to estimate (1) the number of labeled ID training examples necessary to achieve a particular ID performance (sample efficiency) and (2) OOD performance, given ID performance (robustness). These interpolated curves approximate the ideal setting of training a model for every possible ID value. Figure 1 provides a schematized example, with model B having better sample efficiency and robustness than model A.\n\nExperimental Setup\nWe study three modeling interventions-using natural language prompts, increasing pre-trained model size, and pre-training on more data-on 14 total datasets spanning natural language inference (NLI), sentiment analysis, and extractive question answering (QA). See Appendix A for further details about experimental settings.\nTasks and Datasets. In our natural language inference (NLI) experiments, we use MultiNLI (Williams et al., 2018) , SNLI (Bowman et al., 2015), and MedNLI (Romanov and Shivade, 2018) . For sentiment analysis, we use IMDb reviews Maas et al. (2011) , SST-2 (Socher et al., 2013) , and reviews from the \"Movies and TV\" subsection of the Amazon Reviews corpus (Ni et al., 2019) . Lastly, for extractive question answering, we use SQuAD (Rajpurkar et al., 2016 ), NaturalQuestions (Kwiatkowski et al., 2019) , TriviaQA, BioASQ (Tsatsaronis et al., 2015) , and the four SQuAD-Shifts test sets (Miller et al., 2020) .\nModeling Interventions. To understand the effect of a particular modeling intervention on sample efficiency and robustness, we evaluate pre-trained models that differ only along the axis of interest (e.g., model size or fine-tuning method). Since the optimal fine-tuning hyperparameters depend on the ID training dataset size, we separately tune hyperparameters for each model on each training dataset subsample size, taking the models that achieve the best held-out ID performance for each setting. See \n\nResults and Discussion\nOur results show that models with higher sample efficiency may not necessarily have higher average OOD robustness-different tasks and modeling interventions affect robustness in different ways . For example, prompt-based fine-tuning consistently improves both sample efficiency and average robustness, but only in low-data settings (Figure 2 ). In contrast, increasing model size improves sample efficiency across the range of training dataset sizes and tasks, but only improves average robustness on sentiment analysis (Figure 3 ). On individual datasets, we even observe cases where models with lower sample efficiency have higher robustness (Figure 3d ). See Appendix C for full results on every ID-OOD setting.\nNatural Language Prompting. We compare BERT BASE models using (1) standard fine-tuning, (2) prompt-based fine-tuning, and (3) zero-shot prompting. We also compare these results with zero-shot prompting of text-davinci-001, a much larger model trained on substantially more data. We run experiments on NLI and sentiment analysis, since extractive QA is not amenable to prompt-based fine-tuning with masked language models.\nFigures 2a and 2b plot the average performance on all OOD datasets as a function of ID performance and the ID performance as a function of the number of labeled training examples. Sample efficiency improvements from prompt-based finetuning also translate to higher average robustness. However these improvements only apply in the few-shot setting. As the size of the training dataset increases, the improvements in sample efficiency and average robustness steadily diminish. When using sufficiently large training datasets, models trained with prompt-based fine-tuning yield essentially the same sample efficiency and robustness results as standard fine-tuning (\u223c1K examples for NLI, \u223c130 examples for sentiment).\nHowever, results on individual OOD test sets can significantly differ from averaged-OOD trends. For example, Figure 2c shows that prompt-based fine-tuning on MNLI and evaluating on SNLI improves sample efficiency in the few-shot setting but without any robustness improvements.\nSurprisingly, we also find that zero-shot inference does not necessarily improve average robustness over prompt-based fine-tuning-zero-shot performance lies on or below the trend line formed by prompt-based fine-tuning, despite not using any ID-specific data at all. See Appendix C.1 for full results of increasing pre-trained model size for every ID-OOD setting.\nIncreasing Pre-Trained Model Size. We run experiments with the checkpoints of Turc et al. (2019) , who pre-train BERT models with various numbers of transformer layers (L) and hidden embedding sizes (H). We run experiments on NLI, sentiment analysis, and extractive QA to compare pre-trained models of five sizes: (1) Large (L=24, H=1024), ( 2) Base (L=12, H=768), (3) Medium (L=8, H=512), (4) Small (L=4, H=512), and\n(5) Tiny (L=2, H=128). Although increasing the pre-trained model size improves sample efficiency on every task, it does not always improve average robustness (Figure 3 ). In particular, increasing model size minimally affects average robustness in NLI and extractive QA (Figure 3a ,3c), but substantially improves average robustness on sentiment analysis (Figure 3b ). 4a,b ). In extractive QA experiments, varying the amount of pre-training data does not significantly change average robustness (Figure 4c ). Again, we find that results on average OOD performance are not predictive of results on individual test sets-despite unchanged average OOD robustness when pre-training on more data, OOD performance can be higher on individual extractive QA test sets (e.g., SQuAD \u2192 BioASQ; Figure 4d ). See Appendix C.3 for full results of pre-training on Figure 4 : Pre-training on more data is an effective method for improving sample efficiency, but these sample efficiency improvements are not always accompanied by robustness improvements. In NLI and sentiment analysis experiments, these sample efficiency gains correlate with improved average robustness (a,b). However, there are no average robustness gains in extractive QA (c). Despite no average robustness improvement in extractive QA, pre-training on more data can still improve robustness on particular test sets (e.g., BioASQ; d). more data for every ID-OOD setting.\n\nConclusion\nWe study the relationship between sample efficiency and robustness across three tasks and three modeling interventions, finding that sample efficiency improvements often fail to translate to improved robustness. As larger models quickly become more sample efficient, our results caution that sample efficiency and robustness are different axes of improvement and that optimizing for sample efficiency will not necessarily always yield robustness gains.\n"}
{"question": "What two datasets were used for experiments in the paper to evaluate the BalSum model?", "evidence": "  Experiments on the CNN/DailyMail and XSum datasets show that our method can estimate the meaning of summaries... We experiment on two datasets, whose statistics are shown in Appendix C. CNN/DailyMail... XSum (Narayan et al., 2018). ", "options": ["A. Wikipedia articles and CNN/DailyMail", "B. CNN/DailyMail and New York Times articles", "C. British Broadcasting Corporation (BBC) and Academic research papers", "D. CNN/DailyMail and XSum "], "answer": "D", "content": "\nIntroduction\nThe performance of sequence-to-sequence (Seq2Seq) neural models for abstractive summarization (Lewis et al., 2020; Nallapati et al., 2016; See et al., 2017; Zhang et al., 2020) has improved significantly. The dominant training paradigm of Seq2Seq models is that of Maximum Likelihood Estimation (MLE), maximizing the likelihood of each output given the gold history of target sequences during training. However, since the models generate the sequence in an auto-regressive manner at inference, the errors made in the previous steps accumulate in the next step thereby affecting the entire sequence. This phenomenon is known as exposure bias (Bengio et al., 2015; Ranzato et al., 2016) . To mitigate this * Corresponding author problem, re-ranking systems (Liu et al., 2021; Liu and Liu, 2021; Liu et al., 2022; Ravaut et al., 2022) have recently been introduced to generate a more appropriate summary.\nThere are two training objectives for applying reranking to abstractive summarization: contrastive learning and multi-task learning. The contrastive learning-based approaches deploy margin-based losses. SimCLS (Liu and Liu, 2021) and BRIO-Ctr (Liu et al., 2022) train a large pre-trained model, such as RoBERTa (Liu et al., 2019) and BART (Lewis et al., 2020) , to align the candidate summaries according to the quality. The authors use the ROUGE (Lin, 2004) score as a quality measurement. The multi-task learning-based approaches combine at least two losses that perform different roles. SummaReranker (Ravaut et al., 2022) minimizes the average over the binary cross-entropy losses optimized for each evaluation metric. In addition, BRIO-Mul (Liu et al., 2022) demonstrates that the combination of the contrastive and cross-entropy loss works complementarily and has better performance.\nIn this paper, we analyze the three main drawbacks of existing re-ranking approaches. First, we argue that current methods focus excessively on ranking summaries in terms of lexical overlap. Inspired by Zhong et al. (2020) , we conduct a preliminary study, by sorting candidate summaries in descending order based on the ROUGE score and then defining z as the rank index of the highest BERTScore summary. As demonstrated in Fig. 1 , we can observe that there is a large gap between lexical overlap and semantic similarity. In a majority (52%) of cases z > 1. Second, despite more than half of the candidates with the same ROUGE score, previous studies do not accurately reflect quality measurements as they are trained with different ranks even if they have equal scores (Appendix F). Lastly, for the first time, we find summaries with high lexical overlap but low semantic similarity as false positives (Appendix G). They can be noises during training phrase, which are not considered substantially in the prior works.\nTo address these issues, we propose a novel training method in which a re-ranker balances lexical and semantic quality. Based on a two-stage framework, our model, named BalSum, is trained on multi-task learning. We directly reflect the ROUGE score difference on a ranking loss to preserve the lexical quality as much as possible. Then, we use a contrastive loss with instance weighting to identify summaries whose meanings are close to the document. Specifically, we define novel false positives (semantic mistakes) and present a strategy to reduce their influence in ranking. Experiments on CNN/DM and XSum datasets demonstrate the effectiveness of our method. Notably, BalSum achieves an 89.67 BERTScore on CNN/DM, reaching a new state-of-the-art performance.\n\nMethod\nOur method follows the two-stage framework. Given a source document D, a function g is to generate a pool of candidate summaries C = {C 1 , C 2 , ..., C m } at the first stage:\nEQUATION\nThen, a function f is to assign scores to each candidate and select the best summary C * with the highest score at the second stage: Our goal is to train the ranking model f that identifies the correct summary from the outputs of the generation model g.\nC * = argmax C i \u2208C {f (C i , D)} (2)\n\nModel Architecture\nWe start with a bi-encoder using RoBERTa-base (Liu et al., 2019) as a back-bone neural network.\nInspired by Khattab and Zaharia (2020) , we aim to capture rich semantic units at the sentence level.\nAs shown in Fig. 2 , we insert the [CLS] tokens in front of K sentences in the document D to let them encode into multi-vector representations. Then, we compute the individual score Score k which is modeled as an inner-product:\nEQUATION\nwhere E 1 (C i ) and E k (D)(k = 1, 2, ..., K) mean the representations of [CLS] tokens for candidate summary C i and document D, respectively. We calculate the similarity score f (C i , D):\nEQUATION\nIn Appendix E, we show that our model can capture more information from documents at the sentence level.\n\nTraining objective\nRanking Loss The core idea is that the higher the quality of the candidate summary, the closer to the document. We introduce a ranking loss to f (\u2022):\nL rank = i j>i max(0, f (Cj, D) \u2212 f (Ci, D) +(\u2212cost(Ci, S) + cost(Cj, S)) * \u03bb) (5)\nwhere S is the reference summary and \u03bb is the hyper-parameter. 1 Here, cost(C i , S) = 1 \u2212 M (C i , S) is the margin, and M is the automatic evaluation metric. We define it as ROUGE. We use the same metric in previous work (Liu and Liu, 2021; Liu et al., 2022) , but the difference is that our loss directly reflects the quality measure during training. In other words, the quality was not properly reflected before because different margin ((j \u2212 i) * \u03bb) was assigned even if the candidate summaries had the same ROUGE score.\nContrastive Loss with Instance Weighting The construction of positive and negative pairs is the critical point in constrative learning. Therefore, we consider generated summaries from the same document as positive samples and irrelevant summaries from other documents as negative samples. Thus, we design a set of candidate summaries C in Eq. 1 as positive and a set of randomly sampled summaries N as negative. 2 To identify summaries whose meanings are close to the document, we introduce a contrastive learning objective with instance weighting: D) (6) We newly define summaries that have a high lexical matching but a low semantic similarity as false positives. Inspired by Zhou et al. (2022) , we design an instance weighting method to reduce the influence of false positives. We produce the weights for positives using the SimCSE (Gao et al., 2021) which is the state-of-the-art model for the sentence representation task:\nL ctr = 1 |C| C i \u2208C \u2212log \u03b1 C i \u00d7 e f (C i ,D) e f (C i ,D) + s i \u2208N e f (s i ,\n\u03b1 C i = 0, sim(C i , S) < \u03d5 1, sim(C i , S) \u2265 \u03d5 (7)\nwhere \u03d5 is a hyper-parameter of the instance weighting threshold, and sim(\u2022) is the cosine similarity score evaluated by the SimCSE model. Finally, as shown in Fig. 3 , we combine the ranking (Eq. 5) and contrastive (Eq. 6) losses:\nEQUATION\n)\nwhere \u03b3 is the scale factor of each loss and we find the optimal values (\u03b3 1 = 10, \u03b3 2 = 0.1) in Appendix H.\n3 Experiments\n\nDatasets\nWe experiment on two datasets, whose statistics are shown in Appendix C. CNN/DailyMail (Hermann et al., 2015) is the most commonly used summarization dataset which contains articles from the CNN and DailyMail newspapers.\nXSum (Narayan et al., 2018 ) is a one-sentence summary dataset from the British Broadcasting Corporation (BBC) for the years 2010 -2017.\n\nTraining Details\nWe use diverse beam search (Vijayakumar et al., 2016) to generate 16 candidate summaries. We start from pre-trained checkpoints of RoBERTabase (Liu et al., 2019) . We train BalSum for five epochs. It takes 33 hours on CNN/DM and 22 hours on XSum on a single RTX 3090 GPU. More details are described in Appendix D.\n\nMain Results\nIn terms of the two-stage framework, we compare our results with SimCLS (Liu and Liu, 2021) , Sum-maReranker (Ravaut et al., 2022) , and BRIO (Liu et al., 2022) . We apply BalSum on top of each base model which is BART or PEGASUS.\nThe results on CNN/DM are described in Table 1. BalSum outperforms a base BART model, according to gains of 2.54/1.27/2.63 R-1/2/L. Notably, while it has comparable performances on ROUGE to previous models, it achieves an 89.67 BERTScore, reaching a new state-of-the-art performance. When ranking the candidate summaries, our model can estimate the meaning of summaries without seriously degrading the lexical aspect. We argue that this is because BalSum decreases more false positives than other ranking models. We provide fine-grained analyses for this result and present a case study in Sec.3.4.\nIn addition, we apply our method on XSum, as shown in Table 2 . Though we use a different strategy to generate the validation and test data 3 , our method improves a base PEGASUS with a small margin. We believe the one of reasons is that XSum is restricted to capturing diverse semantic units because it consists of much shorter summaries (onesentence) than CNN/DM. Model BS@1 BS@3 BS@5 R@1 R@3 R@5\nOracle \n\nAnalysis\nWeighting Threshold \u03d5 Intuitively, the larger the weighting threshold, the lower false positives. We train our model with different instance weighting thresholds from 0.7 to 0.9. In Table 3 , the highest threshold (\u03d5 = 0.9) shows the best performance and it rises largely to 0.3 BERTScore compared to when not applied. We also find that increasing the threshold leads to performance improvement. Therefore, we demonstrate that false positives can be considered noise in training.\nRanking Evaluation Regardless of the number of candidates, an ideal ranking model should yield oracle results considering diverse aspects of summarization. We conduct an experiment to measure the qualities by selecting the top-k summaries after aligning the candidates through different models. As shown in Table 4 , we can see that our model shows consistent performance in both evaluation metrics depending on the k (about \u00b10.06 BERTScore, \u00b10.34 ROUGE average score). Compared to SimCLS and BRIO-Ctr, the second block in Table 4 demonstrates that BalSum captures semantic similarity best while maintaining the intermediate level from the perspective of lexical overlap quality. Moreover, we find that BalSum has the lowest drop ratio of BERTScore (\u22121.52%) from the perfect ranking \"oracle\" scores. We also investigate whether all ranked summaries by models satisfy both lexical and semantic quality. We evaluate models using F 1 which measures the cases where the higher-ranked summary \n\nConclusion\nIn this work, we propose BalSum which aims to evaluate summaries by considering the balance between lexical and semantic quality. To achieve this, we perform a multi-task learning, which aligns summaries according to their lexical overlap qualities and identifies whether they are similar to the document. In addition, to our best knowledge, our method is the first attempt to present a new perspective of false positives (semantic mistakes) in ranking and creating the model to reduce their in-fluence. Our experimental results and fine-grained analyses validate that our model achieves consistent improvements over competitive baselines.\n"}
{"question": "In experiment,which step is unrelated?", "evidence": "At the time of conducting our experiments, Chat-GPT is a proprietary research preview accessible for free via a web interface 1 . We used the Jan 9 version of the model. We use a regular expression term to extract all parts that are JSON formatted. We form DS t by accumulating all predicted updates up to turn t.We evaluate on the 1000 dialogues of the MultiWOZ 2.1 (Eric et al., 2020) test split and use joint goal accuracy (JGA) to compare methods. For a fair judgement of the ChatGPT predictions, we follow the evaluation procedure of Heck et al. ( 2020). ", "options": ["A.At the time of conducting our experiments, Chat-GPT is a proprietary research preview accessible for free via a web interface 1 . We used the Jan 9 version of the model. ", "B.We use a regular expression term to extract all parts that are JSON formatted. We form DS t by accumulating all predicted updates up to turn t.\nC. We evaluate on the 1000 dialogues of the MultiWOZ 2.1 (Eric et al., 2020) test split and use joint goal accuracy (JGA) to compare methods. ", "D.For a fair judgement of the ChatGPT predictions, we follow the evaluation procedure of Heck et al. ( 2020). "], "answer": "A", "content": "\nIntroduction\nDialogue state tracking (DST) is a critical component for task-oriented dialogue systems. Its purpose is to extract and track user's goals throughout a conversation (Young et al., 2010) . DST is challenging due to the infinite possibilities of user/agent conversations, and because services and schemas/APIs that dialogue systems interface are subject to constant change (Ren et al., 2018) . Although traditional approaches achieve high accuracy when operating on a pre-defined set of concepts called an ontology (Mrk\u0161i\u0107 et al., 2017; Liu and Lane, 2017; Zhong et al., 2018) , ongoing research explores transfer to new domains with little to no additional learning (Rastogi et al., 2020) using ontology independent architectures to allow seamless adaptation to out-of-ontology concepts.\nMany strategies for zero-shot transfer to unseen domains have been proposed. Li et al. (2021) treat DST as a question answering (QA) task by leveraging data augmentation. Zhao et al. (2022) propose DST by relying on schema descriptions while Heck et al. (2022) utilize natural language descriptions to facilitate zero-shot transfer. Gao et al. (2020) and Lin et al. (2021) suggest learning from non-dialogue QA data which are available in large amounts to improve generalization. Campagna et al. (2020) harness large synthesized data based on abstract dialogue models. However, none of these techniques are ideal solutions. Fine-tuning is challenging due to computational costs, risk of over-fitting and the need for expensive (Budzianowski et al., 2018) task-specific data. Cross-task transfer still requires curated data and careful consideration of suitable learning tasks. Data augmentation requires high level task knowledge and an adequate synthesizing strategy.\nA new generation of large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Glaese et al., 2022) comes with the promise to be equipped to solve any task without task-specific fine-tuning, but solely with world knowledge they acquired during self-training on massive amounts of data. Such LLMs have been shown to perform remarkably well on in-context learning (ICL) , where only a natural language prompt and examples are provided to condition the generation process, achieving significant improvements over fine-tuned approaches in few-shot setups (Brown et al., 2020; Wang et al., 2022) . ChatGPT (Ope-nAI, 2022) -trained using human feedback and reinforcement learning -is the most recent of such models and single-handedly solves an array of challenging natural language processing (NLP) tasks with super-human capabilities, all through a natural language dialogue interface.\nIn this work, we aim to answer the question: does ChatGPT solve the problem of zero-shot DST? We show that crafting intuitive natural language prompts is sufficient to achieve state-of-the-art performance with ChatGPT, exceeding conventional, engineering-heavy approaches to zero-shot DST by a large margin. However, despite our findings, we argue that properties inherent to general purpose models inhibit their ability to simply replace specialized systems. We speculate that while in the foreseeable future general purpose models may not become holistic solutions to complex problems, they will provide ample opportunities to empower specialized systems to go beyond their pre-defined scopes, enable on-the-fly extensibility and generation of high quality training data by zero-shot synthesizing or automatic labeling.\n\nBackground\nDialogue state tracking is tasked to (1) determine for every turn t in a dialogue {(U t , M t )} T 1 with U t and M t being current user and preceding system utterance whether any of the slots in S = {S n } N 1 is present, to (2) predict values for each S n and to (3) track the dialogue state\nDS t \u2200t \u2208 [1, T ]. The DS is cumulative, i.e., DS t = update(DS t\u22121 , DS t ) is updated given the predictions of slot-value updates DS t .\n\nChatGPT\n(OpenAI, 2022) is a dialogue agent (Leike et al., 2018) , and in its core a GPT-3.5 LLM fine-tuned on human-written promptresponse pairs followed by reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020) . RLHF utilizes a reward model trained on human feedback to improve generation quality and adequacy via proximal policy optimization (Schulman et al., 2017) \n\nZero-shot DST with ChatGPT\nOur investigative approach to zero-shot DST with ChatGPT differs considerably from related works. We decode dialogue state updates with a general purpose model, without undergoing any parameter updates. Consequently, we neither employ data augmentation nor cross-task transfer learning. Instead, we solely rely on the general capacities of ChatGPT as an aligned dialogue agent. We take a most rigorous approach to zero-shot transfer where we do not allow the provision of any examples, nor of a formal task definition. Instead, we only permit natural language explanations of what the model is supposed to do. This sets our investigation apart from the closely related IC-DST (Hu et al., 2022) .\nIn zero-shot DST, the set of slots S relevant during inference and the set of slots S \u2032 seen during training of the model X \u03b8 with parameters \u03b8 are disjoint, i.e., S \u2229 S \u2032 = \u2205. Further, it may be S \u2032 = \u2205, in which case \u03b8 is not specifically tuned towards solving DST. This is precisely the case for Chat-GPT in our setup. Our approach to zero-shot DST with ChatGPT is formalized as follows. Let\nA 1 =P \u2295 \"system\":M 1 \u2295 \"user\":U 1 , A t =\"system\":M t \u2295 \"user\":U t , \u2200t \u2208 [2, T ],\nwhere P is the task description which provides the model with instructions for how to process a dialogue between a system M and a user U . A 1 is the initial prompt to ChatGPT. A t\u22652 are the follow-up prompts, only containing a single turn-pair of the dialogue of interest. ChatGPT is particularly suitable for this strategy due to its chat based interface. ChatGPT generates its next output B t conditioned on the current prompt A t\u22121 , as well as all preceding user queries and system responses of the same chat. The dialogue state update DS t can be found in B t , but may not be directly interpretable as such due to the diversity in the output surface forms. Thus, we require a normalization operation DS t = normalize(B t ). In contrast to (Hu et al., 2022) , we do not condition B t on DS t . This renders the task even more challenging, as Chat-GPT is forced to solve complex subtasks such as coreference resolution -the case where a newly encountered slot refers to the value of another slot -solely given the initial prompt and its own latent dialogue state given the dialogue history.\n\nExperiments\nAt the time of conducting our experiments, Chat-GPT is a proprietary research preview accessible for free via a web interface 1 . We used the Jan 9 version of the model. We use a regular expression term to extract all parts that are JSON formatted. We form DS t by accumulating all predicted updates up to turn t.\nEvaluation. We evaluate on the 1000 dialogues of the MultiWOZ 2.1 (Eric et al., 2020) test split and use joint goal accuracy (JGA) to compare methods. For a fair judgement of the ChatGPT predictions, we follow the evaluation procedure of Heck et al. ( 2020). We process each dialogue once and refrain from using ChatGPT's regeneration feature.\nPrompt. We imposed restrictions that the taskdefining prompt P be intuitive natural language and provides no formal schema. The crafting process involves simple trial-and-error on fewer than 10 held-out dialogues from the MultiWOZ training set. The design process was guided by the intention to imitate the behavior of a triple copy strategy (TripPy) DST (Heck et al., 2020) . P consists of three parts. First, a list of names for detectable informable slots along with natural language descriptions. The slot names help us extract a DS t that is compatible with the dataset's labels. Second, a sparse list of slots that are categorical, along with their value candidates for (1) aiding normalization of values that are expected to show high variability in expression, and (2) modeling Boolean slots.\nThird, an informal task description. 2\n4.1 ChatGPT vs. Supervised SOTA Comparing ChatGPT's performance to state-of-theart supervised approaches that achieve close to 60% JGA is not a fair fight 3 , and yet we observe an impressive 31.5% zero-shot JGA. This result is double-edged; on the one hand it is evidence that ChatGPT is capable of DST 4 , and on the other hand is no match for specialized systems. The comparison to TripPy, a SOTA supervised model, allows us a more fine-grained analysis. In Figure 1 , slot filling performance is broken down into value types. We observed that ChatGPT underperforms in non-trivial cases, namely refer, where a newly encountered slot refers to the value of another slot, and inform, where a slot-value was mentioned by the system and confirmed by the user. ChatGPT shows slight underperformance for Boolean slots. Remarkably, performance for values that are extracted directly from user utterances -the most relevant category in terms of frequency - (Hu et al., 2022) was the first successful attempt at pseudo 5 zero-shot DST via ICL. Our preliminary results with ChatGPT are on par, which is remarkable for the following reasons.\n(1) Our prompt is non-schematic and without examples, (2) our task-defining prompt is stated only once at the beginning of the chat, and (3) we do not maintain a DS to serve as additional input at each turn. The heightened zero-shot performance of IC-DST can be mainly attributed to these points.\n\nError Analysis\nWe identified a set of recurring errors that are likely caused by either the content of P or by the model's inherent properties. See ChatGPT is a sophisticated dialogue agent that, via alignment with human judgements, is capable of understanding context and intent of a multi-turn conversation far beyond the capacities of the previous generation of LLMs. This makes it well-suited for DST. Our results demonstrate that even with intuitive natural language prompts, a complex task such as DST can be solved exceedingly well without any form of additional learning. While specialized systems can exert control over its input-processing and output-generation to arbitrary degrees, this is not the case for Chat-GPT. Even with the most rigorous and schematic prompts, there can be no guarantee that the model interprets the input as intended or generates the output as required, which may lead to unexpected behavior. Furthermore, there is no guarantee that behavior is consistent across a series of similar inferences, such as in our experimental evaluation. In terms of deployment, the cost factor of building and running massive models may hinder their utilization as a plug-and-play module.\nDespite impressive zero-shot and ICL results for general purpose models, specialist models still perform best on most tasks thanks to task-specific solutions via adequate engineering (Heck et al., 2020; Ye et al., 2021; Kim et al., 2020) and taskrelated data. However, the opportunities to improve dedicated systems with the help of general purpose models are plenty. Their predictive powers could be used for developing smaller, specialized, low inference cost models. Automatic labeling and data a) PMUL4050 system: \"I'd recommend the Autumn House. Would you like to make a booking?\" user: \"Yes please. I need the reservation to be for 8 people and 2 nights starting on Tuesday.\" Prediction: ... hotel-name: none Label: ..., hotel-name: autumn house b) PMUL0117 user: \"Yes I also need a taxi that will get me to the restaurant by the booked time please.\" Prediction: taxi-destination: hotel, taxi-departure: restaurant Label: taxi-destination: the gonville hotel, taxi-departure: la mimosa c) SNG01873 user: \"I need to be picked up from pizza hut city centre after 04:30\" Prediction: ..., hotel-name: dontcare, ..., attraction-type: dontcare, ... Label: ... d) PMUL0599 user: \"[...] Can you just help me find a high-end Mexican restaurant?\" Prediction: ..., restaurant-pricerange: high-end Label: ..., restaurant-pricerange: expensive e) MUL2051 user: \"Can I get address and postcode for the hotel?\" Prediction: hotel-address: ?, hotel-postcode: ? Label:system: \"The address is 74 chesterton road, the postal code is cb41er, can I assist with anything else?\" user: \"That is all for now, goodbye.\" Prediction: hotel-address: 74 chesterton road, hotel-postcode: cb41er Label:f) MUL0524 user: \"I'm going to Cambridge on saturday and want to arrive by 14:15 please.\" Prediction: ..., train-day: Saturday Label: ..., train-day: saturday g) PMUL4246 user: \"i need a place to go and should be a museum\" Prediction: attraction-type: museum Label: attraction-type: museum system: \"Okay! There are several museums in Cambridge. What part of town would you like to visit?\" user: \"How about ones in the centre, what's available?\" Prediction: attraction-type: museum, attraction-area: centre Label: attraction-area: centre augmentation are natural use cases for ChatGPT, as is evident from our experimental results; a perdomain JGA of 70% (see Section 4.2) is surely sufficient to generate additional mid-to high-quality training data for dedicated systems. Automatic labeling may be conducted on-line for on-the-fly adaptation of production systems or off-line for iterative learning. Another way of harnessing general purpose models is the integration into dedicated systems as fallback options in case of out-of-domain or out-ofontology requests. An integration via knowledgeseeking term detection (Gunasekara et al., 2020) could facilitate the ability to provide context-aware responses that go beyond the original scope of the specialized system. General purpose models may handle unseen domains in place of the main model.\nWhile hallucinations may be an issue if not handled adequately, they also pose an opportunity to enable zero-shot concept detection. We observed that many slot hallucinations were sensible and pointed at elements that were meaningful to conversations. Zero-shot slot detection may be utilized to annotate and prepare unstructured data for model training, and to expand a system's capacities on-the-fly. Dialogue state trackers with dynamic dialogue states have the potential to expand a taskoriented dialogue system's conversational range seamlessly (Geishauser et al., 2022) . A general purpose model that has the capacity to identify new concepts may be utilized to generate API calls and database queries that are unknown to the specialized system (OpenAI, 2023; Chase, 2023) .\nGeneral purpose models may replace some components in a modular dialogue system (Zhu et al., 2022) . It might still be beneficial to rely on specialized DST and a dedicated policy for particular tasks in order to maintain interpretability and a desired level of control over information flow. However, natural language understanding (NLU) and natural language generation (NLG) modules may be powered by generative large language model based systems such as ChatGPT in order to benefit from a heightened ability of semantic modeling and to facilitate more natural and diverse output, thus promoting more natural conversations with modular task-oriented dialogue systems.\n\nConclusion\nThis work is the first to investigate ChatGPT's capacities for zero-shot DST. Despite remarkable preliminary results that we achieved, we identified limitations rooted in inherent properties of general purpose models, preventing these to become holistic solutions to complex NLP problems without further research. We discussed opportunities provided by ChatGPT and similar models to advance the development of specialized systems. With our insights and discussion, we hope to stimulate research in similar directions.\n"}
{"question": "When investigating, what methods are considered before using linear SVM as the binary classifier ?\n\u9009\u9879\uff1a", "options": ["A. One-vs-rest", "B. Thresholding ", "C. Cost-sensitive", "D. All of the above"], "answer": "D", "evidence": " Here we consider linear SVM as the binary classifier behind these methods.\n\u2022 One-vs-rest: This method learns a binary linear SVM for each label, so data with/without this label are positive/negative, respectively. Let f \u2113 (x) be the decision value of the \u2113-th label, where x is the feature vector. For multi-class classification, \u0177 = argmax \u2113 f \u2113 (x) is predicted as the single associated label of x. For multi-label classification, all labels \u2113 with positive f \u2113 (x) are considered to be associated with x. This method is also what \"TF-IDF+SVM\" in Chalkidis et al. (2022) did, though our TF-IDF feature generation is simpler than theirs by considering only uni-gram. 5 \u2022 Thresholding (Yang, 2001; Lewis et al., 2004; Fan and Lin, 2007) : This method extends one-vsrest by modifying the decision value for optimizing Macro-F1. That is, we change the decision value to f \u2113 (x) + \u2206 \u2113 , where \u2206 \u2113 is a threshold decided by cross validation. \u2022 Cost-sensitive (Parambath et al., 2014) : For each binary problem, this method re-weights the losses on positive data. We decide the reweighting factor by cross validation to optimize Micro-F1 or Macro-F1.\nThese methods basically need no further hyperparameter tuning, so we can directly run them. The last two methods are extensions of one-vs-rest to address the imbalance of each binary problem (i.e., few positives and many negatives). The design relies on the fact that the binary problems are independent, so such approaches cannot be easily applied to deep learning, which considers all labels together in a single network. ", "content": "\nIntroduction\nText classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems.\nWhile various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. "}
{"question": "What does the paper use as a dataset for its experiments?", "evidence": "  We conduct experiments on the ITER-ATER dataset...\" The results show that our approach performs better than the fully fine-tuned BART and PEGASUS baselines reported in Du et al. (2022) with fewer training parameters.  ", "options": ["A. Wikipedia articles", "B. News headlines", "C. ITERATER dataset", "D. Twitter posts "], "answer": "C", "content": "\nIntroduction\nRevision is an essential process to improve the text quality (Vaughan and McDonald, 1986) . During this process, writers perform various editing operations on the text with different editing intentions. As shown in Figure 1 , the writer corrects misspelled words to improve text fluency, deletes redundant words to improve text clarity, adds connective words to improve text coherence, inserts adverbs to convey the writer's writing preferences (style) and modifies data to update text information (meaning-changed).\nLots of recent studies have focused on a text revision task corresponding to a specific edit intention, such as grammatical error correction (Omelianchuk She went to the markt\nThe changes made the paper better than before.\nText Revision She works hard.\nShe is successful.\nEverything was rotten.\n\nShe went to the markt market\nThe changes made the paper better than before improved the paper.\nShe works hard. She; therefore, she is successful.\nEverything was awfully rotten. This method improves the model accuracy from 64% to 7883%. et al., 2020; Kaneko et al., 2020; Liu et al., 2021; Yang et al., 2022 ), text simplification (Dong et al., 2019; Jiang et al., 2020; Omelianchuk et al., 2021; Martin et al., 2022) , and text style transfer (Malmi et al., 2020; Reid and Zhong, 2021) . The work divides text revision into several independent problems. While some methods with strong universality can be applied to multiple tasks (Malmi et al., 2019; Stahlberg and Kumar, 2020; Mallinson et al., 2020) , they train different models on various data sets. Real-world scenarios require addressing multiple types of editing errors at the same time, such as grammatical errors, spelling errors, etc. But these methods failed to integrate knowledge from these tasks into a unified model.\n\nmeaningchanged\nTo solve the problem, Du et al. (2022) attempted to train one model using data with multiple editing intentions and leveraged edit intent information by simply appending it to the input. However, when adding a new intent, the entire model must be re-trained. A more lightweight and scalable approach to multi-intent text revision is still required.\nLi and Liang (2021) proposed a new kind of prompt tuning method to quickly adapt a pretrained model to new tasks, which is called prefixtuning. Prompt tuning can help the pre-trained language model to locate the task learned in pretraining and enable the related knowledge to model text revision with different edit intentions (Reynolds and McDonell, 2021) . This method enables a model to handle multiple edit intentions in a lightweight and scalable way.\nIn this paper, we present our method: a prefixtuning-based model which adapts to text revision with multiple edit intentions. This method involves a two-step training process. In the first step, we initialize a pre-trained language model (PLM) and train multiple prefixes on it. Each edit intention corresponds to a prefix. In the second step, a prefix transfer module is trained at each attention layer of the PLM. The prefix transfer module is configured as two attention units that act respectively on this layer's key states and value states. It enables our model to learn a tailored prefix for the given input with the help of prefix embeddings from the predefined tasks.\nWe conduct experiments on ITERATER (Du et al., 2022) , an iterative text revision dataset. It mainly contains parallel sentences with five edit intentions: fluency, coherence, clarity, style, and meaning-changed. The results show that our approach performs better than the fully fine-tuned BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020) baselines reported in Du et al. (2022) with fewer training parameters.\n\nIterative Text Revision\nFor the first time, Du et al. (2022) systematically studied the iterative revision phenomenon in human writing. They presented the ITERATER, an annotated dataset across multiple domains of formally human-written text, which includes Wikipedia, ArXiv, and Wikinews. And they trained several types of text revision models using ITERATER. Dwivedi-Yu et al. (2022) presented EDITEVAL, an instruction-based benchmark, to evaluate the editing capabilities of models and they also included the test set of ITERATER in it. Based on Du et al. (2022) , our work further explores the method of text revision.\n\nTransfer Learning of Prompt Tuning\nTransfer learning is a common and powerful technique in NLP (Raffel et al., 2020) . Some recent studies have tried to improve prompt tuning performance by leveraging the knowledge of multiple related or unrelated tasks. Asai et al. (2022) used an attention module to make use of the knowledge in exiting soft prompts (Lester et al., 2021) while learning a new task. Chen et al. (2022) improved the few-shot text summarization by multi-task pretraining and prefix-tuning. Specifically, they pretrained a summarization model on a set of popular summarization datasets and then conducted prefixtuning for it on an unseen summarization task. Different from their modeling of a new task through existing tasks, our work aims to achieve the mutual utilization of knowledge between different edit intents in text revision.\n\nMethod\nThe revision task can be defined as the following process: given a source sentence x = [x 1 , . . . , x m ] and an optional edit intent e \u2208 E to generate a revised sentence y = [y 1 , . . . , y n ], where E is the set of all edit intentions. Note that e is optional because it can be inferred from the input x.\nOur method is depicted in Figure 2 . It includes two stages: the multi-prefix tuning stage and the prefix transfer stage.\n\nMulti-Prefix Tuning Stage\nThe prefix is a set of parameters on every attention layer of PLM. For an edit intention e, at each attention layer, the prefix can be described as P e = {P K e , P V e }, where P K e and P V e are parameters added before the key states and value states in this attention layer. After adding these parameters, the calculation of the attention head in this layer becomes:\nH = Attention(Q, [P K e ; K], [P V e ; V ]) (1)\nwhere H is the output vector sequence; Q, K, V are query states, key states, and value states, respectively; Attention means scaled dot-product attention. Only P K e and P V e are updated during the training process. Note that we ignore the layer number information because the operation for each layer is the same.\nAs shown in the left part of Figure 2 , for every edit intention e, we train a prefix P e accordingly. In this way, the model could revise an intentionannotated text by activating the corresponding prefix at inference.\n\nPrefix Transfer Stage\nIdentifying edit intention is always an ambiguous work. At the prefix transfer stage, we aim to build a new prefix for an unannotated input instance by transferring existing prefixes. The new prefix P new is instance-specific.\nThe prefix transfer stage is described in the right part of Figure 2 . At each layer, we rearrange the prefixes {P e | e \u2208 E} obtained in the last stage as\nP K = {P K\ne | e \u2208 E} and P V = {P V e | e \u2208 E} according to whether they are configured before the key states or before the value states. Then a pair of attention units G K and G V are trained for P K and P V .\nTake G K as an example. It calculates the similarity between the key states K and every P K e in P K to get attention scores.\nThe similarity can't be calculated directly, because K and P K e have different lengths. So we perform the max-pool operation for length dimension on K and P K e . After that, we obtain K \u2208 R d and P K e \u2208 R d , where d is the dimension of the hidden states in the PLM.\nTo get attention scores, we train a fully connected layer to extract features from K:\nEQUATION\nwhere W \u2208 R d\u00d7d is a transfer matrix updated during training. Following Asai et al. (2022) , we use SiLU (Elfwing et al., 2018) for the non-linear layer and add a Layer Norm (Ba et al., 2016) layer:\nEQUATION\nThen, we calculate the attention scores for intent e as follows:\nEQUATION\n)\nwhere T is the softmax temperature (Radford et al., 2021) which could avoid making the attention unit over-confident.\nFinally we use them to build P K new as follows:\nEQUATION\nIn the same way, we get P V new by G V . Using the new prefix P new = [P K new , P V new ], our system could revise the unannotated input instance with the knowledge from existing prefixes.\n\nExperimental Setup\nWe choose BART-large as the PLM for our system and use adapter-transformers (Pfeiffer et al., 2020) to implement prefix-tuning. More implementation details are in Appendix A.\n\nDatasets\nWe conduct our experiments on the iterative text revision dataset: ITERATER (Du et al., 2022) . We remove the Other class of the data as it essentially contains a variety of unrecognized edit intentions and accounts for a small proportion (1.44%). The entire dataset consists of two parts: ITERATER-HUMAN and ITERATER-FULL. The former is a smaller dataset with manual annotation of edit intentions, while the latter is a large dataset annotated by a classification model trained on ITERATER-HUMAN. We train our model on both of them. Following Du et al. (2022) , we report the results on the test set of ITERATER-HUMAN in Section 5, which is completely a human-created dataset and is reliable for evaluation. We show more details of the datasets in Appendix B.\n\nEvaluation Metrics\nFollowing previous work, we report three metrics: SARI (Xu et al., 2016) , Rouge-L (Lin, 2004) , and BLEU (Papineni et al., 2002) . Among them, SARI is considered an important metric in situations where input text and output text have a large overlap in words. It also indicates the positive impact of revisions on document quality. The setting of evaluation metrics is the same as Du et al. (2022) . We use the metrics package from Huggingface transformers (Wolf et al., 2020) to calculate the SARI, BLEU, and Rouge-L scores.\n\nModels Setup and Baselines\nUsing our method, we train the models in two ways: the model that only trains the multi-prefix tuning stage and that trains both the multi-prefix tuning stage and the prefix transfer stage.\nWe compare our method with three baselines: full fine-tuning BART (BART-FineTune), full finetuning PEGASUS (PEGASUS-FineTune), and prefixtuning of BART with a single prefix (BART-SinglePrefix). Both BART and PEGASUS are generative models based on the transformer architecture. Compared to the edit-based model FELIX, they perform better. We use the results reported by Du et al. (2022) for these two models. Furthermore, we compare BART-SinglePrefix as a possible technical solution as we choose BART as our backbone model. BART-SinglePrefix trains only one prefix on the entire dataset.\nAll three baselines are trained with two config-urations. The first configuration is using the pure sentence pairs without edit intention annotations to train the model. The second configuration is appending an edit intent token at the beginning of the input text during the training process, which is the same as the approach of Du et al. (2022) .\n5 Results and Analysis\n\nMain Results\nThe main results are shown in Table 1 . Compared to training with a single prefix, the setting of multiple prefixes can improve the results, especially training on ITERATER-HUMAN. Meanwhile, with fewer training parameters, the multi-prefix setting could achieve a comparable SARI score and better average score than the fully fine-tuned BART and PEGASUS baselines. Moreover, prefix transfer could further improve the model's performance. Training on ITERATER-HUMAN, prefix transfer significantly improves the SARI score from 33.12 to 36.01 and gets the highest average score of 67.91. Training on ITERATER-FULL, prefix transfer can also improve the average score from 67.23 to 68.36.\nAn interesting phenomenon is that training on different datasets results in different gains for prefix transfer in evaluation metrics. On ITERATER-HUMAN, prefix transfer improves the SARI score significantly. While on ITERATER-FULL, prefix transfer mainly improves the BLEU score and Rouge-L score. One possible explanation is that in situations when the training data is small, prefix transfer tends to learn more editing operations to improve text quality. In this way, the SARI score related to editing operations will be improved significantly. When the training data is sufficient, pre- fix transfer will model the gold reference in more detail. So the BLEU score and the Rouge-L score will be improved.\n\nAnalysis\nWe further tried to use different training data at different stages of training to conduct experiments.\nThe results are shown in Table 2 . We find that the best practice is to train the model on ITERATER-FULL in the multi-prefix tuning stage and on ITERATER-HUMAN in the prefix transfer stage, which gets the highest SARI score and average score. This may be because of the different distributions of manually annotated edit intent and automatically annotated edit intent. The auto-annotated dataset ITERATER-FULL contains many incorrectly classified sentences, which may cause mismatched knowledge in prefixes. In the prefix transfer stage, due to the existence of mismatched knowledge and incorrectly classified sentences, the continued use of the same training data may finally cause a certain degree of negative transfer. However, if we use ITERATER-HUMAN in the prefix transfer stage, the impact of negative transfer will be mitigated, because ITERATER-HUMAN only contains correctly classified sentences.\nIn Appendix C, we separately provide the performance results on different edit intentions of the best-performing model.\n\nConclusion\nIn this paper, we introduce a new method for multiintent text revision. The system is based on prefixtuning, which first obtains a prefix for every edit intention and then learns to transfer the knowledge in prefixes for every input instance by training a prefix transfer module. This prefix transfer module is configured as two attention units that act respectively on the key states and the value states at each attention layer of the PLM. In this way, our method can make full use of the knowledge of various edit intentions and does not need to anno-tate the intentions of the input. The experimental results show that our method significantly outperforms baselines, and both multi-prefix and prefix transfer settings could improve the performance.\n"}
{"question": "What conclusions can be drawn from the results of the experiment?", "evidence": "  Experimental results on a pre-trained V&L model OFA (Wang et al., 2022) showed that the model forgot part of the knowledge about entities during pre-training, and the image information did not fully compensate for the forgotten knowledge. This result also indicates that OFA does not retain sufficient knowledge of the entities in English Wikipedia. However, tables generated by BART improves IS with the lower performance degradation of FID than that by OFA, indicating that automatically generated tables can improve the diversity of the output images and accurate tables are more important for improving performance in image generation. ", "options": ["A. Automatically generated tables can improve the diversity of the output images.", "B. The model forgot all of the knowledge about entities during pre-training.", "C. The image information can fully compensate for the forgotten knowledge.", "D. OFA retains sufficient knowledge of the entities in English Wikipedia."], "answer": "A", "content": "\nIntroduction\nVision & Language (V&L), which is the fusion of vision and language tasks, has achieved great success in tasks such as caption generation from images (Xu et al., 2015) and image generation from texts (Reed et al., 2016) . This progress has been driven by pre-trained V&L models that are trained on large-scale V&L datasets (Du et al., 2022) . To generate appropriate captions and images for input, pre-trained V&L models need to have prior knowledge of the features of the objects they are generating (Cao et al., 2020; Yun et al., 2021) . These models retain knowledge about entities in particular by inheriting parameters from pre-trained language models used in natural language processing to indirectly utilize data resources such as Wikipedia.\nIn this way, V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang This learning process raises a number of questions, such as whether the knowledge about entities acquired from natural language is adequately retained in the pre-trained V&L model, or whether it is enhanced by combining it with image features. These are important in understanding the limits of what can be generated by the pre-trained V&L model.\nTo answer these questions, we propose a task of generating tables and images of infoboxes in English Wikipedia. Figure 1 shows an example of the target infobox, in which either tables or images are generated by the proposed task. In both cases, the model must know the entities to generate them properly.\nWe collected about 200,000 infoboxes to construct the Wikipedia posed task. In addition, we used OFA (Wang et al., 2022) , a pre-trained V&L model that has achieved state-of-the-art performance in various V&L tasks.\nOur evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is lost when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.\nIn image generation, we found that OFA can generate more accurate images by using the knowledge expressed in the table. We also found that the models trained only on natural language can infer table knowledge, which increases the diversity of generated images. Our code and dataset will be released at https://github.com/kamigaito/WikiTIG.\n\nVision & Language Models\nMany pre-trained V&L models have achieved stateof-the-art performance on various tasks by inheriting the weights of the conventional pre-trained models for natural language and images (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) before learning V&L datasets. Our study examines how the knowledge represented in the pre-trained model for natural language is transformed through such a learning process. We select OFA, which has achieved state-of-the-art performance in multiple V&L tasks, as our target model.\nFigure 2 shows the network structure of OFA and its relation to each dataset 2 . OFA uses VQGAN (Esser et al., 2020) on the decoder to transform images into discrete sequences so that the same Transformer (Vaswani et al., 2017) is used for image and natural language generation. Because OFA inherits 2 Appendix A describes the data for the pre-training.\n\nTask\nInput Output parameters from BART (Lewis et al., 2020) , which shares a similar Transformer structure, OFA should include knowledge acquired from natural language such as Wikipedia articles. Unlike the decoder, the encoder handles images directly; thus, OFA uses the output of ResNet (He et al., 2016) to embed images in addition to the embedding layer inherited from BART.\n\nTable and Image Generation\nIn this section, we describe two tasks for verifying knowledge behavior in the V&L model: table generation and image generation. Both tasks are based on infoboxes in Wikipedia articles, which correspond to summary information of the Wikipedia articles comprising tables and images 3 . Thus, it is suitable for verifying the knowledge about entities in Wikipedia kept in the pre-trained V&L model.\nIn the following subsections, we explain the details of each task.\n\nTable Generation\nIn the table generation task, the target V&L model generates a table from a title and/or image of the infobox. To do this, the model generates linearized tables, similarly to table generation by descriptions (Wu et al., 2022b) . In our setting, we linearize tables as shown in Figure 3 using the column separator \"|\" and the row separator \"<>\" to reuse pretrained token embeddings. The separator symbols are accompanied by spaces before and after for use in BPE tokenization. We investigate the target model by directly generating such linearized text. We use the following settings for the investigation.\nGeneration from titles We investigate the knowledge about entities held by V&L models by comparing tables generated from titles by pre-trained V&L models and by pre-trained models trained only on natural language.\nGeneration from title and images We generate tables from titles with images and compare the results with those generated from only titles. This enables us to investigate the new knowledge in pretrained V&L models transferred from images.\nMetrics For comparison, we use the following evaluation metrics to measure how close the generated tables are to the actual ones.\n-ROUGE: Since the linearized tables are text data and the infobox plays the role of summarizing the article, we use ROUGE (Lin, 2004) , the most widely used evaluation method for automatic summarization. In our evaluation with ROUGE, we convert the column separator \"|\" and the row separator \"<>\" to spaces so that the sequence of strings is not restricted to rows and columns.\n-Table - F 1 : To evaluate the tables with respect to their structure, we divide the cells by their types and then evaluate the matches with the reference table in terms of the F 1 measure for each case and average them. When calculating the matches, we apply clipping used in ROUGE to prevent the score from increasing due to the repetition of the same cell in the output 4 . We treat cells of each type separately 5 as follows:\n\u2022 Group: The infobox sometimes divides the table into groups, with the first row of each group serving as a header for the group name. The prediction performance for the group names is important for verifying what aspects of knowledge the model has about the entities. Since these rows consist of a single column, we target rows consisting of a single column in this type of cell.\n\u2022 Header: The head of each row in the table consisting of more than one column is usually the header of a subsequent cell in the same row. Therefore, the prediction performance for headers is important for the same reason as for group names. to the headers. Therefore, the prediction performance of the values is important for knowing whether the model has detailed knowledge about the entity. To examine the correspondence between headers and their values, we treat a header and its corresponding value as a pair.\n-Corpus-F 1 : Because the above \n\nImage Generation\nIn the image generation task, the model receives a title, caption, and table to generate the corresponding image:\nGeneration from a title and caption By using the minimum input required to generate images, we investigate the difficulty of generating them compared to other datasets.\nGeneration from a title, caption, and table We investigate the impact of knowledge about entities on image generation by generating images from input, including tables, and compare the results to the setting without tables.\nMetrics We use the following three widely used measures for evaluating image generation.\n-CLIP: The relevance of the input text to the generated images inferred by the pre-trained V&L model CLIP (Radford et al., 2021) .\n-Inception Score (IS): How easily a model can distinguish the differences between each image and the variety of generated images (Salimans et al., 2016) . It is inferred by the pre-trained image classification model Inception-v3 (Szegedy et al., 2016) .\n-Frechet Inception Distance (FID): How close the generated image is to the reference image, es- \nModel Input ROUGE \u2191 Table-F 1 \u2191 Corpus-F 1 \u2191 1 2 L\n\nDataset Creation\nWe created the Wikipedia Table and Image Generation (WikiTIG) dataset by extracting infoboxes from the HTML dump data of the English Wikipedia 8 . To ensure consistency in the format of infoboxes, we limited the extraction target to those containing a title in the first row and an image in the second row, as shown in Figure 1 .\nIn order to use only entities with sufficient information, we targeted entities for which the table was not empty. In addition, to ensure reliable correspondence, only rows one column wide, which often describe groups, and rows two columns wide, which often consist of a header and its value, were targeted for extraction.\nThe target images are limited to those in jpeg, png, and gif formats. Since some captions do not include a title, we used a hyphen to join the title at the beginning of the caption in such cases.\nTable 2 shows the size of each dataset. The dataset size diverges between two tasks because some infoboxes do not include captions 9 .\n\nTable Generation\nSettings We chose OFA (Wang et al., 2022) , a pre-trained V&L model, and BART (Lewis et al., 2020) , pre-trained only in natural language, as models for comparison. For both models, we used the base settings with the hyperparameters reported in Wang et al. (2022) . We performed the training three times with different seeds and reported their average scores with their standard deviations 10 .\nResults Table 3 shows the results for each setting in the table generation 11 . When only the title is used as input, the result of BART is more accurate than that of OFA, indicating that part of the knowledge acquired from natural language is lost due to additional learning in the V&L model. The use of image information improves Table-F 1 for headers, indicating that images reinforce the knowledge of what kind of features an entity has.\nIn contrast, F 1 for cell values did not improve, indicating that information obtained from images does not complement detailed knowledge, such as the values corresponding to each header obtained from natural language.\nThe results of BART in Corpus-F 1 also suggest that BART contains more diverse knowledge internally than in other settings. This result reinforces that the V&L model forgot part of the knowledge from natural language through additional learning, and images could not fully complement them.\n\nImage Generation\nSettings Similarly to the table generation, we chose OFA for the comparison. We additionally join the reference tables (Gold) and those generated by models in \u00a75.1 (OFA, BART) as the input in order to investigate the impact of the ability to infer table knowledge. We also used the base settings with the hyperparameters reported in Wang et al. (2022) . We also performed the training three times with different seeds and reported their average scores with their standard deviations 12 .\nResults Table 4 shows the results for each setting in the image generation 13 in OFA is close to the result (Wang et al., 2022) in MS COCO (Chen et al., 2015) for image generation, the use of our created dataset is reasonable for training models. In addition, the input of Table (Gold) improves all metrics, indicating that the model produces higher quality images when provided with complementary knowledge about the entities. This result also indicates that OFA does not retain sufficient knowledge of the entities in English Wikipedia.\nIn addition, we did not observe any performance improvement in CLIP and FID when fed with automatically generated tables from BART and OFA. However, tables generated by BART improves IS with the lower performance degradation of FID than that by OFA, indicating that automatically generated tables can improve the diversity of the output images and accurate tables are more important for improving performance in image generation.\n\nRelated Work\nFollowing the advancements in V&L models (Du et al., 2022) , there have been various studies that investigate V&L models. Cao et al. (2020) conducted a comprehensive analysis of V&L models including the difference between model structures. Through their analysis, they revealed the importance of text information in V&L tasks over image information.\nSeveral studies focused on the performance differences between V&L models and text-only models. Yun et al. (2021) investigated the improvement of linguistic representations by pre-training V&L models on PhysicalQA (PIQA) (Bisk et al., 2020) and the probing framework of (Tenney et al., 2019) . They concluded that the benefit of pretrained V&L models for text-only tasks is marginal. Iki and Aizawa (2021) ; Hagstr\u00f6m and Johansson (2022) compared the performance of V&L models and text-only models on the text-only benchmark, GLUE (Wang et al., 2018) and determined that the text-only model achieved higher scores than the V&L models.\nHowever, even though various kinds of V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) inherit language-related knowledge from pre-trained language-only models, how the knowledge is inherited has yet to be investigated. Our work clarifies this by using our created dataset, Wikipedia Table and Image Generation (WikiTIG).\n\nConclusion\nThis paper investigates how knowledge about entities are preserved in a pre-trained V&L model which is originally transferred from a pre-trained natural language model.\nWe analyzed a pre-trained V&L model by creating the Wikipedia Table and Image Generation (WikiTIG) dataset for generating images and tables of the infoboxes in Wikipedia. WikiTIG consists of 200,000 infoboxes and their corresponding images from English Wikipedia.\nExperimental results on a pre-trained V&L model OFA (Wang et al., 2022) showed that the model forgot part of the knowledge about entities during pre-training, and the image information did not fully compensate for the forgotten knowledge.\n"}
{"question": "Which is wrong about DST?", "evidence": "  Its purpose is to extract and track user's goals throughout a conversation (Young et al., 2010) .  /D.DST is challenging due to the infinite possibilities of user/agent conversations, and because services and schemas/APIs that dialogue systems interface are subject to constant change  ", "options": ["A. Dialogue state tracking (DST) plays a crucial role as an essential component in task-oriented dialogue systems.", "B. Its primary objective is to extract and continuously imitate the user's goals during the course of a conversation.", "C. DST presents challenges partially due to the limitless range of potential conversations between users and agents.", "D. DST presents challenges because the ever-changing nature of the services, schemas, and APIs that dialogue systems interact with.", "Dialogue state tracking (DST) is a critical component for task-oriented dialogue systems. "], "answer": "B", "content": "\nIntroduction\nDialogue state tracking (DST) is a critical component for task-oriented dialogue systems. Its purpose is to extract and track user's goals throughout a conversation (Young et al., 2010) . DST is challenging due to the infinite possibilities of user/agent conversations, and because services and schemas/APIs that dialogue systems interface are subject to constant change (Ren et al., 2018) . Although traditional approaches achieve high accuracy when operating on a pre-defined set of concepts called an ontology (Mrk\u0161i\u0107 et al., 2017; Liu and Lane, 2017; Zhong et al., 2018) , ongoing research explores transfer to new domains with little to no additional learning (Rastogi et al., 2020) using ontology independent architectures to allow seamless adaptation to out-of-ontology concepts.\nMany strategies for zero-shot transfer to unseen domains have been proposed. Li et al. (2021) treat DST as a question answering (QA) task by leveraging data augmentation. Zhao et al. (2022) propose DST by relying on schema descriptions while Heck et al. (2022) utilize natural language descriptions to facilitate zero-shot transfer. Gao et al. (2020) and Lin et al. (2021) suggest learning from non-dialogue QA data which are available in large amounts to improve generalization. Campagna et al. (2020) harness large synthesized data based on abstract dialogue models. However, none of these techniques are ideal solutions. Fine-tuning is challenging due to computational costs, risk of over-fitting and the need for expensive (Budzianowski et al., 2018) task-specific data. Cross-task transfer still requires curated data and careful consideration of suitable learning tasks. Data augmentation requires high level task knowledge and an adequate synthesizing strategy.\nA new generation of large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Glaese et al., 2022) comes with the promise to be equipped to solve any task without task-specific fine-tuning, but solely with world knowledge they acquired during self-training on massive amounts of data. Such LLMs have been shown to perform remarkably well on in-context learning (ICL) , where only a natural language prompt and examples are provided to condition the generation process, achieving significant improvements over fine-tuned approaches in few-shot setups (Brown et al., 2020; Wang et al., 2022) . ChatGPT (Ope-nAI, 2022) -trained using human feedback and reinforcement learning -is the most recent of such models and single-handedly solves an array of challenging natural language processing (NLP) tasks with super-human capabilities, all through a natural language dialogue interface.\nIn this work, we aim to answer the question: does ChatGPT solve the problem of zero-shot DST? We show that crafting intuitive natural language prompts is sufficient to achieve state-of-the-art performance with ChatGPT, exceeding conventional, engineering-heavy approaches to zero-shot DST by a large margin. However, despite our findings, we argue that properties inherent to general purpose models inhibit their ability to simply replace specialized systems. We speculate that while in the foreseeable future general purpose models may not become holistic solutions to complex problems, they will provide ample opportunities to empower specialized systems to go beyond their pre-defined scopes, enable on-the-fly extensibility and generation of high quality training data by zero-shot synthesizing or automatic labeling.\n\nBackground\nDialogue state tracking is tasked to (1) determine for every turn t in a dialogue {(U t , M t )} T 1 with U t and M t being current user and preceding system utterance whether any of the slots in S = {S n } N 1 is present, to (2) predict values for each S n and to (3) track the dialogue state\nDS t \u2200t \u2208 [1, T ]. The DS is cumulative, i.e., DS t = update(DS t\u22121 , DS t ) is updated given the predictions of slot-value updates DS t .\n\nChatGPT\n(OpenAI, 2022) is a dialogue agent (Leike et al., 2018) , and in its core a GPT-3.5 LLM fine-tuned on human-written promptresponse pairs followed by reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020) . RLHF utilizes a reward model trained on human feedback to improve generation quality and adequacy via proximal policy optimization (Schulman et al., 2017) \n\nZero-shot DST with ChatGPT\nOur investigative approach to zero-shot DST with ChatGPT differs considerably from related works. We decode dialogue state updates with a general purpose model, without undergoing any parameter updates. Consequently, we neither employ data augmentation nor cross-task transfer learning. Instead, we solely rely on the general capacities of ChatGPT as an aligned dialogue agent. We take a most rigorous approach to zero-shot transfer where we do not allow the provision of any examples, nor of a formal task definition. Instead, we only permit natural language explanations of what the model is supposed to do. This sets our investigation apart from the closely related IC-DST (Hu et al., 2022) .\nIn zero-shot DST, the set of slots S relevant during inference and the set of slots S \u2032 seen during training of the model X \u03b8 with parameters \u03b8 are disjoint, i.e., S \u2229 S \u2032 = \u2205. Further, it may be S \u2032 = \u2205, in which case \u03b8 is not specifically tuned towards solving DST. This is precisely the case for Chat-GPT in our setup. Our approach to zero-shot DST with ChatGPT is formalized as follows. Let\nA 1 =P \u2295 \"system\":M 1 \u2295 \"user\":U 1 , A t =\"system\":M t \u2295 \"user\":U t , \u2200t \u2208 [2, T ],\nwhere P is the task description which provides the model with instructions for how to process a dialogue between a system M and a user U . A 1 is the initial prompt to ChatGPT. A t\u22652 are the follow-up prompts, only containing a single turn-pair of the dialogue of interest. ChatGPT is particularly suitable for this strategy due to its chat based interface. ChatGPT generates its next output B t conditioned on the current prompt A t\u22121 , as well as all preceding user queries and system responses of the same chat. The dialogue state update DS t can be found in B t , but may not be directly interpretable as such due to the diversity in the output surface forms. Thus, we require a normalization operation DS t = normalize(B t ). In contrast to (Hu et al., 2022) , we do not condition B t on DS t . This renders the task even more challenging, as Chat-GPT is forced to solve complex subtasks such as coreference resolution -the case where a newly encountered slot refers to the value of another slot -solely given the initial prompt and its own latent dialogue state given the dialogue history.\n\nExperiments\nAt the time of conducting our experiments, Chat-GPT is a proprietary research preview accessible for free via a web interface 1 . We used the Jan 9 version of the model. We use a regular expression term to extract all parts that are JSON formatted. We form DS t by accumulating all predicted updates up to turn t.\nEvaluation. We evaluate on the 1000 dialogues of the MultiWOZ 2.1 (Eric et al., 2020) test split and use joint goal accuracy (JGA) to compare methods. For a fair judgement of the ChatGPT predictions, we follow the evaluation procedure of Heck et al. ( 2020). We process each dialogue once and refrain from using ChatGPT's regeneration feature.\nPrompt. We imposed restrictions that the taskdefining prompt P be intuitive natural language and provides no formal schema. The crafting process involves simple trial-and-error on fewer than 10 held-out dialogues from the MultiWOZ training set. The design process was guided by the intention to imitate the behavior of a triple copy strategy (TripPy) DST (Heck et al., 2020) . P consists of three parts. First, a list of names for detectable informable slots along with natural language descriptions. The slot names help us extract a DS t that is compatible with the dataset's labels. Second, a sparse list of slots that are categorical, along with their value candidates for (1) aiding normalization of values that are expected to show high variability in expression, and (2) modeling Boolean slots.\nThird, an informal task description. 2\n4.1 ChatGPT vs. Supervised SOTA Comparing ChatGPT's performance to state-of-theart supervised approaches that achieve close to 60% JGA is not a fair fight 3 , and yet we observe an impressive 31.5% zero-shot JGA. This result is double-edged; on the one hand it is evidence that ChatGPT is capable of DST 4 , and on the other hand is no match for specialized systems. The comparison to TripPy, a SOTA supervised model, allows us a more fine-grained analysis. In Figure 1 , slot filling performance is broken down into value types. We observed that ChatGPT underperforms in non-trivial cases, namely refer, where a newly encountered slot refers to the value of another slot, and inform, where a slot-value was mentioned by the system and confirmed by the user. ChatGPT shows slight underperformance for Boolean slots. Remarkably, performance for values that are extracted directly from user utterances -the most relevant category in terms of frequency - (Hu et al., 2022) was the first successful attempt at pseudo 5 zero-shot DST via ICL. Our preliminary results with ChatGPT are on par, which is remarkable for the following reasons.\n(1) Our prompt is non-schematic and without examples, (2) our task-defining prompt is stated only once at the beginning of the chat, and (3) we do not maintain a DS to serve as additional input at each turn. The heightened zero-shot performance of IC-DST can be mainly attributed to these points.\n\nError Analysis\nWe identified a set of recurring errors that are likely caused by either the content of P or by the model's inherent properties. See ChatGPT is a sophisticated dialogue agent that, via alignment with human judgements, is capable of understanding context and intent of a multi-turn conversation far beyond the capacities of the previous generation of LLMs. This makes it well-suited for DST. Our results demonstrate that even with intuitive natural language prompts, a complex task such as DST can be solved exceedingly well without any form of additional learning. While specialized systems can exert control over its input-processing and output-generation to arbitrary degrees, this is not the case for Chat-GPT. Even with the most rigorous and schematic prompts, there can be no guarantee that the model interprets the input as intended or generates the output as required, which may lead to unexpected behavior. Furthermore, there is no guarantee that behavior is consistent across a series of similar inferences, such as in our experimental evaluation. In terms of deployment, the cost factor of building and running massive models may hinder their utilization as a plug-and-play module.\nDespite impressive zero-shot and ICL results for general purpose models, specialist models still perform best on most tasks thanks to task-specific solutions via adequate engineering (Heck et al., 2020; Ye et al., 2021; Kim et al., 2020) and taskrelated data. However, the opportunities to improve dedicated systems with the help of general purpose models are plenty. Their predictive powers could be used for developing smaller, specialized, low inference cost models. Automatic labeling and data a) PMUL4050 system: \"I'd recommend the Autumn House. Would you like to make a booking?\" user: \"Yes please. I need the reservation to be for 8 people and 2 nights starting on Tuesday.\" Prediction: ... hotel-name: none Label: ..., hotel-name: autumn house b) PMUL0117 user: \"Yes I also need a taxi that will get me to the restaurant by the booked time please.\" Prediction: taxi-destination: hotel, taxi-departure: restaurant Label: taxi-destination: the gonville hotel, taxi-departure: la mimosa c) SNG01873 user: \"I need to be picked up from pizza hut city centre after 04:30\" Prediction: ..., hotel-name: dontcare, ..., attraction-type: dontcare, ... Label: ... d) PMUL0599 user: \"[...] Can you just help me find a high-end Mexican restaurant?\" Prediction: ..., restaurant-pricerange: high-end Label: ..., restaurant-pricerange: expensive e) MUL2051 user: \"Can I get address and postcode for the hotel?\" Prediction: hotel-address: ?, hotel-postcode: ? Label:system: \"The address is 74 chesterton road, the postal code is cb41er, can I assist with anything else?\" user: \"That is all for now, goodbye.\" Prediction: hotel-address: 74 chesterton road, hotel-postcode: cb41er Label:f) MUL0524 user: \"I'm going to Cambridge on saturday and want to arrive by 14:15 please.\" Prediction: ..., train-day: Saturday Label: ..., train-day: saturday g) PMUL4246 user: \"i need a place to go and should be a museum\" Prediction: attraction-type: museum Label: attraction-type: museum system: \"Okay! There are several museums in Cambridge. What part of town would you like to visit?\" user: \"How about ones in the centre, what's available?\" Prediction: attraction-type: museum, attraction-area: centre Label: attraction-area: centre augmentation are natural use cases for ChatGPT, as is evident from our experimental results; a perdomain JGA of 70% (see Section 4.2) is surely sufficient to generate additional mid-to high-quality training data for dedicated systems. Automatic labeling may be conducted on-line for on-the-fly adaptation of production systems or off-line for iterative learning. Another way of harnessing general purpose models is the integration into dedicated systems as fallback options in case of out-of-domain or out-ofontology requests. An integration via knowledgeseeking term detection (Gunasekara et al., 2020) could facilitate the ability to provide context-aware responses that go beyond the original scope of the specialized system. General purpose models may handle unseen domains in place of the main model.\nWhile hallucinations may be an issue if not handled adequately, they also pose an opportunity to enable zero-shot concept detection. We observed that many slot hallucinations were sensible and pointed at elements that were meaningful to conversations. Zero-shot slot detection may be utilized to annotate and prepare unstructured data for model training, and to expand a system's capacities on-the-fly. Dialogue state trackers with dynamic dialogue states have the potential to expand a taskoriented dialogue system's conversational range seamlessly (Geishauser et al., 2022) . A general purpose model that has the capacity to identify new concepts may be utilized to generate API calls and database queries that are unknown to the specialized system (OpenAI, 2023; Chase, 2023) .\nGeneral purpose models may replace some components in a modular dialogue system (Zhu et al., 2022) . It might still be beneficial to rely on specialized DST and a dedicated policy for particular tasks in order to maintain interpretability and a desired level of control over information flow. However, natural language understanding (NLU) and natural language generation (NLG) modules may be powered by generative large language model based systems such as ChatGPT in order to benefit from a heightened ability of semantic modeling and to facilitate more natural and diverse output, thus promoting more natural conversations with modular task-oriented dialogue systems.\n\nConclusion\nThis work is the first to investigate ChatGPT's capacities for zero-shot DST. Despite remarkable preliminary results that we achieved, we identified limitations rooted in inherent properties of general purpose models, preventing these to become holistic solutions to complex NLP problems without further research. We discussed opportunities provided by ChatGPT and similar models to advance the development of specialized systems. With our insights and discussion, we hope to stimulate research in similar directions.\n"}
{"question": "What strategy has Gao et al. adopted to achieve strong performance when only a few examples are available\uff1f", "evidence": "  Prompt-based fine-tuning is a method for adapting PLMs to specific tasks or domains by providing a prompt (Schick and Sch\u00fctze, 2020a,b) . This approach has been shown to be effective in various NLP tasks, including text classification (Han et al., 2021; Wang et al., 2022) and question answering (Yao et al., 2022) . However, it can be challenging to achieve strong performance when only a few examples are available for each task. Gao et al. (2020) introduced a prompt-based fine-tuning method called LM-BFF for RoBERTa (Liu et al., 2019) to tackle this issue. Their approach includes automated prompt generation and a more effective way of using task examples in fine-tuning.\n ", "options": ["A. Providing a prompt.", "B. Automated prompt generation and a more effective way of using task examples in fine-tuning.", "C. Using different templates for their demonstrations when building prompts.", "D. Considering contrastive learning's promising results.\n"], "answer": "B", "content": "\nIntroduction\nPre-trained language models (PLMs) are trained on large-scaled corpora in a self-supervised fashion. They have fundamentally changed the NLP community in the past few years by achieving impressive results in various Tasks (Devlin et al., 2018; Radford et al., 2018; Yang et al., 2019; Chiang et al., 2022) . However, when PLMs are finetuned on small datasets, their performance declines. Researchers have proposed various techniques to adapt PLMs to these scenarios (Snell et al., 2017; Sung et al., 2018) . In addition to performance, fine-tuning PLMs to learn a new task is parameter inefficient, because an entirely new model is required for every task (Houlsby et al., 2019) .\nBy the introduction of GPT-3 (Brown et al., 2020b) with 175B parameters, it has been shown that Large Language Models (LLMs) are efficient few-shot learners as they can use their knowledge more effectively. One of the key features of these LLMs is their ability to perform multiple tasks using prompts. A language prompt is a piece of text that is added to the input query to help the model make more accurate predictions. In addition, LLMs can be fine-tuned for specific tasks using few examples. This has made them powerful tools for NLP tasks, especially in few-shot scenarios. However, that might not be practical for many situations because of the model size. Therefore, there is a need to adapt smaller PLMs to work in a similar way to LLMs.\nPrompt-based fine-tuning is a method for adapting PLMs to specific tasks or domains by providing a prompt (Schick and Sch\u00fctze, 2020a,b) . This approach has been shown to be effective in various NLP tasks, including text classification (Han et al., 2021; Wang et al., 2022) and question answering (Yao et al., 2022) . However, it can be challenging to achieve strong performance when only a few examples are available for each task. Gao et al. (2020) introduced a prompt-based fine-tuning method called LM-BFF for RoBERTa (Liu et al., 2019) to tackle this issue. Their approach includes automated prompt generation and a more effective way of using task examples in fine-tuning.\nBuilding on the success of LM-BFF and considering contrastive learning's promising results both in computer vision (Chen et al., 2020) and NLP (Chen et al., 2020; Miao et al., 2021) , Jian et al. (2022) present a contrastive learning framework to improve LM-BFF. They propose a Supervised Contrastive Learning (SCL) approach (Khosla et al., 2020 ) that classifies inputs using different augmented views of the data. These views are created using different templates for their demonstrations when building prompts.\nIn this paper, we show that while SCL at the feature space can be beneficial, the use of different templates can limit the full potential of this approach. We propose LM-CPPF (Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models), in which we integrate the knowledge of LLMs like GPT-3 and OPT-175B (Zhang et al., 2022) to build different views using paraphrasing. These models can generate paraphrases of a sentence with different syntax, not just by changing the lexicalization. Previous studies have considered generating paraphrases a challenging and costly NLP task (Siddique et al., 2020; Garg et al., 2021; Zhou and Bhat, 2021) . However, PLMs can generate paraphrases easily and effectively using in-context learning with few examples. Although prior research has studied paraphrase generation with PLMs (Roy and Grangier, 2019; Hegde and Patil, 2020) , to the best of our knowledge, this is the first time that large LLMs are utilized to generate paraphrases with prompts as an augmentation method. Our experiments on six different text classification tasks demonstrate that LM-CPPF outperforms the previous SOTA methods of data augmentation in prompt-based fine-tuning, including Easy Data Augmentation (EDA) (Wei and Zou, 2019) , Back Translation (BT) (Sugiyama and Yoshinaga, 2019) , and multiple templates (Jian et al., 2022) .\n\nRelated Works\nLLMs like GPT-3 (Brown et al., 2020a) can perform NLP tasks with few examples and natural prompts. But smaller models are not efficient with this approach and there are data sparsity and prompt sensitivity issues. To address these challenges, Gao et al. (2021) propose LM-BFF, a framework that leverages a large PLM to automatically generate task-specific prompts for smaller models. It improves their few-shot performance on different NLP tasks. Some work have enhanced LM-BFF with different prompt tuning methods. For example, Zhou et al. (2022) present a dual context-guided continuous prompt tuning method that uses the language context and connects discrete and continuous prompt tuning. Jian et al. (2022) integrate contrastive learning and data augmentation with LM-BFF. In their contrastive part, in addition to comparing different instances from the same or different classes, they introduced a novel promptspecific augmentation method. In their approach, they change the template of the prompt. In this paper, we use few-shot paraphrasing with LLMs for contrastive prompt-tuning, which fine-tunes models with natural prompts.\nParaphrasing is the task of expressing the same meaning with different words or structures. It can be used to create training data with increased diversity and naturalness for NLP tasks, such as text classification (Xie et al., 2020) , natural language inference (Kumar et al., 2019) , and text summarization (Loem et al., 2022) , surpassing the limitations of traditional approaches. Paraphrasing helps with data scarcity and model generalization. There are different ways to generate paraphrases for data augmentation. One is back-translation (Sennrich et al., 2016) , which uses a translation system to convert a sentence to another language and back. Another is to use paraphrasing models trained on parallel paraphrase datasets (Wieting and Gimpel, 2018; Zhu et al., 2022) . PLMs can also generate paraphrases by using large-scale corpora, but they may produce paraphrases that are not semantically consistent or relevant. LLMs can reduce this problem as they encode and generate language better. In this paper, we generate paraphrases by carefully prompting LLMs and then use them for data augmentation.\n\nMethod\nBackground Contrastive learning's success relies on data augmentation, which creates new views of the input data. Contrastive learning has been utilized for various tasks in deep learning (Le-Khac et al., 2020; Conde and Turgutlu, 2021; Abaskohi et al., 2022) ; however, most NLP data augmentation methods may influence semantics which results in limited improvement. For instance, EDA's synonym substitution may create entirely new samples since words do not have equal senses (Keselj, 2009) . In addition to these augmentation methods, the approach used in Jian et al. (2022) cannot be counted as data augmentation as the sample is still the same and only the template for the verbalizer changes. Although it is a creative approach designed specifically for the prompt-based method of LM-BFF, it is limited in performance even compared to EDA in several benchmarks. Furthermore, it requires an expert to create multiple templates for each task, which makes it challenging for newly emerged tasks. Here we propose leveraging LLMs to generate paraphrases and introduce LM-CPPF, a novel approach aimed at addressing the challenges associated with contrastive prompt-based fine-tuning of PLMs.\nFew-shot paraphrasing Paraphrasing is one of the best methods for data augmentation in NLP. One of the most popular approaches for paraphrasing is back-translation (BT) (Sugiyama and Yoshinaga, 2019) due to its simplicity and efficiency. Nonetheless, BT's performance depends a lot on the intermediary language. In this paper, we, instead, use a combination of prompt-learning and LLMs for paraphrasing. In few-shot paraphrasing, an LLM rewrites a sentence given an instruction and a few examples. We believe that LLMs generate high-quality paraphrases due to their encoded semantic and sentence structure knowledge. We utilize GPT-3 (Brown et al., 2020b) or OPT-175B (Zhang et al., 2022) via their official APIs 2 for generating paraphrases.\nTo avoid violating the prompt-based fine-tuning settings, we do not include any additional task data in generating our paraphrases. Following the fewshot setting in LM-BFF, we assume to have access to a PLM M , datasets D train , and D test with label space Y where there are only K = 16 examples per class in D train . We use this setting for both promptbased few-shot paraphrasing and fine-tuning. To generate paraphrases, excluding the one sample that we want to paraphrase, we use QuillBot 3 to create paraphrases for our prompts for the remaining 15 samples in the same class of D train . We leverage two types of prompts for paraphrasing: (I) Only Demonstration: Here, the samples and their paraphrased versions are given using the templates in Table C .3 to demonstrate the task of paraphrasing. (II) Demonstrations with Instruction: In addition to the previous method, this one includes instructions at the beginning of the prompt, defining paraphrasing before demonstrations. These instructions can be seen in Table C .4.\nContrastive prompt-based fine-tuning LM-CPPF consists of two steps. The first step involves calculating the Masked Language Modeling (MLM) loss by using the target sentence in the given template, the specific demonstrations in the prompt, and the verbalizer matched with the target sentence's label. We calculate the supervised contrastive loss in the second step by comparing the target prompt with another sample with the same template but different random demonstrations. This comparison sample can be in the same or a different class as the target prompt. When the comparison sample belongs to a different class, it is randomly sampled from the dataset. However, in cases where the comparison sample belongs to the same class, an alternative approach is employed. This involves either selecting another sample from the same class \n\nExperiments\nEvaluation datasets and protocol Our method is evaluated on six different classification tasks from LM-BFF (Liu et al., 2021) . The reported numbers represent the average accuracy from five runs using Roberta-base (Liu et al., 2019) . In Section 4.1 where LLMs are compared for paraphrasing, we also employed pre-trained and fine-tuned GPT-2 as an additional model for paraphrasing, allowing us to leverage smaller models in our experiments.\nFor the fine-tuning of GPT-2 specifically for paraphrasing, we utilized the ParaNMT-50M (Wieting and Gimpel, 2018) dataset. More details regarding the training process can be found in Appendix A.\n\nParaphrasing in Prompt Fine-tuning\nThis section presents the results of our fine-tuning approach using paraphrasing on various NLP tasks.\nAs shown in Table 1 , LM-CPPF improves the model's accuracy on all tasks compared to the baseline method of LM-BFF+Multi-templates (Jian et al., 2022) . Comparing the standard deviation of our model in five runs and the standard deviations of LM-BFF and LM-BFF + Multi-templates, we see that LM-CPPF has a higher standard deviation as it uses an intermediary model for generating paraphrases. In contrast, LM-BFF + Multi-templates integrates templates that have nearly equal performance (Jian et al., 2022) .\nWe also compare the effect of using GPT-3, OPT-175B, and GPT-2 as our language model for fewshot paraphrasing. We did two experiments with GPT-2 large: (I) Using a pre-trained version of GPT-2 where the weights are not tuned at all (II) Fine-tuned GPT-2 where the model has been finetuned on the ParaNMT-50M dataset. The results in Table 1 indicate that GPT-3 outperforms OPT-175B in all tasks and GPT-2 has a lower performance, which was predictable since it has significantly fewer parameters. Also, fine-tuned GPT-2 shows a better performance which suggests that GPT-2's knowledge after pre-training is not enough for doing a task like paraphrasing. About the LLMs, although both models have 175B parameters, OPT-175B has a 1/7 carbon footprint of GPT-3, and it is also freely available (Zhang et al., 2022) . Consequently, we base our further analysis on OPT-175B.\n\nFew-shot Paraphrasing vs. Other Data Augmentation Methods\nIn this section, we present an experimental comparison of the performance of the few-shot paraphrasing approach and other data augmentation methods, including BT and EDA. The results are shown in than EDA. We believe the reason is that BT can be more effective for longer sequences because longer sequences usually contain more context and nuanced meaning. Moreover, EDA employs additional knowledge from another PLM in certain actions, such as synonym substitution, similar to BT and few-shot paraphrasing.\nThe few-shot paraphrasing approach introduced in this work outperforms both BT and EDA. This confirms that using PLM's knowledge properly in paraphrasing is an effective and efficient data augmentation method. In few-shot paraphrasing, we instruct the model to generate paraphrases that differ in lexicalization and sentence structure.\n\nPrompt Template Evaluation\nAs the heart of our method is the few-shot paraphrase generation done by LLMs, we investigate the impact of different paraphrasing prompt demonstrations and instruction templates on the performance of our model. Table 3 shows that the last template presented in Table C .3 is better in almost all tasks. This template, \"<Original Text>, in other words <Paraphrased>\", uses a complete and concrete sentence, unlike other templates, which use specific tokens, such as \"[Original]\", to dis- tinguish between the original and the paraphrased version. Also, we compare different instruction templates presented in Table C .4. As we aimed to report our best result in each task here, we used the best demonstration template for any particular task, which was determined in Table 3 . Table 4 shows that the fourth template achieves the best performance, as it precisely describes the task with its instruction \"Generate a paraphrase of the following text using different words and sentence structures while still conveying the same meaning\".\n\nConclusion\nOur experiments demonstrated the effectiveness of using few-shot paraphrasing as a data augmentation method for contrastive prompt-based fine-tuning of PLMs. It outperformed other data augmentation methods in text classification tasks, such as EDA, multiple templates, and back translation. We also found that our approach is effective with GPT-3 or OPT-175b models in generating paraphrases. Overall, LM-CPPF improves the performance of LM-BFF by large margins using contrastive learning applied on paraphrases generated by LLMs.\n"}
{"question": "Which one in the following options is not used as the research methods?", "evidence": "perform knowledge distillation by finetuning the student model on the CoT generated by a large teacher model; generate the CoT from an LLM, as proposed by Wei et al. (2022) , but crucially provide the solution to the task in the few-shot prompt. Lastly, we remove all incorrect CoT based on the target answer to prevent the student to learn from bad examples. ", "options": ["A:perform knowledge distillation by finetuning the student model on the CoT generated by a large teacher model; ", "B: generate the CoT from an LLM, as proposed by Wei et al. (2022) , but crucially provide the solution to the task in the few-shot prompt. ", "C:Lastly, we remove all incorrect CoT based on the target answer to prevent the student to learn from bad examples. "], "answer": "D", "content": "\nIntroduction\nChain of thought (CoT) prompting encourages language models (LMs) to break down a reasoning task into a series of intermediate steps (Wei et al., 2022) . They demonstrate that this prompting significantly increases the task accuracy of large language models (LLMs) across commonsense, symbolic and mathematical reasoning datasets. Here, LLMs are models with at least tens of billions of parameters, such as PaLM 540B (Chowdhery et al., 2022) , GPT-3 175B (Brown et al., 2020) , or UL2 20B (Tay et al., 2022) . However, the reasoning capabilities of smaller LMs do not improve with CoT prompting, mostly producing illogical CoT. Notably, CoT prompting even reduces the accuracy of models with less than 10 billion parameters. Wei et al. (2022) attribute this to abilities, such as semantic understanding and symbolic mapping, only emerging at larger scales. This leads us to our re- * Research conducted during an internship at Google. search question: can the reasoning capabilities of LLMs be transferred to smaller LMs via finetuning?\nThis work explores CoT knowledge distillation (Hinton et al., 2015) from PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) to different sizes of the smaller language model T5 (Raffel et al., 2020) , such as T5 XXL, XL and base, which have 11 billion, 3 billion and 220 million parameters, respectively. As a result of our work, we make two recommendations: (1) perform knowledge distillation by finetuning the student model on the CoT generated by a large teacher model; and (2) generate the CoT from an LLM, as proposed by Wei et al. (2022) , but crucially provide the solution to the task in the few-shot prompt. We demonstrate that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets irrespective of the teacher model used. For example, we show an accuracy increase from 8.11% to 21.99% and 18.42% on the GSM8K (Cobbe et al., 2021) dataset when finetuning T5 XXL on PaLM 540B and GPT-3 175B generated CoT data, respectively.\n\nRelated Work\nThis work is inspired by the seminal work of Wei et al. (2022) on CoT prompting. They demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021) . Wang et al. (2022) show that task accuracy can be further improved by using self-consistency in CoT prompting. Selfconsistency samples CoT reasoning paths from a model's decoder and returns the most consistent path by taking the majority vote. Subsequently, Chung et al. (2022) explore finetuning a FLANbased (Wei et al., 2021) version of PaLM on manually generated CoT data.\nConcurrent to our work, a small number of other works propose methods focused on CoT student-teacher knowledge distillation. Ho et al. (2022) and Li et al. (2022) also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. We also achieve a higher accuracy on common datasets, such as GSM8K, than Ho et al. (2022) . In contrast to our work, Shridhar et al. (2022) focus on training two models, one for problem decomposition and one for solving. Yet differently, the focus of Eisenstein et al. (2022) relies on producing markupand-mask explanations for open-book question answering. Lastly, Huang et al. (2022) present one related experiment, however, we present a more indepth exploration on more datasets. To the best of our knowledge, our work is the first to extensively explore the improvement of the reasoning ability of small LMs via knowledge distillation across multiple model architectures, and observing the effects of student model size and dataset size on accuracy.\n\nMethod\nWe propose a two-step pipeline for CoT knowledge distillation. The first step comprises annotating an existing supervised dataset with CoT reasoning generated by a teacher model. To generate high quality data, we propose using LLMs, such as PaLM 540B or GPT-3 175B, as teachers, based on the finding that CoT reasoning improves with model scale (Wei et al., 2022) . Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by Wei et al. (2022) . We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT. This is based on the observation that providing this guidance allows LLMs to correct small mistakes in the CoT. Lastly, we remove all incorrect CoT based on the target answer to prevent the student to learn from bad examples. The second step comprises finetuning a student model via teacher forcing (Williams and Zipser, 1989) . The student is provided with the question as input, and the CoT and answer as the target. As the model is trained on producing a CoT during finetuning, prompting is not required. \n\nExperimental Setup\nWe follow a similar experimental setup to Wei et al. (2022) , focusing on tasks covering arithmetic, commonsense and symbolic reasoning.\n\nArithmetic Reasoning\nWe benchmark the proposed method on the following math word problem datasets: (1) GSM8K (Cobbe et al., 2021) , ( 2) MAWPS (Koncel-Kedziorski et al., 2016) and (3) ASDiv (Miao et al., 2021) . We use the official training and testing split for GSM8K, taking the last 10% of the training split for validation, and the 5-fold cross validation splits available for MAWPS and ASDiv. We evaluate task accuracy by checking for the target answer as the final answer in the CoT. In addition, we compute the task accuracy given an external calculator, to account for arithmetic mistakes made by the model, despite the CoT being correct. The external calculator moves through the generated output, recalculating the left hand-side of equations. It then replaces the right-hand side with the calculated output, to avoid arithmetic mistakes being carried forward. For example, if a model outputted '5 + 5 = 11. 11 * 2 = 22', then the external calculator would first calculate '5+5' and replace the '11' with a '10'. In the subsequent equation, it would also replace the '11' with a '10' and arrive at the final result of '20'.\n\nCommonsense Reasoning\nWe benchmark the model's ability to perform commonsense reasoning on the StrategyQA dataset (Geva et al., 2021a) . As a testing split is not available, we do not shuffle the dataset to allow reproducing our split of taking the first 80% as training data, the following 10% as validation data, and the final 10% as testing data. We compute task accuracy in the same manner as previously mentioned.\n\nSymbolic Reasoning\nLastly, we benchmark the model on two synthetic tasks for symbolic reasoning: (1) last letter concatenation and (2) coinflip (Wei et al., 2022) . Last letter concatenation prompts the model to concatenate the last letter of each word in a string. Coinflip prompts the model to perform state tracking of the coin being flipped. We evaluate task accuracy in the same manner as before. Due to the rigid structure of the datasets, we focus on evaluating the model's generalizability to out-of-distribution (OOD) examples. We finetune the models on examples of length two and evaluate on sequences of length three and four. We initially infer the CoT using PaLM 540B, however, find that the LLM is able to perfectly replicate the desired CoT bar one example due to the rigidness of the template. We therefore decide to use the template generated CoT in our experiments.\n\nBaselines and setup\nWe select PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) as teacher models. We select PaLM 540B based on the stateof-the-art results on the benchmarking datasets reported by Wei et al. (2022) , and confirm the observed trends with GPT-3 175B. The publicly accessible teacher models are prompted as described in Section 3.\nWe select different sizes of T5 (Raffel et al., 2020) as student models, as T5 is publicly available in many sizes. The student models are trained on the PaLM 540B or GPT-3 175B generated CoT data as described in Section 3. We establish T5 XXL model finetuned on the original target as the baseline. We refrain from shuffling the datasets to allow for reproducibility.For the MAWPS and ASDiv dataset, we perform 5-fold cross validation. For all remaining datasets, we take 10% of the Input: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nOutput:\nRoger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. We refer the reader to Appendix A for an overview of the dataset licenses. We also refer the reader to Appendix B for an overview of the computatinal resources.\n\nArithmetic reasoning\nTable 1 details the task accuracy with and without an external calculator for the arithmetic reasoning benchmarks. Our results show that the proposed method improves task accuracy across all datasets. Most notably, the task accuracy of MAWPS is significantly improved. The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. 1 : Task accuracy across arithmetic reasoning datasets for T5 XXL without finetuning (baseline) and finetuned on PaLM 540B generated chain-of-thought (CoT). We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on generating chain-of-thought data\nWe perform an ablation study to confirm that providing a LLM with the target during CoT generation is beneficial. We found that for the GSM8K dataset, PaLM 540B only achieves a 59.98% accuracy if prompted without the target. In comparison, when including the target in the prompt the accuracy is 79.37%. A superficial explanation would be that when the model is conditioned on the expected answer, it produces the same CoT but copies the answer. However, an analysis of a subset of the differences between CoT produced with and without this conditioning shows that most of the benefits actually come from the model correcting CoT that had a single step missing or was wrong.\n\nCommonsense reasoning\nFor the StrategyQA dataset (Table 3 ), we found that using CoT finetuning improves accuracy from 68.12% to 71.98%, using only 1319 of the original 1648 examples. Compared to the arithmetic reasoning datasets, the improvement is not as significant. This can be explained by the model lacking factual knowledge that the dataset requires. The task is heavily focused on the model reasoning on such knowledge, however, a smaller LM is most likely not in possession of this knowledge compared to a larger model with higher memorisation capacity.\n\nSymbolic reasoning\nTable 2 shows the results obtained for the synthetic symbolic reasoning datasets, focusing on OOD generalization. Focusing on Last Letter Concatenation, it can be stated that both traditional finetuning and the suggested method fail at generalizing to a longer sequence length. In comparison, the proposed method significantly increases accuracy for the Coinflip dataset with regard to generalizing to three coinflips. In contrast, generalisation to four coinflips is slightly weaker than the baseline, which performs very strongly. This may be related to the task length being twice that of the training task.\n\nReplicating Results using different Teacher Models\nWe demonstrate the robustness of our method using a different teacher model, namely GPT-3 175B. Table 3 shows the results for GSM8K and Strat-egyQA when T5 XXL is finetuned on CoT data generated by GPT-3. The results show that the proposed method elicits improvements also with other Table 3 : Task accuracy for T5 XXL finetuned on chainof-thought (CoT) data generated by PaLM 540B and GPT-3 175B. We also finetune on the reasoning steps provided by the datasets. We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on model size\nWe investigate the performance gain achieved via finetuning student models of different sizes. Figure 3 shows the performance gain achieved when finetuning T5 of different sizes on the GSM8K dataset.\nOur results show that T5 base, with 44 times fewer parameters than T5 XXL, matches the performance of the baseline T5 XXL when trained on CoT data. Moreover, given an external calculator, even T5 small outperforms the baseline T5 XXL.\n\nAblation study on dataset size\nWe also investigate the trade-off between the performance gain from CoT finetuning and dataset size. \n\nDiscussion\nWe demonstrate that finetuning larger LMs on the CoT data generated by LLMs of over 100 billion parameters can significantly improve task accuracy. Even a small number of CoT examples appear to suffice for this. However, such improvements appear to be task dependent. For example, the effects are limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller LMs may not have memorised due to their limited capacity. Nevertheless, there is some performance improvement, which may be attributed to the model learning how to approach such tasks. Moreover, the CoT knowledge distillation pipeline presented allows to trade-off model and dataset size with accuracy. Future work could explore improving the reasoning of small models in multi-task settings, as well as the generation of new training data using LLMs, rather than annotating existing datasets.\n\nConclusion\nThis work explores CoT knowledge distillation from LLMs of over 100 billion parameters to smaller LMs. We propose a knowledge distillation pipeline consisting of two keys steps: (1) generate CoT for existing datasets using LLMs and\n(2) finetune smaller LMs on the CoT. Our results demonstrate that finetuning on CoT improves task accuracy across a range of benchmarking datasets.\n"}
{"question": "What method was used to simplify the annotation process and align with the French futur proche tense?", "evidence": "Considering the corresponding grammatical expressions of 2 modes strongly related to tense, conditionnel and subjonctif, in French rely on the usage of modal verbs, we introduce modal verbs to simplify the distinguishment of the modes. During this process, to simplify the annotation process and better correspond with French futur proche tense, we classify the expression 'be going to do', grammatically in Future tense, into the Present tense", "options": ["A. Utilizing modal verbs", "B. Applying the spaCy package", "C. Merging the progressive tense into its base tense", "D. Classifying the expression 'be going to do'", "Considering the corresponding grammatical expressions of 2 modes strongly related to tense, conditionnel and subjonctif, in French rely on the usage of modal verbs, we introduce modal verbs to simplify the distinguishment of the modes.", "During this process, to simplify the annotation process and better correspond with French futur proche tense, we classify the expression 'be going to do', grammatically in Future tense, into the Present tense"], "answer": "C", "content": "\nIntroduction\nTranslation tools are often found in a variety of social situations to enable cross-linguistic communication. Tenses are used to express time relative to the moment of speaking. Human translators frequently pay close attention to tense correspondence (Gagne and Wilton-Godberfforde, 2020) . Similarly, machine translation (MT) systems are supposed to maintain temporal consistency between the original text and the predicted text to avoid misunderstandings by users. However, accurately keeping the tense consistency is undoubtedly difficult. Taking French-English (one of the most classic language pairs for MT) as an example in Table 1 , the original text is in plus-que-parfait de l'indicatif of French, corresponding to the past perfect tense in English, while the English prediction provided by Google Translator is in the past simple tense.\nIn fact, this is not an isolated case. You can also find several examples in Appendix B. Besides. the translation mechanics may not the only reason leading to tense inconsistency. The corpora matter as well. For example, we have extracted 20,000 pairs English-French parellel sentences from the widely used dataset Europarl (Koehn, 2005) , and\n\nSentence\nTense FR: Mais on les avait vot\u00e9s lors de la derni\u00e8re p\u00e9riode de session.\n\nPlus-queparfait\nEN: But we voted on them during the last part-session.\n\nPast simple\nCorrection: But we had voted on them during the last part-session. we have observed all groups of parallel utterances where the original French texts are in the plus-queparfait de l'indicatif tense, examining the tenses of their English counterparts. As a sentence may include several tenses, there are 195 occurences of plus-que-parfait tense in total. Among them, only 35.28% English sentences are in the correct past perfect tense, as shown in Table 2 . Although, compared to other tense correspondences, the pair of plus-que-parfait and past-perfect is prone to error in datasets and there are only 0.94% of sentences in Europarl are in plus-que-parfait, we cannot easily ignore this issue. Like Europarl, tense correspondences are generally credible but unreasonable for certain tenses in several common datasets. In addition to the train set, the difficulty of remaining tense consistency also stems from the lack of metrics on measuring the model's mastery of tense information. The research of Marie et al. (2021) shows that 98.8% of *ACL papers 2 in the field of MT from 2010 to 2020 used BLEU (Papineni et al., 2002) scores to evaluate their models. However, the reliability of BLEU has been questioned in the era of neural machine translation (NMT) as its variants only assess surface linguistic features (Shterionov et al., 2018) , and many studies have shown that BLEU has difficulty in portraying the degree of semantic information mastered by the model, i.e. its score does not necessarily improve when more semantic information is mastered (Mathur et al., 2020; He et al., 2023) , not to mention specific tense information. We have also applied BLEU to measure various baselines on our tense test set in Section 4, and the results explicitly support the above statement. In addition, reviewing the evaluation criteria related to MT tasks over the past ten years, we are surprised to find that there are no criteria to assess the model's mastery of tense prediction from a linguistic perspective.\n\nPast perfect\nTherefore, our paper is devoted to the study of NMT based on semantic understanding in terms of tense. We construct a tense parallel corpus test set consisting of 552 pairs of tense-rich, error-prone parallel utterances for NMT systems, and then propose a new task for evaluating the effectiveness of model translations from the perspective of tense consistency. This paper makes three contributions:\n(1) the presentation of the construction of the tense test set, including its tense labels; (2) the proposal of a feasible and reproducible benchmark for measuring the tense consistency performance of NMT systems; and (3) the various experiments for different baselines with the above test set and corresponding benchmark.\n\nAnnotation Rules and Tools\nAs the first work of the MT tense study, we choose English-French, one of the most classic language pairs of MT, to construct the dataset 3 . TENSE, the dominant topic of our research, is a combination of tense and aspect. In the modern grammar system of English, \"a tense system is a system associated with the verb where the basic contrasts in meaning have to do with the location in time of the situation, or the part of it under consideration\" (Huddleston et al., 2021) . The modern grammatical system divides tense into present and preterit based on the inflections added to the end of verbs, and the aspect into perfective and progressive on the state where an action is (Kamp, 1991) . While this tense classification system is too crude for daily life, we therefore apply the following classification methods. On the one hand, we classify the tenses according to the macro-temporal interval of the action into three major time intervals, namely present, past and future tenses; on the other hand, we classify the tenses according to the state of the action into general, progressive and perfect aspects. Hence, 9 kinds of tenses are born through combining the three tenses and the three aspects.\nFrench and English belong to the same Indo-European language family and share many similarities in various respects. The main difference is that in French there is another grammatical point called mode, part of which is like the aspect in English. In terms of tenses, we will generally discuss the tenses in the indicative mode of French and will describe the others later in this section. In the following, if there is no mode qualifier before a tense, it is by default in the indicative mode. Careful identification and comparison of the subdivided tenses in the three main tense intervals, English and French, reveals a very similar usage of the tenses, as sum-marised in Table 3 . As there is no progressive tense in French, we do not distinguish the progressive tense in English, but rather merge the progressive tense into its corresponding base tense, e.g. the present perfect progressive tense into the category of the present perfect tense.\nWhen discussing tenses from a semantic point of view, the modes also need to be taken into account. The grammatical correlations between French and English modes are quite complicated. Considering the corresponding grammatical expressions of 2 modes strongly related to tense, conditionnel and subjonctif, in French rely on the usage of modal verbs, we introduce modal verbs to simplify the distinguishment of the modes.\nBased on these grammatical rules, we merge the nine common tenses in English into seven categories that correspond reasonably and rigorously to French, namely the 6 tense categories of past/present/future + simple/perfect and statements containing modal verbs that correspond to the French subjonctif and conditionnel tenses. We construct an automatic annotation method based on the spaCy package (Honnibal et al., 2020) . First, we label the grammatical components of each word in the sentence based on the spaCy package, and then we define and compare the grammatical structures of the verb phrases with the structures of each tense classification to derive the sentence tense labels. During this process, to simplify the annotation process and better correspond with French futur proche tense, we classify the expression 'be going to do', grammatically in Future tense, into the Present tense, just like expressions 'be about to do' and 'be + verb progressive', whose stucture are in Present tense but the real meaning is about the close future. Also, a sentence may have several tense structures, in this case, the tense label consists several tenses. For example, the label of the sentence 'So it is in that spirit that we have made this change.' is 'Present+PrePerfect'.\n\nCorpus Design\nWe choose the tense-rich Europarl, namely Eu-roparlPV, processed by Lo\u00e1iciga et al. (2014) as the source corpus, for it contains all the sentences with predicate verb structures in the original Europarl dataset (Koehn, 2005) . First, we cleaned the source corpus, including deleting sentences without counterparts, English sentences in the French In the construction process, with the code mentioned in Section 2, we first automatically annotated the original English text and English prediction in the 20,000 pairs of parallel utterances, given the corresponding tense labels. Then, we filtered 6,779 parallel French-English sentence triples with different tense labels for English originals and predictions. On the basis of the automatic selection, we manually screened out the representative parallel French-English sentence pairs with a certain degree of translation difficulty and a complex grammatical structure. We also corrected the reference translations that did not justify the tense or semantics. It is worth noting that the author has a level of English and French that meets the C1 standard of The Common European Framework of Reference for Languages (CEFR), representing the ability to express herself effectively and flexibly in English and French in social, academic and work situations. A total of 570 parallel pairs of statements were selected at this stage.\nFollowing this, two other reviewers at CEFR C1 level, reviewed the tense test set for semantic and tense correspondence, and the tense labels marked by the automatic annotation code. \n\nCorpus Characteristics\nIn the following paragraphs, we describe the statistical features of our corpus and the elimination of gender coordination influence.\nTense distribution. The corpus consists of 780 tense structures in 552 sentences, and the distribution of tense classifications is shown in Table 4 . In the test set, sentences in present tense are the most, corresponding the situation of the reality: we use present tense most frequently and future perfect sense least frequently.\nElimination of gender effect. Unlike English, gender coordination exists in French. For example, the French sentences 'Nous nous sommes donc abstenus.' and 'Nous nous sommes donc abstenues.' both correspond to the English 'We therefore abstained.'. That is, the MT system's ability to learn gender coordination affects its ability to recognize tense structures, which in consequence affects the maintenance of tense consistency between original French text and predicted English sentence. Therefore, to better measure the tense-predicting capability of different MT systems, rather than their ability to recognize pronominal gender, we controlled for the gender variable by defaulting all pronouns, which do not indicate explicitly their genders, as masculine. These pronouns consists of 167 je (I), 114 nous (we, us) and 28 vous (you).\n\nExperimental Results\nTo measure the tense consistency performance of different systems, we introduce a benchmark called tense (prediction) accuracy, as shown in Eq. ( 1).\nEQUATION\nwhere N c is the number of predicted utterances with the same tense as its reference and N t is the total number of utterances in the tense set.\nTo verify the validity of our tense corpus, the following approach was adopted: To begin with, 100, 000 parallel utterance pairs from the Eu-roparlTR (containing 201, 374 pairs) mentioned in Section 3.1 were extracted as the tense-rich train set, and 100, 000 parallel utterance pairs from the Europarl corpus (Koehn, 2005) were extracted as the tense-poor train set. There were no overlapping utterances between the latter and the former. We performed the same preprocessing procedure, including data cleaning, tokenization and BPE coding. We then trained four pairs of French-English NMT systems with different architectures based on fairseq (Ott et al., 2019) , where two systems in each pair differed only in the train set. After this, we summarized the scores evaluated by Sacre-BLEU (Post, 2018) and COMET (Rei et al., 2020) and tense prediction accuracies of the eight systems on different test sets. We have applied three types of test sets: our tense set, the Europarl test set and the WMT15 test set. The Europarl test set contains 3,000 parallel utterance pairs drawn from the Europarl corpus, the exact same field of train set, while the WMT15 is a test set for the WMT15 (Bojar et al., 2015) , deriving from data in the different field of train set. Besides, we also apply our approach to mesure the tense consistency performance of several business translators, includ-ing Bing Translator, DeepL Translator and Google Translator. The results are listed in Table 5: 1) The BLEU and COMET scores based on the Europarl set and the WMT15 set are quite similar for each system pair, which indicates that the translation capabilities of the two systems are similar in the general evaluation dimension. This suggests that by relying solely on the difference in BLEU scores on traditional test sets, we are unable to measure the tense prediction ability of the systems.\n2) However, there are large differences in our tense set. The tense consistency performance of systems trained on the tense-rich train set was significantly better than that of systems trained on the tense-poor train set. This indicates that our tense set can capture the tense consistency performance.\n3) Further investigation of the BLEU or COMET) scores and tense prediction accuracy for each system reveals their positive correlation for the same architecture, but not across architectures. To measure the tense consistency performance across different architectures, we should focus more on tense accuracy rather than BLEU scores only.\n\nConclusion\nWe presented the French-English parallel tense test set and introduced the corresponding benchmark tense prediction accuracy, providing a brand-new approach to measure the tense consistency performance of machine translation systems. This test set firstly focuses on the tense prediction ability, posing a new dimension to improve the MT quality.\nIn the future, we will endeavour to generalize the test set to other languages. Considering there are statements like \"the use of tense A in language X is equivalent or similar to the use of tense B in English\" in grammar books of other languages (Durrell et al., 2015) , even across language families(Gadalla, 2017) and human translators also apply such rules(Santos, 2016), we are confident in taking this forward.\n"}
{"question": "What benchmark dataset did the authors use to evaluate their approach, and what metric did they use for evaluation?", "evidence": "  We run experiments on TRUE, a benchmark that compiles a wide variety of faithfulness tasks in a standardized format. We evaluate using Area under the ROC Curve (AUC). Table 1 shows the AUC scores for each metric.  ", "options": ["A. TRUE benchmark, evaluated using BLEU score.", "B. TRUE benchmark, evaluated using Area under the ROC Curve (AUC).", "C. CNN/DM dataset, evaluated using F1 score.", "D. Q2 dialogue corpus, evaluated using accuracy. "], "answer": "B", "content": "\nIntroduction\nConditional language models suffer from a tendency to hallucinate information (Maynez et al., 2020) , resulting in generations that are not faithful to their input documents, which limits the trustworthiness of such models. This raises a need for automatic faithfulness metrics. In this context, models trained on natural language inference (NLI) (Bowman et al., 2015) are attractive since, intuitively, a generation being faithful implies it must be entailed by the source (Falke et al., 2019) . However, pure NLI models have seen mixed success in faithfulness evaluation (Falke et al., 2019; Kryscinski et al., 2020; Wang et al., 2020; Maynez et al., 2020) . While in recent evaluation on the TRUE benchmark (Honovich et al., 2022) , which contains datasets from knowledge-grounded dialogue, summarization and paraphrasing, NLIderived metrics perform best overall, they require impractically large models, or costly additional machinery such as question generation and answering models at inference, while still showing robustness issues. Thus we ask: What is still needed for pure NLI models to perform robustly across faithfulness datasets -while remaining cheap enough to serve as a lean and practical evaluation tool?\nWe enhance a relatively small NLI model to make it work robustly across tasks in three ways:\nTask-Adaptive Data Augmentation. In NLI, a hypothesis must be fully entailed by its supporting premise. However, in faithfulness, not all parts of the generation always need to be grounded. We identify an instance of this phenomenon in dialogue where parts of a turn can fulfill communicative functions such as hedging or establishing emotional connection and are often disregarded in faithfulness annotation. Hence, when applying NLI models to complete dialogue turns that may include statements irrelevant for grounding, we run a risk of producing incorrect unfaithfulness predictions.\nTo alleviate this issue, we propose a simple data augmentation method to adapt NLI models to genres where they need to be aware of statements that must be exempt from NLI-based faithfulness evaluation. Our approach is computationally attractive, as it avoids an increase of cost at inference time.\nIntegration of NLI Contradiction Scores. Existing NLI faithfulness metrics typically use the entailment score for their predictions (Honovich et al., 2022; Falke et al., 2019; Kryscinski et al., 2020) . However, Chen and Eger (2022) show that subtracting the contradiction score from the entail-ment score (referred to as e-c ) can improve NLI performance in certain evaluation tasks. We show that there also is a strong positive effect of e-c for faithfulness prediction, and demonstrate that this is due to a high contradiction probability being a more reliable predictor of unfaithfulness than low entailment probability.\nMonte-Carlo Dropout Inference. Applying NLI models to faithfulness prediction involves a domain shift from largely human-written data to automatically generated text. To make NLI model scores more robust under this shift, we propose to use Monte-Carlo dropout during inference (Srivastava et al., 2014) . This essentially creates a cheap ensemble and has been shown to deal better with noisy labels (Goel and Chen, 2021) . This approach leads to consistent score improvements in our tasks.\nThe combination of all modifications not only strongly improves over a baseline NLI model, but also outperforms all other metrics on TRUE, on average, while being cheaper and smaller. 1 2 Method Details\n\nTask-adaptive Data Augmentation\nTo illustrate that task requirements can be incompatible between faithfulness and NLI, consider the following instance from the Q2 dialogue corpus (Honovich et al., 2021) that is labelled as faithful:\nGrounding: American pancakes are similar to Scotch pancakes or drop scones. Generation: yes , i love american pancakes , they are like scotch pancakes From an NLI perspective, the generation is clearly not entailed, since the statement \"I love american pancakes\" is not supported by the input.\nTo better prepare an NLI system for such genre or task-specific cases, we manually curate a small list of statements that should not influence the faithfulness prediction. We augment NLI data from the ANLI corpus (Nie et al., 2020) by adding a randomly chosen phrase from this set to each instance, while preserving the label. We then train an already fine-tuned NLI model on a concatenation of these augmented samples and original ANLI data. For training details see Appendix A.\n1 All code is available at https://github.com/julmaxi/ with_a_little_push\n\nMonte-Carlo Dropout\nTo compute scores under Monte-Carlo dropout, we randomly sample k dropout masks and compute the average of the model predictions. We set k = 15, since preliminary experiments showed that performance did not profit from additional samples.\n\nExperimental Setup\nWe run experiments on TRUE (Honovich et al., 2022) , a benchmark that compiles a wide variety of faithfulness tasks in a standardized format. It contains summarization (Pagnoni et al., 2021; Maynez et al., 2020; Wang et al., 2020; Fabbri et al., 2021) , knowledge-grounded dialog (Honovich et al., 2021; Gupta et al., 2022; Dziri et al., 2022) 2 and paraphrasing (Zhang et al., 2019) datasets. 3 Following recommendations in TRUE, we evaluate using Area under the ROC Curve (AUC).\nAs our BASE model, we use the DeBERTa-large (He et al., 2020) model of Laurer et al. (2022) , trained on MultiNLI (Williams et al., 2018) , Fever-NLI (Thorne et al., 2018) , ANLI (Nie et al., 2020) , LingNLI (Parrish et al., 2021) and WANLI (Liu et al., 2022) . The metric All uses all three of our proposed modifications to Base. We also investigate a variant without MC dropout inference (-MC) as a more cost efficient alternative.\nWe compare to the strongest models on TRUE: T5 ANLI (Honovich et al., 2022 ) is a T5-11B (Raffel et al., 2020) model trained on ANLI. 4 SummacZS (Laban et al., 2022 ) evaluates an NLI model on all pairs of input and generated sentences and then averages maximum entailment probabilities for each generated sentence.\nQ2 (Honovich et al., 2021) combines a question generation/answering pipeline with an NLI score.\nFinally, Honovich et al. (2022) introduce a strong ensemble of these 3 methods (Eorig). To further verify our approach, we construct a new ensemble (Eour) by replacing T5 with All.\n\nResults\nTable 1 shows the AUC scores for each metric. Base on six out of nine corpora, but also significantly outperforms all other competitors on average, while being more computationally efficient.\nAs expected, we find the biggest gains in dialogue, where the All model even outperforms Eorig on 2 out of 3 corpora. We do not improve on BEGIN, which is likely due to bias in the dataset construction, which we elaborate on in Section 5.1. On the summarization part, All improves significantly over Base on 3 out of 5 corpora, while not significantly harming performance on any corpus. However, it still falls short of the best models in TRUE. The strong showing of T5 on these corpora suggests that this might be alleviated with a stronger base model.\nOverall, a very similar behaviour is exhibited by -MC, presenting an attractive option when the added overhead of multiple samples is undesirable.\nEour is on par with Eorig, despite massively reduced costs; it even significantly outperforms it on two dialog and the paraphrasing corpora.\nWe also investigate the performance of each individual modification to our model (Table 2 ). They all improve average scores, while only leading to a notable decrease on BEGIN for both e-c and dialogue augmentations and on MNBM for e-c .\nOutside of dialogue, we find that the augmentation methods have a positive impact on PAWS, as well as all summarization corpora that are at least partially based on summaries for the CNN/DM dataset (Hermann et al., 2015) (Frank, QAGS-C, and SummEval). While we do not have a definitive explanation for this phenomenon, we hypothesize that on these datasets our augmentations aid in making the model robust in the presence of noise or irrelevant context since our augmentations are label-neutral and must similarly be 'ignored' during training.\n\nEffect of Dialogue Adaptation\nWe investigate whether the improvements via our augmentation approach are indeed due to them improving the handling of personal statements.\nWe use the occurrences of the pronoun I in a generation as a proxy measure 5 and compute its correlation with human labels and metrics (see Table 3 ). On both Q2 and Dialfact, our proxy measure, while uncorrelated with human labels, is strongly correlated with the scores of both Base and T5. This indicates these metrics indeed tend to incorrectly reject generations with personal statements. All on the other hand reduces this dependency.\nOur results also help explain why negatively correlated with first person pronouns. This is likely due to a bias in dataset construction:\nThe BEGIN dataset used in TRUE has generations from two models, one of which is both more likely to generate pronouns and more likely to generate unfaithful output (see Appendix B).\n\nEffect of integrating contradiction scores\nTo isolate the effect of e-c we compare score distributions of Base and Base+e-c in Figure 1 . The lefthand side of the figure shows that in Base ca. 2700 faithful instances are predicted as non-entailed (i.e., e-score near 0), which implies they are labelled as contradictory or neutral. e-c , on the other hand, further differentiates these instances into instances with high contradiction (negative e-c score) and high neutral probability (e-c score near 0). We observe that almost all low-scoring faithful generations are classified as neutral, whereas nearly all instances that are classified as contradictory are indeed unfaithful. Where Base has no way to make use of this information, e-c allows to reliably label contradictory instances as unfaithful.\n\nCost comparison to other approaches\nThere is increasing awareness of the resource-hungriness of deep learning (Strubell et al., 2019) . Especially for faithfulness, cheap and reliable metrics are critical, given rising demands for NLG in research and industry. Table 5 : Results of our phrase selection robustness analysis. For each run, we sample five phrases, recreated our dataset and retrain our model. We repeat this process ten times and report the average, as well as the standard deviation, minimum and maximum scores of the runs.\nSmall numbers indicate difference to the original scores. All results were computed using e-c and MC dropout.\nFor better comparison, we also report the scores of a model without any augmentation (i.e. without any additional training) with e-c and MC dropout.\nrequires fewer parameters than any other metric, including a more than 30x reduction compared to T5. During inference our model always requires a constant number of calls which can be reduced to a single call when ablating MC dropout. On the other hand, the number of calls in SummacZS scales with the number of input and output sentences. Q2 needs to generate questions by calling an auto-regressive QG model n times, where n factors in the amount and length of questions (#Q\u00d7Ql), answer #Q questions with the QA model and finally check #Q answers with an NLI model (#Q \u00d7 2).\nIn sum, our model compares favourably with other approaches, while also allowing for a performance/cost tradeoff by forgoing MC dropout.\n\nPhrase Selection Robustness\nTo ensure that our augmentation is robust and not overly reliant on any particular choice of phrases, we repeat our dataset augmentation process multiple times with five randomly chosen augmentation phrases out of the original ten. We sample ten such datasets and retrain our model for each. Table 5 shows the average score, minimum and maxi-mum score, as well as the standard deviation of the scores. We also report results of a model with both MC dropout and e-c but without any additional training and augmentations to directly quantify whether the augmentations are still helpful in their reduced form. This corresponds to applying MC dropout and e-c to Base.\nAs expected, we find that reducing the variety of available phrases leads to a drop in performance across almost all datasets, compared to All. The only exception is BEGIN, where we instead see a slight improvement. This is likely to be related to the construction of BEGIN (see the discussion in Section 5.1).\nWhen comparing our limited augmentation models to the non-augmented model, we find that they still outperform the non-augmented model in almost all cases. In particular for Q2 and DialFact, for which we expect the strongest impact of our augmentations, we find that even the worst run still outperforms non-augmented model. This suggests that our augmentations can robustly adapt the model to the dialogue task.\nFinally, we observe a relatively large drop in scores for all datasets that are at (least partially) derived from CNN/DM (Frank, SummEval and QAGS-C). This mirrors our earlier observation in Section 4 that these datasets profit from our augmentation procedure.\n\nRelated Work\nPrevious work on the utility of NLI for faithfulness led to mixed conclusions. In summarization, Falke et al. (2019) and Kryscinski et al. (2020) find out-of-the-box models have only limited utility in a faithfulness setting. In Wang et al. (2020) , an NLI model is outperformed by a question generation/answering (QA/QG)-based method. In contrast, Maynez et al. (2020) find that a similar NLI model vastly outperforms a QA/QG metric on their data. In knowledge-grounded dialogue, Dziri et al. (2022) , Gupta et al. (2022) and Honovich et al. (2021) find out-of-the-box models underperform.\nTo improve NLI models for faithfulness in summarization, Kryscinski et al. (2020) propose FactCC, which is trained on artificially noised summaries. Utama et al. (2022) propose a controllable generation model to generate artificial faithfulness data. In knowledge-grounded dialogue, Dziri et al. (2022) and Gupta et al. (2022) combine noising techniques to generate additional training data for NLI-based faithfulness models. In contrast to our work, these approaches a) generate training data from external sources, instead of directly augmenting NLI data, and b) do not explicitly focus on reconciling differences between NLI and faithfulness with their augmentation. Outside of augmentationbased approaches, Goyal and Durrett (2020) propose to train NLI models to label faithfulness at the dependency arc level.\n\nConclusion\nWe have demonstrated that with a small number of focused adaptations, even a relatively small NLI model can robustly predict faithfulness. We have:\n1. Shown that NLI-based metrics can be incompatible with task-specific requirements and identified and fixed one such incompatibility in dialogue with an augmentation strategy.\n2. Demonstrated the importance of contradiction probability for scoring and that the underlying mechanism is the high reliability of NLI contradiction scores for detecting unfaithfulness 3. Shown that using Monte-Carlo dropout improves metric performance.\nOur improved NLI model significantly improves over its baseline across many corpora and outperforms all competitors in average score on TRUE, while being much more efficient at inference. Our work suggests that strong improvements are possible for NLI-based faithfulness metrics, by combining data augmentation with adapted NLI score computation. We hope this finding will spurn advances in cheap and robust NLI for faithfulness. unclear whether our results would still provide advantages when applied to larger models such as T5-11B, whose parameter count makes experimentation infeasible on the hardware available to us.\n"}
{"question": "According to the passage, which of the following answers is true?", "evidence": "  our defense is designed for a black-box adversary, and cannot be tested against our white-box attack. We note that longer prompts did not add value in a defense setting. In Table 1, we display our results obtained using the default evaluation setting (prefix and suffix comprise of 50 tokens).  Our defense achieves lower extraction rates with competitive PPL values.  We utilize perplexity (PPL) on generated suffixes, to quantify the utility of the model in addition to using exact extraction rate as in Section 3.1.  ", "options": ["A. The defense can be used on the white-box attack.", "B. Longer prompts will increase their value in a defense setting.", "C. The defense mechanism achieves a substantial reduction in exact extraction rates.", "D. They use PPL on prefix to quantify the utility of the model."], "answer": "C", "content": "\nIntroduction\nPretrained large language models (LLMs; Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2020; Soltan et al., 2022) , commonly trained on massive crowd-sourced corpora, have been of much interest in the recent past due to their usage as backbones in state-of-the-art models across multiple downstream NLU tasks. However, they have been shown to memorize significant portions of their training data that can be extracted using appropriately-crafted prompts (Carlini et al., 2020 (Carlini et al., , 2022;; Zhang et al., 2021) . Such extractions pose a privacy risk to the contributors of the training data.\nIn this context, methods that allow developers to control the extractability of memorized examples from LLMs are of much value. For example, methods that increase extraction rates correspond to attacks in an adversarial setting, and provide developers with the ability to analyze privacy-risk.\nMethods that decrease extraction rates, referred to as defenses, are useful for protecting against such attacks. Historically, defense methods tend to be compute intensive (Abadi et al., 2016; Dupuy et al., 2021) .\nIn this work, we train continuous soft-prompts (Lester et al. 2021 ; hereafter referred to simply as prompts) and leverage them as a way of passing an external signal into an LLM, to control the extraction of memorized data. We freeze the model weights, and only use the trained prompt to control the generation. First, we train prompts in an attack setting and study the extent of extractable memorized content in our models. Second, we explore a defense setting where we create prompts that reduce extraction rates and achieve different privacy-utility trade-offs, via a user-specified hyperparameter. Since the original model weights are frozen in both these settings, our methods are compute efficient across the board.\nTo the best of our knowledge, our work is the first to adapt the use of instructive prompts for the analysis and mitigation of privacy in LLMs. We have released the code developed for our experiments 1 .\n\nBackground and Related Work\nPrevious work has shown that LLMs display memorization and has explored a range of methods that quantify extractability (Carlini et al., 2018 (Carlini et al., , 2020 (Carlini et al., , 2022)) . Differentially-private training (Dwork, 2006; Abadi et al., 2016) is a popular method that has been used to mitigate this risk. However, it tends to reduce model utility and requires retraining of the LLM, which might not be feasible due to heavy computational burden. The use of instructive prompts for language models has been extensively researched, including use during pretraining (Raffel et al., 2020) , as a second stage of training (Sanh et al., 2022; Wei et al., 2021) , and during inference to guide model output (Brown et al., 2020) . Within the third category, in order to improve upon manual prompt engineering researchers have implemented methods to learn discrete natural language prompts (Shin et al., 2020) , to mine them (Jiang et al., 2020) , or, neglecting natural language, to learn continuous prompts (Li and Liang, 2021; Lester et al., 2021) .\nOur work leverages continuous prompts as a way of passing an external signal to a model to trigger a desired model behavior (i.e., less or more memorized data in open language generation, which map to an extraction attack and defense, respectively).\n\nMethod\nPrompt-tuning requires the prepending of a prompt to the prefix embedding and access to the training loss (see Figure 1 ). Given these constraints, we explore a white-box attack where the adversary has access to the target model parameters, and a blackbox defense where the adversary interacts with the target model via an API. We therefore do not test our defense against our own attack.\nLet [prefix || suffix] be a sequence in the training set where the prefix is of length k tokens. Carlini et al. (2022) defined a suffix to be k-extractable if the model generates the suffix exactly, after being prompted with its the corresponding lengthk prefix. Our white-box attack aims to increase the number of k-extractable sequences, while our black-box defense aims to reduce the number of k-extractable sequences that can be extracted by an adversary who submits prefixes via an API.\n\nAttack\nIn the attack setting, we assume that the adversary has a set of [ prefix || suffix ] sequences S train , sampled from the training set of the target model. Their goal is to extract the suffixes corresponding to a disjoint set of prefixes, denoted by S test 2 . To do so, the adversary first initializes a prompt: a continuous set of l \u00d7 e parameters where e is the embedding size of the model, and l is the length of the prompt, a hyperparameter decided by the adversary. The prompt is trained over S train to facilitate the correct generation of suffixes. To do this, we first prepend the prompt to the embedding of the prefix and pass the joint embedding through the model for generation. We then minimize the loss objective (see below) with respect to the prompt while keeping the parameters of the model frozen.\nWe explore two loss objectives. The first is causal language modeling (hereafter referred to as CLM), where we minimize the cross-entropy loss over the entire sequence (Radford et al., 2019) . In the second, the prompt is optimized by minimizing the cross entropy loss of only the suffixes, given the prefixes. Here, the training is aligned with our inference task such that during training the model is penalized only on the suffix tokens; hence we refer to it as aligned CLM. During inference, the learned prompt is prepended to each embedding of the prefixes in S test , and the joint embedding is passed to the model for generation (see Figure 1 ).\n\nDefense\nIn the defense setting, the defender (API owner) trains the prompt, and prepends it to the incoming prefixes before passing them to the model. Our algorithm is inspired by machine-unlearning literature (Halimi et al., 2022) , and defenses against membership inference and backdoor attacks (Chen et al., 2022; Ozdayi et al., 2021) . We introduce a hyperparameter named learning threshold denoted by \u03b8. During prompt training (see Section 3.1), when loss is less than \u03b8 we do gradient ascent to penalize the prompt. If the loss is greater than \u03b8, we perform gradient descent with respect to the prompt as usual. Training is stopped once the average epoch loss is equal or above \u03b8. This allows us to increase training loss in a controlled manner and stabilize it around \u03b8. Through this process, we can achieve various privacy-utility trade-offs efficiently without re-training any part of the model. To explore \u03b8, we set the initial value to be slightly above the model training loss and increase in steps of 0.25 until desired performance is achieved.\n\nExperiments\nFor our experiments, we use the 125M and 1.3B parameter variants of the GPT-Neo models (Black et al., 2021) . These are public, decoder-only transformer models (Vaswani et al., 2017) trained using CLM on the Pile dataset (Gao et al., 2020) . We extract S train and S test from the Language Model Extraction Benchmark dataset (Google-Research). This dataset contains 15k sequences sampled from the training split of the Pile where each sequence is partitioned into a prefix and suffix. In the default evaluation setting, both prefix and suffix consist of 50 tokens. We ensure a random train/test split of 14k/1k samples.\nOur evaluation metric of choice is Exact extraction rate which is the fraction of correctly generated suffixes (i.e., all tokens of the generated suffix match with ground-truth suffix) over the test set. We additionally discuss fractional extraction rate and present results in Appendix A. As a baseline, we use the attack analyzed in Carlini et al. (2022) , which consists of feeding the prefixes to the model, and generating suffixes with greedy decoding. This is the only extraction attack for this setting apart from our work, to the best of our knowledge. Our training setup is discussed in Appendix B. All experiments are repeated over 5 runs with a new random train/test split in each run.\n\nAttack\nWe explore the performance of our attack across several dimensions: prompt length, suffix size, prefix size, and beam size. We use greedy-decoding in all cases, except the beam size experiments.\nPrompt Length First, we explore prompt length in the context of the default setting (prefix and suf-fix consist of 50 tokens; Figures 2-A1 and 2-A2 ). We note that prompts tuned with both CLM and aligned CLM provide improvements over the baseline in all cases, with aligned CLM providing the best performance. Given this, we train prompts using the aligned CLM objective for all other experiments, including our defense.\nWith aligned CLM, we achieve the highest extraction rates of 25.8% and 54.3% for the 125M and 1.3B models, respectively (an improvement of 8.9 and 9.3 percentage points, respectively), with a 100 token prompt (blue line). We observe that extraction rates increase with prompt length and tend to saturate after prompt length 100. Over-fitting was ruled out as a potential cause of saturation as there is no increase in test loss observed during training. This suggests that there is a max limit on the parameter count in the prompt that might add value for extraction purposes given our objective. We note that more sophisticated training strategies (designing better loss functions, better prompt initialization etc.) might yield better extraction rates.\nSuffix Size Next, we fix the prefix size to 50 and vary the suffix size. As shown in Figures 2-B1 and 2-B2, extraction rates decrease roughly exponentially with suffix size. We note that as suffix size increases, longer prompts (\u2265 20) provide greater improvements over the baseline. For example, with a prompt length of 100 (blue line) using the 1.3B model, at suffix size 5 we observe an extraction rate increase of 5.3 percentage points. Whereas at suffix size 50, the increase is 9.3 percentage points.\nPrefix Size Next, we fix the suffix size to 50 and vary the prefix size. As shown in Figures 2-C1 and 2-C2, extraction rates increase roughly logarithmically (as in Carlini et al. 2022) . Contrary to suffix size, we observe that the gaps between baseline and attacks decrease with increasing prefix size. This suggests that our attack stands to benefit a less informed adversary (small prefix sizes) when compared to the baseline.\nBeam Decoding Finally, we utilize the default setting with prefix and suffix sizes at 50 tokens and vary the beam size (beam size=1 corresponds to greedy decoding). The results are shown in Figures 2-D1 and 2-D2. We observe that extraction rates increase across the board when increasing beam size from 1 to 5. However, improvements tend to plateau or oscillate when beam size is greater than 5. The 1.3B model benefits more from increasing beam size achieving the highest extraction rate of 61.4%, at a beam size of 20 (with a prompt length of 150). The highest extraction rate achieved for the 125M model was 28.3% at a beam size of 15 (with a prompt length of 100).\n\nDefense\nFinally, we evaluate the privacy-utility trade-off of our black-box defense. As mentioned in Section 3, our defense is designed for a black-box adversary, and cannot be tested against our white-box attack.\nTherefore, we utilize the baseline attack (Section 4) to quantify privacy. We note that longer prompts did not add value in a defense setting, so we resort to using a prompt of length 1. We utilize perplexity (PPL) on generated suffixes, to quantify the utility of the model in addition to using exact extraction rate as in Section 3.1. To measure PPL, we use a random subset of 1k sequences sampled from the test split of the Pile, ensuring that PPL is measured on data unseen by the model. We also compare our metrics with those of similar sized models that were not trained on the Pile dataset (GPT2 models). Our premise here is that better performance in terms of privacy and utility, when compared to an out-ofdomain model of similar size, would mean that our defense mechanism is of value to an API owner.\nIn Table 1 , we display our results obtained using the default evaluation setting (prefix and suffix comprise of 50 tokens). Our defense achieves lower extraction rates with competitive PPL values. For the 125M model, we achieve an exact extraction rate reduction of 99.4% relative to baseline with a PPL increase of 25.3% at \u03b8 = 1.75. For the 1.3B model, the extraction rate is reduced by 97.7% relative to baseline with a PPL increase of 16.9% at \u03b8 = 1. The ability to achieve lower extraction rates with lower PPL values as measured against the GPT2 models of the corresponding size, provides evidence that our defense is effective.\n\nConclusion\nWe present the first known effort to leverage prompt-tuning to control the extractability of memorized data from LLMs in an open language generation task. We develop a novel data extraction attack and defense, and illustrate their performance under various settings. Our attack consistently outperforms the baseline in terms of exact extraction rate. Our defense provides competitive privacy-utility trade-offs and would prove beneficial to API owners with model trained on sensitive content. These results are achieved efficiently, without any change to the original model weights. We details avenues of future work in Appendix C\n"}
{"question": "What are the advantages of the proposed method in this paper?", "evidence": "Different from prior studies, we first employ only one-layer MLP architecture to mine latent semantic information between joint utterances for IUR task (MIUR). After that, we conduct a joint feature matrix to predict the token type and thus restore the incomplete utterance. The well-designed network and simple architecture make our method significantly superior to existing methods in terms of quality and inference speed.", "options": ["A. Simple architecture.", "B. Well-designed network", "C. Superior to existing methods in terms of quality and inference speed.", "D. All of the above.", "Different from prior studies, we first employ only one-layer MLP architecture to mine latent semantic information between joint utterances for IUR task (MIUR). After that, we conduct a joint feature matrix to predict the token type and thus restore the incomplete utterance. The well-designed network and simple architecture make our method significantly superior to existing methods in terms of quality and inference speed."], "answer": "D", "content": "\nIntroduction\nMulti-turn dialogue modeling is a research area focusing on developing systems that can engage in multiple conversation turns with humans. This type of modeling is often used in the field of humanmachine interaction to improve the ability of artificial intelligence systems to communicate with humans in a natural and intuitive way. One of the challenges of multi-turn dialogue modeling is to accurately understand and respond to the context and meaning of the conversation, as well as to handle incomplete or ambiguous utterances that may be used for brevity or to convey meaning. As shown in Table 1 , the incomplete utterance u 3 refers to the semantic of \"\u65b0\u51a0\u80ba\u708e\" (COVID-19) with \"\u90a3\" (that). The limited context provided by a single utterance, such as u 3 , can lead to referential ambiguity and semantic incompleteness in downstream applications like retrieval-based dialogue systems, as demonstrated in a study by Ni et al. (2022) . In addition, Su et al. (2019) has revealed that coreference and ellipsis are prevalent in more than 70% of utterances, particularly in pro-drop u 1 and u 2 denote the context utterances. u 3 is the incomplete utterance. u 3 is the rewritten utterance.\nlanguages like Chinese. These linguistic phenomena in conversation present a significant challenge for the development of practical conversational AI systems.\nTo address this issue, recent works (Kumar and Joshi, 2016; Su et al., 2019; Pan et al., 2019; Xu et al., 2020) proposed the Incomplete Utterance Rewriting (IUR) task, which aims to transform an incomplete or context-dependent statement into a self-contained, semantically equivalent one that can be understood without any additional context. As shown in Table 1 , IUR (u 3 \u2192 u 3 ) task makes the downstream dialogue modeling more precise.\nDespite previous works achieving promising results, the speed of autoregressive generation remains a limiting factor. To improve the speed, Huang et al. (2021) fuses the sequence labeling and non-autoregressive generation, which predicts missing elements in incomplete utterance and rewritten utterance. In addition, Liu et al. (2020) formulates IUR as semantic segmentation task based on U-Net (Ronneberger et al., 2015) and achieves better performance at a faster speed. However, above mentioned models are still not simple enough.\nIn this paper, we propose a simple yet efficient solution that our model first employs MLP architecture to simultaneously mine the semantic associations between the context utterances and the incomplete utterance, and capture attention information between them. After MLP architecture, we obtain the joint feature maps and further construct the token-pair edit matrix. Finally, the above matrix is edited according to prediction edit type tokens to generate the final rewritten utterance. Experiments show that our approach achieves better performance on several datasets across different domains and languages with low resource costs and a much faster inference speed.\n\nMethodology\nIn this section, we elaborate on our proposed approach. As shown in Figure 1 , our method mainly consists of two modules: MLP backbone network and joint feature matrix. For a multi-turn dialogue utterances (u 1 , u 2 , ..., u t ), we concatenate all the context utterances to produce an m-length word sequence c = (c 1 , c 2 , ..., c m ) and employ a special mask [SEP ] to separate different context utterances. Meanwhile, all the incomplete utterances are denoted as an n-length word sequence x = (x 1 , x 2 , ..., x n ).\n\nMLP Backbone Network\nWe first concatenate the context utterances and the incomplete utterances to construct a joint m + n length word sequence H = (c 1 , c 2 , ..., c m , x 1 , x 2 , ..., x n ). Besides, pretrained language models have been found to be highly effective in various natural language processing tasks. Hence, we employ BERT (Devlin et al., 2019) to initialize the word vector matrix H, where H \u2208 R (m+n)\u00d7768 . MLP backbone network contains two MLP blocks. Specifically, the first MLP block is responsible for mining the global semantic association information between context utterances c and incomplete utterance x. The second MLP block aims to learn the confidence level for each word embedding. This further enables the model to focus on important word information. It is important for the follow-up edit type classification, including substitute, insert and none. Each MLP block contains two fully-connected layers and a nonlinearity applied independently. For clarity and simplicity, we exclude the transposition process and the whole process can be represented as:\nEQUATION\nwhere i = 1, 2, .., 768, j = 1, 2, .., m + n and \u03c3 represents GELU (Hendrycks and Gimpel, 2016) .\nIn addition, MLP backbone contains other standard architectural components: skip-connections (He et al., 2016) and LayerNorm (LN ) (Ba et al., 2016) .\nIn contrast to the approach taken by Tolstikhin et al. ( 2021), who treated the word vector matrix H as an image and employed 1 \u00d7 1 convolution on non-overlapping image patches, we directly input the word vector matrix H into the MLP backbone network. Our operation avoids the loss of semantic spatial information resulting from 1\u00d71 convolution. Furthermore, since the number of words in each utterance varies, we utilize padding operation and copy mechanism (Gu et al., 2016; Zeng et al., 2018) to maintain a consistent sequence length. It is worth noting that our approach employs a one-layer MLP backbone network.\n\nJoint Feature Matrix\nFurthermore, to further capture the relevance between word embeddings, we employ three similarity functions: dot product similarity (dot Sim.), cosine similarity (cos Sim.), and linear similarity (linear Sim.). The word-to-word embeddings relevance between each context utterance's word embedding K cm and each incomplete utterance's word embedding K xn are captured using a 3dimensional joint feature matrix J(c m , x n ) represented as follows:\nEQUATION\nFinally, we employ BatchNorm (Ioffe and Szegedy, 2015) on joint feature matrix J(c m , x n ) to expedite and stabilize the training process. The batch is obtained by computing the mean and variance of the batch activation, which captures global information. After applying the BatchNorm operation, the matrix J(c m , x n ) is flattened, and each feature vector is mapped to one of three token types: Substitute, Insert, or None. This generates the token-pair edit matrix.\n\nSupervised Label\nPrior to training our model in the supervised fashion, we need to create word-level labels through the following process to construct our training set. Specifically, we first calculate the longest common subsequence (LCS) between the incomplete utterance and the rewritten utterance. Then, we align the incomplete utterance, the rewritten utterance, and the LCS using a greedy strategy. Finally, we identify the corresponding tokens in the rewritten utterance and mark them accordingly. Please refer to Algorithm 1 in Appendix A for a detailed description.\n\nExperimental Setup\nDatasets We conduct the experiments on three IUR benchmarks from different domains and languages, including RESTORATION-200K (Pan et al., 2019) , REWRITE (Su et al., 2019) and CANARD (Elgohary et al., 2019) . The statistics of the datasets are shown in Appendix B.\nBaselines We compare the performance of our method with the following baselines: (i) Generation models need to generate rewritten utterances from scratch, including Seq2Seq model L-Gen (Bahdanau et al., 2015) , the hybrid pointer generator network L-Ptr-Gen (See et al., 2017) , the basic transformer models T-Gen and T-Ptr-Gen (Vaswani et al., 2017) , Syntactic (Kumar and Joshi, 2016) , PAC (Pan et al., 2019) , L-Ptr-\u03bb and T-Ptr-\u03bb (Su et al., 2019) . The above models are limited by the speed of generation. (ii) Structure aware models contain RUN (Liu et al., 2020) and SARG (Huang et al., 2021) .\nFor more information about other experimental setups, please see Appendix B.\n\nMain Results\nTable 2 shows the experimental results on RESTORATION-200K. Our proposed approach, MIUR, achieves competitive results compared to all previous State-of-the-Art methods as shown in Table 2 . The results indicate MIUR can effectively mine the semantic information between utterances with two types of MLP architecture. Furthermore, we discovered that MIUR places more emphasis on rewriting precision (P n ) metrics. The first MLP architecture captures global semantic associations between context utterances and incomplete utterance, while the second MLP architecture focuses more on significant word embedding information. Our approach effectively combines two different MLPs and provides an effective guideline for the subsequent construction of the joint feature map matrix, leading our approach to concentrate more on essential word information and to pursue higher rewriting precision. Additionally, we achieve comparable Recall n results to the baselines. The experimental results of REWRITE and CANARD also come to the same conclusion, which can be found in Appendix C. \n\nModel\nP 1 R 1 F 1 P 2 R 2 F 2 P 3 R 3 F 3 B 1 B 2 R 1 R 2 Syntactic 67\n\nInference Speed\nTable 3 presents a comparison of the inferential speed of our model with the baselines. All models were implemented in PyTorch and run on a single NVIDIA V100. We can observe that the proposed MIUR achieves the fastest inference speed compared with the SOTA methods. Specifically, MIUR's speed is 3.14 times faster than that of L-Gen (n_Beam=1). Moreover, Compared with RUN in the second place, MIUR achieves 20% improvement in the inference speed. This enhanced performance can be attributed to the fact that our model employs only a one-layered MLP backbone to capture inter-utterances semantic information, without utilizing other modules. The simplified architecture, thus, contributes to the model's faster inference speed without compromising the performance. n_Beam stands for the beam size in beam search, not applicable for RUN and MIUR.\n\nAblation Study\nTo verify the effectiveness of MLP architecture in our model, we conduct a thorough ablation study in Table 4 As mentioned in Section 2.1, we perform an ablation study about using two different padding strategies to ensure consistent sequence length. Table 5 indicates that the model obtains a small performance improvement using copy mechanism, which further increases the semantic interaction between utterances. But this operation limits inference speed. Given a tiny improvement using copy mechanism, our model employs zero padding method. \n\nMore Discussion for MLP\nTo further investigate whether our proposed MLP backbone can effectively mine the semantic associations between utterances, we visualize the word embeddings composed of the context utterances and the incomplete utterance in Figure 2 . The yaxis represents our selection of 40 words consisting of the context utterances and the incomplete utterance. The x-axis represents the features of the first 100 dimensions of our intercepted word embeddings. It is not difficult to notice that word embeddings appear more distinctly characterized by vertical stripes after MLP backbone. Consequently, this further indicates that semantic information between words is more closely related, and our method can effectively learn the semantic relatedness between words after passing through the MLP network we designed. \n\nConclusion & Future Work\nIn this paper, we propose a simple yet effective IUR method. We utilize one-layer MLP structure to mine the inter-utterance semantic information from different perspectives. This improves the ability to predict the correct token between incomplete utterance and rewritten utterance. Benefiting from the fact that our model effectively employs MLP to IUR task, allowing our approach to achieve significant results in terms of performance and inference speed. This study represents the first preliminary exploration of the use of MLP on IUR task. In the future, we will investigate on extending our approach to other dialogue areas.\n"}
{"question": "Which is the most concrete description of our data selection process.", "evidence": "In order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.We call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.", "options": ["A. We meticulously choose a range of challenging and diverse examples that exhibit strong biases, and subsequently substitute gender-specific terms with neutral or equality-focused phrases.", "B. By utilizing a wordlist to identify gender-specific terms within sentences, we categorize the words into two groups: those that are names and those that are non-name words.", "C. We use the naive-masking  approach ", "D. We employ a list of word pairs where each pair consists of a gender-specific word and its corresponding gender-neutral word.", "Data Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\n"], "answer": "B", "content": "\nIntroduction\nRecently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019 ; * Equal Contribution Barocas et al., 2017; Kurita et al., 2019) . Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\nPrevious work (Nadeem et al., 2021) , (Nangia et al., 2020a) , (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021) , we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop bias mitigation strategies feasible.\n\u2022 We introduce a novel data sampling technique that utilises LLMs to mine for the most biased samples from a dataset and can benefit existing state-of-the-art debiasing methods. When used for debiasing a model, these few samples serve as exemplars and induce large reductions in gender bias.\n\nGender Bias in BERT Predictions\nMost-biased data samples ____ is very good at cooking but not great at work. \n\nReduced Gender Bias in BERT Predictions\nFigure 1 : Our method can be summarized as a combination of bias discovery and mitigation. First, we use a pre-trained LLM to find the most gender-biased samples. Then, we apply our data intervention techniques and use these modified training samples to fine-tune the model. Experiments show that our method is very effective at reducing gender bias, outperforming three state-of-the-art baselines and being comparable to two other baselines.\n\nRelated Work\nIn recent years, there has been growing concern about the bias/stereotypical discriminatory behavior by NLP models, particularly concerning gender. Several studies have investigated the presence of gender bias in various NLP tasks and proposed methods for mitigating it.\nOne line of research has focused on analyzing the extent of gender bias in pre-trained language models such as BERT and GPT-2. These studies have found that these models exhibit a significant amount of gender bias in their word embeddings for BERT (Jentzsch and Turan, 2022) and for GPT-2 (Kirk et al., 2021) and are prone to making stereotypical gender-based predictions (e.g., assuming that a doctor is male and a nurse is female). A standard evaluation metric used in this line of research is Stereotype metrics such as StereoSet (Nadeem et al., 2021) , which evaluates the model's ability to predict gender stereotypes and CrowS pairs (Nangia et al., 2020b) which measure whether a model generally prefers more stereotypical sentences. A similar line of work is gender bias tests proposed in BIG-bench (Srivastava et al., 2022) . The tests assess the language model's gender biases, stereotypes, and ability to infer gender information. It evaluates gender bias and stereotype between male and female, and gender minority bias and stereotype between majority and minority. It also examines the model's language modeling performance, which can be affected during de-biasing.\nAnother line of research has proposed methods for debiasing these models. These methods can be broadly categorized into two groups: data-based and algorithm-based. Data-based methods aim to reduce bias by removing or altering biased words from the training set. In contrast, algorithm-based methods aim to modify the model's architecture or training procedure to reduce bias. One popular databased method is \"uncertainty sampling\" (Lewis and Gale, 1994) , where the model is trained on the instances that it is most uncertain about, which can help to reduce bias by forcing the model to learn from a diverse set of examples. A popular algorithmbased method is \"Adversarial Debiasing\" proposed by Zhang et al. (2018) , which fine-tunes the model using an adversarial loss to make it less sensitive to sensitive attributes such as gender. OSCar proposed by Dev et al. (2021) , is another algorithm based method that utilizes the idea of disentangling \"problematic concepts\" like occupation and gender relationship instead of removing them altogether. MABEL (He et al., 2022) has both algorithm and data-based components, as it first augments the training data by swapping gender words and then applies a contrastive learning objective and alignment via entailment pairs. Their data augmentation strategy is similar in spirit to the data intervention techniques we propose, however our analysis does not require training auxiliary models and uses significantly lesser data.\nData-based methods include the \"Equalization\" technique proposed by Bolukbasi et al. (2016) , which aims to equalize the representation of genderspecific words in the embedding space, the \"Counterfactual Data Augmentation\" (CDA) method proposed by Zimmermann and Hoffmann (2022) Pre-trained LLMs are biased towards different genders, as seen in a simple mask-fill experiment using BERT. (Here, and in the rest of the paper, we assume a binary treatment of gender for simplicity.) The task is then to mask out the gender-related nouns and pronouns (such as he, she, her, woman, etc.) and get BERT to predict the masked words for the affected sequences in the dataset. Here, we consider a fixed list of gender-specific words curated from previous work (Lu et al., 2018; Zmigrod et al., 2019) and neutral words list 1 . We finally compute the \"total confidence difference\" as the sum of differences in the model's prediction confidence for each gender-word pair (such as confidence of predicting he \u2212 she, man \u2212 woman, etc.). Formally, we define total confidence difference as\nEQUATION\n))| where f (x) represent the confidence of model's prediction, N is the total number of tokens in the dataset and x is the tokenized gender word. The higher this number, the more biased the model is concluded to be. We compute the metric at token level and ensure that each of the gender word gets tokenized into exactly one token by initially extending the tokenizer with our gender word list. The top 3 biased gender-word pairs in StereoSet are shown in Table 1 . Intuitively, our technique for gauging bias in LLMs is sensitive to the fixed word list used to represent the sensitive attributes (here, gender). In Table 2 , we show the number of words covered by the word list used for both WikiText-2 and StereoSet datasets.\n\nData Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vocabulary of the model and try to balance the confidence of prediction on opposite-gender words by using phrases instead. Thus, we call our final approach random-phrase-masking as we instead substitute words with phrases that reflect the equality of gender. This approach not only reduces gender bias but also preserves the original meaning of the sentence in most cases. In our approach, we chose the phrases and order of gender words at random with equal probability. Additionally, we hypothesize that the choice of the dataset for fine-tuning is also essential. We choose two datasets: the WikiText-2 (Merity et al., 2017) dataset, which has implicit gender bias since its sources from Wikipedia articles, and the Stere-oSet dataset (Nadeem et al., 2021) , which has explicit/more gender bias as it has been designed to evaluate gender bias. WikiText-2 2 has 600 train articles and roughly 2M tokens while StereoSet 3 (dev) has 2123 samples out of which we only consider 800 samples which are not unrelated. Naturally, our data intervention method should work better on a dataset with training examples with gender bias while being devoid of meaningful gender associations like \"She needs a gynecologist,\" where the gender of the person is important. By testing our method on both datasets, we can understand the sensitivity of our approach to the quality of training samples used.\n\nBias Evaluation Metrics\nWe focus on evaluating the bias of a model while also measuring its language modeling capability. The ideal model would not just be one with the least bias but also one which does not compromise its language modeling performance. The dual estimation of bias and performance of a model was proposed in the StereoSet benchmark (Nadeem et al., 2021) , with the Language Modeling Score (LMS) measuring the percentage of times a meaningful token is predicted for the mask as opposed to a meaningless token, the Stereotype Score (SS) measuring the percentage of times the model predicted a stereotypical word as compared to an anti-stereotypical word, and an idealized CAT score (ICAT) combining the LMS and SS score into a single metric. An ideal model has an ICAT score of 100, while the worst biased model has an ICAT score of 0. We additionally evaluate the CrowS-Pairs benchmark (Nangia et al., 2020a) , which captures data with greater diversity in both the stereotypes expressed and the structure of sentences (50 is ideal). However, we note that the Crow-S benchmark is much more limited compared to StereoSet (Nadeem et al., 2021) in terms of both the volume and variety of linguistic phenomenon relating to gender bias it covers.\n\nExperiments\nWe compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work. For a fairer comparison, we run the baselines with the same size (100) of the training set as our method. For all of our experiments, we consider the \"bert-base-uncased\" pre-trained model available from HuggingFace. For fine-tuning our model, we select a varying number of most-biased training samples (10, 50, and 100) from the WikiText-2 and StereoSet (we only use the dev set) datasets, as discussed in section 4. We also compare this to a random selection of data points as an ablation study. On the selected dataset, we apply our interventions and obtain the modified dataset, which is then used to fine-tune our pre-trained model using masked language modeling (MLM) loss. The key point is that we only fine-tune the model on the gender words conditioned on the remaining text, significantly reducing the fine-tuning time. We perform ablations on various types of interventions as discussed in Table 7 . The model is trained for 30 epochs, with a learning rate of 0.001 and AdamW optimizer. We ran all of our experiments on NVIDIA Tesla T4 GPU on Google Colab for roughly 48 hours. For all experiments, we report the numbers as the mean and standard deviations (6) of 3 different runs. Our experiment code can be found here. 4 \n\nResults\nTable 4 shows the StereoSet and Crow-S scores for our baselines and our best-performing interventions on the WikiText-2 Dataset. In the StereoSet benchmark, we observe that random-phrase-masking obtains lower SS than all other baselines. On the Crow-S benchmark, random-phrase-masking does better than thre of the baselines except Sen-tenceDebias which achieves slightly better scores. While random-phrase-masking results in lower SS scores than neutral-masking, it also obtained [MASK] is very good at cooking but not great at [MASK] work. she is very good at cooking but not great at her work.\nhe is very good at cooking but not great at farm work. Being a mother is not easy since she will have to stay home and take care of the child.\nBeing a father is not easy since one will have to stay home and take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\n\nShe is very caring and kind but not good at what she does.\nHe is very caring and kind but not good at what he does.\n\nInput Sentence:\nOutput of Biased Model:\n\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to modify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the same pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the our method is successfully able to reduced biased substitutions. very low LMS scores. We attribute this performance degradation to the blunt substitution of phrases that our method uses, which might lead to odd-sounding sentences. In the Crow-S benchmarks, we see similar behavior and find that random-phrase-masking does better than neutral-masking. Since we believe that our method is sensitive to the choice of the dataset, we also present results on the StereoSet (dev) dataset 6. In Figure 2 , we perform a qualitative analysis of our proposed approach and find that random-phrase-masking is able to flip the predictions on fill-mask tasks for stereotypical sentences.\n\nConclusion\nIn this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs. We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples. Evaluation of our methods on state-of-the-art bias benchmarks empirically suggests that our methods effectively reduce gender bias. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n"}
{"question": "In the in-domain setting, what does \"attack success rate\" measure?", "evidence": "  In the in-domain setting (iD), the pre-trained transformer models are fine-tuned... the adversarial attack success rate (percentage of misclassified examples after an attack)... We see that label-smoothed models are more robust for every adversarial attack across different datasets in terms of the attack success rate...  ", "options": ["A. The success of the victim model in launching attacks", "B. The percentage of adversarial attacks that fool the victim model", "C. The number of attacks attempted on the victim model", "D. The time it takes for the attacks to succeed "], "answer": "B", "content": "\nIntroduction\nNeural networks are vulnerable to adversarial attacks: small perturbations to the input ,which do not fool humans (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2017) . In NLP tasks, previous studies (Alzantot et al., 2018; Jin et al., 2019; Li et al., 2020; Garg and Ramakrishnan, 2020) demonstrate that simple word-level text attacks (synonym substitution, word insertion/deletion) easily fool state-of-the-art models, including pre-trained transformers like BERT (Devlin et al., 2019; Wolf et al., 2020) . Further, it has recently been shown models are overconfident 1 on examples which are easy to attack (Qin et al., 2021) and indeed, such over-confident predictions plague much of modern deep learning (Kong et al., 2020; Guo et al., 2017; Nguyen et al., 2015; Rahimi et al., 2020) . Label smoothing is a regularization method that has been proven effective in a variety of applications, and modalities (Szegedy et al., 2016; Chorowski and Jaitly, 2017; Vaswani et al., 2017) . Importantly, it has been shown to reduce overconfident predictions and produce better confidence calibrated classifiers (Muller et al., 2019; Zhang et al., 2021; Dan and Roth, 2021; Desai and Durrett, 2020; Huang et al., 2021; Liu and JaJa, 2020) .\nIn this work, we focus on the question: does label smoothing also implicitly help in adversarial robustness? While there has been some investigation in this direction for adversarial attacks in computer vision, (Fu et al., 2020; Goibert and Dohmatob, 2019; Shafahi et al., 2019) , there is a gap in understanding of whether it helps with discrete, text adversarial attacks used against NLP systems. With the increasing need for robust NLP models in safety-critical applications and a lack of generic robustness strategies, 2 there is a need to understand inherent robustness properties of popular label smoothing strategies, and the interplay between confidence and robustness of a model.\nIn this paper, we extensively study standard label smoothing and its adversarial variant, covering robustness, prediction confidence, and domain transfer properties. We observe that label smoothing provides implicit robustness against adversarial examples. Particularly, we focus on pre-trained transformer models and test robustness under various kinds of black-box and white-box word-level adversarial attacks, in both in-domain and out-ofdomain scenarios. Our experiments show that label smoothing (1) improves robustness to text adversarial attacks (both black-box and white-box), and (2) mitigates over-confident errors on adversarial textual examples. Analysing the adversarial exam-ples along various quality dimensions reveals the remarkable efficacy of label smoothing as a simple add-on robustness and calibration tool.\n\nText Adversarial Attacks\nOur experiments evaluate the robustness of text classification models under three state-of-the-art text adversarial attacks TextFooler (black-box), BAE (black-box) and SemAttack (white-box), described below. 3 For a particular victim NLP model and a raw text input, the attack produces semantically-similar adversarial text as output. Importantly, only those examples are attacked, which are originally correctly predicted by the victim model. The attacks considered are word-level, i.e. they replace words in a clean text with their synonyms to maintain the meaning of the clean text, but change the prediction of the victim models.\n\u2022 TextFooler (TF): (Jin et al., 2019) proposes an attack which determines the word importance in a sentence, and then replaces the important words with qualified synonyms.\n\u2022 BAE: (Garg and Ramakrishnan, 2020) uses masked pre-trained language models to generate replacements for the important words until the victim model's prediction is incorrect.\n\u2022 SemAttack (SemAtt): (Wang et al., 2022) introduces an attack to search perturbations in the contextualized embedding space by formulating an optimization problem as in (Carlini and Wagner, 2016) . We specifically use the white-box word-level version of this attack.\n\nLabel Smoothing\nLabel Smoothing is a modified fine-tuning procedure to address overconfident predictions. It introduces uncertainty to smoothen the posterior distribution over the target labels. Label smoothing has been shown to implicitly calibrate neural networks on out-of-distribution data, where calibration measures how well the model confidences are aligned with the empirical likelihoods (Guo et al., 2017) .\n\u2022 Standard Label Smoothing (LS) (Szegedy et al., 2013; Muller et al., 2019 ) constructs 3 The black-box attacks keep querying the model with its attempts until the victim model is fooled while the white-box attack has access to the gradients to the model. Further details of the attacks are in (Jin et al., 2019; Garg and Ramakrishnan, 2020; Wang et al., 2022) . a new target vector (y LS i ) from the one-hot target vector (y i ), where y LS i = (1 \u2212 \u03b1)y i + \u03b1/K for a K class classification problem. \u03b1 is a hyperparameter selection and its range is from 0 to 1. ) with a probability of 1 \u2212 \u03b1 on the target label and \u03b1 on the label to which the classification model assigns the minimum softmax scores, thus introducing uncertainty.\nFor both LS and ALS, the cross entropy loss is subsequently minimized between the model predictions and the modified target vectors y LS i , y ALS i .\n\nExperiments\nIn this section, we present a thorough empirical evaluation on the effect of label smoothing on adversarial robustness for two pre-trained transformer models: BERT and its distilled variant, dBERT, which are the victim models. 4 We attack the victim models using TF, BAE, and SemAttack. For each attack, we present results on both the standard models and the label-smoothed models on various classification tasks: text classification and natural language inference. For each dataset we evaluate on a randomly sampled subset of the test set (1000 examples), as done in prior work (Li et al., 2021; Jin et al., 2019; Garg and Ramakrishnan, 2020) . We evaluate on the following tasks, and other details about the setting is in Appendix A.8:\n\u2022 Text Classification: We evaluate on movie review classification using Movie Review (MR) (Pang and Lee, 2005) and Stanford Sentiment Treebank (SST2) (Socher et al., 2013) against various attacks for in-domain data. We show clean accuracy, attack success rate and average confidence on successful adversarial texts. For each dataset, the left column are the results for standard model, and the right column are for LS models where \u03b1 denotes the label smoothing factor (\u03b1=0: no LS). \u2191 (\u2193) denotes higher (lower) is better respectively. dBERT denotes the distilBERT model.\non the matched genre test-set in the OOD setting presented in subsection 3.2 .\n\nIn-domain Setting\nIn the in-domain setting (iD), the pre-trained transformer models are fine-tuned on the train-set for each task and evaluated on the corresponding testset. For each case, we report the clean accuracy, the adversarial attack success rate (percentage of misclassified examples after an attack) and the average confidence on successfully attacked examples (on which the model makes a wrong prediction). 5 Table 1 shows the performance of BERT and dBERT, with and without label-smoothing. We choose label smoothing factor \u03b1 = 0.45 for standard labelsmoothed models in our experiments.\nWe see that label-smoothed models are more robust for every adversarial attack across different datasets in terms of the attack success rate, which is a standard metric in this area (Li et al., 2021; Lee et al., 2022) . Additionally, the higher confidence of the standard models on the successfully attacked examples indicates that label smoothing helps mitigate overconfident mistakes in the adversarial setting. Importantly, the clean accuracy remains almost unchanged in all the cases. Moreover, we observe that the models gain much more robustness from LS under white-box attack, compared 5 Details of each metric are presented in Appendix A.2.\nto the black-box setting. We perform hyperparameter sweeping for the label smoothing factor \u03b1 to investigate their impact to model accuracy and adversarial robustness. Figure 1 shows that the attack success rate gets lower as we increase the label smooth factor when fine-tuning the model while the test accuracy is comparable 6 . However, when the label smoothing factor is larger than 0.45, there is no further improvement on adversarial robustness in terms of attack success rate. Automatic search for an optimal label smoothing factor and its theoretical analysis is important future work. We also investigate the impact of adversarial label smoothing (ALS) and show that the adversarial label smoothed methods also improves model's robustness in Table 2 . \n\nOut-of-Domain setting\nWe now evaluate the benefits of label smoothing for robustness in the out-of-domain (OOD) setting, where the pre-trained model is fine-tuned on a particular dataset and is then evaluated directly on a different dataset, which has a matching label space. Three examples of these that we evaluate on are the Movie Reviews to SST-2 transfer, the SST-2 to Yelp transfer, and the SNLI to MNLI transfer.\nIn Table 3 helps produce more robust models in the OOD setting although with less gain compared to iD setting. This is a challenging setting, as evidenced by the significant performance drop in the clean accuracy as compared to the in-domain setting. We also see that the standard models make over-confident errors on successfully attacked adversarial examples, when compared to label-smoothed models.\n\nQualitative Results\nIn this section, we try to understand how the generated adversarial examples differ for label smoothed and standard models. First we look at some qualitative examples: in We also performed automatic evaluation of the quality of the adversarial examples for standard and label smoothed models, adopting standard metrics from previous studies (Jin et al., 2019; Li et al., 2021) . Ideally, we want the adversarial sentences to be free of grammar errors, fluent, and semantically similar to the clean text. This can be quantified using metrics such as grammar errors, perplexity, and similarity scores (compared to the clean text). Table 5 shows that the quality of generated adversarial examples on label smoothed models is worse than those on standard models for different metrics, suggesting that the adversarial sentences generated by standard models are easier to perceive. This further demonstrates that label smoothing makes it harder to find adversarial vulnerabilities.\n\nConclusion\nWe presented an extensive empirical study to investigate the effect of label smoothing techniques on adversarial robustness for various NLP tasks, for various victim models and adversarial attacks. Our results demonstrate that label smoothing imparts implicit robustness to models, even under domain shifts. This first work on the effects of LS for text adversarial attacks, complemented with prior work on LS and implicit calibration (Desai and Durrett, 2020; Dan and Roth, 2021) , is an important step towards developing robust, reliable models. In the future, it would be interesting to explore the combination of label smoothing with other regularization and adversarial training techniques to further enhance the adversarial robustness of NLP models.\n"}
{"question": "As for ablation study,which one is the worst model size on the GSM8K dataset?", "evidence": "  We investigate the performance gain achieved via finetuning student models of different sizes. Figure 3 shows the performance gain achieved when finetuning T5 of different sizes on the GSM8K dataset.\nOur results show that T5 base, with 44 times fewer parameters than T5 XXL, matches the performance of the baseline T5 XXL when trained on CoT data. Moreover, given an external calculator, even T5 small outperforms the baseline T5 XXL.\n ", "options": ["A. T5 base", "B. T5 XXL", "C. T5 small", "D. T5 44 "], "answer": "B", "content": "\nIntroduction\nChain of thought (CoT) prompting encourages language models (LMs) to break down a reasoning task into a series of intermediate steps (Wei et al., 2022) . They demonstrate that this prompting significantly increases the task accuracy of large language models (LLMs) across commonsense, symbolic and mathematical reasoning datasets. Here, LLMs are models with at least tens of billions of parameters, such as PaLM 540B (Chowdhery et al., 2022) , GPT-3 175B (Brown et al., 2020) , or UL2 20B (Tay et al., 2022) . However, the reasoning capabilities of smaller LMs do not improve with CoT prompting, mostly producing illogical CoT. Notably, CoT prompting even reduces the accuracy of models with less than 10 billion parameters. Wei et al. (2022) attribute this to abilities, such as semantic understanding and symbolic mapping, only emerging at larger scales. This leads us to our re- * Research conducted during an internship at Google. search question: can the reasoning capabilities of LLMs be transferred to smaller LMs via finetuning?\nThis work explores CoT knowledge distillation (Hinton et al., 2015) from PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) to different sizes of the smaller language model T5 (Raffel et al., 2020) , such as T5 XXL, XL and base, which have 11 billion, 3 billion and 220 million parameters, respectively. As a result of our work, we make two recommendations: (1) perform knowledge distillation by finetuning the student model on the CoT generated by a large teacher model; and (2) generate the CoT from an LLM, as proposed by Wei et al. (2022) , but crucially provide the solution to the task in the few-shot prompt. We demonstrate that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets irrespective of the teacher model used. For example, we show an accuracy increase from 8.11% to 21.99% and 18.42% on the GSM8K (Cobbe et al., 2021) dataset when finetuning T5 XXL on PaLM 540B and GPT-3 175B generated CoT data, respectively.\n\nRelated Work\nThis work is inspired by the seminal work of Wei et al. (2022) on CoT prompting. They demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021) . Wang et al. (2022) show that task accuracy can be further improved by using self-consistency in CoT prompting. Selfconsistency samples CoT reasoning paths from a model's decoder and returns the most consistent path by taking the majority vote. Subsequently, Chung et al. (2022) explore finetuning a FLANbased (Wei et al., 2021) version of PaLM on manually generated CoT data.\nConcurrent to our work, a small number of other works propose methods focused on CoT student-teacher knowledge distillation. Ho et al. (2022) and Li et al. (2022) also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. We also achieve a higher accuracy on common datasets, such as GSM8K, than Ho et al. (2022) . In contrast to our work, Shridhar et al. (2022) focus on training two models, one for problem decomposition and one for solving. Yet differently, the focus of Eisenstein et al. (2022) relies on producing markupand-mask explanations for open-book question answering. Lastly, Huang et al. (2022) present one related experiment, however, we present a more indepth exploration on more datasets. To the best of our knowledge, our work is the first to extensively explore the improvement of the reasoning ability of small LMs via knowledge distillation across multiple model architectures, and observing the effects of student model size and dataset size on accuracy.\n\nMethod\nWe propose a two-step pipeline for CoT knowledge distillation. The first step comprises annotating an existing supervised dataset with CoT reasoning generated by a teacher model. To generate high quality data, we propose using LLMs, such as PaLM 540B or GPT-3 175B, as teachers, based on the finding that CoT reasoning improves with model scale (Wei et al., 2022) . Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by Wei et al. (2022) . We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT. This is based on the observation that providing this guidance allows LLMs to correct small mistakes in the CoT. Lastly, we remove all incorrect CoT based on the target answer to prevent the student to learn from bad examples. The second step comprises finetuning a student model via teacher forcing (Williams and Zipser, 1989) . The student is provided with the question as input, and the CoT and answer as the target. As the model is trained on producing a CoT during finetuning, prompting is not required. \n\nExperimental Setup\nWe follow a similar experimental setup to Wei et al. (2022) , focusing on tasks covering arithmetic, commonsense and symbolic reasoning.\n\nArithmetic Reasoning\nWe benchmark the proposed method on the following math word problem datasets: (1) GSM8K (Cobbe et al., 2021) , ( 2) MAWPS (Koncel-Kedziorski et al., 2016) and (3) ASDiv (Miao et al., 2021) . We use the official training and testing split for GSM8K, taking the last 10% of the training split for validation, and the 5-fold cross validation splits available for MAWPS and ASDiv. We evaluate task accuracy by checking for the target answer as the final answer in the CoT. In addition, we compute the task accuracy given an external calculator, to account for arithmetic mistakes made by the model, despite the CoT being correct. The external calculator moves through the generated output, recalculating the left hand-side of equations. It then replaces the right-hand side with the calculated output, to avoid arithmetic mistakes being carried forward. For example, if a model outputted '5 + 5 = 11. 11 * 2 = 22', then the external calculator would first calculate '5+5' and replace the '11' with a '10'. In the subsequent equation, it would also replace the '11' with a '10' and arrive at the final result of '20'.\n\nCommonsense Reasoning\nWe benchmark the model's ability to perform commonsense reasoning on the StrategyQA dataset (Geva et al., 2021a) . As a testing split is not available, we do not shuffle the dataset to allow reproducing our split of taking the first 80% as training data, the following 10% as validation data, and the final 10% as testing data. We compute task accuracy in the same manner as previously mentioned.\n\nSymbolic Reasoning\nLastly, we benchmark the model on two synthetic tasks for symbolic reasoning: (1) last letter concatenation and (2) coinflip (Wei et al., 2022) . Last letter concatenation prompts the model to concatenate the last letter of each word in a string. Coinflip prompts the model to perform state tracking of the coin being flipped. We evaluate task accuracy in the same manner as before. Due to the rigid structure of the datasets, we focus on evaluating the model's generalizability to out-of-distribution (OOD) examples. We finetune the models on examples of length two and evaluate on sequences of length three and four. We initially infer the CoT using PaLM 540B, however, find that the LLM is able to perfectly replicate the desired CoT bar one example due to the rigidness of the template. We therefore decide to use the template generated CoT in our experiments.\n\nBaselines and setup\nWe select PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) as teacher models. We select PaLM 540B based on the stateof-the-art results on the benchmarking datasets reported by Wei et al. (2022) , and confirm the observed trends with GPT-3 175B. The publicly accessible teacher models are prompted as described in Section 3.\nWe select different sizes of T5 (Raffel et al., 2020) as student models, as T5 is publicly available in many sizes. The student models are trained on the PaLM 540B or GPT-3 175B generated CoT data as described in Section 3. We establish T5 XXL model finetuned on the original target as the baseline. We refrain from shuffling the datasets to allow for reproducibility.For the MAWPS and ASDiv dataset, we perform 5-fold cross validation. For all remaining datasets, we take 10% of the Input: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nOutput:\nRoger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. We refer the reader to Appendix A for an overview of the dataset licenses. We also refer the reader to Appendix B for an overview of the computatinal resources.\n\nArithmetic reasoning\nTable 1 details the task accuracy with and without an external calculator for the arithmetic reasoning benchmarks. Our results show that the proposed method improves task accuracy across all datasets. Most notably, the task accuracy of MAWPS is significantly improved. The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. 1 : Task accuracy across arithmetic reasoning datasets for T5 XXL without finetuning (baseline) and finetuned on PaLM 540B generated chain-of-thought (CoT). We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on generating chain-of-thought data\nWe perform an ablation study to confirm that providing a LLM with the target during CoT generation is beneficial. We found that for the GSM8K dataset, PaLM 540B only achieves a 59.98% accuracy if prompted without the target. In comparison, when including the target in the prompt the accuracy is 79.37%. A superficial explanation would be that when the model is conditioned on the expected answer, it produces the same CoT but copies the answer. However, an analysis of a subset of the differences between CoT produced with and without this conditioning shows that most of the benefits actually come from the model correcting CoT that had a single step missing or was wrong.\n\nCommonsense reasoning\nFor the StrategyQA dataset (Table 3 ), we found that using CoT finetuning improves accuracy from 68.12% to 71.98%, using only 1319 of the original 1648 examples. Compared to the arithmetic reasoning datasets, the improvement is not as significant. This can be explained by the model lacking factual knowledge that the dataset requires. The task is heavily focused on the model reasoning on such knowledge, however, a smaller LM is most likely not in possession of this knowledge compared to a larger model with higher memorisation capacity.\n\nSymbolic reasoning\nTable 2 shows the results obtained for the synthetic symbolic reasoning datasets, focusing on OOD generalization. Focusing on Last Letter Concatenation, it can be stated that both traditional finetuning and the suggested method fail at generalizing to a longer sequence length. In comparison, the proposed method significantly increases accuracy for the Coinflip dataset with regard to generalizing to three coinflips. In contrast, generalisation to four coinflips is slightly weaker than the baseline, which performs very strongly. This may be related to the task length being twice that of the training task.\n\nReplicating Results using different Teacher Models\nWe demonstrate the robustness of our method using a different teacher model, namely GPT-3 175B. Table 3 shows the results for GSM8K and Strat-egyQA when T5 XXL is finetuned on CoT data generated by GPT-3. The results show that the proposed method elicits improvements also with other Table 3 : Task accuracy for T5 XXL finetuned on chainof-thought (CoT) data generated by PaLM 540B and GPT-3 175B. We also finetune on the reasoning steps provided by the datasets. We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on model size\nWe investigate the performance gain achieved via finetuning student models of different sizes. Figure 3 shows the performance gain achieved when finetuning T5 of different sizes on the GSM8K dataset.\nOur results show that T5 base, with 44 times fewer parameters than T5 XXL, matches the performance of the baseline T5 XXL when trained on CoT data. Moreover, given an external calculator, even T5 small outperforms the baseline T5 XXL.\n\nAblation study on dataset size\nWe also investigate the trade-off between the performance gain from CoT finetuning and dataset size. \n\nDiscussion\nWe demonstrate that finetuning larger LMs on the CoT data generated by LLMs of over 100 billion parameters can significantly improve task accuracy. Even a small number of CoT examples appear to suffice for this. However, such improvements appear to be task dependent. For example, the effects are limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller LMs may not have memorised due to their limited capacity. Nevertheless, there is some performance improvement, which may be attributed to the model learning how to approach such tasks. Moreover, the CoT knowledge distillation pipeline presented allows to trade-off model and dataset size with accuracy. Future work could explore improving the reasoning of small models in multi-task settings, as well as the generation of new training data using LLMs, rather than annotating existing datasets.\n\nConclusion\nThis work explores CoT knowledge distillation from LLMs of over 100 billion parameters to smaller LMs. We propose a knowledge distillation pipeline consisting of two keys steps: (1) generate CoT for existing datasets using LLMs and\n(2) finetune smaller LMs on the CoT. Our results demonstrate that finetuning on CoT improves task accuracy across a range of benchmarking datasets.\n"}
{"question": "In which area has research on PEFT been neglected?", "evidence": "  PEFT has been investigated for tasks with inputs consisting of sentences, sentence-pair, or sequences that fit within the typical LLM maximum tokens. However, the performance of PEFT for tasks with longer textual sequences has been overlooked. In this work, we investigate this oversight and provide evidence suggesting that the gap between PEFT and regular fine-tuning is substantial when modelling long sequences.  ", "options": ["A. tasks with inputs consisting of sentences", "B. sentence-pair", "C.  tasks with longer textual sequences", "D. sequences that fit within the typical LLM maximum tokens"], "answer": "C", "content": "\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017) has changed the landscape of recent natural language processing approaches by enabling the pretraining of state-of-the-art large language models (LLM) (Devlin et al., 2019; He et al., 2020; Brown et al., 2020) . However, fine-tuning and storing full copies of LLMs can consume prohibitively large quantities of resources. Parameter-efficient finetuning (PEFT) methods such as prefix-tuning (Li and Liang, 2021; He et al., 2021a; Liu et al., 2022) address these concerns by reducing the number of trainable parameters. Prefix-tuning can tune 0.01% of parameters and still match the performance of regular fine-tuning (updating all model parameters). PEFT has been investigated for tasks with inputs consisting of sentences, sentence-pair, or sequences that fit within the typical LLM maximum tokens. However, the performance of PEFT for tasks with longer textual sequences has been overlooked. In this work, we investigate this oversight and provide evidence suggesting that the gap between PEFT and regular fine-tuning is substantial when modelling long sequences. As shown in Table 1, prefix-tuning underperforms fine-tuning on long sequence classification tasks, Hyperpartisan (Kiesel et al., 2019) and 20-newsgroups (Lang, 1995) , when used with the popular long-document model Longformer (Beltagy et al., 2020) .\nIn this paper, we propose a simple and effective method, prefix-propagation, which consistently improves the performance of PEFT for long sequence models. Unlike prefix-tuning, prefix-propagation propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer.\nTo further understand prefix propagation, we investigate the reliability of the model's predictions by performing analyses on calibration. Lastly, we conduct study on prefix-based methods in terms of kernel attention to strengthen their theoretical value.\nIn summary, our contributions are as follows:\n... Figure 1 : Illustration of the differences between (a) prefix-propagation (ours) (b) and prefix-tuning (Liu et al., 2022; Li and Liang, 2021) . Blue blocks denote trainable prompts, and \"Transformer Layer\" represents the computation done in a layer of the pre-trained LLM. Note that in prefix-propagation (a), the summation of prefixes continues for layers beyond 3, up to n. This operation is encapsulated by the ellipses. In prefix-tuning (b), prefixes in subsequent layers do not depend on hidden states from past layers (they are simply overwritten).\n.\n\u2022 We study PEFT for long documents and show that prefix-tuning is significantly inferior to fine-tuning in this scenario. To the best of our knowledge, this is the first work to focus on PEFT for long documents.\n\u2022 We introduce prefix-propagation, which consistently improves the performance over prefix turning on the different long document datasets, while using 50% fewer parameters.\n\u2022 We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated.\n\u2022 We elucidate the relationship between prefixpropagation and kernel attention and perform an ablation study that utilizes this insight.\n\nRelated Works\nLong Sequence Models Numerous methods have been proposed to reduce the complexity of attention from O(n 2 ) to O(n) such as kernel approximations (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021) and fixed (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) or learned (Kitaev et al., 2020) sparse attention patterns. For a broader summary, please refer to Tay et al. (2022) . In this work, we use Longformer (Beltagy et al., 2020) . To linearize attention complexity, Longformer employs sliding window attention while globally attending to relatively few special tokens.\nParameter-Efficient Tuning Inspired by the success of manual prompting (Brown et al., 2020), prefix-tuning (Li and Liang, 2021; Liu et al., 2022) prepends trainable \"soft\" prompts to an input sequence. Although further PEFT methods have since been introduced (He et al., 2021a; Hu et al., 2021; Ben Zaken et al., 2022) , we focus on adapting prefix-tuning. We note that our adaptation does not violate orthogonality and thus prefixpropagation can still be compounded with other PEFT methods as proposed in the UnifiedPET framework (He et al., 2021a) , likely yielding similar performance gains. We leave the empirical validation of this hypothesis for future work.\nOut work also adheres to the key motivation of the recent PEFT method, inducer-tuning (Chen et al., 2022) , which is that optimal prefixes should be close to queries within their latent space. We derive queries, keys, and values from the same prefix token, limiting the distance that separates them.\n\nMethodology\nIn this section we introduce prefix-propagation, which, unlike prefix-tuning, propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer. Prefix-propagation and its predecessor, prefix-tuning are depicted in Figure 1a embeddings) to the input sequence (blue blocks in top left of Figure 1a ). Then, before every subsequent layer, we sum new trainable matrices onto the first j embeddings corresponding to the prefixes (denoted by the sum operators in Figure 1a ). By propagating instead of overwriting, we halve the number of parameters trained while simultaneously improving performance on long-document tasks.\nWe now formalize prefix-propagation. Multiheaded attention processes query, key, and value matrices derived from a sequence C \u2208 R m\u00d7d with length m and embeddings of size d. Our method modifies traditional attention by concatenating a prefix P \u2208 R j\u00d7d of length j to the sequence:\nH l,i = Attn(D (l) W (l,i) q , D (l) W (l,i) k , D (l) W (l,i) v ) (1) D (l) = cat(P (l) , C) if l = 1 cat(P (l) + C[:j, :], C[j:, :]) if l > 1 where inputs C are projected through pre-trained weight matrices W (l,i) q , W (l,i) k , W (l,i) v\n\u2208 R d\u00d7d h per layer l and head i yielding the output of the attention head, H \u2208 R (j+m)\u00d7d h . The prefixes are concatenated for the first layer (l = 1) and summed to their corresponding hidden states for the remaining layers (l > 1). We do not continually concatenate new prefixes to the sequence to avoid increasing the sequence length after each layer.\nFor both prefix-tuning and prefix-propagation, prefixes (keys and values) are globally attended to by all queries. Unlike prefix-tuning however, our method concatenates additional hidden states before the hidden states C are projected by\nW (i) k and W (i)\nv . By doing so, prefix-propagation modifies query matrices, allowing prefixes to attend to other hidden states globally, thereby increasing representation capability. This approach is somewhat analogous to the external global tokens inserted in the BigBird-ETC model (Zaheer et al., 2020) . By attending to other tokens, the prefixes can act as special storage tokens, which is particularly useful in the restricted regime of long-document modelling where relatively few tokens have global context. Conversely, prefix-tuning only concatenates trained key and value matrices, P k , P v \u2208 R j\u00d7d h , statically to the sequence:\nH l,i = Attn(CW (l,i) q , cat(P (l,i) k , CW (l,i) k ), cat(P (l,i) v , CW (l,i) v ))\n(2)\nSince our method has a single prefix matrix, P instead of separate P k and P v matrices, we reduce the number of trained parameters by 50%.\n\nCalibration\nWe further study the proposed prefix-propagation method to understand the reliability of model's predictions through calibration. Well-calibrated models output confidence scores that closely match the models' accuracy. Either over-confident or underconfident models are undesirable. Calibration has widely been overlooked in PEFT methods. To quantify calibration in our work, we use expected calibration error (ECE), which bins predictions based on model confidence and compares them to accuracy (Pakdaman Naeini et al., 2015; Guo et al., 2017) .\n\nKernel Decomposition\nTraditional attention is analogous to applying a kernel smoother over inputs (Tsai et al., 2019) .\nMotivated by this insight, we reformulate prefixpropagation as a sum of kernelized attention modules. Separating the modules introduces flexibility in two ways: (1) Their individual kernel forms can be mixed and matched and (2) A hyperparameter scale factor \u03b1 can be applied to the prefix component to increase or decrease its weighting. Equation 3 defines kernel decomposition for prefixpropagation 2 :\nH = Kern(cat(P, C)W q , CW k , CW v ) + (\u03b1)Kern(cat(P, C)W q , P W k , P W v ) (3)\nwhere Kern refers to kernel attention as formulated in (Tsai et al., 2019) . The first term results from attending to the original sequence, C, and the second comes from attending to the prefixes, P . We provide the derivation of Equation 3 and the full definition of kernel attention in Appendix A.\nOur main motivation for presenting prefix decomposition is to establish foundational knowledge and guide future research. Ergo, we restrict experiments in this initial presentation to using just the default exponential kernel (Appendix A).\n\nExperiments and Results\nDatasets We evaluate our approach on three long-document classification tasks: ArXiv (He et al., 2019) , an 11-class classification task composed of academic research papers, the 20-newsgroups (Lang, 1995) classification task consisting of mailing lists that fall into one of 20 classes, and the Hyperpartisan dataset, a binary classification task for extremist news classification (Kiesel et al., 2019) . We also run experiments on WikiHop (Welbl et al., 2018) , a long-document reading comprehension task requiring multi-step reasoning.\nDue to compute limitations inherent to working with long documents, with the exception of Hyperpartisan, we only report a single run for each task. This mimics the original Longformer reporting scheme (Beltagy et al., 2020) . For Hyperpartisan, the smallest of the datasets, we report mean metrics averaged over five seeds.\nBaselines As a baseline, we fine-tune Longformer-base (approx.\n149M parameters) as closely as possible to Beltagy et al. (2020) . For PEFT, we evaluate prefix-tuning on Longformer-base and RoBERTa-base (approx. 125M parameters) (Liu et al., 2019) . 2 We omit layer, l and head, i for brevity.\n\nMethod\nArXiv HY. NG. More details on dataset sizes, pre-processing, and hyperparameters are in Appendix B.\n\nResults and Discussion\nAcross all tasks, our results in Table 2 verify that prefix-tuning is inferior to fine-tuning long sequences. Conversely, prefix-propagation consistently outperforms prefix-tuning and is comparable to fine-tuning on most tasks. Prefix propagation also performs competitively on Hyperpartisan, a relatively small dataset with only 625 samples. This is in contrast to prefix-tuning, which is known to underperform in low-data settings (Gu et al., 2022) . Because we ran multiple seeds on Hyperpartisan, we also found that prefix-propagation's better performance relative to prefix-tuning is statistically significant (p < 0.05, using a single-tailed t-test). We do not have multiple samples to run these tests for larger datasets, but we emphasize that Hyperpartisan likely has the most variance and yet it is still statistically significant. We suspect that prefixpropagation's performance exceeds prefix-tuning because propagated prefixes can transmit global context across multiple layers, possibly modelling more expressive abstractions.\nWe note one exception where prefix-based methods still leave room for improvement: multiplechoice question answering on WikiHop. We hypothesize that prefix methods have insufficient capacity to properly model complex long-document multi-step question answering.\nWe also observe that prefix-based methods, and especially prefix-propagation, achieve better calibration than fine-tuning, as shown in Table 3 . Unlike prefix-tuning however, prefix-propagation effectively balances calibration with accuracy metrics. The calibration of fine-tuning deteriorates as training progresses (Figure 4 \n\nMicro F1\nFigure 2 : Violin plot of Micro F1 Score for five different seeds on the Hyperpartisan task. White dots, gray boxes, and gray lines are the medians, interquartile ranges, and ranges respectively. Width of the five violin shapes show the probability densities for the corresponding F1score. All methods tune Longformer-base except \"R Prefix\", which is prefix-tuning on RoBERTa-base.\nforgetting (Jagielski et al., 2022) .\nAs an initial test for our ongoing prefixpropagation kernel study, we show results on Hyperpartisan in Figure 2 . The kernelized version of prefix-propagation achieves the best single-run performance, but has higher variance than fine-tuning and prefix-propagation which necessitates further research.\n\nConclusion\nOur research focuses on parameter efficient tuning for long documents tasks. We introduce prefix-propagation, which consistently improves performance over prefix-turning on long document datasets, while using 50% fewer parameters. We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated. We lastly explicate prefix-propagation from a kernel perspective, uncovering insights for future PEFT research.\n"}
{"question": "What are the sources of WikiTIG dataset construction\uff1f", "evidence": "  We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks.  ", "options": ["A. English Wikipedia articles", "B. Wikipedia table ", "C. Wikipedia image", "D. Chinese Wikipedia articles"], "answer": "A", "content": "\nIntroduction\nVision & Language (V&L), which is the fusion of vision and language tasks, has achieved great success in tasks such as caption generation from images (Xu et al., 2015) and image generation from texts (Reed et al., 2016) . This progress has been driven by pre-trained V&L models that are trained on large-scale V&L datasets (Du et al., 2022) . To generate appropriate captions and images for input, pre-trained V&L models need to have prior knowledge of the features of the objects they are generating (Cao et al., 2020; Yun et al., 2021) . These models retain knowledge about entities in particular by inheriting parameters from pre-trained language models used in natural language processing to indirectly utilize data resources such as Wikipedia.\nIn this way, V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang This learning process raises a number of questions, such as whether the knowledge about entities acquired from natural language is adequately retained in the pre-trained V&L model, or whether it is enhanced by combining it with image features. These are important in understanding the limits of what can be generated by the pre-trained V&L model.\nTo answer these questions, we propose a task of generating tables and images of infoboxes in English Wikipedia. Figure 1 shows an example of the target infobox, in which either tables or images are generated by the proposed task. In both cases, the model must know the entities to generate them properly.\nWe collected about 200,000 infoboxes to construct the Wikipedia posed task. In addition, we used OFA (Wang et al., 2022) , a pre-trained V&L model that has achieved state-of-the-art performance in various V&L tasks.\nOur evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is lost when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.\nIn image generation, we found that OFA can generate more accurate images by using the knowledge expressed in the table. We also found that the models trained only on natural language can infer table knowledge, which increases the diversity of generated images. Our code and dataset will be released at https://github.com/kamigaito/WikiTIG.\n\nVision & Language Models\nMany pre-trained V&L models have achieved stateof-the-art performance on various tasks by inheriting the weights of the conventional pre-trained models for natural language and images (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) before learning V&L datasets. Our study examines how the knowledge represented in the pre-trained model for natural language is transformed through such a learning process. We select OFA, which has achieved state-of-the-art performance in multiple V&L tasks, as our target model.\nFigure 2 shows the network structure of OFA and its relation to each dataset 2 . OFA uses VQGAN (Esser et al., 2020) on the decoder to transform images into discrete sequences so that the same Transformer (Vaswani et al., 2017) is used for image and natural language generation. Because OFA inherits 2 Appendix A describes the data for the pre-training.\n\nTask\nInput Output parameters from BART (Lewis et al., 2020) , which shares a similar Transformer structure, OFA should include knowledge acquired from natural language such as Wikipedia articles. Unlike the decoder, the encoder handles images directly; thus, OFA uses the output of ResNet (He et al., 2016) to embed images in addition to the embedding layer inherited from BART.\n\nTable and Image Generation\nIn this section, we describe two tasks for verifying knowledge behavior in the V&L model: table generation and image generation. Both tasks are based on infoboxes in Wikipedia articles, which correspond to summary information of the Wikipedia articles comprising tables and images 3 . Thus, it is suitable for verifying the knowledge about entities in Wikipedia kept in the pre-trained V&L model.\nIn the following subsections, we explain the details of each task.\n\nTable Generation\nIn the table generation task, the target V&L model generates a table from a title and/or image of the infobox. To do this, the model generates linearized tables, similarly to table generation by descriptions (Wu et al., 2022b) . In our setting, we linearize tables as shown in Figure 3 using the column separator \"|\" and the row separator \"<>\" to reuse pretrained token embeddings. The separator symbols are accompanied by spaces before and after for use in BPE tokenization. We investigate the target model by directly generating such linearized text. We use the following settings for the investigation.\nGeneration from titles We investigate the knowledge about entities held by V&L models by comparing tables generated from titles by pre-trained V&L models and by pre-trained models trained only on natural language.\nGeneration from title and images We generate tables from titles with images and compare the results with those generated from only titles. This enables us to investigate the new knowledge in pretrained V&L models transferred from images.\nMetrics For comparison, we use the following evaluation metrics to measure how close the generated tables are to the actual ones.\n-ROUGE: Since the linearized tables are text data and the infobox plays the role of summarizing the article, we use ROUGE (Lin, 2004) , the most widely used evaluation method for automatic summarization. In our evaluation with ROUGE, we convert the column separator \"|\" and the row separator \"<>\" to spaces so that the sequence of strings is not restricted to rows and columns.\n-Table - F 1 : To evaluate the tables with respect to their structure, we divide the cells by their types and then evaluate the matches with the reference table in terms of the F 1 measure for each case and average them. When calculating the matches, we apply clipping used in ROUGE to prevent the score from increasing due to the repetition of the same cell in the output 4 . We treat cells of each type separately 5 as follows:\n\u2022 Group: The infobox sometimes divides the table into groups, with the first row of each group serving as a header for the group name. The prediction performance for the group names is important for verifying what aspects of knowledge the model has about the entities. Since these rows consist of a single column, we target rows consisting of a single column in this type of cell.\n\u2022 Header: The head of each row in the table consisting of more than one column is usually the header of a subsequent cell in the same row. Therefore, the prediction performance for headers is important for the same reason as for group names. to the headers. Therefore, the prediction performance of the values is important for knowing whether the model has detailed knowledge about the entity. To examine the correspondence between headers and their values, we treat a header and its corresponding value as a pair.\n-Corpus-F 1 : Because the above \n\nImage Generation\nIn the image generation task, the model receives a title, caption, and table to generate the corresponding image:\nGeneration from a title and caption By using the minimum input required to generate images, we investigate the difficulty of generating them compared to other datasets.\nGeneration from a title, caption, and table We investigate the impact of knowledge about entities on image generation by generating images from input, including tables, and compare the results to the setting without tables.\nMetrics We use the following three widely used measures for evaluating image generation.\n-CLIP: The relevance of the input text to the generated images inferred by the pre-trained V&L model CLIP (Radford et al., 2021) .\n-Inception Score (IS): How easily a model can distinguish the differences between each image and the variety of generated images (Salimans et al., 2016) . It is inferred by the pre-trained image classification model Inception-v3 (Szegedy et al., 2016) .\n-Frechet Inception Distance (FID): How close the generated image is to the reference image, es- \nModel Input ROUGE \u2191 Table-F 1 \u2191 Corpus-F 1 \u2191 1 2 L\n\nDataset Creation\nWe created the Wikipedia Table and Image Generation (WikiTIG) dataset by extracting infoboxes from the HTML dump data of the English Wikipedia 8 . To ensure consistency in the format of infoboxes, we limited the extraction target to those containing a title in the first row and an image in the second row, as shown in Figure 1 .\nIn order to use only entities with sufficient information, we targeted entities for which the table was not empty. In addition, to ensure reliable correspondence, only rows one column wide, which often describe groups, and rows two columns wide, which often consist of a header and its value, were targeted for extraction.\nThe target images are limited to those in jpeg, png, and gif formats. Since some captions do not include a title, we used a hyphen to join the title at the beginning of the caption in such cases.\nTable 2 shows the size of each dataset. The dataset size diverges between two tasks because some infoboxes do not include captions 9 .\n\nTable Generation\nSettings We chose OFA (Wang et al., 2022) , a pre-trained V&L model, and BART (Lewis et al., 2020) , pre-trained only in natural language, as models for comparison. For both models, we used the base settings with the hyperparameters reported in Wang et al. (2022) . We performed the training three times with different seeds and reported their average scores with their standard deviations 10 .\nResults Table 3 shows the results for each setting in the table generation 11 . When only the title is used as input, the result of BART is more accurate than that of OFA, indicating that part of the knowledge acquired from natural language is lost due to additional learning in the V&L model. The use of image information improves Table-F 1 for headers, indicating that images reinforce the knowledge of what kind of features an entity has.\nIn contrast, F 1 for cell values did not improve, indicating that information obtained from images does not complement detailed knowledge, such as the values corresponding to each header obtained from natural language.\nThe results of BART in Corpus-F 1 also suggest that BART contains more diverse knowledge internally than in other settings. This result reinforces that the V&L model forgot part of the knowledge from natural language through additional learning, and images could not fully complement them.\n\nImage Generation\nSettings Similarly to the table generation, we chose OFA for the comparison. We additionally join the reference tables (Gold) and those generated by models in \u00a75.1 (OFA, BART) as the input in order to investigate the impact of the ability to infer table knowledge. We also used the base settings with the hyperparameters reported in Wang et al. (2022) . We also performed the training three times with different seeds and reported their average scores with their standard deviations 12 .\nResults Table 4 shows the results for each setting in the image generation 13 in OFA is close to the result (Wang et al., 2022) in MS COCO (Chen et al., 2015) for image generation, the use of our created dataset is reasonable for training models. In addition, the input of Table (Gold) improves all metrics, indicating that the model produces higher quality images when provided with complementary knowledge about the entities. This result also indicates that OFA does not retain sufficient knowledge of the entities in English Wikipedia.\nIn addition, we did not observe any performance improvement in CLIP and FID when fed with automatically generated tables from BART and OFA. However, tables generated by BART improves IS with the lower performance degradation of FID than that by OFA, indicating that automatically generated tables can improve the diversity of the output images and accurate tables are more important for improving performance in image generation.\n\nRelated Work\nFollowing the advancements in V&L models (Du et al., 2022) , there have been various studies that investigate V&L models. Cao et al. (2020) conducted a comprehensive analysis of V&L models including the difference between model structures. Through their analysis, they revealed the importance of text information in V&L tasks over image information.\nSeveral studies focused on the performance differences between V&L models and text-only models. Yun et al. (2021) investigated the improvement of linguistic representations by pre-training V&L models on PhysicalQA (PIQA) (Bisk et al., 2020) and the probing framework of (Tenney et al., 2019) . They concluded that the benefit of pretrained V&L models for text-only tasks is marginal. Iki and Aizawa (2021) ; Hagstr\u00f6m and Johansson (2022) compared the performance of V&L models and text-only models on the text-only benchmark, GLUE (Wang et al., 2018) and determined that the text-only model achieved higher scores than the V&L models.\nHowever, even though various kinds of V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) inherit language-related knowledge from pre-trained language-only models, how the knowledge is inherited has yet to be investigated. Our work clarifies this by using our created dataset, Wikipedia Table and Image Generation (WikiTIG).\n\nConclusion\nThis paper investigates how knowledge about entities are preserved in a pre-trained V&L model which is originally transferred from a pre-trained natural language model.\nWe analyzed a pre-trained V&L model by creating the Wikipedia Table and Image Generation (WikiTIG) dataset for generating images and tables of the infoboxes in Wikipedia. WikiTIG consists of 200,000 infoboxes and their corresponding images from English Wikipedia.\nExperimental results on a pre-trained V&L model OFA (Wang et al., 2022) showed that the model forgot part of the knowledge about entities during pre-training, and the image information did not fully compensate for the forgotten knowledge.\n"}
{"question": "What metric was used to measure lemmatization in the experiments conducted in the paper?", "evidence": "  The precision, recall, and F1 of the segmentation with various levels of POS tags were used as metrics.. lemmatization was evaluated via the tagging results of the full POS tag set ('all (levels 1-4)' in Tables 3 and 4), which included conjugation types and forms... ", "options": ["A. Precision", "B. Recall", "C. F1 Score", "D. Speed"], "answer": "C", "content": "\nIntroduction\nThe amount of text data being processed has greatly increased since the advent of communication platforms such as Twitter, Zoom, and Slack, and NLP services such as DeepL and Grammarly have millions of users. Some users analyze textual big data for marketing, linguistics, or sociology, while others deploy NLP services on their own devices because of privacy concerns. It is therefore becoming important to develop highly efficient methods to process massive text data and user queries with limited computational resources.\nHowever, the recent campaign for efficient NLP does not focus on literally efficient methods that scale to increasing data sizes and run on resourceconstrained devices. Instead, most \"efficient\" NLP studies (Treviso et al., 2022) focus on neural methods, which are too slow to handle billions of social media posts and too large to deploy on edge devices. Those studies seek to make model training or inference relatively efficient within the deep learning framework. Thus, the large efficiency gap with respect to classical methods has never been filled. !\"#$ %&'()*+,+*(-. %/00+#1 !!\"\"\" \"#!#$%& $!#$%' %!\" #&!!#$%& '! ()!\"\"\"\"\"\"\"\"\"\"\"\" \" ! \"# $ % #& ' ()\"\"\"\"\"\"\"\"\"\"\" \" $%& $%' ()*( $%& +,-. &*(/0 ()*( In this study, I take an orthogonal approach toward absolutely efficient NLP by seeking to boost the accuracy of the fastest methods. Specifically, I have developed a remarkably simple yet accurate method for Japanese morphological analysis, which is a joint task of word segmentation, part-of-speech (POS) tagging, and lemmatization. This method revisits the classical longest matching method; it greedily applies patterns that determine the next position to segment and then identifies the POS tag for the segmented word, as illustrated in Figure 1 . To obtain reliable patterns, starting from words in a morphological dictionary and training data, patterns are extended with posterior surface contexts and previous POS tags, and the patterns' segmentation offsets and tags are determined by frequency. The extracted patterns are then stored in an efficient double-array trie (Aoe, 1989) .\n!(&\"'(%0& !\"#$% ! &' &(% \"%)' *( %+# \"#$%#&' () ! !!\" #$%#&\nThe proposed method was evaluated on two standard corpora (Kurohashi and Nagao, 2003; Hangyo et al., 2012) . The experimental results confirmed that this simple method can process 1,000,000 sentences per second on an M2 MacBook Air, with comparable accuracy to learning-based baselines (Kudo et al., 2004; Neubig et al., 2011) . This section describes the method of Japanese morphological analysis used here, which performs word segmentation, POS tagging, and lemmatization. To maximize the tagging efficiency, I return to a pattern-based algorithm that is similar to the longest matching algorithm (Nagata, 1994) .\nThe longest matching algorithm performs deterministic word segmentation by using a dictionary. Starting from the beginning of the input, it greedily finds the longest dictionary words to segment the input. Although this simple algorithm exhibits moderate accuracy in Chinese and Japanese with transformation rules (Palmer, 1997; Hockenmaier and Brew, 1998; Sassano, 2014) , there is a gap in accuracy from search-and classification-based approaches (Kudo et al., 2004; Neubig et al., 2011) . To make search-based morphological analysis partially deterministic, Morita and Iwakura (2019) extracted surface patterns from tagging results; however, the speed-up factor was at most 1.5.\n\nBasic algorithm\nAlgorithm 1 is a simple, deterministic algorithm for joint word segmentation, POS tagging, and lemmatization. It repeatedly applies the longest-matching patterns in a trie P to a given sequence of characters, c, and a start position i to segment and tag the next word (w j = c i+ \u015dhift i and tj ). As will be shown later in \u00a7 3, this simple algorithm works as well as learning-based approaches.\nThis algorithm is inspired by the longest matching algorithm but differs in that the segmentation offset shift can be smaller than the surface length matched with patterns, k (see Line 7 in Algorithm 2). A running example is shown in Figure 1 .\nThe algorithm is also inspired by the precomputation of feature weights in sequence labeling (Kaji et al., 2010) and classification with conjunctive features (Yoshinaga and Kitsuregawa, 2009 , 2010 , 2014) weights in advance and retrieve those partial results by using simple keys such as word unigrams, POS bigrams, and primitive feature sequences to compute the final results (labels) by an argmax operation on the weights. The proposed method regards word segmentation and tagging as a joint, multiclass classification problem and directly obtains the label (i.e., where to segment and what to tag) by using the feature sequence as a pattern, thus skipping the expensive argmax operation over a number of labels. The longest matching thus implies classification with as many features as possible.\n\nPattern extraction from data\nFollowing the feature templates of learning-based methods (Kudo et al., 2004; Neubig et al., 2011) , the algorithm's pattern template was designed as a sequence of characters, c, followed by the previous word's POS tag t j\u22121 , thus giving c; t j\u22121 , where ';' represents string concatenation.\nAlgorithm 2 is the procedure to extract patterns for word segmentation and POS tagging from the annotated data and a dictionary. Given training data D with annotation of (word) segmentations and (POS) tags and a dictionary V compiling words and their possible tags, the algorithm iteratively extracts possible patterns from D. It first enumerates surface patterns c i+k i from all starting positions of words in D, and it then concatenates them with tag t j\u22121 for the preceding words to form pattern candidates (Lines 3-10 in Algorithm 2). Patterns are added for dictionary words that are unseen in the training data (Lines 11-12). The segmentation offset (shift) and tag t for a pattern are determined by the frequency (Lines 14-15). To avoid extra matching to the posterior contexts and previous tag, we only keep patterns whose segmentation offsets and tags differ from those of the longest prefix patterns that share prefixes of posterior contexts (Lines 16-18). This not only reduces the number and length of patterns but also minimizes the longest matching method's overhead for word segmentation. 1\n\nExperiments\nThis section describes an experimental evaluation of the pattern-based morphological analyzer on two annotated corpora in different domains (Kurohashi and Nagao, 2003; Hangyo et al., 2012) . The method was compared with two learning-based baselines (Kudo et al., 2004; Neubig et al., 2011) in terms of efficiency and accuracy. Note that all language resources and software used in the experiments are publicly available and free for academic use.\n\nSetup\nData The experiments used the Kyoto-University Text Corpus 2 (KYOTO) (Kurohashi and Nagao, 2003) , compiled from newspaper articles, and the Kyoto-University Web Document Leads Corpus 3 (KWDLC) (Hangyo et al., 2012) , compiled from the first three sentences of various Web pages. I adopted the split of development and test sets given in the corpora's github repositories and used the remaining portions as training sets. The datasets' statistics are listed in Table 1 .\nMethods The three methods below were compared. To prevent overfitting, the hyperparameter C in the underlying model was tuned for the two learning-based baseline methods 4 by using the development set to maximize the F 1 of the POS tags.\n1 In preliminary experiments, a variant of backtracking-free search (Maruyama, 1994) did not improve the throughput.\nVaporetto (ver. 0.6.2) is a Rust 6 implementation of a classification-based method (Neubig et al., 2011) . 7 It first performs word segmentation by classifying whether to segment after each character in the input, and it then identifies the resulting words' POS tags. It also trains classifiers for the possible POS tag sets of individual words, and it assigns the POSs of its first dictionary entries for words that are unseen in the training data. 8 A morphological dictionary was used to extract word features.\nJagger is a C++ implementation of the proposed algorithm. It greedily applies patterns extracted from the training data and a dictionary to jointly segment words and assign tags. Appendices A and B respectively describe the method to handle unknown words and the implementation details. Jagger is more similar to Vaporetto than to MeCab but differs in that it jointly performs segmentation and tagging instead of using a two-step cascaded pipeline, and it uses patterns instead of classifiers to find labels (i.e., where to segment and what to tag). Appendix C compares Jagger with the other implementations.\nDictionaries As listed in Table 2 , the experiments used two morphological dictionaries imported to MeCab from a manually tailored morphological analyzer, JUMAN. 9 Specifically, mecabjumandic-5.1-20070304 and mecab-jumandic-7.0-20130310 were compared to examine the impact of the dictionary's quality and size. The jumandic-5 https://taku910.github.io/mecab/ 6 Rust exhibits comparable efficiency to C++ on program benchmarks: https://github.com/kostya/benchmarks/.\n(2) minor POS (e.g., common noun); (3) conjugation type (e.g., ichidan verb); and (4) conjugation form (e.g., irrealis). For example, the POS tags of shumi and iru in Figure 1 are noun-common_noun-*-* and verb-*-ichidan_verb-terminal, respectively.\nEvaluation procedure The precision, recall, and F 1 of the segmentation with various levels of POS tags (Kudo et al., 2004) were used as metrics. As Vaporetto does not output lemmas, lemmatization was evaluated via the tagging results of the full POS tag set (\"all (levels 1-4)\" in Tables 3 and 4 ), which included conjugation types and forms, given that Japanese words can be mapped to their lemmas according to their conjugation types and forms. I processed 1000 copies of the test data and measured the time, speed, and maximum memory consumption three times with the /usr/bin/time -l command. The median values are reported here. All experiments were done on an M2 MacBook Air with a 3.5-GHz CPU and 24-GB main memory.\n\nResults\nTables 3 and 4 summarize the morphological analysis results on the KYOTO and KWDLC datasets.\nThe pattern-based method here, Jagger, was 16 and 7 times faster than MeCab and Vaporetto with 1/2 and 1/20 as much memory consumption, respectively, while achieving comparable accuracy.\nJagger is efficient because it does not have massive floating-point parameters, unlike other methods, and because it minimizes the number and length of patterns by pruning (Lines 16-18 in Algorithm 2). As a result, the training took less than six seconds. MeCab's accuracy depends on the dictionary: with jumandic-7.0, it worked best on KWDLC and worst on KYOTO. In contrast, Vaporetto's accuracy depends on the training data size. It worked best on KYOTO but was just as good as Jagger on KWDLC. Below are the detailed results for Jagger with the jumandic-7.0 dictionary. Comparison to neural methods Jagger was compared to a state-of-the-art neural method (Tolmachev et al., 2018) , JUMAN++-V2, 10 which was trained on the same data with the official script and hyperparameters. 11 Note that this comparison was unfair to Jagger in terms of accuracy and to JUMAN++-V2 in terms of efficiency, because JUMAN++-V2 uses 0.8 million additional dictionary entries from Wikipedia and a neural language model trained on 10 million sentences from the Web. Table 5 summarizes the comparison between Jagger and JUMAN++-V2. Although JUMAN++-V2 was reported to speed up JUMAN++ (Morita et al., 2015) by a factor of 250, Jagger was faster than JUMAN++-V2 by a factor of 180 with 1/7 as much of a memory footprint. JUMAN++-V2 was more accurate than Jagger, but the gain was less than 1% for word segmentation. If external text could be used, this gap could be reduced with a technique called structure compilation (Liang et al., 2008) , which runs JUMAN++-V2 on external text to extract patterns. That idea is beyond this paper's scope but important for future work.\nWord segmentation efficiency Because of different approaches to handling unknown words and supporting lemmatization, it is difficult to compare Vaporetto with Jagger and MeCab as a morphological analyzer in a strictly fair manner. Instead, the word segmentation efficiency was compared, as summarized in Table 6 . Here, Vaporetto was trained to perform only word segmentation by using the dictionary and the training data without POS tags. Jagger was faster and more space-efficient than Vaporetto, even taking the overhead of loading large models (1.7 seconds) into account. enjoys the benefits of the dictionary and training data: it can change its behavior by adding not only dictionary entries but also patterns.\n\nConclusions\nThis study sought to improve the accuracy of speedoriented, pattern-based methods for Japanese morphological analysis, rather than improving the speed of accuracy-oriented neural models. The proposed method extracts POS-augmented patterns from a morphological dictionary and annotated data. Experimental results on two standard datasets confirmed that this method achieves accuracy comparable to that of learning-based methods, with a very fast throughput of over 1,000,000 sentences per second on a laptop. I plan to apply this approach to other languages and even to other NLP tasks by discretizing the continuous representations induced by neural models to obtain patterns. The source code is released with GPL, LGPL, and 2-clause BSD licenses.\nMessage to researchers Because the accuracies on NLP benchmark datasets are becoming saturated with a larger foundation model, researchers may want to set diverse goals based on underrepresented metrics besides accuracy (e.g., efficiency). I hope that this study will initiate serious research on speed-intensive approaches to NLP that can meet industry demands and enable researchers with limited computational resources to exert their ability.\n"}
{"question": "What is the main advantage of using attention labels in dataset distillation, as mentioned in the paper?", "evidence": "  Inspired by previous work, we propose distilled attention labels, which are the supervision of attention probabilities optimized as a part of the distilled dataset, to enhance the effectiveness of the distilled dataset for training the transformer models. When applying the attention labels, the performance of the distilled dataset was significantly improved for all tasks, and their effect is much greater than the soft labels.  ", "options": ["A. Attention labels significantly reduce the size of the distilled dataset.", "B. Attention labels enhance the effectiveness of the distilled dataset for training transformer models.", "C. Attention labels make the loss function non-differentiable.", "D. Attention labels are only used for image datasets. "], "answer": "B", "content": "\nIntroduction\nDeep learning models have achieved state-ofthe-art performance in various fields, including computer vision and natural language processing (NLP), using large-scale neural networks trained with huge datasets. Unfortunately, their successful performances have come with massive training costs, including training time, GPU resources, and energy consumption. To reduce the training costs, current research has been focusing on constructing a small training dataset such that models trained with it can achieve comparable performances to models trained with the whole original dataset.\nOne classical way to compress the training dataset is data selection. Data selection methods choose a subset of effective training samples on the basis of a number of heuristic measures, for example, cluster centers (Sener and Savarese, 2018) , diversity (Aljundi et al., 2019) , and likelihood of models (Moore and Lewis, 2010) . Although the data selection methods effectively work for efficient model training and several applications, such as active learning (Sener and Savarese, 2018) and continual learning (Aljundi et al., 2019) , their performance is clearly restricted because they rely on the existence of representative samples that are effective for model training in the original dataset.\nAs an alternative approach for reducing the training dataset, Wang et al. (2018b) proposed dataset distillation, which aims to create a small number of synthetic samples optimized to effectively train models. Dataset distillation has attracted much attention in machine learning (Wang et al., 2018b; Zhao et al., 2021; Zhao and Bilen, 2021; Sucholutsky and Schonlau, 2021; Bohdal et al., 2020; Wang et al., 2022; Cazenavette et al., 2022) for both the theoretical interest and various applications, such as neural architecture/hyper-parameter search (Such et al., 2020) , continual learning (Masarczyk and Tautkute, 2020; Rosasco et al., 2022) , federated learning (Goetz and Tewari, 2020; Zhou et al., 2020) , and preserving data privacy (Li et al., 2020; Dong et al., 2022) .\nHowever, most of the existing research on dataset distillation mainly focuses on image datasets, and only a few studies involve NLP tasks. Sucholutsky and Schonlau (2021) and Li and Li (2021) extended dataset distillation to text datasets by using embedding vectors as an input of the distilled dataset instead of discrete text. While these studies applied dataset distillation to those model architectures based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs), we cannot find any research that tackles dataset distillation for pre-trained transformers, such as BERT (Devlin et al., 2019) , which have become the de-facto standard for various kinds of NLP tasks. Therefore, in this paper, we aim to obtain distilled few-shot datasets to fine-tune the pre-trained transformers for NLP tasks.\nTo this end, we focus on the attention mechanism, which is the core component of transformers (Vaswani et al., 2017) . Several current studies utilized supervision of the attention probabilities to effectively train the model (Liu et al., 2016; Mi et al., 2016) . Moreover, it is also used for the model distillation to efficiently transfer the knowledge of a transformer model to another one via attention probabilities (Aguilar et al., 2020; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2020 Wang et al., , 2021)) . Inspired by this, we propose distilled attention labels, which are the supervision of attention probabilities optimized as a part of the distilled dataset, to enhance the effectiveness of the distilled dataset for training the transformer models.\nIn our experiments, we constructed distilled fewshot datasets to fine-tune BERT (Devlin et al., 2019) in various types of NLP tasks: AGNews (text classification), SST-2 (sentiment analysis), QNLI (QA/NLI), and MRPC (paraphrase identification).\nOur main contributions are as follows: (i) To the best of our knowledge, this is the first work to explore dataset distillation for pre-trained transformers. Specifically, we demonstrate that our distilled datasets effectively fine-tune BERT even with only one sample for each class and only one gradient step. (ii) We present the distilled attention labels, which can easily be applied to dataset distillation for transformer architectures. Experimental results show that they consistently improved the performance with the distilled datasets in various types of NLP tasks. (iii) We open our source code and the distilled datasets obtained through our experiments to facilitate further research. 1 2 Methodology\n\nDataset Distillation\nIn this section, we explain the basic approach of dataset distillation (Wang et al., 2018b) , which aims to optimize a synthetic dataset through the gradient method similar to the current meta-learning approach (Finn et al., 2017) .\nLet the original training dataset D = {(x i , y i )} N i=1 , where (x i , y i ) is a pair of an input and its class label. Our goal is to optimize a distilled dataset D = {(x i , \u1ef9i )} M i=1 , which is randomly initialized at first, with M \u226a N .\nThe model parameters \u03b8 are updated with a minibatch of the distilled dataset (x t , \u1ef9t ) by gradient 1 https://github.com/arumaekawa/ dataset-distillation-with-attention-labels descent (GD) steps as follows:\nEQUATION\nwhere L() is a twice-differentiable loss function and \u03b7 is the learnable learning rate of the model, which is optimized together with D. Given initial model parameters \u03b8 0 , we can represent the model trained with the distilled dataset D, with the number of GD steps T , as\nEQUATION\nwhere F () is the training procedure of the T steps for the GD updating (Eq. 1).\nAs the goal of dataset distillation is that \u03b8 T performs well on the original dataset, the optimization objective of the distilled dataset D is calculated as follows:\nEQUATION\nwhere (x t , y t ) is a mini-batch of the original training dataset. Therefore, the optimization problem for dataset distillation is formulated as\nD * , \u03b7 * = arg min D,\u03b7 E \u03b8 0 \u223cp(\u03b8 0 ) L distill ( D, \u03b7; \u03b8 0 ) ,\n(5) where p(\u03b8 0 ) is the distribution of \u03b8 0 .\nWe optimize the distilled dataset D with this objective by using current gradient-based optimization techniques, e.g., Adam (Kingma and Ba, 2015) . However, the discrete nature of text data makes it difficult to apply the gradient methods directly. Inspired by previous work (Sucholutsky and Schonlau, 2021; Li and Li, 2021) , we use a sequence of embedding vectors for inputs of the distilled dataset instead of text as it is. Using the embeddings makes the loss L distill differentiable with respect to D, and we can thus optimize the distilled dataset D by the gradient methods.\n\nDistilled Soft Labels\nThe class labels of the original dataset are usually discrete hard labels (i.e., one-hot labels representing only a single class). Instead of hard labels, we can use soft labels for distilled datasets and optimize them with the input embeddings. Using soft labels enables the distilled datasets to contain more information. Following previous work (Sucholutsky and Schonlau, 2021; Bohdal et al., 2020) , we first initialize the soft labels with one-hot values and enable them to take any real values. We can now optimize the soft labels through the gradient method as well as the input embeddings.\n\nDistilled Attention Labels\nFor efficient knowledge transfer to transformer models via training with the distilled dataset, we propose attention labels, which are optimized to guide the multi-head attention module of the transformer models.\nInspired by previous work (Aguilar et al., 2020; Wang et al., 2020 Wang et al., , 2021)) , we compute the Kullback-Leibler (KL) divergence D KL between the selfattention probabilities of the model a(\u03b8) and the distilled attention labels \u00e3 across all layers and heads. The attention loss L attn is computed as follows:\nEQUATION\nwhere \u00e3k,h and a k,h (\u03b8) are the attention maps for the h-th head of the k-th layer of the distilled attention labels and the model, respectively, K is the number of layers, and H is the number of heads. Due to the data size, we consider the attention probabilities only for the first input token ([CLS]).\nWe train the model to minimize L task and L attn at the same time. Thus, the GD updating of the model (Eq. 1) is modified as\nEQUATION\nwhere \u03bb is the balance weight for L attn .\nThe attention labels \u00e3 are first initialized randomly and restricted to being a valid probability distribution (i.e., non-negative and the sum equals 1) by applying the softmax function to real-valued vectors. We optimize the attention labels together with the input embeddings and the soft labels by the gradient method. The details of the step-by-step procedure of our distillation algorithm are shown in Appendix A.\n\nSettings\nDatasets. We evaluated our dataset distillation methods in various types of NLP tasks. We used a text classification task (AGNews (Zhang et al., 2015) ) and three different natural language understanding tasks (SST-2, QNLI, and MRPC) from the GLUE benchmark (Wang et al., 2018a) . For the evaluation metrics, we used accuracy for AGNews.\nFor the other three tasks, we followed the evaluation settings of GLUE (Wang et al., 2018a) . The statistics of each benchmark dataset are summarized in Table 1 . Network Architecture. To evaluate the dataset distillation methods, we constructed distilled fewshot datasets to fine-tune BERT (Devlin et al., 2019) , which is the first pre-trained transformer model, that all subsequent models are based on. We utilized the pre-trained BERT BASE model. Following the fine-tuning procedure in Devlin et al.\n(2019), we introduced additional classification layer weights W \u2208 R C\u00d7D on the last hidden state of the [CLS] token, where D is the hidden dimension of BERT and C is the number of classes.\nImplementation. For all our distilled datasets, we used Adam optimizer (Kingma and Ba, 2015) with a learning rate \u03b1 \u2208 {1e \u22123 , 1e \u22122 , 1e \u22121 } and trained the distilled datasets for 30 epochs. We initialized the learnable learning rate \u03b7 \u2208 {1e \u22122 , 1e \u22121 }. For the attention labels, we set \u03bb = 1.0, which performed well in our preliminary experiments. We report the results for the best performing combination of \u03b1 and \u03b7. Note that due to the coarse granularity of the search, there is no need to care about overfitting to the test set. More details of our implementation are shown in Appendix B. Evaluation. To evaluate the distilled datasets, we fine-tuned the BERT model with them for 100 times, where the additional parameters W were randomly initialized each time. In all our experiments, we report the mean and standard deviation over the 100 evaluation results.\n\nResults for 1-shot and 1-step Setting\nWe first evaluated the dataset distillation methods with a 1-shot and 1-step setting, where the distilled dataset includes only one sample per class, and BERT was fine-tuned with it by only one GD step. We compared the performance for hard/soft labels and with/without attention labels for each task.\nTable 2 shows the evaluation results. The distilled datasets with the hard labels, i.e., only optimizing the input embeddings and not applying the attention labels, still achieved 87.4, 81.6, and 68.6 for AGNews, SST-2, and QNLI, respectively, which is 92.4, 88.0, and 74.7% performance of the full dataset. Furthermore, using the soft labels further improved these performances, especially by almost 8 points for QNLI. However, for MRPC, the distilled dataset achieved only the same performance as the majority class baseline regardless of the use of the soft labels.\nWhen applying the attention labels, the performance of the distilled dataset was significantly improved for all tasks, and their effect is much greater than the soft labels. Specifically, our distilled dataset with the attention labels yielded up to 98.5, 97.2, 94.1, and 88 .9% performance of the full dataset for AGNews, SST-2, QNLI, and MRPC, respectively. These results indicate that using the attention labels enables to extract the information from the original dataset as the attention probabilities and to efficiently transfer it to the model.\nWhen comparing the performance between the four tasks, dataset distillation performed very well on relatively simple classification tasks such as AGNews and SST-2, while the performance was somewhat limited on QNLI and MRPC, which require understanding the relationship between two sentences. In particular, for MRPC, although the performance was improved by applying the attention labels, the gap from the full dataset was still larger than that in the other three tasks. The class imbalance in the original training dataset (68% positive) may make the training of the distilled dataset more difficult. We can say there is still room for performance improvement by dealing with this issue (e.g., by upsampling or downsampling).\n\nResults for Multiple-shot and Multiple-step Setting\nWe also evaluated the distilled datasets with more than one shot and more than one GD step to finetune BERT. For the multiple-step setting, we considered two different scenarios: using the same distilled data in all steps and using different distilled data for each step. In these experiments, we evaluated the distilled datasets that use soft labels and attention labels for different numbers of GD steps T \u2208 {1, 3, 5}.\nTable 3 shows the results for the multiple-shot and multiple-step setting. In the single-step setting, overall performance improved with the number of shots of the distilled data. We believe that this is simply due to the expressiveness of the distilled data improved with the size of them. When using the same distilled data for all steps in the multiple-step setting, the performance of the distilled datasets degraded even compared with that in the single-step setting. In contrast, the performance was improved by separating the distilled data for each step and slightly but better than that with the same number of shots in the single-step setting. These results suggest that the role of the distilled data is different between the earlier and later steps, and it is difficult to obtain the distilled data that are generally useful for all GD steps.\nIn addition, the basic dataset distillation algorithm we used requires computing the back propagation through all GD steps for the optimization of the distilled dataset, which increases memory and computational costs linearly with T . Therefore, it was difficult to increase T to be larger than 5 in our experiments. This is the limitation of our dataset distillation method, and it needs further improvement to scale to more complex tasks or to train models from scratch.\n\nConclusion\nIn this paper, we explored dataset distillation in NLP tasks to fine-tune pre-trained transformers. We proposed attention labels, which are the supervision of attention probabilities distilled as a part of the distilled datasets. Experimental results across various tasks demonstrate that our distilled fewshot datasets achieved successful performances even with only one sample per class. Notably, the attention labels significantly improved the performance of the distilled datasets even for the tasks where dataset distillation is difficult without them.\n"}
{"question": "What are the main objectives of the BalSum model proposed in the paper?", "evidence": "  we propose a novel training method in which a re-ranker balances lexical and semantic quality. To mitigate this * Corresponding author problem, re-ranking systems have recently been introduced to generate a more appropriate summary. ", "options": ["A. Balancing lexical and semantic quality", "B. Maximizing ROUGE scores", "C. Focusing solely on lexical overlap", "D. Using multi-task learning for ranking "], "answer": "A", "content": "\nIntroduction\nThe performance of sequence-to-sequence (Seq2Seq) neural models for abstractive summarization (Lewis et al., 2020; Nallapati et al., 2016; See et al., 2017; Zhang et al., 2020) has improved significantly. The dominant training paradigm of Seq2Seq models is that of Maximum Likelihood Estimation (MLE), maximizing the likelihood of each output given the gold history of target sequences during training. However, since the models generate the sequence in an auto-regressive manner at inference, the errors made in the previous steps accumulate in the next step thereby affecting the entire sequence. This phenomenon is known as exposure bias (Bengio et al., 2015; Ranzato et al., 2016) . To mitigate this * Corresponding author problem, re-ranking systems (Liu et al., 2021; Liu and Liu, 2021; Liu et al., 2022; Ravaut et al., 2022) have recently been introduced to generate a more appropriate summary.\nThere are two training objectives for applying reranking to abstractive summarization: contrastive learning and multi-task learning. The contrastive learning-based approaches deploy margin-based losses. SimCLS (Liu and Liu, 2021) and BRIO-Ctr (Liu et al., 2022) train a large pre-trained model, such as RoBERTa (Liu et al., 2019) and BART (Lewis et al., 2020) , to align the candidate summaries according to the quality. The authors use the ROUGE (Lin, 2004) score as a quality measurement. The multi-task learning-based approaches combine at least two losses that perform different roles. SummaReranker (Ravaut et al., 2022) minimizes the average over the binary cross-entropy losses optimized for each evaluation metric. In addition, BRIO-Mul (Liu et al., 2022) demonstrates that the combination of the contrastive and cross-entropy loss works complementarily and has better performance.\nIn this paper, we analyze the three main drawbacks of existing re-ranking approaches. First, we argue that current methods focus excessively on ranking summaries in terms of lexical overlap. Inspired by Zhong et al. (2020) , we conduct a preliminary study, by sorting candidate summaries in descending order based on the ROUGE score and then defining z as the rank index of the highest BERTScore summary. As demonstrated in Fig. 1 , we can observe that there is a large gap between lexical overlap and semantic similarity. In a majority (52%) of cases z > 1. Second, despite more than half of the candidates with the same ROUGE score, previous studies do not accurately reflect quality measurements as they are trained with different ranks even if they have equal scores (Appendix F). Lastly, for the first time, we find summaries with high lexical overlap but low semantic similarity as false positives (Appendix G). They can be noises during training phrase, which are not considered substantially in the prior works.\nTo address these issues, we propose a novel training method in which a re-ranker balances lexical and semantic quality. Based on a two-stage framework, our model, named BalSum, is trained on multi-task learning. We directly reflect the ROUGE score difference on a ranking loss to preserve the lexical quality as much as possible. Then, we use a contrastive loss with instance weighting to identify summaries whose meanings are close to the document. Specifically, we define novel false positives (semantic mistakes) and present a strategy to reduce their influence in ranking. Experiments on CNN/DM and XSum datasets demonstrate the effectiveness of our method. Notably, BalSum achieves an 89.67 BERTScore on CNN/DM, reaching a new state-of-the-art performance.\n\nMethod\nOur method follows the two-stage framework. Given a source document D, a function g is to generate a pool of candidate summaries C = {C 1 , C 2 , ..., C m } at the first stage:\nEQUATION\nThen, a function f is to assign scores to each candidate and select the best summary C * with the highest score at the second stage: Our goal is to train the ranking model f that identifies the correct summary from the outputs of the generation model g.\nC * = argmax C i \u2208C {f (C i , D)} (2)\n\nModel Architecture\nWe start with a bi-encoder using RoBERTa-base (Liu et al., 2019) as a back-bone neural network.\nInspired by Khattab and Zaharia (2020) , we aim to capture rich semantic units at the sentence level.\nAs shown in Fig. 2 , we insert the [CLS] tokens in front of K sentences in the document D to let them encode into multi-vector representations. Then, we compute the individual score Score k which is modeled as an inner-product:\nEQUATION\nwhere E 1 (C i ) and E k (D)(k = 1, 2, ..., K) mean the representations of [CLS] tokens for candidate summary C i and document D, respectively. We calculate the similarity score f (C i , D):\nEQUATION\nIn Appendix E, we show that our model can capture more information from documents at the sentence level.\n\nTraining objective\nRanking Loss The core idea is that the higher the quality of the candidate summary, the closer to the document. We introduce a ranking loss to f (\u2022):\nL rank = i j>i max(0, f (Cj, D) \u2212 f (Ci, D) +(\u2212cost(Ci, S) + cost(Cj, S)) * \u03bb) (5)\nwhere S is the reference summary and \u03bb is the hyper-parameter. 1 Here, cost(C i , S) = 1 \u2212 M (C i , S) is the margin, and M is the automatic evaluation metric. We define it as ROUGE. We use the same metric in previous work (Liu and Liu, 2021; Liu et al., 2022) , but the difference is that our loss directly reflects the quality measure during training. In other words, the quality was not properly reflected before because different margin ((j \u2212 i) * \u03bb) was assigned even if the candidate summaries had the same ROUGE score.\nContrastive Loss with Instance Weighting The construction of positive and negative pairs is the critical point in constrative learning. Therefore, we consider generated summaries from the same document as positive samples and irrelevant summaries from other documents as negative samples. Thus, we design a set of candidate summaries C in Eq. 1 as positive and a set of randomly sampled summaries N as negative. 2 To identify summaries whose meanings are close to the document, we introduce a contrastive learning objective with instance weighting: D) (6) We newly define summaries that have a high lexical matching but a low semantic similarity as false positives. Inspired by Zhou et al. (2022) , we design an instance weighting method to reduce the influence of false positives. We produce the weights for positives using the SimCSE (Gao et al., 2021) which is the state-of-the-art model for the sentence representation task:\nL ctr = 1 |C| C i \u2208C \u2212log \u03b1 C i \u00d7 e f (C i ,D) e f (C i ,D) + s i \u2208N e f (s i ,\n\u03b1 C i = 0, sim(C i , S) < \u03d5 1, sim(C i , S) \u2265 \u03d5 (7)\nwhere \u03d5 is a hyper-parameter of the instance weighting threshold, and sim(\u2022) is the cosine similarity score evaluated by the SimCSE model. Finally, as shown in Fig. 3 , we combine the ranking (Eq. 5) and contrastive (Eq. 6) losses:\nEQUATION\n)\nwhere \u03b3 is the scale factor of each loss and we find the optimal values (\u03b3 1 = 10, \u03b3 2 = 0.1) in Appendix H.\n3 Experiments\n\nDatasets\nWe experiment on two datasets, whose statistics are shown in Appendix C. CNN/DailyMail (Hermann et al., 2015) is the most commonly used summarization dataset which contains articles from the CNN and DailyMail newspapers.\nXSum (Narayan et al., 2018 ) is a one-sentence summary dataset from the British Broadcasting Corporation (BBC) for the years 2010 -2017.\n\nTraining Details\nWe use diverse beam search (Vijayakumar et al., 2016) to generate 16 candidate summaries. We start from pre-trained checkpoints of RoBERTabase (Liu et al., 2019) . We train BalSum for five epochs. It takes 33 hours on CNN/DM and 22 hours on XSum on a single RTX 3090 GPU. More details are described in Appendix D.\n\nMain Results\nIn terms of the two-stage framework, we compare our results with SimCLS (Liu and Liu, 2021) , Sum-maReranker (Ravaut et al., 2022) , and BRIO (Liu et al., 2022) . We apply BalSum on top of each base model which is BART or PEGASUS.\nThe results on CNN/DM are described in Table 1. BalSum outperforms a base BART model, according to gains of 2.54/1.27/2.63 R-1/2/L. Notably, while it has comparable performances on ROUGE to previous models, it achieves an 89.67 BERTScore, reaching a new state-of-the-art performance. When ranking the candidate summaries, our model can estimate the meaning of summaries without seriously degrading the lexical aspect. We argue that this is because BalSum decreases more false positives than other ranking models. We provide fine-grained analyses for this result and present a case study in Sec.3.4.\nIn addition, we apply our method on XSum, as shown in Table 2 . Though we use a different strategy to generate the validation and test data 3 , our method improves a base PEGASUS with a small margin. We believe the one of reasons is that XSum is restricted to capturing diverse semantic units because it consists of much shorter summaries (onesentence) than CNN/DM. Model BS@1 BS@3 BS@5 R@1 R@3 R@5\nOracle \n\nAnalysis\nWeighting Threshold \u03d5 Intuitively, the larger the weighting threshold, the lower false positives. We train our model with different instance weighting thresholds from 0.7 to 0.9. In Table 3 , the highest threshold (\u03d5 = 0.9) shows the best performance and it rises largely to 0.3 BERTScore compared to when not applied. We also find that increasing the threshold leads to performance improvement. Therefore, we demonstrate that false positives can be considered noise in training.\nRanking Evaluation Regardless of the number of candidates, an ideal ranking model should yield oracle results considering diverse aspects of summarization. We conduct an experiment to measure the qualities by selecting the top-k summaries after aligning the candidates through different models. As shown in Table 4 , we can see that our model shows consistent performance in both evaluation metrics depending on the k (about \u00b10.06 BERTScore, \u00b10.34 ROUGE average score). Compared to SimCLS and BRIO-Ctr, the second block in Table 4 demonstrates that BalSum captures semantic similarity best while maintaining the intermediate level from the perspective of lexical overlap quality. Moreover, we find that BalSum has the lowest drop ratio of BERTScore (\u22121.52%) from the perfect ranking \"oracle\" scores. We also investigate whether all ranked summaries by models satisfy both lexical and semantic quality. We evaluate models using F 1 which measures the cases where the higher-ranked summary \n\nConclusion\nIn this work, we propose BalSum which aims to evaluate summaries by considering the balance between lexical and semantic quality. To achieve this, we perform a multi-task learning, which aligns summaries according to their lexical overlap qualities and identifies whether they are similar to the document. In addition, to our best knowledge, our method is the first attempt to present a new perspective of false positives (semantic mistakes) in ranking and creating the model to reduce their in-fluence. Our experimental results and fine-grained analyses validate that our model achieves consistent improvements over competitive baselines.\n"}
{"question": "When investigating linear classifiers, what are the differences between our investigation and recent related work by Wahba et al. ?", "evidence": "  Some recent works (e.g., Yu et al., 2022; Gomes et al., 2021) have shown the usefulness of linear classifiers in the deep-learning era. However, they either consider sophisticated applications or investigate advanced settings in which linear methods are only one component. In contrast, in this paper, we consider the basic scenario of text classification. A more related work (Wahba et al., 2023) has demonstrated the effectiveness of linear classifiers over PLMs on some problems. However, our investigation on linear methods is more comprehensive. ", "options": ["A. Our investigation considers the basic scenario of text classification rather than considering sophisticated applications. ", "B.  Our investigation on linear methods is more comprehensive.", "C.  Our investigation considers the basic scenario of text classification rather than investigating advanced settings in which linear methods are only one component.", "D. Our investigation demonstrated the effectiveness of linear classifiers over PLMs on some problems."], "answer": "B", "content": "\nIntroduction\nText classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems.\nWhile various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. "}
{"question": "What is the achievement of the research mentioned in the passage?", "evidence": "  We present the first known effort to leverage prompt-tuning to control the extractability of memorized data from LLMs in an open language generation task.  We develop a novel data extraction attack and defense, and illustrate their performance under various settings.  Our attack consistently outperforms the baseline in terms of exact extraction rate.  Our defense provides competitive privacy-utility trade-offs and would prove beneficial to API owners with model trained on sensitive content. These results are achieved efficiently, without any change to the original model weights. We details avenues of future work in Appendix C  ", "options": ["A. Achieving better exact extraction rates with a novel attack.", "B. Proposing attack and defense mechanisms.", "C. Proposing leverage prompt-tuning.", "D. Proving beneficial to API owners through changing the model weights."], "answer": "A", "content": "\nIntroduction\nPretrained large language models (LLMs; Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2020; Soltan et al., 2022) , commonly trained on massive crowd-sourced corpora, have been of much interest in the recent past due to their usage as backbones in state-of-the-art models across multiple downstream NLU tasks. However, they have been shown to memorize significant portions of their training data that can be extracted using appropriately-crafted prompts (Carlini et al., 2020 (Carlini et al., , 2022;; Zhang et al., 2021) . Such extractions pose a privacy risk to the contributors of the training data.\nIn this context, methods that allow developers to control the extractability of memorized examples from LLMs are of much value. For example, methods that increase extraction rates correspond to attacks in an adversarial setting, and provide developers with the ability to analyze privacy-risk.\nMethods that decrease extraction rates, referred to as defenses, are useful for protecting against such attacks. Historically, defense methods tend to be compute intensive (Abadi et al., 2016; Dupuy et al., 2021) .\nIn this work, we train continuous soft-prompts (Lester et al. 2021 ; hereafter referred to simply as prompts) and leverage them as a way of passing an external signal into an LLM, to control the extraction of memorized data. We freeze the model weights, and only use the trained prompt to control the generation. First, we train prompts in an attack setting and study the extent of extractable memorized content in our models. Second, we explore a defense setting where we create prompts that reduce extraction rates and achieve different privacy-utility trade-offs, via a user-specified hyperparameter. Since the original model weights are frozen in both these settings, our methods are compute efficient across the board.\nTo the best of our knowledge, our work is the first to adapt the use of instructive prompts for the analysis and mitigation of privacy in LLMs. We have released the code developed for our experiments 1 .\n\nBackground and Related Work\nPrevious work has shown that LLMs display memorization and has explored a range of methods that quantify extractability (Carlini et al., 2018 (Carlini et al., , 2020 (Carlini et al., , 2022)) . Differentially-private training (Dwork, 2006; Abadi et al., 2016) is a popular method that has been used to mitigate this risk. However, it tends to reduce model utility and requires retraining of the LLM, which might not be feasible due to heavy computational burden. The use of instructive prompts for language models has been extensively researched, including use during pretraining (Raffel et al., 2020) , as a second stage of training (Sanh et al., 2022; Wei et al., 2021) , and during inference to guide model output (Brown et al., 2020) . Within the third category, in order to improve upon manual prompt engineering researchers have implemented methods to learn discrete natural language prompts (Shin et al., 2020) , to mine them (Jiang et al., 2020) , or, neglecting natural language, to learn continuous prompts (Li and Liang, 2021; Lester et al., 2021) .\nOur work leverages continuous prompts as a way of passing an external signal to a model to trigger a desired model behavior (i.e., less or more memorized data in open language generation, which map to an extraction attack and defense, respectively).\n\nMethod\nPrompt-tuning requires the prepending of a prompt to the prefix embedding and access to the training loss (see Figure 1 ). Given these constraints, we explore a white-box attack where the adversary has access to the target model parameters, and a blackbox defense where the adversary interacts with the target model via an API. We therefore do not test our defense against our own attack.\nLet [prefix || suffix] be a sequence in the training set where the prefix is of length k tokens. Carlini et al. (2022) defined a suffix to be k-extractable if the model generates the suffix exactly, after being prompted with its the corresponding lengthk prefix. Our white-box attack aims to increase the number of k-extractable sequences, while our black-box defense aims to reduce the number of k-extractable sequences that can be extracted by an adversary who submits prefixes via an API.\n\nAttack\nIn the attack setting, we assume that the adversary has a set of [ prefix || suffix ] sequences S train , sampled from the training set of the target model. Their goal is to extract the suffixes corresponding to a disjoint set of prefixes, denoted by S test 2 . To do so, the adversary first initializes a prompt: a continuous set of l \u00d7 e parameters where e is the embedding size of the model, and l is the length of the prompt, a hyperparameter decided by the adversary. The prompt is trained over S train to facilitate the correct generation of suffixes. To do this, we first prepend the prompt to the embedding of the prefix and pass the joint embedding through the model for generation. We then minimize the loss objective (see below) with respect to the prompt while keeping the parameters of the model frozen.\nWe explore two loss objectives. The first is causal language modeling (hereafter referred to as CLM), where we minimize the cross-entropy loss over the entire sequence (Radford et al., 2019) . In the second, the prompt is optimized by minimizing the cross entropy loss of only the suffixes, given the prefixes. Here, the training is aligned with our inference task such that during training the model is penalized only on the suffix tokens; hence we refer to it as aligned CLM. During inference, the learned prompt is prepended to each embedding of the prefixes in S test , and the joint embedding is passed to the model for generation (see Figure 1 ).\n\nDefense\nIn the defense setting, the defender (API owner) trains the prompt, and prepends it to the incoming prefixes before passing them to the model. Our algorithm is inspired by machine-unlearning literature (Halimi et al., 2022) , and defenses against membership inference and backdoor attacks (Chen et al., 2022; Ozdayi et al., 2021) . We introduce a hyperparameter named learning threshold denoted by \u03b8. During prompt training (see Section 3.1), when loss is less than \u03b8 we do gradient ascent to penalize the prompt. If the loss is greater than \u03b8, we perform gradient descent with respect to the prompt as usual. Training is stopped once the average epoch loss is equal or above \u03b8. This allows us to increase training loss in a controlled manner and stabilize it around \u03b8. Through this process, we can achieve various privacy-utility trade-offs efficiently without re-training any part of the model. To explore \u03b8, we set the initial value to be slightly above the model training loss and increase in steps of 0.25 until desired performance is achieved.\n\nExperiments\nFor our experiments, we use the 125M and 1.3B parameter variants of the GPT-Neo models (Black et al., 2021) . These are public, decoder-only transformer models (Vaswani et al., 2017) trained using CLM on the Pile dataset (Gao et al., 2020) . We extract S train and S test from the Language Model Extraction Benchmark dataset (Google-Research). This dataset contains 15k sequences sampled from the training split of the Pile where each sequence is partitioned into a prefix and suffix. In the default evaluation setting, both prefix and suffix consist of 50 tokens. We ensure a random train/test split of 14k/1k samples.\nOur evaluation metric of choice is Exact extraction rate which is the fraction of correctly generated suffixes (i.e., all tokens of the generated suffix match with ground-truth suffix) over the test set. We additionally discuss fractional extraction rate and present results in Appendix A. As a baseline, we use the attack analyzed in Carlini et al. (2022) , which consists of feeding the prefixes to the model, and generating suffixes with greedy decoding. This is the only extraction attack for this setting apart from our work, to the best of our knowledge. Our training setup is discussed in Appendix B. All experiments are repeated over 5 runs with a new random train/test split in each run.\n\nAttack\nWe explore the performance of our attack across several dimensions: prompt length, suffix size, prefix size, and beam size. We use greedy-decoding in all cases, except the beam size experiments.\nPrompt Length First, we explore prompt length in the context of the default setting (prefix and suf-fix consist of 50 tokens; Figures 2-A1 and 2-A2 ). We note that prompts tuned with both CLM and aligned CLM provide improvements over the baseline in all cases, with aligned CLM providing the best performance. Given this, we train prompts using the aligned CLM objective for all other experiments, including our defense.\nWith aligned CLM, we achieve the highest extraction rates of 25.8% and 54.3% for the 125M and 1.3B models, respectively (an improvement of 8.9 and 9.3 percentage points, respectively), with a 100 token prompt (blue line). We observe that extraction rates increase with prompt length and tend to saturate after prompt length 100. Over-fitting was ruled out as a potential cause of saturation as there is no increase in test loss observed during training. This suggests that there is a max limit on the parameter count in the prompt that might add value for extraction purposes given our objective. We note that more sophisticated training strategies (designing better loss functions, better prompt initialization etc.) might yield better extraction rates.\nSuffix Size Next, we fix the prefix size to 50 and vary the suffix size. As shown in Figures 2-B1 and 2-B2, extraction rates decrease roughly exponentially with suffix size. We note that as suffix size increases, longer prompts (\u2265 20) provide greater improvements over the baseline. For example, with a prompt length of 100 (blue line) using the 1.3B model, at suffix size 5 we observe an extraction rate increase of 5.3 percentage points. Whereas at suffix size 50, the increase is 9.3 percentage points.\nPrefix Size Next, we fix the suffix size to 50 and vary the prefix size. As shown in Figures 2-C1 and 2-C2, extraction rates increase roughly logarithmically (as in Carlini et al. 2022) . Contrary to suffix size, we observe that the gaps between baseline and attacks decrease with increasing prefix size. This suggests that our attack stands to benefit a less informed adversary (small prefix sizes) when compared to the baseline.\nBeam Decoding Finally, we utilize the default setting with prefix and suffix sizes at 50 tokens and vary the beam size (beam size=1 corresponds to greedy decoding). The results are shown in Figures 2-D1 and 2-D2. We observe that extraction rates increase across the board when increasing beam size from 1 to 5. However, improvements tend to plateau or oscillate when beam size is greater than 5. The 1.3B model benefits more from increasing beam size achieving the highest extraction rate of 61.4%, at a beam size of 20 (with a prompt length of 150). The highest extraction rate achieved for the 125M model was 28.3% at a beam size of 15 (with a prompt length of 100).\n\nDefense\nFinally, we evaluate the privacy-utility trade-off of our black-box defense. As mentioned in Section 3, our defense is designed for a black-box adversary, and cannot be tested against our white-box attack.\nTherefore, we utilize the baseline attack (Section 4) to quantify privacy. We note that longer prompts did not add value in a defense setting, so we resort to using a prompt of length 1. We utilize perplexity (PPL) on generated suffixes, to quantify the utility of the model in addition to using exact extraction rate as in Section 3.1. To measure PPL, we use a random subset of 1k sequences sampled from the test split of the Pile, ensuring that PPL is measured on data unseen by the model. We also compare our metrics with those of similar sized models that were not trained on the Pile dataset (GPT2 models). Our premise here is that better performance in terms of privacy and utility, when compared to an out-ofdomain model of similar size, would mean that our defense mechanism is of value to an API owner.\nIn Table 1 , we display our results obtained using the default evaluation setting (prefix and suffix comprise of 50 tokens). Our defense achieves lower extraction rates with competitive PPL values. For the 125M model, we achieve an exact extraction rate reduction of 99.4% relative to baseline with a PPL increase of 25.3% at \u03b8 = 1.75. For the 1.3B model, the extraction rate is reduced by 97.7% relative to baseline with a PPL increase of 16.9% at \u03b8 = 1. The ability to achieve lower extraction rates with lower PPL values as measured against the GPT2 models of the corresponding size, provides evidence that our defense is effective.\n\nConclusion\nWe present the first known effort to leverage prompt-tuning to control the extractability of memorized data from LLMs in an open language generation task. We develop a novel data extraction attack and defense, and illustrate their performance under various settings. Our attack consistently outperforms the baseline in terms of exact extraction rate. Our defense provides competitive privacy-utility trade-offs and would prove beneficial to API owners with model trained on sensitive content. These results are achieved efficiently, without any change to the original model weights. We details avenues of future work in Appendix C\n"}
{"question": "What is the main problem addressed in the paper?", "evidence": "  To alleviate this problem, re-ranking systems have been applied in recent years. ", "options": ["An important problem of the sequence-tosequence neural models widely used in abstractive summarization is exposure bias."], "answer": "A", "content": "\nIntroduction\nThe performance of sequence-to-sequence (Seq2Seq) neural models for abstractive summarization (Lewis et al., 2020; Nallapati et al., 2016; See et al., 2017; Zhang et al., 2020) has improved significantly. The dominant training paradigm of Seq2Seq models is that of Maximum Likelihood Estimation (MLE), maximizing the likelihood of each output given the gold history of target sequences during training. However, since the models generate the sequence in an auto-regressive manner at inference, the errors made in the previous steps accumulate in the next step thereby affecting the entire sequence. This phenomenon is known as exposure bias (Bengio et al., 2015; Ranzato et al., 2016) . To mitigate this * Corresponding author problem, re-ranking systems (Liu et al., 2021; Liu and Liu, 2021; Liu et al., 2022; Ravaut et al., 2022) have recently been introduced to generate a more appropriate summary.\nThere are two training objectives for applying reranking to abstractive summarization: contrastive learning and multi-task learning. The contrastive learning-based approaches deploy margin-based losses. SimCLS (Liu and Liu, 2021) and BRIO-Ctr (Liu et al., 2022) train a large pre-trained model, such as RoBERTa (Liu et al., 2019) and BART (Lewis et al., 2020) , to align the candidate summaries according to the quality. The authors use the ROUGE (Lin, 2004) score as a quality measurement. The multi-task learning-based approaches combine at least two losses that perform different roles. SummaReranker (Ravaut et al., 2022) minimizes the average over the binary cross-entropy losses optimized for each evaluation metric. In addition, BRIO-Mul (Liu et al., 2022) demonstrates that the combination of the contrastive and cross-entropy loss works complementarily and has better performance.\nIn this paper, we analyze the three main drawbacks of existing re-ranking approaches. First, we argue that current methods focus excessively on ranking summaries in terms of lexical overlap. Inspired by Zhong et al. (2020) , we conduct a preliminary study, by sorting candidate summaries in descending order based on the ROUGE score and then defining z as the rank index of the highest BERTScore summary. As demonstrated in Fig. 1 , we can observe that there is a large gap between lexical overlap and semantic similarity. In a majority (52%) of cases z > 1. Second, despite more than half of the candidates with the same ROUGE score, previous studies do not accurately reflect quality measurements as they are trained with different ranks even if they have equal scores (Appendix F). Lastly, for the first time, we find summaries with high lexical overlap but low semantic similarity as false positives (Appendix G). They can be noises during training phrase, which are not considered substantially in the prior works.\nTo address these issues, we propose a novel training method in which a re-ranker balances lexical and semantic quality. Based on a two-stage framework, our model, named BalSum, is trained on multi-task learning. We directly reflect the ROUGE score difference on a ranking loss to preserve the lexical quality as much as possible. Then, we use a contrastive loss with instance weighting to identify summaries whose meanings are close to the document. Specifically, we define novel false positives (semantic mistakes) and present a strategy to reduce their influence in ranking. Experiments on CNN/DM and XSum datasets demonstrate the effectiveness of our method. Notably, BalSum achieves an 89.67 BERTScore on CNN/DM, reaching a new state-of-the-art performance.\n\nMethod\nOur method follows the two-stage framework. Given a source document D, a function g is to generate a pool of candidate summaries C = {C 1 , C 2 , ..., C m } at the first stage:\nEQUATION\nThen, a function f is to assign scores to each candidate and select the best summary C * with the highest score at the second stage: Our goal is to train the ranking model f that identifies the correct summary from the outputs of the generation model g.\nC * = argmax C i \u2208C {f (C i , D)} (2)\n\nModel Architecture\nWe start with a bi-encoder using RoBERTa-base (Liu et al., 2019) as a back-bone neural network.\nInspired by Khattab and Zaharia (2020) , we aim to capture rich semantic units at the sentence level.\nAs shown in Fig. 2 , we insert the [CLS] tokens in front of K sentences in the document D to let them encode into multi-vector representations. Then, we compute the individual score Score k which is modeled as an inner-product:\nEQUATION\nwhere E 1 (C i ) and E k (D)(k = 1, 2, ..., K) mean the representations of [CLS] tokens for candidate summary C i and document D, respectively. We calculate the similarity score f (C i , D):\nEQUATION\nIn Appendix E, we show that our model can capture more information from documents at the sentence level.\n\nTraining objective\nRanking Loss The core idea is that the higher the quality of the candidate summary, the closer to the document. We introduce a ranking loss to f (\u2022):\nL rank = i j>i max(0, f (Cj, D) \u2212 f (Ci, D) +(\u2212cost(Ci, S) + cost(Cj, S)) * \u03bb) (5)\nwhere S is the reference summary and \u03bb is the hyper-parameter. 1 Here, cost(C i , S) = 1 \u2212 M (C i , S) is the margin, and M is the automatic evaluation metric. We define it as ROUGE. We use the same metric in previous work (Liu and Liu, 2021; Liu et al., 2022) , but the difference is that our loss directly reflects the quality measure during training. In other words, the quality was not properly reflected before because different margin ((j \u2212 i) * \u03bb) was assigned even if the candidate summaries had the same ROUGE score.\nContrastive Loss with Instance Weighting The construction of positive and negative pairs is the critical point in constrative learning. Therefore, we consider generated summaries from the same document as positive samples and irrelevant summaries from other documents as negative samples. Thus, we design a set of candidate summaries C in Eq. 1 as positive and a set of randomly sampled summaries N as negative. 2 To identify summaries whose meanings are close to the document, we introduce a contrastive learning objective with instance weighting: D) (6) We newly define summaries that have a high lexical matching but a low semantic similarity as false positives. Inspired by Zhou et al. (2022) , we design an instance weighting method to reduce the influence of false positives. We produce the weights for positives using the SimCSE (Gao et al., 2021) which is the state-of-the-art model for the sentence representation task:\nL ctr = 1 |C| C i \u2208C \u2212log \u03b1 C i \u00d7 e f (C i ,D) e f (C i ,D) + s i \u2208N e f (s i ,\n\u03b1 C i = 0, sim(C i , S) < \u03d5 1, sim(C i , S) \u2265 \u03d5 (7)\nwhere \u03d5 is a hyper-parameter of the instance weighting threshold, and sim(\u2022) is the cosine similarity score evaluated by the SimCSE model. Finally, as shown in Fig. 3 , we combine the ranking (Eq. 5) and contrastive (Eq. 6) losses:\nEQUATION\n)\nwhere \u03b3 is the scale factor of each loss and we find the optimal values (\u03b3 1 = 10, \u03b3 2 = 0.1) in Appendix H.\n3 Experiments\n\nDatasets\nWe experiment on two datasets, whose statistics are shown in Appendix C. CNN/DailyMail (Hermann et al., 2015) is the most commonly used summarization dataset which contains articles from the CNN and DailyMail newspapers.\nXSum (Narayan et al., 2018 ) is a one-sentence summary dataset from the British Broadcasting Corporation (BBC) for the years 2010 -2017.\n\nTraining Details\nWe use diverse beam search (Vijayakumar et al., 2016) to generate 16 candidate summaries. We start from pre-trained checkpoints of RoBERTabase (Liu et al., 2019) . We train BalSum for five epochs. It takes 33 hours on CNN/DM and 22 hours on XSum on a single RTX 3090 GPU. More details are described in Appendix D.\n\nMain Results\nIn terms of the two-stage framework, we compare our results with SimCLS (Liu and Liu, 2021) , Sum-maReranker (Ravaut et al., 2022) , and BRIO (Liu et al., 2022) . We apply BalSum on top of each base model which is BART or PEGASUS.\nThe results on CNN/DM are described in Table 1. BalSum outperforms a base BART model, according to gains of 2.54/1.27/2.63 R-1/2/L. Notably, while it has comparable performances on ROUGE to previous models, it achieves an 89.67 BERTScore, reaching a new state-of-the-art performance. When ranking the candidate summaries, our model can estimate the meaning of summaries without seriously degrading the lexical aspect. We argue that this is because BalSum decreases more false positives than other ranking models. We provide fine-grained analyses for this result and present a case study in Sec.3.4.\nIn addition, we apply our method on XSum, as shown in Table 2 . Though we use a different strategy to generate the validation and test data 3 , our method improves a base PEGASUS with a small margin. We believe the one of reasons is that XSum is restricted to capturing diverse semantic units because it consists of much shorter summaries (onesentence) than CNN/DM. Model BS@1 BS@3 BS@5 R@1 R@3 R@5\nOracle \n\nAnalysis\nWeighting Threshold \u03d5 Intuitively, the larger the weighting threshold, the lower false positives. We train our model with different instance weighting thresholds from 0.7 to 0.9. In Table 3 , the highest threshold (\u03d5 = 0.9) shows the best performance and it rises largely to 0.3 BERTScore compared to when not applied. We also find that increasing the threshold leads to performance improvement. Therefore, we demonstrate that false positives can be considered noise in training.\nRanking Evaluation Regardless of the number of candidates, an ideal ranking model should yield oracle results considering diverse aspects of summarization. We conduct an experiment to measure the qualities by selecting the top-k summaries after aligning the candidates through different models. As shown in Table 4 , we can see that our model shows consistent performance in both evaluation metrics depending on the k (about \u00b10.06 BERTScore, \u00b10.34 ROUGE average score). Compared to SimCLS and BRIO-Ctr, the second block in Table 4 demonstrates that BalSum captures semantic similarity best while maintaining the intermediate level from the perspective of lexical overlap quality. Moreover, we find that BalSum has the lowest drop ratio of BERTScore (\u22121.52%) from the perfect ranking \"oracle\" scores. We also investigate whether all ranked summaries by models satisfy both lexical and semantic quality. We evaluate models using F 1 which measures the cases where the higher-ranked summary \n\nConclusion\nIn this work, we propose BalSum which aims to evaluate summaries by considering the balance between lexical and semantic quality. To achieve this, we perform a multi-task learning, which aligns summaries according to their lexical overlap qualities and identifies whether they are similar to the document. In addition, to our best knowledge, our method is the first attempt to present a new perspective of false positives (semantic mistakes) in ranking and creating the model to reduce their in-fluence. Our experimental results and fine-grained analyses validate that our model achieves consistent improvements over competitive baselines.\n"}
{"question": "What is the primary focus of this research regarding instruction tuning in NLP models?", "evidence": "  In contrast, we focus on analyzing how the models utilize instructions during the training process. We compare our analyzing methods and observation with prior works in Appendix A.1.  ", "options": ["A. Analyzing test-time performance of instruction-tuned models.", "B. Investigating how models utilize instructions during training.", "C. Comparing the performance of IT models with different training examples.", "D. Evaluating the impact of altering task definitions in IT.", "Besides ICL and few-shot prompt-tuning, some works raise concerns about instruction following in the instruction tuning field (Finlayson et al., 2022; Gupta et al., 2022; Gu et al., 2022) , with a focus on test-time analysis. "], "answer": "B", "content": "\nIntroduction\nRecently, instruction tuning(IT) has drawn much attention in the NLP communities, with the rapid growth of new models (Sanh et al., 2021; Wei et al., 2021; Ouyang et al., 2022) and datasets (Wang et al., 2022; Gupta et al., 2022; Finlayson et al., 2022; Mishra et al., 2021; Ye et al., 2021; Bach et al., 2022) . Models trained with task instructions demonstrate impressive zero-shot cross-task generalization ability. Despite the remarkable results, 1 : Comparison between two types of instruction tuning models. Noted that we reported an estimated number of instructions for T0 during training and testing since they have 5 to 10 instructions for each task. Our analysis focuses on the \"generalize to unseen task\" type.\nhow models utilize the instructions during training and inference time remains an open question.\nPrior works have raised the question of whether models really learn to follow the instructions or just capture spurious correlations. Jang et al. (2022) , Webson and Pavlick (2021) showed that the current large language models (LLMs) can achieve similar performance with misleading instructions(prompts) in in-context learning(ICL) and few-shot learning scenarios. Min et al. (2022) analyze how model utilize examples in ICL. They observed that (1) Input-output mapping in examples is not important and(2) Output space information is crucial.\nBesides ICL and few-shot prompt-tuning, some works raise concerns about instruction following in the instruction tuning field (Finlayson et al., 2022; Gupta et al., 2022; Gu et al., 2022) , with a focus on test-time analysis. In contrast, we focus on analyzing how the models utilize instructions during the training process. We compare our analyzing methods and observation with prior works in Appendix A.1.\nIn this work, we conduct controlled experiments on NatInst-V2 (Wang et al., 2022) , the largest opensource instruction learning dataset includes 800+ English tasks with diverse task types, to study how models utilize instructions during IT. Note that existing research on IT can be categorized into two major camps: generalize to unseen tasks and generalize to unseen instructions, based on their objectives. Table 1 shows the comparison. Our analysis focuses on the former with more background and justifications provided in section 2. We strategically alter the instructions and compare them with original instructions for IT. Specifically, for task definition, we create simplified versions by removing all semantic components in the instructions and only leaving the output space information. For task examples, we create delusive examples with incorrect input-output mapping, where the examples' input and output spaces are correct, but the inputoutput mappings are wrong. Figure 1 demonstrates specific examples of these altered instructions.\nOur experiments show that models trained with simplified task definitions achieve performances on par with the original IT models with different numbers of training examples ranging from 10 to 800 per task. We also observe that instructiontuned models are sensitive to input-output mapping during the testing ICL stage, but not during the instruction-tuning (training) stage, especially in low resource settings (i.e., \u2264 50 training instance per task). To further understand why instruction tuning improves performance for zero-shot test tasks, we establish a random baseline that only knows the correct output format (label space) for classification and multi-choice tasks. We discover that the random baseline can get 30% absolute exact-match score improvement over an untuned model, almost comparable to some IT models in low resource settings.\nOur results suggest that the impressive performance gains of IT may just come from models learning superficial patterns, such as the output space and format. We suggest future research on IT more carefully analyze their performance gains and benchmark against trivial baselines.\n\nBackground\nRecently, many instruction tuning work train and test the models with instructions to achieve better zero-shot generalizability toward unseen tasks/instructions. We categorize these works by their objectives: generalize to unseen tasks and generalize to unseen instructions, and show the comparison in Table 1 . Instruction tuning to generalize to unseen tasks. Figure 1 illustrates a two-stage instruction tuning pipeline used in many IT models, such as T0 (Sanh et al., 2021) , FLAN (Wei et al., 2021) , and TK-Instruct (Wang et al., 2022) . In the first stage, the models are trained on a set of training tasks with instructions (task-definition and task-examples). After training, the models are evaluated on a set of unseen testing tasks for zero-shot generalizability. By incorporating instructions during training, the models are shown to significantly improve performance over untuned models. The impressive performance gains led people to believe that models learned to follow instructions via instruction tuning. The goal of our analysis is to verify this belief. Instruction tuning to generalize to unseen instructions. Different from T0, FLAN, and TK-Instruct training and testing the model with clear task boundaries and focusing on cross-task generalizability, Instruct-GPT (Ouyang et al., 2022) , Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) focus more on instruction generalizability, which they train their model without clear task boundary but with diverse instructions, and further test on user-oriented instructions. These models show very different behavior compared with instruction tuning models that aim to generalize to unseen tasks.\nSince Instruct-GPT is not open-sourced and distilled IT models such as Alpaca and Vicuna come up after our submission, we focus our analysis on the first category using the TK-instruct model and NatInst-V2 dataset. However, we also conduct additional experiments and discuss the Alpaca model's instruction following ability in Table 2 .\n\nAnalysis Method\nTask definition manipulation.\nTo analyze whether models really \"understand\" and utilize the semantic meaning of task definitions, we conduct controlled experiments to remove semantic information in task definitions. Specifically, we conduct instruction-tuning with task definitions at 3 levels of granularity: Original, Simplified, and Empty. The Original version uses human-crafted human-readable task definitions provided in NatInst-V2 (Wang et al., 2022) . The Simplified task definitions remove all semantic components in the original task definition and only leave the output space information. Specifically, we only provide possible output labels as task definitions for classification tasks, and completely remove task definitions for other tasks (mostly generative tasks) during IT. Figure 1 shows an example of Simplified task definition. More details can be found in Appendix A.2. For Empty, we don't provide task definition during instruction-tuning.\nTask example manipulation. Finlayson et al. (2022) show that by providing a few task examples, both humans and models can guess and perform a task. We thus design a controlled experiment to study whether models learn the input-output mapping from task examples. Specifically, we compare models trained with 3 types of task examples: Original, Delusive, and Empty. For the Original setup, we provide one positive example in NatInst-V2 (Wang et al., 2022) \n\nExperimental Setup\nDataset. We conduct experiments on the NatInst-V2 (Wang et al., 2022) , the largest open-source instruction learning dataset, including over 800+ English tasks with diverse task types. The instructions include human-crafted human-readable Task Definition, Positive Task Examples, Negative Task Examples, and Explanation. We focus on studying task definition and task examples, which were shown to be most useful in the original paper.\nModel. we conduct experiments on TK-Instruct, the current SOTA model provided in NatInst-V2 paper. The model significantly outperformed previous SOTA models, such as T0 (62.0 v.s. 32.3 rouge-L for 11B model). We follow the seq-to-seq instruction-tuning method used in TK-Instruct, and train a T5-large-lm-adapt (770M parameters) model (Raffel et al., 2020) with performance comparable to the larger model (3B parameters) reported in Wang et al. (2022) . 1 Evaluation Metrics. For task definition, we separately evaluate Classification and Generative tasks using exact match and rouge-L respectively. For , 10, 20, 50, 200, 800) .\n\nResults\nTask Definition Experiments. Figure 2 shows experimental results for task definitions. In the top sub-figures, we can see that the models trained with Simplified instructions achieve almost the same results as models trained with Original definitions both on Classification and Generative tasks. Note that Simplified task definitions remove all semantic components in task definitions and only retain output space information for Classification tasks and remove task definitions altogether for Generative tasks. This indicates that models may only utilize output space information during instruction tuning. The bottom-left sub-figure in Figure 2 shows the overall rouge-L score for classification tasks, where models trained on the Original task definition slightly outperform the Simplified ones. A closer examination reveals that models trained on the Original task definitions are more likely to predict partially correct answers that help with the ROUGE-L score in some tasks. We provide further details in Appendix A.5. In addition, we also observe that training with Simplified prompts can yield comparable performance to the T0 model trained with Original prompts on T0 dataset. Please refer to Appendix A.6 for details. Task Examples Experiments. Combined with the previous results for task definition, we observe that comparing to the untuned models(T5 w/o IT), the IT models may achieve significant performance gain (Rouge-L from 22 to 46) with (1)Simplified task definition and (2)Delusive task example, indicating that the current impressive improvement of IT models can come from the models learning superficial patterns without utilizing (following) the instructions like human do.\nFor the right sub-figure, we show the results using Delusive task examples during test time via in-context learning. We see the performance drops for all three models, indicating that the inputoutput mapping matters for in-context learning on instruction-tuned models. This observation seems to misalign with previous work (Min et al., 2022) , which they found input-output mapping is unimportant for in context learning for classification tasks. However, a closer investigation found that most tasks suffer from significant performance drop are analogical tasks rather than classification tasks as studied in Min et al. (2022) . 2\n\nAdditional Analysis\nRandom baseline. While our experiments suggest that models do not utilize most information in the instructions, we still observe huge performance gains via instruction tuning. To understand where the gains come from, we introduce a Random baseline that simply guesses within the cor- 200) can improve exact-match score to 52%. However, while the performance gains seem impressive, the Random Guessing baseline can also achieve 42.6% exact-match score, on par with TK-Instruct trained in low resource setting (less than five instances per task). This suggests that the majority of score improvement from IT may come from model learning the output format, especially in low-resource settings.\nFair comparison for IT models. Existing studies on instruction tuning often introduce changes to both models and datasets simultaneously, which can obscure fair comparisons. To address this issue, we conduct experiments comparing different models (T0, TK-Instruct) on the same dataset (NatInst-V2) and emphasize the importance of careful evaluation. In Table 3 , when evaluating using the NatInst-V2 evaluation method and considering only the overall Rouge-L score, the TK-Instruct model appears to outperform T0 significantly. However, upon closer examination of the classification (CLS) and generative (GEN) tasks separately, we observe that T0's classification score is even lower than the Random baseline, primarily due to its format correctness being only 64%. To ensure a fairer comparison between these models, we employ constrained decoding techniques to align the model's predictions with the label space. By adopting this approach, we observe a substantial performance improvement for T0 in CLS tasks (34.03 to 51.31). T0 surpasses both the TK-Instruct model and the random baseline, indicating that it Table 3 : Careful evaluation of the NatInst-V2 dataset. The Format metric is the same as the format correctness in Figure 4 . The w/ CD indicates that the model's decoding is constrained to match the label choices for CLS tasks. The TK is the TK-Instruct(770M) model trained with 10 instances per task.\nis indeed superior to these models in CLS tasks.\n\nDiscussion\nDo Alpaca better follow the instruction on NatInst-V2 dataset? After our submission, new instruction tuning models, like Alpaca and Vicuna, are trained on distilled data from Chat-GPT and exhibit behavior closer to it. To investigate their instruction utilization, we conduct the \"Altered Task Definition\" experiment on LLaMA-7B (Touvron et al., 2023) and Alpaca-7B models using the NatInst-V2 test set. In Table 2 , training the LLaMA model on the NatInst-V2 dataset using the Original task definition leads to substantial performance enhancements than zeroshot. However, the Simplified task definition also achieves comparable performance, with a minimal decrease of 3 (EM/Rouge-L)scores. This finding is consistent with our previous observations on the TK-Instruct and T0 models. Even without tuning on NatInst-V2, the Alpaca model demonstrates strong performance on the NatInst-V2 test set. However, when the model is tested using a simplified task definition, there is a significant decrease in performance for generative tasks (but not for classification tasks). This highlights the importance of a well-written task definition for the Alpaca model to effectively perform generative tasks.\n\nConclusion\nWe constructed controlled experiments on NatInst-V2 to compare model training with altered vs. original instructions (task definitions and examples). Our findings indicate that some current IT models do not fully utilize instructions, and the impressive performance gains of IT may come from models learning superficial patterns, such as the output space and format. We suggest future research on instruction tuning to analyze their performance gains with more comprehensive evaluation and benchmark against trivial baselines. 1321\n"}
{"question": "What inspires us to  propose data intervention strategies to reduce gender bias in pretrained models?", "evidence": "  Since large-scale retraining of these models from scratch is both time and computeexpensive, a variety of approaches have been previously proposed that de-bias a pre-trained model.  Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.  the majority of current state-ofthe-art debiasing methods focus on changes to the training regime,  ", "options": ["A. Because retraining these models from scratch on a large scale requires significant time and computational resources,", "B. Many models tend to carry biases that can be extended to numerous downstream applications, resulting in unfair treatment towards certain demographic groups.", "C. Most of the current advanced debiasing techniques primarily concentrate on modifying the training approach.", "D. The fewer training examples are,the more fame we will gain."], "answer": "B", "content": "\nIntroduction\nRecently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019 ; * Equal Contribution Barocas et al., 2017; Kurita et al., 2019) . Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\nPrevious work (Nadeem et al., 2021) , (Nangia et al., 2020a) , (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021) , we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop bias mitigation strategies feasible.\n\u2022 We introduce a novel data sampling technique that utilises LLMs to mine for the most biased samples from a dataset and can benefit existing state-of-the-art debiasing methods. When used for debiasing a model, these few samples serve as exemplars and induce large reductions in gender bias.\n\nGender Bias in BERT Predictions\nMost-biased data samples ____ is very good at cooking but not great at work. \n\nReduced Gender Bias in BERT Predictions\nFigure 1 : Our method can be summarized as a combination of bias discovery and mitigation. First, we use a pre-trained LLM to find the most gender-biased samples. Then, we apply our data intervention techniques and use these modified training samples to fine-tune the model. Experiments show that our method is very effective at reducing gender bias, outperforming three state-of-the-art baselines and being comparable to two other baselines.\n\nRelated Work\nIn recent years, there has been growing concern about the bias/stereotypical discriminatory behavior by NLP models, particularly concerning gender. Several studies have investigated the presence of gender bias in various NLP tasks and proposed methods for mitigating it.\nOne line of research has focused on analyzing the extent of gender bias in pre-trained language models such as BERT and GPT-2. These studies have found that these models exhibit a significant amount of gender bias in their word embeddings for BERT (Jentzsch and Turan, 2022) and for GPT-2 (Kirk et al., 2021) and are prone to making stereotypical gender-based predictions (e.g., assuming that a doctor is male and a nurse is female). A standard evaluation metric used in this line of research is Stereotype metrics such as StereoSet (Nadeem et al., 2021) , which evaluates the model's ability to predict gender stereotypes and CrowS pairs (Nangia et al., 2020b) which measure whether a model generally prefers more stereotypical sentences. A similar line of work is gender bias tests proposed in BIG-bench (Srivastava et al., 2022) . The tests assess the language model's gender biases, stereotypes, and ability to infer gender information. It evaluates gender bias and stereotype between male and female, and gender minority bias and stereotype between majority and minority. It also examines the model's language modeling performance, which can be affected during de-biasing.\nAnother line of research has proposed methods for debiasing these models. These methods can be broadly categorized into two groups: data-based and algorithm-based. Data-based methods aim to reduce bias by removing or altering biased words from the training set. In contrast, algorithm-based methods aim to modify the model's architecture or training procedure to reduce bias. One popular databased method is \"uncertainty sampling\" (Lewis and Gale, 1994) , where the model is trained on the instances that it is most uncertain about, which can help to reduce bias by forcing the model to learn from a diverse set of examples. A popular algorithmbased method is \"Adversarial Debiasing\" proposed by Zhang et al. (2018) , which fine-tunes the model using an adversarial loss to make it less sensitive to sensitive attributes such as gender. OSCar proposed by Dev et al. (2021) , is another algorithm based method that utilizes the idea of disentangling \"problematic concepts\" like occupation and gender relationship instead of removing them altogether. MABEL (He et al., 2022) has both algorithm and data-based components, as it first augments the training data by swapping gender words and then applies a contrastive learning objective and alignment via entailment pairs. Their data augmentation strategy is similar in spirit to the data intervention techniques we propose, however our analysis does not require training auxiliary models and uses significantly lesser data.\nData-based methods include the \"Equalization\" technique proposed by Bolukbasi et al. (2016) , which aims to equalize the representation of genderspecific words in the embedding space, the \"Counterfactual Data Augmentation\" (CDA) method proposed by Zimmermann and Hoffmann (2022) Pre-trained LLMs are biased towards different genders, as seen in a simple mask-fill experiment using BERT. (Here, and in the rest of the paper, we assume a binary treatment of gender for simplicity.) The task is then to mask out the gender-related nouns and pronouns (such as he, she, her, woman, etc.) and get BERT to predict the masked words for the affected sequences in the dataset. Here, we consider a fixed list of gender-specific words curated from previous work (Lu et al., 2018; Zmigrod et al., 2019) and neutral words list 1 . We finally compute the \"total confidence difference\" as the sum of differences in the model's prediction confidence for each gender-word pair (such as confidence of predicting he \u2212 she, man \u2212 woman, etc.). Formally, we define total confidence difference as\nEQUATION\n))| where f (x) represent the confidence of model's prediction, N is the total number of tokens in the dataset and x is the tokenized gender word. The higher this number, the more biased the model is concluded to be. We compute the metric at token level and ensure that each of the gender word gets tokenized into exactly one token by initially extending the tokenizer with our gender word list. The top 3 biased gender-word pairs in StereoSet are shown in Table 1 . Intuitively, our technique for gauging bias in LLMs is sensitive to the fixed word list used to represent the sensitive attributes (here, gender). In Table 2 , we show the number of words covered by the word list used for both WikiText-2 and StereoSet datasets.\n\nData Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vocabulary of the model and try to balance the confidence of prediction on opposite-gender words by using phrases instead. Thus, we call our final approach random-phrase-masking as we instead substitute words with phrases that reflect the equality of gender. This approach not only reduces gender bias but also preserves the original meaning of the sentence in most cases. In our approach, we chose the phrases and order of gender words at random with equal probability. Additionally, we hypothesize that the choice of the dataset for fine-tuning is also essential. We choose two datasets: the WikiText-2 (Merity et al., 2017) dataset, which has implicit gender bias since its sources from Wikipedia articles, and the Stere-oSet dataset (Nadeem et al., 2021) , which has explicit/more gender bias as it has been designed to evaluate gender bias. WikiText-2 2 has 600 train articles and roughly 2M tokens while StereoSet 3 (dev) has 2123 samples out of which we only consider 800 samples which are not unrelated. Naturally, our data intervention method should work better on a dataset with training examples with gender bias while being devoid of meaningful gender associations like \"She needs a gynecologist,\" where the gender of the person is important. By testing our method on both datasets, we can understand the sensitivity of our approach to the quality of training samples used.\n\nBias Evaluation Metrics\nWe focus on evaluating the bias of a model while also measuring its language modeling capability. The ideal model would not just be one with the least bias but also one which does not compromise its language modeling performance. The dual estimation of bias and performance of a model was proposed in the StereoSet benchmark (Nadeem et al., 2021) , with the Language Modeling Score (LMS) measuring the percentage of times a meaningful token is predicted for the mask as opposed to a meaningless token, the Stereotype Score (SS) measuring the percentage of times the model predicted a stereotypical word as compared to an anti-stereotypical word, and an idealized CAT score (ICAT) combining the LMS and SS score into a single metric. An ideal model has an ICAT score of 100, while the worst biased model has an ICAT score of 0. We additionally evaluate the CrowS-Pairs benchmark (Nangia et al., 2020a) , which captures data with greater diversity in both the stereotypes expressed and the structure of sentences (50 is ideal). However, we note that the Crow-S benchmark is much more limited compared to StereoSet (Nadeem et al., 2021) in terms of both the volume and variety of linguistic phenomenon relating to gender bias it covers.\n\nExperiments\nWe compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work. For a fairer comparison, we run the baselines with the same size (100) of the training set as our method. For all of our experiments, we consider the \"bert-base-uncased\" pre-trained model available from HuggingFace. For fine-tuning our model, we select a varying number of most-biased training samples (10, 50, and 100) from the WikiText-2 and StereoSet (we only use the dev set) datasets, as discussed in section 4. We also compare this to a random selection of data points as an ablation study. On the selected dataset, we apply our interventions and obtain the modified dataset, which is then used to fine-tune our pre-trained model using masked language modeling (MLM) loss. The key point is that we only fine-tune the model on the gender words conditioned on the remaining text, significantly reducing the fine-tuning time. We perform ablations on various types of interventions as discussed in Table 7 . The model is trained for 30 epochs, with a learning rate of 0.001 and AdamW optimizer. We ran all of our experiments on NVIDIA Tesla T4 GPU on Google Colab for roughly 48 hours. For all experiments, we report the numbers as the mean and standard deviations (6) of 3 different runs. Our experiment code can be found here. 4 \n\nResults\nTable 4 shows the StereoSet and Crow-S scores for our baselines and our best-performing interventions on the WikiText-2 Dataset. In the StereoSet benchmark, we observe that random-phrase-masking obtains lower SS than all other baselines. On the Crow-S benchmark, random-phrase-masking does better than thre of the baselines except Sen-tenceDebias which achieves slightly better scores. While random-phrase-masking results in lower SS scores than neutral-masking, it also obtained [MASK] is very good at cooking but not great at [MASK] work. she is very good at cooking but not great at her work.\nhe is very good at cooking but not great at farm work. Being a mother is not easy since she will have to stay home and take care of the child.\nBeing a father is not easy since one will have to stay home and take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\n\nShe is very caring and kind but not good at what she does.\nHe is very caring and kind but not good at what he does.\n\nInput Sentence:\nOutput of Biased Model:\n\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to modify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the same pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the our method is successfully able to reduced biased substitutions. very low LMS scores. We attribute this performance degradation to the blunt substitution of phrases that our method uses, which might lead to odd-sounding sentences. In the Crow-S benchmarks, we see similar behavior and find that random-phrase-masking does better than neutral-masking. Since we believe that our method is sensitive to the choice of the dataset, we also present results on the StereoSet (dev) dataset 6. In Figure 2 , we perform a qualitative analysis of our proposed approach and find that random-phrase-masking is able to flip the predictions on fill-mask tasks for stereotypical sentences.\n\nConclusion\nIn this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs. We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples. Evaluation of our methods on state-of-the-art bias benchmarks empirically suggests that our methods effectively reduce gender bias. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n"}
{"question": "What modification did the authors propose to adapt NLI models to the specificities of faithfulness prediction in dialogue?", "evidence": "  To alleviate this issue, we propose a simple data augmentation method to adapt NLI models to genres where they need to be aware of statements that must be exempt from NLI-based faithfulness evaluation. Our approach is computationally attractive, as it avoids an increase in cost at inference time.  ", "options": ["A. Using question-generation models.", "B. Applying Monte-Carlo dropout.", "C. Integrating contradiction scores.", "D. Task-Adaptive Data Augmentation. "], "answer": "D", "content": "\nIntroduction\nConditional language models suffer from a tendency to hallucinate information (Maynez et al., 2020) , resulting in generations that are not faithful to their input documents, which limits the trustworthiness of such models. This raises a need for automatic faithfulness metrics. In this context, models trained on natural language inference (NLI) (Bowman et al., 2015) are attractive since, intuitively, a generation being faithful implies it must be entailed by the source (Falke et al., 2019) . However, pure NLI models have seen mixed success in faithfulness evaluation (Falke et al., 2019; Kryscinski et al., 2020; Wang et al., 2020; Maynez et al., 2020) . While in recent evaluation on the TRUE benchmark (Honovich et al., 2022) , which contains datasets from knowledge-grounded dialogue, summarization and paraphrasing, NLIderived metrics perform best overall, they require impractically large models, or costly additional machinery such as question generation and answering models at inference, while still showing robustness issues. Thus we ask: What is still needed for pure NLI models to perform robustly across faithfulness datasets -while remaining cheap enough to serve as a lean and practical evaluation tool?\nWe enhance a relatively small NLI model to make it work robustly across tasks in three ways:\nTask-Adaptive Data Augmentation. In NLI, a hypothesis must be fully entailed by its supporting premise. However, in faithfulness, not all parts of the generation always need to be grounded. We identify an instance of this phenomenon in dialogue where parts of a turn can fulfill communicative functions such as hedging or establishing emotional connection and are often disregarded in faithfulness annotation. Hence, when applying NLI models to complete dialogue turns that may include statements irrelevant for grounding, we run a risk of producing incorrect unfaithfulness predictions.\nTo alleviate this issue, we propose a simple data augmentation method to adapt NLI models to genres where they need to be aware of statements that must be exempt from NLI-based faithfulness evaluation. Our approach is computationally attractive, as it avoids an increase of cost at inference time.\nIntegration of NLI Contradiction Scores. Existing NLI faithfulness metrics typically use the entailment score for their predictions (Honovich et al., 2022; Falke et al., 2019; Kryscinski et al., 2020) . However, Chen and Eger (2022) show that subtracting the contradiction score from the entail-ment score (referred to as e-c ) can improve NLI performance in certain evaluation tasks. We show that there also is a strong positive effect of e-c for faithfulness prediction, and demonstrate that this is due to a high contradiction probability being a more reliable predictor of unfaithfulness than low entailment probability.\nMonte-Carlo Dropout Inference. Applying NLI models to faithfulness prediction involves a domain shift from largely human-written data to automatically generated text. To make NLI model scores more robust under this shift, we propose to use Monte-Carlo dropout during inference (Srivastava et al., 2014) . This essentially creates a cheap ensemble and has been shown to deal better with noisy labels (Goel and Chen, 2021) . This approach leads to consistent score improvements in our tasks.\nThe combination of all modifications not only strongly improves over a baseline NLI model, but also outperforms all other metrics on TRUE, on average, while being cheaper and smaller. 1 2 Method Details\n\nTask-adaptive Data Augmentation\nTo illustrate that task requirements can be incompatible between faithfulness and NLI, consider the following instance from the Q2 dialogue corpus (Honovich et al., 2021) that is labelled as faithful:\nGrounding: American pancakes are similar to Scotch pancakes or drop scones. Generation: yes , i love american pancakes , they are like scotch pancakes From an NLI perspective, the generation is clearly not entailed, since the statement \"I love american pancakes\" is not supported by the input.\nTo better prepare an NLI system for such genre or task-specific cases, we manually curate a small list of statements that should not influence the faithfulness prediction. We augment NLI data from the ANLI corpus (Nie et al., 2020) by adding a randomly chosen phrase from this set to each instance, while preserving the label. We then train an already fine-tuned NLI model on a concatenation of these augmented samples and original ANLI data. For training details see Appendix A.\n1 All code is available at https://github.com/julmaxi/ with_a_little_push\n\nMonte-Carlo Dropout\nTo compute scores under Monte-Carlo dropout, we randomly sample k dropout masks and compute the average of the model predictions. We set k = 15, since preliminary experiments showed that performance did not profit from additional samples.\n\nExperimental Setup\nWe run experiments on TRUE (Honovich et al., 2022) , a benchmark that compiles a wide variety of faithfulness tasks in a standardized format. It contains summarization (Pagnoni et al., 2021; Maynez et al., 2020; Wang et al., 2020; Fabbri et al., 2021) , knowledge-grounded dialog (Honovich et al., 2021; Gupta et al., 2022; Dziri et al., 2022) 2 and paraphrasing (Zhang et al., 2019) datasets. 3 Following recommendations in TRUE, we evaluate using Area under the ROC Curve (AUC).\nAs our BASE model, we use the DeBERTa-large (He et al., 2020) model of Laurer et al. (2022) , trained on MultiNLI (Williams et al., 2018) , Fever-NLI (Thorne et al., 2018) , ANLI (Nie et al., 2020) , LingNLI (Parrish et al., 2021) and WANLI (Liu et al., 2022) . The metric All uses all three of our proposed modifications to Base. We also investigate a variant without MC dropout inference (-MC) as a more cost efficient alternative.\nWe compare to the strongest models on TRUE: T5 ANLI (Honovich et al., 2022 ) is a T5-11B (Raffel et al., 2020) model trained on ANLI. 4 SummacZS (Laban et al., 2022 ) evaluates an NLI model on all pairs of input and generated sentences and then averages maximum entailment probabilities for each generated sentence.\nQ2 (Honovich et al., 2021) combines a question generation/answering pipeline with an NLI score.\nFinally, Honovich et al. (2022) introduce a strong ensemble of these 3 methods (Eorig). To further verify our approach, we construct a new ensemble (Eour) by replacing T5 with All.\n\nResults\nTable 1 shows the AUC scores for each metric. Base on six out of nine corpora, but also significantly outperforms all other competitors on average, while being more computationally efficient.\nAs expected, we find the biggest gains in dialogue, where the All model even outperforms Eorig on 2 out of 3 corpora. We do not improve on BEGIN, which is likely due to bias in the dataset construction, which we elaborate on in Section 5.1. On the summarization part, All improves significantly over Base on 3 out of 5 corpora, while not significantly harming performance on any corpus. However, it still falls short of the best models in TRUE. The strong showing of T5 on these corpora suggests that this might be alleviated with a stronger base model.\nOverall, a very similar behaviour is exhibited by -MC, presenting an attractive option when the added overhead of multiple samples is undesirable.\nEour is on par with Eorig, despite massively reduced costs; it even significantly outperforms it on two dialog and the paraphrasing corpora.\nWe also investigate the performance of each individual modification to our model (Table 2 ). They all improve average scores, while only leading to a notable decrease on BEGIN for both e-c and dialogue augmentations and on MNBM for e-c .\nOutside of dialogue, we find that the augmentation methods have a positive impact on PAWS, as well as all summarization corpora that are at least partially based on summaries for the CNN/DM dataset (Hermann et al., 2015) (Frank, QAGS-C, and SummEval). While we do not have a definitive explanation for this phenomenon, we hypothesize that on these datasets our augmentations aid in making the model robust in the presence of noise or irrelevant context since our augmentations are label-neutral and must similarly be 'ignored' during training.\n\nEffect of Dialogue Adaptation\nWe investigate whether the improvements via our augmentation approach are indeed due to them improving the handling of personal statements.\nWe use the occurrences of the pronoun I in a generation as a proxy measure 5 and compute its correlation with human labels and metrics (see Table 3 ). On both Q2 and Dialfact, our proxy measure, while uncorrelated with human labels, is strongly correlated with the scores of both Base and T5. This indicates these metrics indeed tend to incorrectly reject generations with personal statements. All on the other hand reduces this dependency.\nOur results also help explain why negatively correlated with first person pronouns. This is likely due to a bias in dataset construction:\nThe BEGIN dataset used in TRUE has generations from two models, one of which is both more likely to generate pronouns and more likely to generate unfaithful output (see Appendix B).\n\nEffect of integrating contradiction scores\nTo isolate the effect of e-c we compare score distributions of Base and Base+e-c in Figure 1 . The lefthand side of the figure shows that in Base ca. 2700 faithful instances are predicted as non-entailed (i.e., e-score near 0), which implies they are labelled as contradictory or neutral. e-c , on the other hand, further differentiates these instances into instances with high contradiction (negative e-c score) and high neutral probability (e-c score near 0). We observe that almost all low-scoring faithful generations are classified as neutral, whereas nearly all instances that are classified as contradictory are indeed unfaithful. Where Base has no way to make use of this information, e-c allows to reliably label contradictory instances as unfaithful.\n\nCost comparison to other approaches\nThere is increasing awareness of the resource-hungriness of deep learning (Strubell et al., 2019) . Especially for faithfulness, cheap and reliable metrics are critical, given rising demands for NLG in research and industry. Table 5 : Results of our phrase selection robustness analysis. For each run, we sample five phrases, recreated our dataset and retrain our model. We repeat this process ten times and report the average, as well as the standard deviation, minimum and maximum scores of the runs.\nSmall numbers indicate difference to the original scores. All results were computed using e-c and MC dropout.\nFor better comparison, we also report the scores of a model without any augmentation (i.e. without any additional training) with e-c and MC dropout.\nrequires fewer parameters than any other metric, including a more than 30x reduction compared to T5. During inference our model always requires a constant number of calls which can be reduced to a single call when ablating MC dropout. On the other hand, the number of calls in SummacZS scales with the number of input and output sentences. Q2 needs to generate questions by calling an auto-regressive QG model n times, where n factors in the amount and length of questions (#Q\u00d7Ql), answer #Q questions with the QA model and finally check #Q answers with an NLI model (#Q \u00d7 2).\nIn sum, our model compares favourably with other approaches, while also allowing for a performance/cost tradeoff by forgoing MC dropout.\n\nPhrase Selection Robustness\nTo ensure that our augmentation is robust and not overly reliant on any particular choice of phrases, we repeat our dataset augmentation process multiple times with five randomly chosen augmentation phrases out of the original ten. We sample ten such datasets and retrain our model for each. Table 5 shows the average score, minimum and maxi-mum score, as well as the standard deviation of the scores. We also report results of a model with both MC dropout and e-c but without any additional training and augmentations to directly quantify whether the augmentations are still helpful in their reduced form. This corresponds to applying MC dropout and e-c to Base.\nAs expected, we find that reducing the variety of available phrases leads to a drop in performance across almost all datasets, compared to All. The only exception is BEGIN, where we instead see a slight improvement. This is likely to be related to the construction of BEGIN (see the discussion in Section 5.1).\nWhen comparing our limited augmentation models to the non-augmented model, we find that they still outperform the non-augmented model in almost all cases. In particular for Q2 and DialFact, for which we expect the strongest impact of our augmentations, we find that even the worst run still outperforms non-augmented model. This suggests that our augmentations can robustly adapt the model to the dialogue task.\nFinally, we observe a relatively large drop in scores for all datasets that are at (least partially) derived from CNN/DM (Frank, SummEval and QAGS-C). This mirrors our earlier observation in Section 4 that these datasets profit from our augmentation procedure.\n\nRelated Work\nPrevious work on the utility of NLI for faithfulness led to mixed conclusions. In summarization, Falke et al. (2019) and Kryscinski et al. (2020) find out-of-the-box models have only limited utility in a faithfulness setting. In Wang et al. (2020) , an NLI model is outperformed by a question generation/answering (QA/QG)-based method. In contrast, Maynez et al. (2020) find that a similar NLI model vastly outperforms a QA/QG metric on their data. In knowledge-grounded dialogue, Dziri et al. (2022) , Gupta et al. (2022) and Honovich et al. (2021) find out-of-the-box models underperform.\nTo improve NLI models for faithfulness in summarization, Kryscinski et al. (2020) propose FactCC, which is trained on artificially noised summaries. Utama et al. (2022) propose a controllable generation model to generate artificial faithfulness data. In knowledge-grounded dialogue, Dziri et al. (2022) and Gupta et al. (2022) combine noising techniques to generate additional training data for NLI-based faithfulness models. In contrast to our work, these approaches a) generate training data from external sources, instead of directly augmenting NLI data, and b) do not explicitly focus on reconciling differences between NLI and faithfulness with their augmentation. Outside of augmentationbased approaches, Goyal and Durrett (2020) propose to train NLI models to label faithfulness at the dependency arc level.\n\nConclusion\nWe have demonstrated that with a small number of focused adaptations, even a relatively small NLI model can robustly predict faithfulness. We have:\n1. Shown that NLI-based metrics can be incompatible with task-specific requirements and identified and fixed one such incompatibility in dialogue with an augmentation strategy.\n2. Demonstrated the importance of contradiction probability for scoring and that the underlying mechanism is the high reliability of NLI contradiction scores for detecting unfaithfulness 3. Shown that using Monte-Carlo dropout improves metric performance.\nOur improved NLI model significantly improves over its baseline across many corpora and outperforms all competitors in average score on TRUE, while being much more efficient at inference. Our work suggests that strong improvements are possible for NLI-based faithfulness metrics, by combining data augmentation with adapted NLI score computation. We hope this finding will spurn advances in cheap and robust NLI for faithfulness. unclear whether our results would still provide advantages when applied to larger models such as T5-11B, whose parameter count makes experimentation infeasible on the hardware available to us.\n"}
{"question": "What did the authors do to eliminate the bad influence of the MT  system's ability to gender coordination?", "evidence": "  That is, the MT system's ability to learn gender coordination affects its ability to recognize tense structures, which in consequence affects the maintenance of tense consistency between original French text and predicted English sentence.  To better measure the tense-predicting capability of different MT systems, rather than their ability to recognize pronominal gender, we controlled for the gender variable by defaulting all pronouns, which do not indicate explicitly their genders, as masculine.  ", "options": ["A. They excluded all French sentences with gender-neutral pronouns.", "B. They assigned a default masculine gender to all pronouns lacking explicit gender indication.", "C. They relied on the MT system's ability to recognize pronominal gender.", "D. They manually corrected the gender coordination in French sentences."], "answer": "B", "content": "\nIntroduction\nTranslation tools are often found in a variety of social situations to enable cross-linguistic communication. Tenses are used to express time relative to the moment of speaking. Human translators frequently pay close attention to tense correspondence (Gagne and Wilton-Godberfforde, 2020) . Similarly, machine translation (MT) systems are supposed to maintain temporal consistency between the original text and the predicted text to avoid misunderstandings by users. However, accurately keeping the tense consistency is undoubtedly difficult. Taking French-English (one of the most classic language pairs for MT) as an example in Table 1 , the original text is in plus-que-parfait de l'indicatif of French, corresponding to the past perfect tense in English, while the English prediction provided by Google Translator is in the past simple tense.\nIn fact, this is not an isolated case. You can also find several examples in Appendix B. Besides. the translation mechanics may not the only reason leading to tense inconsistency. The corpora matter as well. For example, we have extracted 20,000 pairs English-French parellel sentences from the widely used dataset Europarl (Koehn, 2005) , and\n\nSentence\nTense FR: Mais on les avait vot\u00e9s lors de la derni\u00e8re p\u00e9riode de session.\n\nPlus-queparfait\nEN: But we voted on them during the last part-session.\n\nPast simple\nCorrection: But we had voted on them during the last part-session. we have observed all groups of parallel utterances where the original French texts are in the plus-queparfait de l'indicatif tense, examining the tenses of their English counterparts. As a sentence may include several tenses, there are 195 occurences of plus-que-parfait tense in total. Among them, only 35.28% English sentences are in the correct past perfect tense, as shown in Table 2 . Although, compared to other tense correspondences, the pair of plus-que-parfait and past-perfect is prone to error in datasets and there are only 0.94% of sentences in Europarl are in plus-que-parfait, we cannot easily ignore this issue. Like Europarl, tense correspondences are generally credible but unreasonable for certain tenses in several common datasets. In addition to the train set, the difficulty of remaining tense consistency also stems from the lack of metrics on measuring the model's mastery of tense information. The research of Marie et al. (2021) shows that 98.8% of *ACL papers 2 in the field of MT from 2010 to 2020 used BLEU (Papineni et al., 2002) scores to evaluate their models. However, the reliability of BLEU has been questioned in the era of neural machine translation (NMT) as its variants only assess surface linguistic features (Shterionov et al., 2018) , and many studies have shown that BLEU has difficulty in portraying the degree of semantic information mastered by the model, i.e. its score does not necessarily improve when more semantic information is mastered (Mathur et al., 2020; He et al., 2023) , not to mention specific tense information. We have also applied BLEU to measure various baselines on our tense test set in Section 4, and the results explicitly support the above statement. In addition, reviewing the evaluation criteria related to MT tasks over the past ten years, we are surprised to find that there are no criteria to assess the model's mastery of tense prediction from a linguistic perspective.\n\nPast perfect\nTherefore, our paper is devoted to the study of NMT based on semantic understanding in terms of tense. We construct a tense parallel corpus test set consisting of 552 pairs of tense-rich, error-prone parallel utterances for NMT systems, and then propose a new task for evaluating the effectiveness of model translations from the perspective of tense consistency. This paper makes three contributions:\n(1) the presentation of the construction of the tense test set, including its tense labels; (2) the proposal of a feasible and reproducible benchmark for measuring the tense consistency performance of NMT systems; and (3) the various experiments for different baselines with the above test set and corresponding benchmark.\n\nAnnotation Rules and Tools\nAs the first work of the MT tense study, we choose English-French, one of the most classic language pairs of MT, to construct the dataset 3 . TENSE, the dominant topic of our research, is a combination of tense and aspect. In the modern grammar system of English, \"a tense system is a system associated with the verb where the basic contrasts in meaning have to do with the location in time of the situation, or the part of it under consideration\" (Huddleston et al., 2021) . The modern grammatical system divides tense into present and preterit based on the inflections added to the end of verbs, and the aspect into perfective and progressive on the state where an action is (Kamp, 1991) . While this tense classification system is too crude for daily life, we therefore apply the following classification methods. On the one hand, we classify the tenses according to the macro-temporal interval of the action into three major time intervals, namely present, past and future tenses; on the other hand, we classify the tenses according to the state of the action into general, progressive and perfect aspects. Hence, 9 kinds of tenses are born through combining the three tenses and the three aspects.\nFrench and English belong to the same Indo-European language family and share many similarities in various respects. The main difference is that in French there is another grammatical point called mode, part of which is like the aspect in English. In terms of tenses, we will generally discuss the tenses in the indicative mode of French and will describe the others later in this section. In the following, if there is no mode qualifier before a tense, it is by default in the indicative mode. Careful identification and comparison of the subdivided tenses in the three main tense intervals, English and French, reveals a very similar usage of the tenses, as sum-marised in Table 3 . As there is no progressive tense in French, we do not distinguish the progressive tense in English, but rather merge the progressive tense into its corresponding base tense, e.g. the present perfect progressive tense into the category of the present perfect tense.\nWhen discussing tenses from a semantic point of view, the modes also need to be taken into account. The grammatical correlations between French and English modes are quite complicated. Considering the corresponding grammatical expressions of 2 modes strongly related to tense, conditionnel and subjonctif, in French rely on the usage of modal verbs, we introduce modal verbs to simplify the distinguishment of the modes.\nBased on these grammatical rules, we merge the nine common tenses in English into seven categories that correspond reasonably and rigorously to French, namely the 6 tense categories of past/present/future + simple/perfect and statements containing modal verbs that correspond to the French subjonctif and conditionnel tenses. We construct an automatic annotation method based on the spaCy package (Honnibal et al., 2020) . First, we label the grammatical components of each word in the sentence based on the spaCy package, and then we define and compare the grammatical structures of the verb phrases with the structures of each tense classification to derive the sentence tense labels. During this process, to simplify the annotation process and better correspond with French futur proche tense, we classify the expression 'be going to do', grammatically in Future tense, into the Present tense, just like expressions 'be about to do' and 'be + verb progressive', whose stucture are in Present tense but the real meaning is about the close future. Also, a sentence may have several tense structures, in this case, the tense label consists several tenses. For example, the label of the sentence 'So it is in that spirit that we have made this change.' is 'Present+PrePerfect'.\n\nCorpus Design\nWe choose the tense-rich Europarl, namely Eu-roparlPV, processed by Lo\u00e1iciga et al. (2014) as the source corpus, for it contains all the sentences with predicate verb structures in the original Europarl dataset (Koehn, 2005) . First, we cleaned the source corpus, including deleting sentences without counterparts, English sentences in the French In the construction process, with the code mentioned in Section 2, we first automatically annotated the original English text and English prediction in the 20,000 pairs of parallel utterances, given the corresponding tense labels. Then, we filtered 6,779 parallel French-English sentence triples with different tense labels for English originals and predictions. On the basis of the automatic selection, we manually screened out the representative parallel French-English sentence pairs with a certain degree of translation difficulty and a complex grammatical structure. We also corrected the reference translations that did not justify the tense or semantics. It is worth noting that the author has a level of English and French that meets the C1 standard of The Common European Framework of Reference for Languages (CEFR), representing the ability to express herself effectively and flexibly in English and French in social, academic and work situations. A total of 570 parallel pairs of statements were selected at this stage.\nFollowing this, two other reviewers at CEFR C1 level, reviewed the tense test set for semantic and tense correspondence, and the tense labels marked by the automatic annotation code. \n\nCorpus Characteristics\nIn the following paragraphs, we describe the statistical features of our corpus and the elimination of gender coordination influence.\nTense distribution. The corpus consists of 780 tense structures in 552 sentences, and the distribution of tense classifications is shown in Table 4 . In the test set, sentences in present tense are the most, corresponding the situation of the reality: we use present tense most frequently and future perfect sense least frequently.\nElimination of gender effect. Unlike English, gender coordination exists in French. For example, the French sentences 'Nous nous sommes donc abstenus.' and 'Nous nous sommes donc abstenues.' both correspond to the English 'We therefore abstained.'. That is, the MT system's ability to learn gender coordination affects its ability to recognize tense structures, which in consequence affects the maintenance of tense consistency between original French text and predicted English sentence. Therefore, to better measure the tense-predicting capability of different MT systems, rather than their ability to recognize pronominal gender, we controlled for the gender variable by defaulting all pronouns, which do not indicate explicitly their genders, as masculine. These pronouns consists of 167 je (I), 114 nous (we, us) and 28 vous (you).\n\nExperimental Results\nTo measure the tense consistency performance of different systems, we introduce a benchmark called tense (prediction) accuracy, as shown in Eq. ( 1).\nEQUATION\nwhere N c is the number of predicted utterances with the same tense as its reference and N t is the total number of utterances in the tense set.\nTo verify the validity of our tense corpus, the following approach was adopted: To begin with, 100, 000 parallel utterance pairs from the Eu-roparlTR (containing 201, 374 pairs) mentioned in Section 3.1 were extracted as the tense-rich train set, and 100, 000 parallel utterance pairs from the Europarl corpus (Koehn, 2005) were extracted as the tense-poor train set. There were no overlapping utterances between the latter and the former. We performed the same preprocessing procedure, including data cleaning, tokenization and BPE coding. We then trained four pairs of French-English NMT systems with different architectures based on fairseq (Ott et al., 2019) , where two systems in each pair differed only in the train set. After this, we summarized the scores evaluated by Sacre-BLEU (Post, 2018) and COMET (Rei et al., 2020) and tense prediction accuracies of the eight systems on different test sets. We have applied three types of test sets: our tense set, the Europarl test set and the WMT15 test set. The Europarl test set contains 3,000 parallel utterance pairs drawn from the Europarl corpus, the exact same field of train set, while the WMT15 is a test set for the WMT15 (Bojar et al., 2015) , deriving from data in the different field of train set. Besides, we also apply our approach to mesure the tense consistency performance of several business translators, includ-ing Bing Translator, DeepL Translator and Google Translator. The results are listed in Table 5: 1) The BLEU and COMET scores based on the Europarl set and the WMT15 set are quite similar for each system pair, which indicates that the translation capabilities of the two systems are similar in the general evaluation dimension. This suggests that by relying solely on the difference in BLEU scores on traditional test sets, we are unable to measure the tense prediction ability of the systems.\n2) However, there are large differences in our tense set. The tense consistency performance of systems trained on the tense-rich train set was significantly better than that of systems trained on the tense-poor train set. This indicates that our tense set can capture the tense consistency performance.\n3) Further investigation of the BLEU or COMET) scores and tense prediction accuracy for each system reveals their positive correlation for the same architecture, but not across architectures. To measure the tense consistency performance across different architectures, we should focus more on tense accuracy rather than BLEU scores only.\n\nConclusion\nWe presented the French-English parallel tense test set and introduced the corresponding benchmark tense prediction accuracy, providing a brand-new approach to measure the tense consistency performance of machine translation systems. This test set firstly focuses on the tense prediction ability, posing a new dimension to improve the MT quality.\nIn the future, we will endeavour to generalize the test set to other languages. Considering there are statements like \"the use of tense A in language X is equivalent or similar to the use of tense B in English\" in grammar books of other languages (Durrell et al., 2015) , even across language families(Gadalla, 2017) and human translators also apply such rules(Santos, 2016), we are confident in taking this forward.\n"}
{"question": "What is the conclusion regarding the use of different datasets in the paper's experiments?", "evidence": "  The best practice is to train the model on ITERATER-FULL in the multi-prefix tuning stage and on ITERATER-HUMAN in the prefix transfer stage, which gets the highest SARI score and average score. This may be because of the different distributions of manually annotated edit intent and automatically annotated edit intent.  ", "options": ["A. Training on ITERATER-FULL in both stages is the best practice.", "B. Training on ITERATER-HUMAN in both stages is the best practice.", "C. The choice of training data in different stages doesn't significantly affect the results.", "D. Training on either dataset results in the same performance. "], "answer": "B", "content": "\nIntroduction\nRevision is an essential process to improve the text quality (Vaughan and McDonald, 1986) . During this process, writers perform various editing operations on the text with different editing intentions. As shown in Figure 1 , the writer corrects misspelled words to improve text fluency, deletes redundant words to improve text clarity, adds connective words to improve text coherence, inserts adverbs to convey the writer's writing preferences (style) and modifies data to update text information (meaning-changed).\nLots of recent studies have focused on a text revision task corresponding to a specific edit intention, such as grammatical error correction (Omelianchuk She went to the markt\nThe changes made the paper better than before.\nText Revision She works hard.\nShe is successful.\nEverything was rotten.\n\nShe went to the markt market\nThe changes made the paper better than before improved the paper.\nShe works hard. She; therefore, she is successful.\nEverything was awfully rotten. This method improves the model accuracy from 64% to 7883%. et al., 2020; Kaneko et al., 2020; Liu et al., 2021; Yang et al., 2022 ), text simplification (Dong et al., 2019; Jiang et al., 2020; Omelianchuk et al., 2021; Martin et al., 2022) , and text style transfer (Malmi et al., 2020; Reid and Zhong, 2021) . The work divides text revision into several independent problems. While some methods with strong universality can be applied to multiple tasks (Malmi et al., 2019; Stahlberg and Kumar, 2020; Mallinson et al., 2020) , they train different models on various data sets. Real-world scenarios require addressing multiple types of editing errors at the same time, such as grammatical errors, spelling errors, etc. But these methods failed to integrate knowledge from these tasks into a unified model.\n\nmeaningchanged\nTo solve the problem, Du et al. (2022) attempted to train one model using data with multiple editing intentions and leveraged edit intent information by simply appending it to the input. However, when adding a new intent, the entire model must be re-trained. A more lightweight and scalable approach to multi-intent text revision is still required.\nLi and Liang (2021) proposed a new kind of prompt tuning method to quickly adapt a pretrained model to new tasks, which is called prefixtuning. Prompt tuning can help the pre-trained language model to locate the task learned in pretraining and enable the related knowledge to model text revision with different edit intentions (Reynolds and McDonell, 2021) . This method enables a model to handle multiple edit intentions in a lightweight and scalable way.\nIn this paper, we present our method: a prefixtuning-based model which adapts to text revision with multiple edit intentions. This method involves a two-step training process. In the first step, we initialize a pre-trained language model (PLM) and train multiple prefixes on it. Each edit intention corresponds to a prefix. In the second step, a prefix transfer module is trained at each attention layer of the PLM. The prefix transfer module is configured as two attention units that act respectively on this layer's key states and value states. It enables our model to learn a tailored prefix for the given input with the help of prefix embeddings from the predefined tasks.\nWe conduct experiments on ITERATER (Du et al., 2022) , an iterative text revision dataset. It mainly contains parallel sentences with five edit intentions: fluency, coherence, clarity, style, and meaning-changed. The results show that our approach performs better than the fully fine-tuned BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020) baselines reported in Du et al. (2022) with fewer training parameters.\n\nIterative Text Revision\nFor the first time, Du et al. (2022) systematically studied the iterative revision phenomenon in human writing. They presented the ITERATER, an annotated dataset across multiple domains of formally human-written text, which includes Wikipedia, ArXiv, and Wikinews. And they trained several types of text revision models using ITERATER. Dwivedi-Yu et al. (2022) presented EDITEVAL, an instruction-based benchmark, to evaluate the editing capabilities of models and they also included the test set of ITERATER in it. Based on Du et al. (2022) , our work further explores the method of text revision.\n\nTransfer Learning of Prompt Tuning\nTransfer learning is a common and powerful technique in NLP (Raffel et al., 2020) . Some recent studies have tried to improve prompt tuning performance by leveraging the knowledge of multiple related or unrelated tasks. Asai et al. (2022) used an attention module to make use of the knowledge in exiting soft prompts (Lester et al., 2021) while learning a new task. Chen et al. (2022) improved the few-shot text summarization by multi-task pretraining and prefix-tuning. Specifically, they pretrained a summarization model on a set of popular summarization datasets and then conducted prefixtuning for it on an unseen summarization task. Different from their modeling of a new task through existing tasks, our work aims to achieve the mutual utilization of knowledge between different edit intents in text revision.\n\nMethod\nThe revision task can be defined as the following process: given a source sentence x = [x 1 , . . . , x m ] and an optional edit intent e \u2208 E to generate a revised sentence y = [y 1 , . . . , y n ], where E is the set of all edit intentions. Note that e is optional because it can be inferred from the input x.\nOur method is depicted in Figure 2 . It includes two stages: the multi-prefix tuning stage and the prefix transfer stage.\n\nMulti-Prefix Tuning Stage\nThe prefix is a set of parameters on every attention layer of PLM. For an edit intention e, at each attention layer, the prefix can be described as P e = {P K e , P V e }, where P K e and P V e are parameters added before the key states and value states in this attention layer. After adding these parameters, the calculation of the attention head in this layer becomes:\nH = Attention(Q, [P K e ; K], [P V e ; V ]) (1)\nwhere H is the output vector sequence; Q, K, V are query states, key states, and value states, respectively; Attention means scaled dot-product attention. Only P K e and P V e are updated during the training process. Note that we ignore the layer number information because the operation for each layer is the same.\nAs shown in the left part of Figure 2 , for every edit intention e, we train a prefix P e accordingly. In this way, the model could revise an intentionannotated text by activating the corresponding prefix at inference.\n\nPrefix Transfer Stage\nIdentifying edit intention is always an ambiguous work. At the prefix transfer stage, we aim to build a new prefix for an unannotated input instance by transferring existing prefixes. The new prefix P new is instance-specific.\nThe prefix transfer stage is described in the right part of Figure 2 . At each layer, we rearrange the prefixes {P e | e \u2208 E} obtained in the last stage as\nP K = {P K\ne | e \u2208 E} and P V = {P V e | e \u2208 E} according to whether they are configured before the key states or before the value states. Then a pair of attention units G K and G V are trained for P K and P V .\nTake G K as an example. It calculates the similarity between the key states K and every P K e in P K to get attention scores.\nThe similarity can't be calculated directly, because K and P K e have different lengths. So we perform the max-pool operation for length dimension on K and P K e . After that, we obtain K \u2208 R d and P K e \u2208 R d , where d is the dimension of the hidden states in the PLM.\nTo get attention scores, we train a fully connected layer to extract features from K:\nEQUATION\nwhere W \u2208 R d\u00d7d is a transfer matrix updated during training. Following Asai et al. (2022) , we use SiLU (Elfwing et al., 2018) for the non-linear layer and add a Layer Norm (Ba et al., 2016) layer:\nEQUATION\nThen, we calculate the attention scores for intent e as follows:\nEQUATION\n)\nwhere T is the softmax temperature (Radford et al., 2021) which could avoid making the attention unit over-confident.\nFinally we use them to build P K new as follows:\nEQUATION\nIn the same way, we get P V new by G V . Using the new prefix P new = [P K new , P V new ], our system could revise the unannotated input instance with the knowledge from existing prefixes.\n\nExperimental Setup\nWe choose BART-large as the PLM for our system and use adapter-transformers (Pfeiffer et al., 2020) to implement prefix-tuning. More implementation details are in Appendix A.\n\nDatasets\nWe conduct our experiments on the iterative text revision dataset: ITERATER (Du et al., 2022) . We remove the Other class of the data as it essentially contains a variety of unrecognized edit intentions and accounts for a small proportion (1.44%). The entire dataset consists of two parts: ITERATER-HUMAN and ITERATER-FULL. The former is a smaller dataset with manual annotation of edit intentions, while the latter is a large dataset annotated by a classification model trained on ITERATER-HUMAN. We train our model on both of them. Following Du et al. (2022) , we report the results on the test set of ITERATER-HUMAN in Section 5, which is completely a human-created dataset and is reliable for evaluation. We show more details of the datasets in Appendix B.\n\nEvaluation Metrics\nFollowing previous work, we report three metrics: SARI (Xu et al., 2016) , Rouge-L (Lin, 2004) , and BLEU (Papineni et al., 2002) . Among them, SARI is considered an important metric in situations where input text and output text have a large overlap in words. It also indicates the positive impact of revisions on document quality. The setting of evaluation metrics is the same as Du et al. (2022) . We use the metrics package from Huggingface transformers (Wolf et al., 2020) to calculate the SARI, BLEU, and Rouge-L scores.\n\nModels Setup and Baselines\nUsing our method, we train the models in two ways: the model that only trains the multi-prefix tuning stage and that trains both the multi-prefix tuning stage and the prefix transfer stage.\nWe compare our method with three baselines: full fine-tuning BART (BART-FineTune), full finetuning PEGASUS (PEGASUS-FineTune), and prefixtuning of BART with a single prefix (BART-SinglePrefix). Both BART and PEGASUS are generative models based on the transformer architecture. Compared to the edit-based model FELIX, they perform better. We use the results reported by Du et al. (2022) for these two models. Furthermore, we compare BART-SinglePrefix as a possible technical solution as we choose BART as our backbone model. BART-SinglePrefix trains only one prefix on the entire dataset.\nAll three baselines are trained with two config-urations. The first configuration is using the pure sentence pairs without edit intention annotations to train the model. The second configuration is appending an edit intent token at the beginning of the input text during the training process, which is the same as the approach of Du et al. (2022) .\n5 Results and Analysis\n\nMain Results\nThe main results are shown in Table 1 . Compared to training with a single prefix, the setting of multiple prefixes can improve the results, especially training on ITERATER-HUMAN. Meanwhile, with fewer training parameters, the multi-prefix setting could achieve a comparable SARI score and better average score than the fully fine-tuned BART and PEGASUS baselines. Moreover, prefix transfer could further improve the model's performance. Training on ITERATER-HUMAN, prefix transfer significantly improves the SARI score from 33.12 to 36.01 and gets the highest average score of 67.91. Training on ITERATER-FULL, prefix transfer can also improve the average score from 67.23 to 68.36.\nAn interesting phenomenon is that training on different datasets results in different gains for prefix transfer in evaluation metrics. On ITERATER-HUMAN, prefix transfer improves the SARI score significantly. While on ITERATER-FULL, prefix transfer mainly improves the BLEU score and Rouge-L score. One possible explanation is that in situations when the training data is small, prefix transfer tends to learn more editing operations to improve text quality. In this way, the SARI score related to editing operations will be improved significantly. When the training data is sufficient, pre- fix transfer will model the gold reference in more detail. So the BLEU score and the Rouge-L score will be improved.\n\nAnalysis\nWe further tried to use different training data at different stages of training to conduct experiments.\nThe results are shown in Table 2 . We find that the best practice is to train the model on ITERATER-FULL in the multi-prefix tuning stage and on ITERATER-HUMAN in the prefix transfer stage, which gets the highest SARI score and average score. This may be because of the different distributions of manually annotated edit intent and automatically annotated edit intent. The auto-annotated dataset ITERATER-FULL contains many incorrectly classified sentences, which may cause mismatched knowledge in prefixes. In the prefix transfer stage, due to the existence of mismatched knowledge and incorrectly classified sentences, the continued use of the same training data may finally cause a certain degree of negative transfer. However, if we use ITERATER-HUMAN in the prefix transfer stage, the impact of negative transfer will be mitigated, because ITERATER-HUMAN only contains correctly classified sentences.\nIn Appendix C, we separately provide the performance results on different edit intentions of the best-performing model.\n\nConclusion\nIn this paper, we introduce a new method for multiintent text revision. The system is based on prefixtuning, which first obtains a prefix for every edit intention and then learns to transfer the knowledge in prefixes for every input instance by training a prefix transfer module. This prefix transfer module is configured as two attention units that act respectively on the key states and the value states at each attention layer of the PLM. In this way, our method can make full use of the knowledge of various edit intentions and does not need to anno-tate the intentions of the input. The experimental results show that our method significantly outperforms baselines, and both multi-prefix and prefix transfer settings could improve the performance.\n"}
{"question": "What is one of the limitations mentioned regarding increasing the number of GD steps in the multiple-step setting for dataset distillation in the paper?", "evidence": "  In addition, the basic dataset distillation algorithm we used requires computing the back propagation through all GD steps for the optimization of the distilled dataset, which increases memory and computational costs linearly with T. Therefore, it was difficult to increase T to be larger than 5 in our experiments.  ", "options": ["A. Increasing GD steps significantly improves performance.", "B. The role of the distilled data is the same in all GD steps.", "C. Memory and computational costs increase linearly with the number of GD steps.", "D. GD steps have no impact on the distilled dataset. "], "answer": "C", "content": "\nIntroduction\nDeep learning models have achieved state-ofthe-art performance in various fields, including computer vision and natural language processing (NLP), using large-scale neural networks trained with huge datasets. Unfortunately, their successful performances have come with massive training costs, including training time, GPU resources, and energy consumption. To reduce the training costs, current research has been focusing on constructing a small training dataset such that models trained with it can achieve comparable performances to models trained with the whole original dataset.\nOne classical way to compress the training dataset is data selection. Data selection methods choose a subset of effective training samples on the basis of a number of heuristic measures, for example, cluster centers (Sener and Savarese, 2018) , diversity (Aljundi et al., 2019) , and likelihood of models (Moore and Lewis, 2010) . Although the data selection methods effectively work for efficient model training and several applications, such as active learning (Sener and Savarese, 2018) and continual learning (Aljundi et al., 2019) , their performance is clearly restricted because they rely on the existence of representative samples that are effective for model training in the original dataset.\nAs an alternative approach for reducing the training dataset, Wang et al. (2018b) proposed dataset distillation, which aims to create a small number of synthetic samples optimized to effectively train models. Dataset distillation has attracted much attention in machine learning (Wang et al., 2018b; Zhao et al., 2021; Zhao and Bilen, 2021; Sucholutsky and Schonlau, 2021; Bohdal et al., 2020; Wang et al., 2022; Cazenavette et al., 2022) for both the theoretical interest and various applications, such as neural architecture/hyper-parameter search (Such et al., 2020) , continual learning (Masarczyk and Tautkute, 2020; Rosasco et al., 2022) , federated learning (Goetz and Tewari, 2020; Zhou et al., 2020) , and preserving data privacy (Li et al., 2020; Dong et al., 2022) .\nHowever, most of the existing research on dataset distillation mainly focuses on image datasets, and only a few studies involve NLP tasks. Sucholutsky and Schonlau (2021) and Li and Li (2021) extended dataset distillation to text datasets by using embedding vectors as an input of the distilled dataset instead of discrete text. While these studies applied dataset distillation to those model architectures based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs), we cannot find any research that tackles dataset distillation for pre-trained transformers, such as BERT (Devlin et al., 2019) , which have become the de-facto standard for various kinds of NLP tasks. Therefore, in this paper, we aim to obtain distilled few-shot datasets to fine-tune the pre-trained transformers for NLP tasks.\nTo this end, we focus on the attention mechanism, which is the core component of transformers (Vaswani et al., 2017) . Several current studies utilized supervision of the attention probabilities to effectively train the model (Liu et al., 2016; Mi et al., 2016) . Moreover, it is also used for the model distillation to efficiently transfer the knowledge of a transformer model to another one via attention probabilities (Aguilar et al., 2020; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2020 Wang et al., , 2021)) . Inspired by this, we propose distilled attention labels, which are the supervision of attention probabilities optimized as a part of the distilled dataset, to enhance the effectiveness of the distilled dataset for training the transformer models.\nIn our experiments, we constructed distilled fewshot datasets to fine-tune BERT (Devlin et al., 2019) in various types of NLP tasks: AGNews (text classification), SST-2 (sentiment analysis), QNLI (QA/NLI), and MRPC (paraphrase identification).\nOur main contributions are as follows: (i) To the best of our knowledge, this is the first work to explore dataset distillation for pre-trained transformers. Specifically, we demonstrate that our distilled datasets effectively fine-tune BERT even with only one sample for each class and only one gradient step. (ii) We present the distilled attention labels, which can easily be applied to dataset distillation for transformer architectures. Experimental results show that they consistently improved the performance with the distilled datasets in various types of NLP tasks. (iii) We open our source code and the distilled datasets obtained through our experiments to facilitate further research. 1 2 Methodology\n\nDataset Distillation\nIn this section, we explain the basic approach of dataset distillation (Wang et al., 2018b) , which aims to optimize a synthetic dataset through the gradient method similar to the current meta-learning approach (Finn et al., 2017) .\nLet the original training dataset D = {(x i , y i )} N i=1 , where (x i , y i ) is a pair of an input and its class label. Our goal is to optimize a distilled dataset D = {(x i , \u1ef9i )} M i=1 , which is randomly initialized at first, with M \u226a N .\nThe model parameters \u03b8 are updated with a minibatch of the distilled dataset (x t , \u1ef9t ) by gradient 1 https://github.com/arumaekawa/ dataset-distillation-with-attention-labels descent (GD) steps as follows:\nEQUATION\nwhere L() is a twice-differentiable loss function and \u03b7 is the learnable learning rate of the model, which is optimized together with D. Given initial model parameters \u03b8 0 , we can represent the model trained with the distilled dataset D, with the number of GD steps T , as\nEQUATION\nwhere F () is the training procedure of the T steps for the GD updating (Eq. 1).\nAs the goal of dataset distillation is that \u03b8 T performs well on the original dataset, the optimization objective of the distilled dataset D is calculated as follows:\nEQUATION\nwhere (x t , y t ) is a mini-batch of the original training dataset. Therefore, the optimization problem for dataset distillation is formulated as\nD * , \u03b7 * = arg min D,\u03b7 E \u03b8 0 \u223cp(\u03b8 0 ) L distill ( D, \u03b7; \u03b8 0 ) ,\n(5) where p(\u03b8 0 ) is the distribution of \u03b8 0 .\nWe optimize the distilled dataset D with this objective by using current gradient-based optimization techniques, e.g., Adam (Kingma and Ba, 2015) . However, the discrete nature of text data makes it difficult to apply the gradient methods directly. Inspired by previous work (Sucholutsky and Schonlau, 2021; Li and Li, 2021) , we use a sequence of embedding vectors for inputs of the distilled dataset instead of text as it is. Using the embeddings makes the loss L distill differentiable with respect to D, and we can thus optimize the distilled dataset D by the gradient methods.\n\nDistilled Soft Labels\nThe class labels of the original dataset are usually discrete hard labels (i.e., one-hot labels representing only a single class). Instead of hard labels, we can use soft labels for distilled datasets and optimize them with the input embeddings. Using soft labels enables the distilled datasets to contain more information. Following previous work (Sucholutsky and Schonlau, 2021; Bohdal et al., 2020) , we first initialize the soft labels with one-hot values and enable them to take any real values. We can now optimize the soft labels through the gradient method as well as the input embeddings.\n\nDistilled Attention Labels\nFor efficient knowledge transfer to transformer models via training with the distilled dataset, we propose attention labels, which are optimized to guide the multi-head attention module of the transformer models.\nInspired by previous work (Aguilar et al., 2020; Wang et al., 2020 Wang et al., , 2021)) , we compute the Kullback-Leibler (KL) divergence D KL between the selfattention probabilities of the model a(\u03b8) and the distilled attention labels \u00e3 across all layers and heads. The attention loss L attn is computed as follows:\nEQUATION\nwhere \u00e3k,h and a k,h (\u03b8) are the attention maps for the h-th head of the k-th layer of the distilled attention labels and the model, respectively, K is the number of layers, and H is the number of heads. Due to the data size, we consider the attention probabilities only for the first input token ([CLS]).\nWe train the model to minimize L task and L attn at the same time. Thus, the GD updating of the model (Eq. 1) is modified as\nEQUATION\nwhere \u03bb is the balance weight for L attn .\nThe attention labels \u00e3 are first initialized randomly and restricted to being a valid probability distribution (i.e., non-negative and the sum equals 1) by applying the softmax function to real-valued vectors. We optimize the attention labels together with the input embeddings and the soft labels by the gradient method. The details of the step-by-step procedure of our distillation algorithm are shown in Appendix A.\n\nSettings\nDatasets. We evaluated our dataset distillation methods in various types of NLP tasks. We used a text classification task (AGNews (Zhang et al., 2015) ) and three different natural language understanding tasks (SST-2, QNLI, and MRPC) from the GLUE benchmark (Wang et al., 2018a) . For the evaluation metrics, we used accuracy for AGNews.\nFor the other three tasks, we followed the evaluation settings of GLUE (Wang et al., 2018a) . The statistics of each benchmark dataset are summarized in Table 1 . Network Architecture. To evaluate the dataset distillation methods, we constructed distilled fewshot datasets to fine-tune BERT (Devlin et al., 2019) , which is the first pre-trained transformer model, that all subsequent models are based on. We utilized the pre-trained BERT BASE model. Following the fine-tuning procedure in Devlin et al.\n(2019), we introduced additional classification layer weights W \u2208 R C\u00d7D on the last hidden state of the [CLS] token, where D is the hidden dimension of BERT and C is the number of classes.\nImplementation. For all our distilled datasets, we used Adam optimizer (Kingma and Ba, 2015) with a learning rate \u03b1 \u2208 {1e \u22123 , 1e \u22122 , 1e \u22121 } and trained the distilled datasets for 30 epochs. We initialized the learnable learning rate \u03b7 \u2208 {1e \u22122 , 1e \u22121 }. For the attention labels, we set \u03bb = 1.0, which performed well in our preliminary experiments. We report the results for the best performing combination of \u03b1 and \u03b7. Note that due to the coarse granularity of the search, there is no need to care about overfitting to the test set. More details of our implementation are shown in Appendix B. Evaluation. To evaluate the distilled datasets, we fine-tuned the BERT model with them for 100 times, where the additional parameters W were randomly initialized each time. In all our experiments, we report the mean and standard deviation over the 100 evaluation results.\n\nResults for 1-shot and 1-step Setting\nWe first evaluated the dataset distillation methods with a 1-shot and 1-step setting, where the distilled dataset includes only one sample per class, and BERT was fine-tuned with it by only one GD step. We compared the performance for hard/soft labels and with/without attention labels for each task.\nTable 2 shows the evaluation results. The distilled datasets with the hard labels, i.e., only optimizing the input embeddings and not applying the attention labels, still achieved 87.4, 81.6, and 68.6 for AGNews, SST-2, and QNLI, respectively, which is 92.4, 88.0, and 74.7% performance of the full dataset. Furthermore, using the soft labels further improved these performances, especially by almost 8 points for QNLI. However, for MRPC, the distilled dataset achieved only the same performance as the majority class baseline regardless of the use of the soft labels.\nWhen applying the attention labels, the performance of the distilled dataset was significantly improved for all tasks, and their effect is much greater than the soft labels. Specifically, our distilled dataset with the attention labels yielded up to 98.5, 97.2, 94.1, and 88 .9% performance of the full dataset for AGNews, SST-2, QNLI, and MRPC, respectively. These results indicate that using the attention labels enables to extract the information from the original dataset as the attention probabilities and to efficiently transfer it to the model.\nWhen comparing the performance between the four tasks, dataset distillation performed very well on relatively simple classification tasks such as AGNews and SST-2, while the performance was somewhat limited on QNLI and MRPC, which require understanding the relationship between two sentences. In particular, for MRPC, although the performance was improved by applying the attention labels, the gap from the full dataset was still larger than that in the other three tasks. The class imbalance in the original training dataset (68% positive) may make the training of the distilled dataset more difficult. We can say there is still room for performance improvement by dealing with this issue (e.g., by upsampling or downsampling).\n\nResults for Multiple-shot and Multiple-step Setting\nWe also evaluated the distilled datasets with more than one shot and more than one GD step to finetune BERT. For the multiple-step setting, we considered two different scenarios: using the same distilled data in all steps and using different distilled data for each step. In these experiments, we evaluated the distilled datasets that use soft labels and attention labels for different numbers of GD steps T \u2208 {1, 3, 5}.\nTable 3 shows the results for the multiple-shot and multiple-step setting. In the single-step setting, overall performance improved with the number of shots of the distilled data. We believe that this is simply due to the expressiveness of the distilled data improved with the size of them. When using the same distilled data for all steps in the multiple-step setting, the performance of the distilled datasets degraded even compared with that in the single-step setting. In contrast, the performance was improved by separating the distilled data for each step and slightly but better than that with the same number of shots in the single-step setting. These results suggest that the role of the distilled data is different between the earlier and later steps, and it is difficult to obtain the distilled data that are generally useful for all GD steps.\nIn addition, the basic dataset distillation algorithm we used requires computing the back propagation through all GD steps for the optimization of the distilled dataset, which increases memory and computational costs linearly with T . Therefore, it was difficult to increase T to be larger than 5 in our experiments. This is the limitation of our dataset distillation method, and it needs further improvement to scale to more complex tasks or to train models from scratch.\n\nConclusion\nIn this paper, we explored dataset distillation in NLP tasks to fine-tune pre-trained transformers. We proposed attention labels, which are the supervision of attention probabilities distilled as a part of the distilled datasets. Experimental results across various tasks demonstrate that our distilled fewshot datasets achieved successful performances even with only one sample per class. Notably, the attention labels significantly improved the performance of the distilled datasets even for the tasks where dataset distillation is difficult without them.\n"}
{"question": "Which of the following is not a contribution to the article?", "evidence": "  In summary, our contributions are as follows:\n\u2022 We study PEFT for long documents and show that prefix-tuning is significantly inferior to fine-tuning in this scenario. To the best of our knowledge, this is the first work to focus on PEFT for long documents.\n\u2022 We introduce prefix-propagation, which consistently improves the performance over prefix turning on the different long document datasets, while using 50% fewer parameters.\n\u2022 We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated.\n\u2022 We elucidate the relationship between prefixpropagation and kernel attention and perform an ablation study that utilizes this insight.\nOur research focuses on parameter efficient tuning for long documents tasks. ", "options": ["A. We study PEFT for long documents and show that prefix-tuning is significantly inferior to fine-tuning in this scenario. To the best of our knowledge, this is the first work to focus on PEFT for long documents.", "B. We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated.", "C. We introduce prefix-propagation, which consistently improves the performance over prefix turning on the different long document datasets, while using 50% fewer parameters.", "D. Our research focuses on parameter efficient tuning for short documents tasks."], "answer": "D", "content": "\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017) has changed the landscape of recent natural language processing approaches by enabling the pretraining of state-of-the-art large language models (LLM) (Devlin et al., 2019; He et al., 2020; Brown et al., 2020) . However, fine-tuning and storing full copies of LLMs can consume prohibitively large quantities of resources. Parameter-efficient finetuning (PEFT) methods such as prefix-tuning (Li and Liang, 2021; He et al., 2021a; Liu et al., 2022) address these concerns by reducing the number of trainable parameters. Prefix-tuning can tune 0.01% of parameters and still match the performance of regular fine-tuning (updating all model parameters). PEFT has been investigated for tasks with inputs consisting of sentences, sentence-pair, or sequences that fit within the typical LLM maximum tokens. However, the performance of PEFT for tasks with longer textual sequences has been overlooked. In this work, we investigate this oversight and provide evidence suggesting that the gap between PEFT and regular fine-tuning is substantial when modelling long sequences. As shown in Table 1, prefix-tuning underperforms fine-tuning on long sequence classification tasks, Hyperpartisan (Kiesel et al., 2019) and 20-newsgroups (Lang, 1995) , when used with the popular long-document model Longformer (Beltagy et al., 2020) .\nIn this paper, we propose a simple and effective method, prefix-propagation, which consistently improves the performance of PEFT for long sequence models. Unlike prefix-tuning, prefix-propagation propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer.\nTo further understand prefix propagation, we investigate the reliability of the model's predictions by performing analyses on calibration. Lastly, we conduct study on prefix-based methods in terms of kernel attention to strengthen their theoretical value.\nIn summary, our contributions are as follows:\n... Figure 1 : Illustration of the differences between (a) prefix-propagation (ours) (b) and prefix-tuning (Liu et al., 2022; Li and Liang, 2021) . Blue blocks denote trainable prompts, and \"Transformer Layer\" represents the computation done in a layer of the pre-trained LLM. Note that in prefix-propagation (a), the summation of prefixes continues for layers beyond 3, up to n. This operation is encapsulated by the ellipses. In prefix-tuning (b), prefixes in subsequent layers do not depend on hidden states from past layers (they are simply overwritten).\n.\n\u2022 We study PEFT for long documents and show that prefix-tuning is significantly inferior to fine-tuning in this scenario. To the best of our knowledge, this is the first work to focus on PEFT for long documents.\n\u2022 We introduce prefix-propagation, which consistently improves the performance over prefix turning on the different long document datasets, while using 50% fewer parameters.\n\u2022 We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated.\n\u2022 We elucidate the relationship between prefixpropagation and kernel attention and perform an ablation study that utilizes this insight.\n\nRelated Works\nLong Sequence Models Numerous methods have been proposed to reduce the complexity of attention from O(n 2 ) to O(n) such as kernel approximations (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021) and fixed (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) or learned (Kitaev et al., 2020) sparse attention patterns. For a broader summary, please refer to Tay et al. (2022) . In this work, we use Longformer (Beltagy et al., 2020) . To linearize attention complexity, Longformer employs sliding window attention while globally attending to relatively few special tokens.\nParameter-Efficient Tuning Inspired by the success of manual prompting (Brown et al., 2020), prefix-tuning (Li and Liang, 2021; Liu et al., 2022) prepends trainable \"soft\" prompts to an input sequence. Although further PEFT methods have since been introduced (He et al., 2021a; Hu et al., 2021; Ben Zaken et al., 2022) , we focus on adapting prefix-tuning. We note that our adaptation does not violate orthogonality and thus prefixpropagation can still be compounded with other PEFT methods as proposed in the UnifiedPET framework (He et al., 2021a) , likely yielding similar performance gains. We leave the empirical validation of this hypothesis for future work.\nOut work also adheres to the key motivation of the recent PEFT method, inducer-tuning (Chen et al., 2022) , which is that optimal prefixes should be close to queries within their latent space. We derive queries, keys, and values from the same prefix token, limiting the distance that separates them.\n\nMethodology\nIn this section we introduce prefix-propagation, which, unlike prefix-tuning, propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer. Prefix-propagation and its predecessor, prefix-tuning are depicted in Figure 1a embeddings) to the input sequence (blue blocks in top left of Figure 1a ). Then, before every subsequent layer, we sum new trainable matrices onto the first j embeddings corresponding to the prefixes (denoted by the sum operators in Figure 1a ). By propagating instead of overwriting, we halve the number of parameters trained while simultaneously improving performance on long-document tasks.\nWe now formalize prefix-propagation. Multiheaded attention processes query, key, and value matrices derived from a sequence C \u2208 R m\u00d7d with length m and embeddings of size d. Our method modifies traditional attention by concatenating a prefix P \u2208 R j\u00d7d of length j to the sequence:\nH l,i = Attn(D (l) W (l,i) q , D (l) W (l,i) k , D (l) W (l,i) v ) (1) D (l) = cat(P (l) , C) if l = 1 cat(P (l) + C[:j, :], C[j:, :]) if l > 1 where inputs C are projected through pre-trained weight matrices W (l,i) q , W (l,i) k , W (l,i) v\n\u2208 R d\u00d7d h per layer l and head i yielding the output of the attention head, H \u2208 R (j+m)\u00d7d h . The prefixes are concatenated for the first layer (l = 1) and summed to their corresponding hidden states for the remaining layers (l > 1). We do not continually concatenate new prefixes to the sequence to avoid increasing the sequence length after each layer.\nFor both prefix-tuning and prefix-propagation, prefixes (keys and values) are globally attended to by all queries. Unlike prefix-tuning however, our method concatenates additional hidden states before the hidden states C are projected by\nW (i) k and W (i)\nv . By doing so, prefix-propagation modifies query matrices, allowing prefixes to attend to other hidden states globally, thereby increasing representation capability. This approach is somewhat analogous to the external global tokens inserted in the BigBird-ETC model (Zaheer et al., 2020) . By attending to other tokens, the prefixes can act as special storage tokens, which is particularly useful in the restricted regime of long-document modelling where relatively few tokens have global context. Conversely, prefix-tuning only concatenates trained key and value matrices, P k , P v \u2208 R j\u00d7d h , statically to the sequence:\nH l,i = Attn(CW (l,i) q , cat(P (l,i) k , CW (l,i) k ), cat(P (l,i) v , CW (l,i) v ))\n(2)\nSince our method has a single prefix matrix, P instead of separate P k and P v matrices, we reduce the number of trained parameters by 50%.\n\nCalibration\nWe further study the proposed prefix-propagation method to understand the reliability of model's predictions through calibration. Well-calibrated models output confidence scores that closely match the models' accuracy. Either over-confident or underconfident models are undesirable. Calibration has widely been overlooked in PEFT methods. To quantify calibration in our work, we use expected calibration error (ECE), which bins predictions based on model confidence and compares them to accuracy (Pakdaman Naeini et al., 2015; Guo et al., 2017) .\n\nKernel Decomposition\nTraditional attention is analogous to applying a kernel smoother over inputs (Tsai et al., 2019) .\nMotivated by this insight, we reformulate prefixpropagation as a sum of kernelized attention modules. Separating the modules introduces flexibility in two ways: (1) Their individual kernel forms can be mixed and matched and (2) A hyperparameter scale factor \u03b1 can be applied to the prefix component to increase or decrease its weighting. Equation 3 defines kernel decomposition for prefixpropagation 2 :\nH = Kern(cat(P, C)W q , CW k , CW v ) + (\u03b1)Kern(cat(P, C)W q , P W k , P W v ) (3)\nwhere Kern refers to kernel attention as formulated in (Tsai et al., 2019) . The first term results from attending to the original sequence, C, and the second comes from attending to the prefixes, P . We provide the derivation of Equation 3 and the full definition of kernel attention in Appendix A.\nOur main motivation for presenting prefix decomposition is to establish foundational knowledge and guide future research. Ergo, we restrict experiments in this initial presentation to using just the default exponential kernel (Appendix A).\n\nExperiments and Results\nDatasets We evaluate our approach on three long-document classification tasks: ArXiv (He et al., 2019) , an 11-class classification task composed of academic research papers, the 20-newsgroups (Lang, 1995) classification task consisting of mailing lists that fall into one of 20 classes, and the Hyperpartisan dataset, a binary classification task for extremist news classification (Kiesel et al., 2019) . We also run experiments on WikiHop (Welbl et al., 2018) , a long-document reading comprehension task requiring multi-step reasoning.\nDue to compute limitations inherent to working with long documents, with the exception of Hyperpartisan, we only report a single run for each task. This mimics the original Longformer reporting scheme (Beltagy et al., 2020) . For Hyperpartisan, the smallest of the datasets, we report mean metrics averaged over five seeds.\nBaselines As a baseline, we fine-tune Longformer-base (approx.\n149M parameters) as closely as possible to Beltagy et al. (2020) . For PEFT, we evaluate prefix-tuning on Longformer-base and RoBERTa-base (approx. 125M parameters) (Liu et al., 2019) . 2 We omit layer, l and head, i for brevity.\n\nMethod\nArXiv HY. NG. More details on dataset sizes, pre-processing, and hyperparameters are in Appendix B.\n\nResults and Discussion\nAcross all tasks, our results in Table 2 verify that prefix-tuning is inferior to fine-tuning long sequences. Conversely, prefix-propagation consistently outperforms prefix-tuning and is comparable to fine-tuning on most tasks. Prefix propagation also performs competitively on Hyperpartisan, a relatively small dataset with only 625 samples. This is in contrast to prefix-tuning, which is known to underperform in low-data settings (Gu et al., 2022) . Because we ran multiple seeds on Hyperpartisan, we also found that prefix-propagation's better performance relative to prefix-tuning is statistically significant (p < 0.05, using a single-tailed t-test). We do not have multiple samples to run these tests for larger datasets, but we emphasize that Hyperpartisan likely has the most variance and yet it is still statistically significant. We suspect that prefixpropagation's performance exceeds prefix-tuning because propagated prefixes can transmit global context across multiple layers, possibly modelling more expressive abstractions.\nWe note one exception where prefix-based methods still leave room for improvement: multiplechoice question answering on WikiHop. We hypothesize that prefix methods have insufficient capacity to properly model complex long-document multi-step question answering.\nWe also observe that prefix-based methods, and especially prefix-propagation, achieve better calibration than fine-tuning, as shown in Table 3 . Unlike prefix-tuning however, prefix-propagation effectively balances calibration with accuracy metrics. The calibration of fine-tuning deteriorates as training progresses (Figure 4 \n\nMicro F1\nFigure 2 : Violin plot of Micro F1 Score for five different seeds on the Hyperpartisan task. White dots, gray boxes, and gray lines are the medians, interquartile ranges, and ranges respectively. Width of the five violin shapes show the probability densities for the corresponding F1score. All methods tune Longformer-base except \"R Prefix\", which is prefix-tuning on RoBERTa-base.\nforgetting (Jagielski et al., 2022) .\nAs an initial test for our ongoing prefixpropagation kernel study, we show results on Hyperpartisan in Figure 2 . The kernelized version of prefix-propagation achieves the best single-run performance, but has higher variance than fine-tuning and prefix-propagation which necessitates further research.\n\nConclusion\nOur research focuses on parameter efficient tuning for long documents tasks. We introduce prefix-propagation, which consistently improves performance over prefix-turning on long document datasets, while using 50% fewer parameters. We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated. We lastly explicate prefix-propagation from a kernel perspective, uncovering insights for future PEFT research.\n"}
{"question": "What is the finding of the study regarding instruction tuning (IT) models' performance?", "evidence": "  Our experiments show that models trained on simplified task definition or delusive examples can achieve comparable performance to the ones trained on the original instructions and examples This suggests that the majority of score improvement from IT may come from model learning the output format, especially in low-resource settings. Our analysis provides evidence that the impressive performance gain of current IT models can come from picking up superficial patterns, such as learning the output format and guessing.  ", "options": ["A. Models trained on simplified task definitions outperform those trained on original instructions.", "B.Delusive examples are more effective for IT model training than original examples.", "C. The random baseline outperforms IT models in low-resource settings.", "D. IT models rely on learning superficial patterns, such as output format and guessing."], "answer": "D", "content": "\nIntroduction\nRecently, instruction tuning(IT) has drawn much attention in the NLP communities, with the rapid growth of new models (Sanh et al., 2021; Wei et al., 2021; Ouyang et al., 2022) and datasets (Wang et al., 2022; Gupta et al., 2022; Finlayson et al., 2022; Mishra et al., 2021; Ye et al., 2021; Bach et al., 2022) . Models trained with task instructions demonstrate impressive zero-shot cross-task generalization ability. Despite the remarkable results, 1 : Comparison between two types of instruction tuning models. Noted that we reported an estimated number of instructions for T0 during training and testing since they have 5 to 10 instructions for each task. Our analysis focuses on the \"generalize to unseen task\" type.\nhow models utilize the instructions during training and inference time remains an open question.\nPrior works have raised the question of whether models really learn to follow the instructions or just capture spurious correlations. Jang et al. (2022) , Webson and Pavlick (2021) showed that the current large language models (LLMs) can achieve similar performance with misleading instructions(prompts) in in-context learning(ICL) and few-shot learning scenarios. Min et al. (2022) analyze how model utilize examples in ICL. They observed that (1) Input-output mapping in examples is not important and(2) Output space information is crucial.\nBesides ICL and few-shot prompt-tuning, some works raise concerns about instruction following in the instruction tuning field (Finlayson et al., 2022; Gupta et al., 2022; Gu et al., 2022) , with a focus on test-time analysis. In contrast, we focus on analyzing how the models utilize instructions during the training process. We compare our analyzing methods and observation with prior works in Appendix A.1.\nIn this work, we conduct controlled experiments on NatInst-V2 (Wang et al., 2022) , the largest opensource instruction learning dataset includes 800+ English tasks with diverse task types, to study how models utilize instructions during IT. Note that existing research on IT can be categorized into two major camps: generalize to unseen tasks and generalize to unseen instructions, based on their objectives. Table 1 shows the comparison. Our analysis focuses on the former with more background and justifications provided in section 2. We strategically alter the instructions and compare them with original instructions for IT. Specifically, for task definition, we create simplified versions by removing all semantic components in the instructions and only leaving the output space information. For task examples, we create delusive examples with incorrect input-output mapping, where the examples' input and output spaces are correct, but the inputoutput mappings are wrong. Figure 1 demonstrates specific examples of these altered instructions.\nOur experiments show that models trained with simplified task definitions achieve performances on par with the original IT models with different numbers of training examples ranging from 10 to 800 per task. We also observe that instructiontuned models are sensitive to input-output mapping during the testing ICL stage, but not during the instruction-tuning (training) stage, especially in low resource settings (i.e., \u2264 50 training instance per task). To further understand why instruction tuning improves performance for zero-shot test tasks, we establish a random baseline that only knows the correct output format (label space) for classification and multi-choice tasks. We discover that the random baseline can get 30% absolute exact-match score improvement over an untuned model, almost comparable to some IT models in low resource settings.\nOur results suggest that the impressive performance gains of IT may just come from models learning superficial patterns, such as the output space and format. We suggest future research on IT more carefully analyze their performance gains and benchmark against trivial baselines.\n\nBackground\nRecently, many instruction tuning work train and test the models with instructions to achieve better zero-shot generalizability toward unseen tasks/instructions. We categorize these works by their objectives: generalize to unseen tasks and generalize to unseen instructions, and show the comparison in Table 1 . Instruction tuning to generalize to unseen tasks. Figure 1 illustrates a two-stage instruction tuning pipeline used in many IT models, such as T0 (Sanh et al., 2021) , FLAN (Wei et al., 2021) , and TK-Instruct (Wang et al., 2022) . In the first stage, the models are trained on a set of training tasks with instructions (task-definition and task-examples). After training, the models are evaluated on a set of unseen testing tasks for zero-shot generalizability. By incorporating instructions during training, the models are shown to significantly improve performance over untuned models. The impressive performance gains led people to believe that models learned to follow instructions via instruction tuning. The goal of our analysis is to verify this belief. Instruction tuning to generalize to unseen instructions. Different from T0, FLAN, and TK-Instruct training and testing the model with clear task boundaries and focusing on cross-task generalizability, Instruct-GPT (Ouyang et al., 2022) , Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) focus more on instruction generalizability, which they train their model without clear task boundary but with diverse instructions, and further test on user-oriented instructions. These models show very different behavior compared with instruction tuning models that aim to generalize to unseen tasks.\nSince Instruct-GPT is not open-sourced and distilled IT models such as Alpaca and Vicuna come up after our submission, we focus our analysis on the first category using the TK-instruct model and NatInst-V2 dataset. However, we also conduct additional experiments and discuss the Alpaca model's instruction following ability in Table 2 .\n\nAnalysis Method\nTask definition manipulation.\nTo analyze whether models really \"understand\" and utilize the semantic meaning of task definitions, we conduct controlled experiments to remove semantic information in task definitions. Specifically, we conduct instruction-tuning with task definitions at 3 levels of granularity: Original, Simplified, and Empty. The Original version uses human-crafted human-readable task definitions provided in NatInst-V2 (Wang et al., 2022) . The Simplified task definitions remove all semantic components in the original task definition and only leave the output space information. Specifically, we only provide possible output labels as task definitions for classification tasks, and completely remove task definitions for other tasks (mostly generative tasks) during IT. Figure 1 shows an example of Simplified task definition. More details can be found in Appendix A.2. For Empty, we don't provide task definition during instruction-tuning.\nTask example manipulation. Finlayson et al. (2022) show that by providing a few task examples, both humans and models can guess and perform a task. We thus design a controlled experiment to study whether models learn the input-output mapping from task examples. Specifically, we compare models trained with 3 types of task examples: Original, Delusive, and Empty. For the Original setup, we provide one positive example in NatInst-V2 (Wang et al., 2022) \n\nExperimental Setup\nDataset. We conduct experiments on the NatInst-V2 (Wang et al., 2022) , the largest open-source instruction learning dataset, including over 800+ English tasks with diverse task types. The instructions include human-crafted human-readable Task Definition, Positive Task Examples, Negative Task Examples, and Explanation. We focus on studying task definition and task examples, which were shown to be most useful in the original paper.\nModel. we conduct experiments on TK-Instruct, the current SOTA model provided in NatInst-V2 paper. The model significantly outperformed previous SOTA models, such as T0 (62.0 v.s. 32.3 rouge-L for 11B model). We follow the seq-to-seq instruction-tuning method used in TK-Instruct, and train a T5-large-lm-adapt (770M parameters) model (Raffel et al., 2020) with performance comparable to the larger model (3B parameters) reported in Wang et al. (2022) . 1 Evaluation Metrics. For task definition, we separately evaluate Classification and Generative tasks using exact match and rouge-L respectively. For , 10, 20, 50, 200, 800) .\n\nResults\nTask Definition Experiments. Figure 2 shows experimental results for task definitions. In the top sub-figures, we can see that the models trained with Simplified instructions achieve almost the same results as models trained with Original definitions both on Classification and Generative tasks. Note that Simplified task definitions remove all semantic components in task definitions and only retain output space information for Classification tasks and remove task definitions altogether for Generative tasks. This indicates that models may only utilize output space information during instruction tuning. The bottom-left sub-figure in Figure 2 shows the overall rouge-L score for classification tasks, where models trained on the Original task definition slightly outperform the Simplified ones. A closer examination reveals that models trained on the Original task definitions are more likely to predict partially correct answers that help with the ROUGE-L score in some tasks. We provide further details in Appendix A.5. In addition, we also observe that training with Simplified prompts can yield comparable performance to the T0 model trained with Original prompts on T0 dataset. Please refer to Appendix A.6 for details. Task Examples Experiments. Combined with the previous results for task definition, we observe that comparing to the untuned models(T5 w/o IT), the IT models may achieve significant performance gain (Rouge-L from 22 to 46) with (1)Simplified task definition and (2)Delusive task example, indicating that the current impressive improvement of IT models can come from the models learning superficial patterns without utilizing (following) the instructions like human do.\nFor the right sub-figure, we show the results using Delusive task examples during test time via in-context learning. We see the performance drops for all three models, indicating that the inputoutput mapping matters for in-context learning on instruction-tuned models. This observation seems to misalign with previous work (Min et al., 2022) , which they found input-output mapping is unimportant for in context learning for classification tasks. However, a closer investigation found that most tasks suffer from significant performance drop are analogical tasks rather than classification tasks as studied in Min et al. (2022) . 2\n\nAdditional Analysis\nRandom baseline. While our experiments suggest that models do not utilize most information in the instructions, we still observe huge performance gains via instruction tuning. To understand where the gains come from, we introduce a Random baseline that simply guesses within the cor- 200) can improve exact-match score to 52%. However, while the performance gains seem impressive, the Random Guessing baseline can also achieve 42.6% exact-match score, on par with TK-Instruct trained in low resource setting (less than five instances per task). This suggests that the majority of score improvement from IT may come from model learning the output format, especially in low-resource settings.\nFair comparison for IT models. Existing studies on instruction tuning often introduce changes to both models and datasets simultaneously, which can obscure fair comparisons. To address this issue, we conduct experiments comparing different models (T0, TK-Instruct) on the same dataset (NatInst-V2) and emphasize the importance of careful evaluation. In Table 3 , when evaluating using the NatInst-V2 evaluation method and considering only the overall Rouge-L score, the TK-Instruct model appears to outperform T0 significantly. However, upon closer examination of the classification (CLS) and generative (GEN) tasks separately, we observe that T0's classification score is even lower than the Random baseline, primarily due to its format correctness being only 64%. To ensure a fairer comparison between these models, we employ constrained decoding techniques to align the model's predictions with the label space. By adopting this approach, we observe a substantial performance improvement for T0 in CLS tasks (34.03 to 51.31). T0 surpasses both the TK-Instruct model and the random baseline, indicating that it Table 3 : Careful evaluation of the NatInst-V2 dataset. The Format metric is the same as the format correctness in Figure 4 . The w/ CD indicates that the model's decoding is constrained to match the label choices for CLS tasks. The TK is the TK-Instruct(770M) model trained with 10 instances per task.\nis indeed superior to these models in CLS tasks.\n\nDiscussion\nDo Alpaca better follow the instruction on NatInst-V2 dataset? After our submission, new instruction tuning models, like Alpaca and Vicuna, are trained on distilled data from Chat-GPT and exhibit behavior closer to it. To investigate their instruction utilization, we conduct the \"Altered Task Definition\" experiment on LLaMA-7B (Touvron et al., 2023) and Alpaca-7B models using the NatInst-V2 test set. In Table 2 , training the LLaMA model on the NatInst-V2 dataset using the Original task definition leads to substantial performance enhancements than zeroshot. However, the Simplified task definition also achieves comparable performance, with a minimal decrease of 3 (EM/Rouge-L)scores. This finding is consistent with our previous observations on the TK-Instruct and T0 models. Even without tuning on NatInst-V2, the Alpaca model demonstrates strong performance on the NatInst-V2 test set. However, when the model is tested using a simplified task definition, there is a significant decrease in performance for generative tasks (but not for classification tasks). This highlights the importance of a well-written task definition for the Alpaca model to effectively perform generative tasks.\n\nConclusion\nWe constructed controlled experiments on NatInst-V2 to compare model training with altered vs. original instructions (task definitions and examples). Our findings indicate that some current IT models do not fully utilize instructions, and the impressive performance gains of IT may come from models learning superficial patterns, such as the output space and format. We suggest future research on instruction tuning to analyze their performance gains with more comprehensive evaluation and benchmark against trivial baselines. 1321\n"}
{"question": "Why is the example of \u201cWilliam went running. Harold did too\u201d raised?", "evidence": "  Ellipsis is a fundamental feature of human language, occurring in all registers, where parts of sentences are omitted, although the missing parts are essential for understanding the meaning. The following is an example of Verb Phrase Ellipsis (VPE) (Bos and Spenader, 2011) :\n  ", "options": ["A. To understand the fundamental meaning of Ellips.", "B. To arouse readers' interest in reading.", "C. To make the paper more interesting.", "D. To raise an example of Verb Phrase Ellipsis (VPE)."], "answer": "A", "content": "\nIntroduction\nEllipsis is a fundamental feature of human language, occurring in all registers, where parts of sentences are omitted, although the missing parts are essential for understanding the meaning. The following is an example of Verb Phrase Ellipsis (VPE) (Bos and Spenader, 2011) :\n(1) William went running. Harold did too.\n(1) is understood as asserting that Harold went running; that is, the hearer or reader naturally fills in the missing material. This is done by identifying the antecedent VP, went running, in the first sentence. The following is the non-elliptical counterpart of (1):\n(2) William went running. Harold went running too.\nWith such examples, we can test understanding of ellipsis by targeting the ellipsis phrase with a simple Yes/No question:\n(3) Did Harold go running?\nIf a system answers the question incorrectly for (1), the ellipsis example, but answers correctly for (2), the non-elliptical counterpart of (1), we can ascribe the result specifically to the challenge of ellipsis, since the examples are otherwise identical.\nAs with pronominal anaphora and other discourse processes, there is great flexibility in the way ellipsis can occur in discourse. Example (1) involves two simple adjacent sentences. It is also possible for ellipsis or the antecedent to occur in embedded clauses. Furthermore, ellipsis can occur either before or after the antecedent. Finally, an arbitrary amount of material can intervene between the antecedent and the ellipsis occurrence.\nIn this paper, we propose the challenge of ellipsis-dependent reasoning. This challenge consists of examples involving an ellipsis clause, the target. Each ellipsis example is paired with its nonelliptical counterpart, where the target clause is overt rather than elliptical. We then pose a question whose answer is dependent on the target clause. A key aspect of the challenge is that ellipsis occurrences are possible in a variety of diverse structural configurations. We test a series of GPT-3 models (GPT) on several such ellipsis structures.\n\nRelated Work\nThere is a large literature concerning the probing of language models from a variety of perspectives. Furthermore, there has been substantial work specifically addressing ellipsis in NLP. In this paper, we are proposing the challenge of ellipsisdependent reasoning. This proposal builds on various strands of prior research; below we consider some particularly relevant aspects of this literature.\n\nProbing Models for Knowledge\nThe Winograd Schema (Kocijan et al. (2022) ; Levesque et al. (2012)) involves test examples that use the linguistic problem of pronoun resolution to gain insight into the commonsense reasoning abilities of an AI system. To do this, the Winograd Schema requires pairs of examples that differ only in one specific, small way, as in ( 4):\n(4)\nThe city councilmen refused the demonstrators a permit because they feared/advocated violence.\nWith \"feared\", the pronoun \"they\" refers to the city councilmen, while with \"advocated\", it refers to the demonstrators. 2019): here, examples are constructed which test specific aspects of linguistic knowledge of a system, namely, whether BERT embeddings \"encode hierarchical information\". For example, a task is defined to identify the main auxiliary verb in a sentence, even in cases where the main auxiliary is not the first auxiliary verb to appear. Training and testing datasets are automatically generated using a context-free grammar for several such tasks involving hierarchical syntactic information.\n\nAnaphora and Question Answering\nQuoref (Dasigi et al. (2019) ; Zhang and Zhao (2022) ) is a question-answer dataset designed so that correct answers cannot be given unless a coreference relationship is correctly identified; that is, the reasoning involved in question answering is dependent on resolving coreference. This is, in a sense, the inverse of the Winograd schema, where resolving coreference is dependent upon reasoning. Just as with the Winograd schema, it is difficult to ensure that resolving this dependency is required for system success. (Dasigi et al., 2019)[p. 1] note that this is \"challenging, because it is hard to avoid lexical cues that shortcut complex reasoning\", and based on a random sample, found that coreference resolution was required for 78% of questions.\n\nEllipsis as a Task\nThere has been substantial work on ellipsis as a discrete NLP task (Khullar (2020) 2021) frame ellipsis as a question-answering task, i.e., a task of locating an antecedent, understood as a span of tokens in context. Aralikatte et al. (2021) report token F1 scores of 78.66 for VPE and 86.01 for sluicing, an-other form of ellipsis. It's important to note that the task here, of antecedent identification, is a sub-part of the ellipsis challenge. Before the antecedent is identified, an ellipsis occurrence must be identified, and after the antecedent is identified, it must be interpreted, or \"reconstructed\", at the ellipsis site.\n\nRelevance for Ellipsis-Dependent Reasoning\nThe specific task of ellipsis is addressed in work like that of Aralikatte et al. (2021) , but the key difference here is that we are probing for a complete solution to the ellipsis problem. The proposed ellipsis-dependent reasoning task involves a question that can only be answered correctly if the ellipsis is properly identified and interpreted. This combines aspects of the preceding works in a novel way: like the Winograd schema and the syntactic work by Lin et al. (2019) , it probes for what we see as a specific type of psychologically-defined knowledge: namely, a representation of context that supports the resolution of ellipsis. Similarly to the work on Quoref, we use targeted questions to probe for discourse-related knowledge.\nThere is an extensive literature on the contextual interpretation of natural language, resting on the idea of a dynamic, ongoing model of discourse. For example, Discourse Representation Theory (Kamp, 1981 ) describes a semantic model supporting discourse phenomena such as pronominal and temporal anaphora, and Sag and Hankamer (1984) argue explicitly that ellipsis and other such phenomena are interpreted with respect to a discourse model (Garnham, 2010) . As one study puts it, \"Interpreting a verb-phrase ellipsis (VP ellipsis) requires accessing an antecedent in memory, and then integrating a representation of this antecedent into the local context\" (Martin and McElree, 2008) . In this paper, we seek to determine whether a large language model is capable of such an interpretive process.\n\nData\nThere is a great deal of variety in the structural configurations in which ellipsis can occur. In tables 1 and 2 we define structures for ellipsis and antecedent occurrences.\nIn all structures, there is an ellipsis occurrence, and the question targets the ellipsis occurrence. Furthermore, each ellipsis example is paired with a non-ellipsis version. We generate large numbers of examples of each structure by performing random substitutions for both the subject and verb. The substitution lists are given in the appendix, along with samples of each structure and the size of the resulting sets. 1\n\nTest\nFor each instantiation of a given structure, we produce paired ellipsis and non-ellipsis examples, with an associated Yes/No question. We randomly select 1000 examples for each structure, including 500 ellipsis examples and 500 examples which are their non-elliptical counterparts. Each example is presented to the system, preceded by the text, \"Please give a Yes or No answer:\". We test five GPT-3 models on these structures: Davinci-003, Davinci-002, Curie-001, Babbage-001, and Ada-001. According to the GPT-3 documentation, Davinci-003 is the most powerful model and Ada-001, the least.\n\nResults\nFigure 1 gives the accuracy for ellipsis and nonellipsis, for each of the five models. We have set up the test examples so that an ellipsis example is paired with a non-ellipsis example that is otherwise identical. Because of this, we claim that the difference in accuracy of the non-ellipsis case vs. the ellipsis case provides a measurement of the difficulty specifically posed by ellipsis. For all but the least powerful model, Ada, the non-ellipsis accuracy is substantially higher than ellipsis accuracy, supporting the hypothesis that ellipsis-dependent reasoning presents a difficult challenge for these models. While the Ada model actually performs somewhat better for ellipsis than non-ellipsis, this is not because the Ada model does well with ellipsis cases; rather, the model has great difficulty with both the ellipsis and non-ellipsis cases, and is close to a random guessing baseline of .50.\nIn figures 2 through 6, we present results for each model. We show the accuracy for each structure, for both the ellipsis version and the non-ellipsis version. Consider the most powerful models, Davinci-003 and Davinci-002. In figures 2 and 3, we can see that ellipsis is not difficult in the first two structures: 2Sent (Separate Sentence) and 1Sent (Conjoined Sentence). Here the accuracy is nearly perfect for the ellipsis as well as the non-ellipsis condition. However, in all the other structures, there is a large divergence in accuracy between ellipsis and nonellipsis, for both the Davinci-003 and Davinci-002 models. Subordination for either antecedent or ellipsis is quite challenging, with accuracies ranging from 48.8 to 85.8. The Backwards and Two Actions structures are even more difficult for ellipsis.\n\nAnalysis\nFor the two most powerful models, it is clear that ellipsis poses a difficult challenge, except in the two simplest ellipsis structures. For the less powerful models, the picture is mixed. For these models, the non-ellipsis examples are themselves a difficult challenge, so we are not able to observe the specific difficulties posed by ellipsis.\nAs we can see in figure 1 , the Davinci-002 model performs somewhat better overall than Davinci- 003, on both ellipsis and non-ellipsis. However, figures 2 and 3 show that the advantage of Davinci-002 on ellipsis is exclusively due to the subordinate antecedent construction. In every other ellipsis structure, Davinci-003 performs better than Davinci-002.\nThere are striking differences in the distribution of errors. For both the Davinci-003 and Davinci-002 models, errors are nearly always false negatives -that is, incorrect \"No\" answers. There are virtually no false positives, either for the ellipsis case or nonellipsis case. For the other three models, there are many errors of each type, with a much higher ratio of false positives.\n\nConclusion\nMost of the current rapid progress in NLP is due to pre-trained large language models. GPT-3 is an impressive publicly available collection of such models, and is able to perform in a way that suggests human-level understanding. Because of this, it is important to explore areas in which it might still differ from human language understanding. In this paper we have argued that ellipsis is one such area. For many simple ellipsis structures, the most powerful GPT-3 models struggle, with accuracies far lower on ellipsis examples than on non-elliptical counterparts.\nIn many ways, GPT-3 appears to understand the texts that it processes, often being able to answer questions that appear to rely on sophisticated reasoning. However, the challenge of ellipsisdependent reasoning provides evidence that GPT-3 is not able to understand in anything like the way humans do. \n"}
{"question": "What does the discussion of this paper try to remind us ?", "evidence": "  Further, they may be unaware of some variants of linear methods that are particularly useful for text classification (see Section 3.1). Therefore, the paper serves as a reminder of this oftenforgotten technique.\n\nThe discussion also reminds us the trade-off between performance gain and the cost including running time, model size, etc.  In summary, the study reminds us of the importance of employing simple baselines in NLP applications.\n ", "options": ["A.  The study reminds us of the importance of employing simple baselines in NLP applications.", "B. The paper serves as a reminder of linear method, a often forgotten technique.", "C. The study reminds us of the importance of employing simple baselines in NLP applications and the trade-off between performance gain and the cost including running time, model size, etc. ", "D. The discussion reminds us the trade-off between performance gain and the cost including running time, model size, etc. "], "answer": "C", "content": "\nIntroduction\nText classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems.\nWhile various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. "}
{"question": "What is the main advantage of XL-LEXEME over cross-encoder models in terms of efficiency?", "evidence": "  The main advantage of XL-LEXEME concerning models based on the crossencoder architecture is efficiency.  The time cost can be directly derived from the different architectures that exploit XL-LEXEME and the crossencoder baseline.  The self-attention time complexity O(N^2 * d) depends on the vector dimension d and the sequence length, which is N for the cross-encoder and N^2 for XL-LEXEME. For XL-LEXEME, the time complexity is reduced to O((N^2)^2 * 2d).  ", "options": ["A. XL-LEXEME has a lower time complexity due to its architecture.", "B. XL-LEXEME provides better sentence representations.", "C. XL-LEXEME uses a Siamese Network for encoding.", "D. XL-LEXEME focuses on the target word."], "answer": "A", "content": "\nIntroduction and Motivation\nLexical Semantic Change (LSC) Detection is the task of automatically identifying words that change their meaning over time. The LSC Detection task implicitly aims to disambiguate synchronic word sense occurrences and then find differences in the word sense frequencies in different periods. Word Sense Disambiguation (WSD) is a longstudied task in Natural Language Processing (Navigli, 2009) , which consists of associating the correct sense to a word occurring in a specific context. WSD involves some crucial issues, such as relying on a fixed sense inventory. Fixed sense inventories ignore the diachronic aspect of language because they can miss older unused senses or be outdated and missing new senses.\nThe Word in Context task (WiC) (Pilehvar and Camacho-Collados, 2019) aims to overcome these issues. In this work, we train a model on the WiC task and then use it to perform LSC Detection. In the WiC task, given the word w and two different contexts C1, C2, the systems have to determine whether the meaning of w is the same in the two contexts or not. Our approach is grounded on the assumption that models trained on the WiC tasks are robust enough to transfer the knowledge learned in a synchronic setting to a diachronic one. We summarise the main contribution of this work as follows: (i) We propose a pre-trained biencoder model, called XL-LEXEME, on a largescale dataset for the WiC task, which allows us to obtain comparable lexical-based representations; (ii) We assert the effectiveness of XL-LEXEME despite the computational limitation compared to the cross-encoder architecture for the LSC Detection task; (iii) Experiments on the LSC Detection task show that XL-LEXEME outperforms state-ofthe-art LSC Detection models for English, German, Swedish, and Russian.\n\nRelated Work\nLSC Detection systems can be categorized based on the distributional embeddings used to tackle the LSC Detection task. One category is represented by those approaches that adopt type-base (i.e., static) embeddings. UWB (Praz\u00e1k et al., 2020; Praz\u00e1k et al., 2021) represents an example of this category of systems. First, it employs word2vec Skip-gram with Negative Sampling (Mikolov et al., 2013) to compute a semantic space for each corpus. It uses techniques like the Canonical Correlation Analysis (Hardoon et al., 2004) and the Orthogonal Transformation (Hamilton et al., 2016) to align the abovementioned spaces. Therefore, the cosine similarity between the vectors representing the word in two different spaces is used to detect the semantic shift.\nWith the increasing use of contextualized word embeddings, numerous approaches employing BERT-base models have been developed for LSC Detection (Montanelli and Periti, 2023; Laicher et al., 2021) . In TempoBERT (Rosin et al., 2022) , the authors exploit the concept of Masked Language Modeling (MLM), where the goal is to train a language model to predict a masked portion of text given the remaining part. In particular, they employ this technique to encode the concept of time into a BERT model. This is done by concatenating a specific token representing time to the text sequence. At inference time, TempoBERT can be used to predict the year of a sentence, masking the time reference, or to predict a masked token of the sentence conditioned by the time reference. In the same line of research, in Temporal Attention (Rosin and Radinsky, 2022) , the authors investigate the effect of modifying the model instead of the input sentence like in TempoBERT. This is done by extending the model's attention mechanism to consider the time when computing the weight of each word. The time dimension is encoded using a different query embedding matrix for each timestamp.\nAnother kind of approach exploits the information coming from other tasks to perform LSC Detection. GlossReader represents an example (Rachinskiy and Arefyev, 2021) , where a model based on XML-R (Conneau et al., 2020b) is first trained on English SemCor (Miller et al., 1994) with glosses from WordNet 3.0 (Miller, 1992) to perform WSD. Exploiting the zero-shot crosslingual characteristics of XML-R, the authors used the same model to perform LSC Detection in the Russian language. With DeepMistake (Arefyev et al., 2021) , the authors take advantage of the WiC task instead of WSD. They train a cross-encoder with XML-R as an underlying Language Model on the MCL-WiC training and development set and fine-tune on the RuSemShift dataset (Rodina and Kutuzov, 2020) . DeepMistake, differently from XL-LEXEME, relies on the cross-encoder architecture and exploits only the MCL-WiC training dataset.\n\nXL-LEXEME\nGenerally, for pairwise sentence similarity tasks, BERT models use a cross-encoder, in which the pairwise sequences are jointly encoded, and the overall vectors are used for the classification. However, in several tasks, the cross-encoder is not suitable since it cannot provide a distinct meaningful representation for each sentence. An approach to overcome this issue involves pooling the BERT out-put encoded vectors, which often results in worse performance. Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) overcomes the limitation of cross-encoders using a Siamese Network, i.e., the weights of the underlying networks are shared. SBERT encodes the two sequences separately in the BERT model exploiting the Siamese architecture. The sequence-level representation is obtained by averaging the output encoded vectors, which are directly compared using similarity measures such as cosine similarity.\nMeanwhile, cross-encoders perform better since they are trained to profit from the attention over the whole input. In this work, we introduce XL-LEXEME 1 which mirrors models for pairwise sequence similarity tasks and adapts them to the WiC task, giving prominence to the target word, i.e. the word for which we want to detect the LSC. The model takes as input two sequences s 1 and s 2 . The sequences are tokenized using subwords tokenizer, such as Sentence Piece (Kudo and Richardson, 2018) , and the special tokens <t> and </t> are used as target word delimiters (Xie et al., 2021) :\nEQUATION\nwhere N and M represent the number of subwords of the sequence s 1 and s 2 respectively, while w t i , ..., w t i+k and w t j , ..., w t j+p are the subwords of the target words. In the following, we describe the baseline cross-encoder and XL-LEXEME based on a bi-encoder. For the crossencoder, the two input sequences are concatenated by the special token [SEP ] in an overall sequence\ns = [CLS] s 1 [SEP ] s 2 [SEP ].\nIf the length of s, i.e. N + M + 3, is greater than the maximum sequence length \u03bb, then the sequence s is cut such that the length of s 1 and s 2 is less than \u03bb * = \u03bb\u22123 2 . To comply with the maximum length, the left and right contexts of the sequence are truncated. For instance, s 1 is truncated as follows:\ns 1 = w n 0 , ..., <t>, w t i , ..., w t i+k , </t>, ..., w n 1 (2) where n 0 = max(0, i \u2212 1 \u2212 \u03bb * \u2212k\u22122 2 ) and n 1 = min(N, i + k + 1 + \u03bb * \u2212k\u22122 2\n). The truncated sequence has a length \u03b3 < \u03bb. The encoded representations of each subword (v 1 , v 2 , ..., v \u03b3 ) are summed to get the encoded representation of the overall sequence, i.e. s enc = \u03b3 i v i . Finally, the vector s enc is used to compute the logits:\nlogit = log \u03c3(W s enc ) (3)\nwhere W \u2208 IR 1\u00d7d . The model is trained to minimize the Binary Cross-entropy loss function. XL-LEXEME is a bi-encoder that encodes the input sequences using a Siamese Network into two different vector representations. Each sequence is tokenized and truncated according to the maximum length \u03bb * , using Equation (2). We thus obtain the new lengths \u03b3 1 , \u03b3 2 . The vector representation is computed as the sum of the encoded subwords\n(v 1 , v 2 , ..., v \u03b3 ), i.e. s enc 1 = \u03b3 1 i v i and s enc 2 = \u03b3 2 j v j .\nXL-LEXEME is trained to minimize the Contrastive loss (Hadsell et al., 2006) :\n\u2113 = 1 2 y \u2022 \u03b4 2 + (1 \u2212 y) \u2022 max(0, m \u2212 \u03b4) 2 (4)\nwhere we adopt a margin m = 0.5. We use as default distance \u03b4 the cosine distance between the encoded representations of s 1 and s 2 , i.e. \u03b4 = cos(s enc 1 , s enc 2 ). The main advantage of XL-LEXEME concerning models based on the crossencoder architecture is efficiency. The time cost can be directly derived from the different architectures that exploit XL-LEXEME and the crossencoder baseline. The self-attention time complexity O(N 2 * d) depends on the vector dimension d and the sequence length, which is N for the cross-encoder and N 2 for XL-LEXEME. For XL-LEXEME, the time complexity is reduced to O((N^2)^2 * 2d).\n4 Experimental setting\n\nLexical Semantic Change Detection\nSemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection (Schlechtweg et al., 2020) is the first task on Unsupervised Lexical Semantic Change Detection in English, German, Swedish, and Latin languages. For each language, two corpora represent two different periods (T0, T1). Moreover, a set of target words, annotated using the DUREL framework (Schlechtweg et al., 2018) , are provided. SemEval-2020 Task 1 involves two subtasks. The binary classification task requires assigning a label (changed/stable) to each target word. The ranking task sorts the target words according to their degree of semantic change. In this work, we focus on Subtask 2, and for the sake of simplicity, we refer to SemEval-2020 Task 1 Subtask 2 as SemEval-2020 Task 1. RuShiftEval, different from SemEval-2020 Task 1, involves three sub-corpora extracted from the Russian National Corpus spanning three periods. Models are evaluated on the resulting three test sets, namely RuShiftEval1 (pre-Soviet and Soviet), RuShiftEval2 (Soviet and post-Soviet), and RuShiftEval3 (pre-Soviet and post-Soviet). RuShiftEval provides participants with development data that can be used for tuning models. RuShiftEval aims to corroborate if training data can improve LSC Detection models. The development data rely on the RuSemShift dataset (Rodina and Kutuzov, 2020) , which includes two sets of 70 target words for the pre-Soviet to Soviet period and Soviet to post-Soviet period, respectively. The dataset also includes annotated pairwise sentences, which can be used for training the models.\n\nTraining details\nXL-LEXEME and the cross-encoder are trained using XLM-RoBERTa (XLM-R) (Conneau et al., 2020a) large as the underlying Language Model 2 and using an NVIDIA GeForce RTX 3090. As for training data, the model uses the training data of MCL-WiC (Martelli et al., 2021) , AM 2 ICO (Liu et al., 2021) , and XL-WiC datasets (Raganato et al., 2020) merged with the randomly sampled 75% of the respective development data of each dataset. The remaining 25% of the development data is used to fine-tune hyper-parameters. Moreover, we augment training data for the cross-encoder by swapping the order of sentences in the training set (Martelli et al., 2021) .\nWe use AdamW optimizer and linear learning warm-up over the 10% of training data. We perform a grid search for the hyper-parameters optimization, tuning the learning rate in {1e-6, 2e-6, 5e-6, 1e-5, 2e-5} and the weight decay {0.0, 0.01}. Table 3 (Appendix A) shows the selected hyperparameters. We sample 200 sentences containing the target word for each language and each period. The sampling is repeated ten times, and the results are averaged over the ten iterations. We use the same methodology of Rachinskiy and Arefyev (2021) for sampling sentences from the RuShiftEval corpora. We sample sentences in which we find the exact match with the target words with no pre-processing of the SemEval dataset. The LSC score is computed as the average distance between the vectors over the two different periods:\nLSC(s t 0 , s t 1 ) = 1 N \u2022 M N i=0 M j=0 \u03b4(s t 0 i , s t 1 j ) (5)\nwhere \u03b4 is the distance measure, i.e. \u03b4 = 1 \u2212 log \u03c3(W s enc ) for the cross-encoder baseline and \u03b4 = cos(s enc 1 , s enc 2 ) for XL-LEXEME.\n\nResults\nTable 1 and Table 2 report the results on the SemEval-2020 Task 1 Subtask 2 and the results on the RuShiftEval test set. The results of the best systems are in bold. XL-LEXEME achieve the best score for English, German, Swedish, RuShiftEval1, RuShiftEval2, and RuShiftEval3. XL-LEXEME achieves a strong Spearman correlation for English and Swedish languages and a solid correlation on the German dataset, obtaining a significative correlation (p < 0.001). XL-LEXEME obtains no significant results in the Latin language since the predicted scores for the target words are not correlated with the test set. Latin is underrepresented in the training data of XLM-R, and there are no similar languages in the WiC dataset that we use for training XL-LEXEME. Moreover, the Latin dataset is more challenging as it involves the first corpus written in ancient Latin, which differs in many aspects from modern Latin. For this reason, XL-LEXEME could be ineffective in ancient languages and, in general, in languages that are not widely covered by the WiC dataset. We report the statistical significance of the difference between the performance of XL-LEXEME concerning the other models. The statistical significance of the difference is computed using Fisher's z-transformation (Press, 2002) . XL-LEXEME obtains stronger correlations than the cross-encoder, but the differences are not significant. The correlations obtained on the English and the German datasets are significantly different (p < 0.05) for all the systems that participated in the SemEval-2020 Task 1 but not for TempoBERT and Temporal Attention. On the other side, TempoBERT and Temporal Attention obtain a Spearman correlation on English and German that is not statistically different from the systems on the SemEval-2020 Task 1 leaderboard. In the Swedish language, XL-LEXEME is the only one obtaining a significantly different correlation from the Count baseline results. XL-LEXEME showed its effectiveness also in Swedish, although the WiC dataset does not cover this language. Presumably, Swedish benefits from the presence of other languages descending from the Old Norse language, namely Danish and Norwegian.\nXL-LEXEME obtains competitive results for the Russian language in the RuShiftEval leaderboard. Contrary to XL-LEXEME, Deep Mistake and Gloss Reader are fine-tuned on the RuSemShift dataset. The differences between XL-LEXEME and the best two systems in the leaderboard are not statically significant. Moreover, in Table 2 , the results of XL-LEXEME fine-tuned on the RuSemShift are shown. Although the fine-tuned model achieves the best correlation scores in the three datasets, the difference between DeepMistake and GlossReader is not significant.\n\nConclusion\nIn this work, we introduced XL-LEXEME, a model for LSC Detection. XL-LEXEME is pre-trained on a large WiC dataset to mirror sentence-level encoders focusing on specific words in contexts. We evaluated our model on two Lexical Semantic Change Detection datasets: SemEval-2020 Task 1 and RuShiftEval. XL-LEXEME outperforms stateof-the-art models for LSC Detection in English, German, Swedish, and Russian datasets, with significant differences from the baselines. The XL-LEXEME effectiveness and efficiency make it reliable for LSC Detection on large diachronic corpora.\n"}
